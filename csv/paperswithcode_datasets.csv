dataset_id,dataset_name,description,source_url,license,modalities,languages,year_published,paper_title,paper_url,dataset_size,dataset_splits,num_classes,associated_tasks,benchmark_urls,pwc_url,area,subtask,task
Clear_Weather,Clear Weather Dataset,"We introduce an object detection dataset in challenging adverse weather conditions covering 12000 samples in real-world driving scenes and 1500 samples in controlled weather conditions within a fog chamber. The dataset includes different weather conditions like fog, snow, and rain and was acquired by over 10,000 km of driving in northern Europe. The driven route with cities along the road is shown on the right. In total, 100k Objekts were labeled with accurate 2D and 3D bounding boxes. The main contributions of this dataset are: - We provide a proving ground for a broad range of algorithms covering signal enhancement, domain adaptation, object detection, or multi-modal sensor fusion, focusing on the learning of robust redundancies between sensors, especially if they fail asymmetrically in different weather conditions. - The dataset was created with the initial intention to showcase methods, which learn of robust redundancies between the sensor and enable a raw data sensor fusion in case of asymmetric sensor failure induced through adverse weather effects. - In our case we departed from proposal level fusion and applied an adaptive fusion driven by measurement entropy enabling the detection also in case of unknown adverse weather effects. This method outperforms other reference fusion methods, which even drop in below single image methods. - Please check out our paper for more information.",https://production-media.paperswithcode.com/datasets/05d90334-6dd6-4297-a27b-fbeb4c12945a.png,Edithttps://github.com/princeton-computational-imaging/SeeingThroughFog/blob/master/LICENSE,"3D, Image",,,,,12000 samples,,,"Object Detection, 2D Object Detection, 3D Object Detection","2d-object-detection-on-clear-weather, 3d-object-detection-on-clear-weather",,See all 1951 tasks,2D Object Detection458 benchma,2D Object Detection458 benchma
COCO-O,COCO-O Dataset,"COCO-O(ut-of-distribution) contains 6 domains (sketch, cartoon, painting, weather, handmake, tattoo) of COCO objects which are hard to be detected by most existing detectors. The dataset has a total of 6,782 images and 26,624 labelled bounding boxes.",https://production-media.paperswithcode.com/datasets/db4e3643-d048-4b2f-88be-21284cac11b3.png,EditUnknown,Image,English,,,,782 images,,,"Object Detection, 2D Object Detection",object-detection-on-coco-o,,See all 1951 tasks,2D Object Detection458 benchma,2D Object Detection458 benchma
Dense_Fog,Dense Fog Dataset,"We introduce an object detection dataset in challenging adverse weather conditions covering 12000 samples in real-world driving scenes and 1500 samples in controlled weather conditions within a fog chamber. The dataset includes different weather conditions like fog, snow, and rain and was acquired by over 10,000 km of driving in northern Europe. The driven route with cities along the road is shown on the right. In total, 100k Objekts were labeled with accurate 2D and 3D bounding boxes. The main contributions of this dataset are:
- We provide a proving ground for a broad range of algorithms covering signal enhancement, domain adaptation, object detection, or multi-modal sensor fusion, focusing on the learning of robust redundancies between sensors, especially if they fail asymmetrically in different weather conditions.
- The dataset was created with the initial intention to showcase methods, which learn of robust redundancies between the sensor and enable a raw data sensor fusion in case of asymmetric sensor failure induced through adverse weather effects.
- In our case we departed from proposal level fusion and applied an adaptive fusion driven by measurement entropy enabling the detection also in case of unknown adverse weather effects. This method outperforms other reference fusion methods, which even drop in below single image methods.
- Please check out our paper for more information.",https://production-media.paperswithcode.com/datasets/DatasetPicture.png,Edithttps://github.com/princeton-computational-imaging/SeeingThroughFog/blob/master/LICENSE,"3D, Image",,,,,12000 samples,,,"Object Detection, 2D Object Detection, 3D Object Detection","2d-object-detection-on-dense-fog, 3d-object-detection-on-stf",,See all 1951 tasks,2D Object Detection458 benchma,2D Object Detection458 benchma
ExDark,ExDark Dataset,"The Exclusively Dark (ExDARK) dataset is a collection of 7,363 low-light images from very low-light environments to twilight (i.e 10 different conditions) with 12 object classes (similar to PASCAL VOC) annotated on both image class level and local object bounding boxes.",https://github.com/cs-chan/Exclusively-Dark-Image-Dataset,EditUnknown,Image,,,,,,,,"Object Detection, 2D Object Detection, Low-Light Image Enhancement, Image Enhancement",2d-object-detection-on-exdark,,See all 1951 tasks,2D Object Detection458 benchma,2D Object Detection458 benchma
FlickrLogos-32,FlickrLogos-32 Dataset,"Object detection benchmark for logo detection.

Images are natural scenes. Each image contains multiple objects, and each image has a total of 1 logo. Logo detection & classification labels are provided.",https://production-media.paperswithcode.com/datasets/d0a9e09e-c807-4488-ae1f-51ffd1137955.jpg,EditUnknown,Image,,,,,,,,"Object Detection, Image Classification, 2D Object Detection, Traffic Sign Recognition","object-detection-on-flickrlogos-32, traffic-sign-recognition-on-flickrlogos-32, image-classification-on-flickrlogos-32",,See all 1951 tasks,2D Object Detection458 benchma,2D Object Detection458 benchma
KITTI_MOTS,KITTI MOTS Dataset,"The Multi-Object and Segmentation (MOTS) benchmark [2] consists of 21 training sequences and 29 test sequences. It is based on the KITTI Tracking Evaluation 2012 and extends the annotations to the Multi-Object and Segmentation (MOTS) task. To this end, we added dense pixel-wise segmentation labels for every object. We evaluate submitted results using the metrics HOTA, CLEAR MOT, and MT/PT/ML. We rank methods by HOTA [1]. Our development kit and GitHub evaluation code provide details about the data format as well as utility functions for reading and writing the label files. (adapted for the segmentation case). Evaluation is performed using the code from the TrackEval repository.

[1] J. Luiten, A. Os̆ep, P. Dendorfer, P. Torr, A. Geiger, L. Leal-Taixé, B. Leibe: HOTA: A Higher Order Metric for Evaluating Multi-object Tracking. IJCV 2020.
[2] P. Voigtlaender, M. Krause, A. Os̆ep, J. Luiten, B. Sekar, A. Geiger, B. Leibe: MOTS: Multi-Object Tracking and Segmentation. CVPR 2019.",https://production-media.paperswithcode.com/datasets/31c8042e-2eff-4210-8948-f06f76b41b54.jpg,EditCreative Commons Attribution-NonCommercial-ShareAlike 3.0,"Image, Video",English,2012,,,,,,"2D Semantic Segmentation, Multi-Object Tracking, Multi-Object Tracking and Segmentation, 2D Object Detection, Object Tracking, Multiple Object Tracking",multi-object-tracking-and-segmentation-on-1,,See all 1951 tasks,2D Object Detection458 benchma,2D Object Detection458 benchma
PACO,PACO Dataset,"Parts and Attributes of Common Objects (PACO) is a detection dataset that goes beyond traditional object boxes and masks and provides richer annotations such as part masks and attributes. It spans 75 object categories, 456 object-part categories and 55 attributes across image (LVIS) and video (Ego4D) datasets. The dataset contains 641K part masks annotated across 260K object boxes, with half of them exhaustively annotated with attributes as well.",https://arxiv.org/pdf/2301.01795v1.pdf,EditMIT License,Image,,,,,,,,2D Object Detection,,,See all 1951 tasks,2D Object Detection458 benchma,2D Object Detection458 benchma
RADIATE,RADIATE Dataset,"RADIATE (RAdar Dataset In Adverse weaThEr) is new automotive dataset created by Heriot-Watt University which includes Radar, Lidar, Stereo Camera and GPS/IMU.
The data is collected in different weather scenarios (sunny, overcast, night, fog, rain and snow) to help the research community to develop new methods of vehicle perception.
The radar images are annotated in 7 different scenarios: Sunny (Parked), Sunny/Overcast (Urban), Overcast (Motorway), Night (Motorway), Rain (Suburban), Fog (Suburban) and Snow (Suburban). The dataset contains 8 different types of objects (car, van, truck, bus, motorbike, bicycle, pedestrian and group of pedestrians).",https://github.com/marcelsheeny/radiate_sdk,EditUnknown,"Image, Video",,,,,,,,"Object Detection, 2D Object Detection, Scene Understanding, Multiple Object Tracking","multiple-object-tracking-on-radiate, 2d-object-detection-on-radiate",,See all 1951 tasks,2D Object Detection458 benchma,2D Object Detection458 benchma
CaDIS,CaDIS Dataset,CaDIS: a Cataract Dataset for Image Segmentation is a dataset for semantic segmentation created by Digital Surgery Ltd. on top of the CATARACTS dataset. CaDIS consists of 4670 images sampled from the 25 videos on CATARACTS' training set. Each pixel in each image is labeled with its respective instrument or anatomical class from a set of 36 identified classes. More details about the dataset could be found in the paper (https://arxiv.org/pdf/1906.11586.pdf).,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,1906,,,4670 images,,,"2D Semantic Segmentation, 2D Semantic Segmentation task 1 (8 classes), 2D Semantic Segmentation task 2 (17 classes), 2D Semantic Segmentation task 3 (25 classes)","2d-semantic-segmentation-task-1-8-classes-on, 2d-semantic-segmentation-task-3-25-classes-on, 2d-semantic-segmentation-task-2-17-classes-on",,See all 1951 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
CamVid,CamVid Dataset,"CamVid (Cambridge-driving Labeled Video Database) is a road/driving scene understanding database which was originally captured as five video sequences with a 960×720 resolution camera mounted on the dashboard of a car. Those sequences were sampled (four of them at 1 fps and one at 15 fps) adding up to 701 frames. Those stills were manually annotated with 32 classes: void, building, wall, tree, vegetation, fence, sidewalk, parking block, column/pole, traffic cone, bridge, sign, miscellaneous text, traffic light, sky, tunnel, archway, road, road shoulder, lane markings (driving), lane markings (non-driving), animal, pedestrian, child, cart luggage, bicyclist, motorcycle, car, SUV/pickup/truck, truck/bus, train, and other moving object",https://arxiv.org/abs/1704.06857,EditUnknown,"Image, Video",,,,,,,32,"2D Semantic Segmentation, Real-Time Semantic Segmentation, Semantic Segmentation, Video Semantic Segmentation","2d-semantic-segmentation-on-camvid, semantic-segmentation-on-camvid, real-time-semantic-segmentation-on-camvid, video-semantic-segmentation-on-camvid",,See all 1951 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
Open_Images_V7,Open Images V7 Dataset,"Open Images is a computer vision dataset covering ~9 million images with labels spanning thousands of object categories. A subset of 1.9M includes diverse annotations types.


15,851,536 boxes on 600 classes
2,785,498 instance segmentations on 350 classes
3,284,280 relationship annotations on 1,466 relationships
675,155 localized narratives (synchronized voice, mouse trace, and text caption)
66,391,027 point-level annotations on 5,827 classes
61,404,966 image-level labels on 20,638 classes

Images are under a  CC BY 2.0 license, annotations under  CC BY 4.0 license.",https://production-media.paperswithcode.com/datasets/fcd9ec8c-20c9-4224-9dd6-5a11f97e9029.png,EditCC BY 4.0,"Audio, Image, Text",English,,,,,,600,"2D Semantic Segmentation, Object Detection, Visual Relationship Detection, Instance Segmentation, Image Captioning, 2D Object Detection, Semantic Segmentation, Speech Recognition, Image Classification",,,See all 1951 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
SpaceNet_7,SpaceNet 7 Dataset,"Satellite imagery analytics have numerous human development and disaster response applications, particularly when time series methods are involved. For example, quantifying population statistics is fundamental to 67 of the 232 United Nations Sustainable Development Goals, but the World Bank estimates that more than 100 countries currently lack effective Civil Registration systems. The SpaceNet 7 Multi-Temporal Urban Development Challenge aims to help address this deficit and develop novel computer vision methods for non-video time series data. In this challenge, participants will identify and track buildings in satellite imagery time series collected over rapidly urbanizing areas. The competition centers around a new open source dataset of Planet satellite imagery mosaics, which includes 24 images (one per month) covering ~100 unique geographies. The dataset will comprise over 40,000 square kilometers of imagery and exhaustive polygon labels of building footprints in the imagery, totaling over 10 million individual annotations. Challenge participants will be asked to track building construction over time, thereby directly assessing urbanization.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-ShareAlike 4.0 International License,Image,,,,,24 images,,,2D Semantic Segmentation,,,See all 1951 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
WaterScenes,WaterScenes Dataset,"A Multi-Task 4D Radar-Camera Fusion Dataset for Autonomous Driving on Water Surfaces description of the dataset 


WaterScenes, the first multi-task 4D radar-camera fusion dataset on water surfaces, which offers data from multiple sensors, including a 4D radar, monocular camera, GPS, and IMU. It can be applied in multiple tasks, such as object detection, instance segmentation, semantic segmentation, free-space segmentation, and waterline segmentation.
Our dataset covers diverse time conditions (daytime, nightfall, night), lighting conditions (normal, dim, strong), weather conditions (sunny, overcast, rainy, snowy) and waterway conditions (river, lake, canal, moat). An information list is also offered for retrieving specific data for experiments under different conditions.
We provide 2D box-level and pixel-level annotations for camera images, and 3D point-level annotations for radar point clouds. We also offer precise timestamps for the synchronization of different sensors, as well as intrinsic and extrinsic parameters.
We provide a toolkit for radar point clouds that includes: pre-processing, labeling, projection and visualization, assisting researchers in processing and analyzing our dataset.",https://production-media.paperswithcode.com/datasets/da2004b6-79c9-4bf1-8a8b-c196b656f822.png,EditUnknown,"3D, Image",,,,,,,,"2D Semantic Segmentation, Object Detection, Instance Segmentation, Panoptic Segmentation, Line Segment Detection, Semantic Segmentation, Point Cloud Segmentation","2d-semantic-segmentation-on-waterscenes, object-detection-on-waterscenes",,See all 1951 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
xBD,xBD Dataset,"The xBD dataset contains over 45,000KM2 of polygon labeled pre and post disaster imagery. The dataset provides the post-disaster imagery with transposed polygons from pre over the buildings, with damage classification labels.",https://github.com/DIUx-xView/xview2-baseline,EditCC BY-NC-SA 4.0,Image,,,,,,,,"2D Semantic Segmentation, Disaster Response, Data Augmentation, Semantic Segmentation, Extracting Buildings In Remote Sensing Images","2d-semantic-segmentation-on-xbd, extracting-buildings-in-remote-sensing-images",,See all 1951 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
Assembly101,Assembly101 Dataset,"Assembly101 is a new procedural activity dataset featuring 4321 videos of people assembling and disassembling 101 ""take-apart"" toy vehicles. Participants work without fixed instructions, and the sequences feature rich and natural variations in action ordering, mistakes, and corrections. Assembly101 is the first multi-view action dataset, with simultaneous static (8) and egocentric (4) recordings. Sequences are annotated with more than 100K coarse and 1M fine-grained action segments, and 18M 3D hand poses. We benchmark on three action understanding tasks: recognition, anticipation and temporal segmentation. Additionally, we propose a novel task of detecting mistakes. The unique recording format and rich set of annotations allow us to investigate generalization to new toys, cross-view transfer, long-tailed distributions, and pose vs. appearance. We envision that Assembly101 will serve as a new challenge to investigate various activity understanding problems.",https://production-media.paperswithcode.com/datasets/463c9974-6756-46ed-b381-ad97dffa795d.jpg,EditUnknown,"3D, Image, Video",,,,,,,,"Open Vocabulary Action Recognition, Action Anticipation, 3D Action Recognition, Action Recognition, Mistake Detection, Action Segmentation","action-anticipation-on-assembly101, 3d-action-recognition-on-assembly101, open-vocabulary-action-recognition-on, action-segmentation-on-assembly101",,See all 1951 tasks,3D Action Recognition65 benchm,3D Action Recognition65 benchm
BABEL,BABEL Dataset,"BABEL is a large dataset with language labels describing the actions being performed in mocap sequences. BABEL consists of action labels for about 43 hours of mocap sequences from AMASS. Action labels are at two levels of abstraction --  sequence labels describe the overall action in the sequence, and frame labels describe all actions in every frame of the sequence. Each frame label is precisely aligned with the duration of the corresponding action in the mocap sequence, and multiple actions can overlap. There are over 28k sequence labels, and 63k frame labels in BABEL, which belong to over 250 unique action categories. Labels from BABEL can be leveraged for tasks like action recognition, temporal action localization, motion synthesis, etc.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-06-18_at_11.15.40.png,EditCustom,"3D, Image, Time Series, Video",,,,,,,,"Temporal Action Localization, Action Recognition, Action Classification, 3D Action Recognition",action-classification-on-babel,,See all 1951 tasks,3D Action Recognition65 benchm,3D Action Recognition65 benchm
Florence3D,Florence3D Dataset,"The dataset collected at the University of Florence during 2012, has been captured using a Kinect camera. It includes 9 activities: wave, drink from a bottle, answer phone,clap, tight lace, sit down, stand up, read watch, bow. During acquisition, 10 subjects were asked to perform the above actions for 2/3 times. This resulted in a total of 215 activity samples.",https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/,"EditCustom (research-only, non-commercial)","3D, Image, Time Series, Video",,2012,,,,,,"Temporal Action Localization, Action Recognition, Skeleton Based Action Recognition, 3D Action Recognition",skeleton-based-action-recognition-on-florence,,See all 1951 tasks,3D Action Recognition65 benchm,3D Action Recognition65 benchm
FR-FS,FR-FS Dataset,"The FR-FS dataset contains 417 videos collected from FIV dataset and Pingchang 2018 Winter Olympic Games. FR-FS contains the critical movements of the athlete’s take-off, rotation, and landing. Among them, 276 are smooth landing videos, and 141 are fall videos.
To test the generalization performance of our proposed model, we randomly select 50% of the videos from the fall and landing videos as the training set and the testing set.",https://production-media.paperswithcode.com/datasets/89e52ea6-202f-4ef0-b479-c6e9a9e4030a.jpg,EditUnknown,"3D, Image, Video",,2018,,,,,,3D Action Recognition,,,See all 1951 tasks,3D Action Recognition65 benchm,3D Action Recognition65 benchm
MultiviewC,MultiviewC Dataset,"The MultiviewC dataset mainly contributes to multiview cattle action recognition, 3D objection detection and tracking. We build a novel synthetic dataset MultiviewC through UE4 based on real cattle video dataset which is offered by CISRO. The format of our data set has been adjusted on the basis of MultiviewX for set-up, annotation and files structure.",https://production-media.paperswithcode.com/datasets/be4eedb8-ff12-4598-bcfc-d6d18cdc15cd.png,EditUnknown,"3D, Image, Video",,,,,,,,"3D Action Recognition, Multiview Detection, 3D Object Detection",,,See all 1951 tasks,3D Action Recognition65 benchm,3D Action Recognition65 benchm
Navigation_Turing_Test,Navigation Turing Test Dataset,"Replay data from human players and AI agents navigating in a 3D game environment.

Introduced in ""Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation"" [ICML 2021] to learn how to evaluate humanlike behavior in agents.",https://production-media.paperswithcode.com/datasets/video_input.png,EditMicrosoft Research License Agreement (MSR-LA),"3D, Image, Video",,2021,,,,,,3D Action Recognition,,,See all 1951 tasks,3D Action Recognition65 benchm,3D Action Recognition65 benchm
UAV-Human,UAV-Human Dataset,"UAV-Human is a large dataset for human behavior understanding with UAVs. It contains 67,428 multi-modal video sequences and 119 subjects for action recognition, 22,476 frames for pose estimation, 41,290 frames and 1,144 identities for person re-identification, and 22,263 frames for attribute recognition. The dataset was collected by a flying UAV in multiple urban and rural districts in both daytime and nighttime over three months, hence covering extensive diversities w.r.t subjects, backgrounds, illuminations, weathers, occlusions, camera motions, and UAV flying attitudes. This dataset can be used for UAV-based human behavior understanding, including action recognition, pose estimation, re-identification, and attribute recognition.",https://production-media.paperswithcode.com/datasets/Untitled.jpg,EditUnknown,"3D, Image, Video",,,,,,,,"Pose Estimation, Skeleton Based Action Recognition, Pedestrian Attribute Recognition, 2D Human Pose Estimation, Person Re-Identification, 3D Action Recognition, Action Recognition","pedestrian-attribute-recognition-on-uav-human, action-recognition-on-uav-human, skeleton-based-action-recognition-on-uav, pose-estimation-on-uav-human, person-re-identification-on-uav-human",,See all 1951 tasks,3D Action Recognition65 benchm,3D Action Recognition65 benchm
ReferIt3D,ReferIt3D Dataset,"ReferIt3D provides two large-scale and complementary visio-linguistic datasets: i) Sr3D, which contains 83.5K template-based utterances leveraging spatial relations among fine-grained object classes to localize a referred object in a scene, and ii) Nr3D which contains 41.5K natural, free-form, utterances collected by deploying a 2-player object reference game in 3D scenes. This dataset can be used for 3D visual grounding and 3D dense captioning tasks.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"3D, Image, Text",English,,,,,,,"3D dense captioning, Natural Language Visual Grounding",,,See all 1951 tasks,3D dense captioning11 papers w,3D dense captioning11 papers w
Aria_Digital_Twin_Dataset,Aria Digital Twin Dataset Dataset,"A real-world dataset, with hyper-accurate digital counterpart & comprehensive ground-truth annotation.

Dataset Content
- 200 sequences (~400 mins)
- 398 objects (324 stationary, 74 dynamic)
- 2 real indoor scenes
- Single + multi-user activities

Sensor Data per device
- 2 x outward-facing monochrome camera streams
- 1 x outward-facing RGB camera stream
- 2 x IMU streams
- 2 x Internal-facing eye tracking cameras
- Complete sensor calibrations",https://production-media.paperswithcode.com/datasets/2fe25d8f-10f6-434d-9a64-6b3bb571eb20.png,EditUnknown,"3D, Image, Video",,,,,,,,"2D Semantic Segmentation, 3D Object Tracking, 2D Object Detection, 3D Depth Estimation, 3D Reconstruction, 3D Object Detection",3d-reconstruction-on-aria-digital-twin,,See all 1951 tasks,3D Depth Estimation2 benchmark,3D Depth Estimation2 benchmark
DRACO20K,DRACO20K Dataset,"DRACO20K dataset is used for evaluating object canonicalization on methods that estimate a canonical frame from a monocular input image.

Provides:
1. Mixed Reality Multi-view RGB-D images rendered from ShapeNet objects
2. Camera poses
3. NOCS maps
4. Semantic 2D keypoints with visibility
5. Object-centric mask",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT License,"3D, Image",,,,,,,,"3D Depth Estimation, 3D Reconstruction, 3D Pose Estimation",,,See all 1951 tasks,3D Depth Estimation2 benchmark,3D Depth Estimation2 benchmark
DurLAR,DurLAR Dataset,"DurLAR is a high-fidelity 128-channel 3D LiDAR dataset with panoramic ambient (near infrared) and reflectivity imagery for multi-modal autonomous driving applications. Compared to existing autonomous driving task datasets, DurLAR has the following novel features:  


High vertical resolution LiDAR with 128 channels, which is twice that of any existing datasets, full 360 degree depth, range accuracy to ±2 cm at 20-50m.  
Ambient illumination (near infrared) and reflectivity panoramic imagery are made available in the Mono16 format (2048 × 128 resolution), with this being only dataset to make this provision.  
No rolling shutter effect, as our flash LiDAR captures all 128 channels simultaneously.  
Ambient illumination data is recorded via an on-board lux meter, which is again not available in previous datasets.  
High-fidelity GNSS/INS available via an onboard OxTS navigation unit operating at 100 Hz and receiving position and timing data from multiple GNSS con-stellations in addition to GPS.  
KITTI data format adopted as the de facto dataset format such that it can be parsed using both the DurLAR development kit and existing KITTI-compatible tools.   
Diversity over repeated locations such that the dataset has been collected under diverse environmental and weather conditions over the same driving route with additional variations in the time of day relative to environmental conditions.

Sensor placement


LiDAR: Ouster OS1-128 LiDAR sensor with 128 channels vertical resolution



Stereo Camera: Carnegie Robotics MultiSense S21 stereo camera with grayscale, colour, and IR enhanced imagers, 2048x1088 @ 2MP resolution



GNSS/INS: OxTS RT3000v3 global navigation satellite and inertial navigation system, supporting localization from GPS, GLONASS, BeiDou, Galileo, PPP and SBAS constellations



Lux Meter: Yocto Light V3, a USB ambient light sensor (lux meter), measuring ambient light up to 100,000 lux",https://production-media.paperswithcode.com/datasets/2b7eea0d-6a81-4ade-8c8f-2deba1c71b3d.png,EditCC,3D,,2048,,,,,,"3D Depth Estimation, Autonomous Driving, Depth Estimation",,,See all 1951 tasks,3D Depth Estimation2 benchmark,3D Depth Estimation2 benchmark
EUEN17037_Daylight_and_View_Standard_TestDataSet,EUEN17037_Daylight_and_View_Standard_TestDataSet Dataset,EUEN17037 Daylight and View Standard Test Dataset.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC-BY,"3D, Image",,,,,,,,"3D Depth Estimation, 3D Object Classification, 3D Geometry Perception",,,See all 1951 tasks,3D Depth Estimation2 benchmark,3D Depth Estimation2 benchmark
HUMAN4D,HUMAN4D Dataset,"HUMAN4D is a large and multimodal 4D dataset that contains a variety of human activities simultaneously captured by a professional marker-based MoCap, a volumetric capture and an audio recording system. By capturing 2 female and $2$ male professional actors performing various full-body movements and expressions, HUMAN4D provides a diverse set of motions and poses encountered as part of single- and multi-person daily, physical and social activities (jumping, dancing, etc. ), along with multi-RGBD (mRGBD), volumetric and audio data.

Description from: HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media",https://production-media.paperswithcode.com/datasets/5249357e-85f8-48d4-ba43-d7a447cc94d9.jpg,EditUnknown,"3D, Image",,,,,,,,"3D Human Shape Estimation, Pose Estimation, Monocular Depth Estimation, Depth Image Estimation, 3D Pose Estimation, Depth Estimation, 3D Shape Reconstruction, 3D Human Pose Estimation, 3D Point Cloud Reconstruction, 3D Depth Estimation, 3D Shape Retrieval, Indoor Monocular Depth Estimation",,,See all 1951 tasks,3D Depth Estimation2 benchmark,3D Depth Estimation2 benchmark
IBISCape,IBISCape Dataset,A Simulated Benchmark for multi-modal SLAM Systems Evaluation in Large-scale Dynamic Environments.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,"3D, Image, Video",,,,,,,,"3D Scene Reconstruction, 2D Semantic Segmentation task 3 (25 classes), 3D Pose Estimation, 3D Object Detection From Stereo Images, 3D Object Tracking, 3D Point Cloud Reconstruction, 3D Depth Estimation",,,See all 1951 tasks,3D Depth Estimation2 benchmark,3D Depth Estimation2 benchmark
Pano3D,Pano3D Dataset,"Pano3D  is a new benchmark for depth estimation from spherical panoramas. Its goal is to drive progress for this task in a consistent and holistic manner.  The Pano3D 360 depth estimation benchmark provides a standard Matterport3D train and test split, as well as a secondary GibsonV2 partioning for testing and training as well. The latter is used for zero-shot cross dataset transfer performance assessment and decomposes it into 3 different splits, each one focusing on a specific generalization axis.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-09-08_at_15.22.09.jpg,EditCustom,"3D, Image",,,,,,,,"Zero-Shot Learning + Domain Generalization, Depth Estimation, Zero-Shot Out-of-Domain Detection, Surface Normals Estimation, Domain Adaptation, Out-of-Distribution Detection, 3D Depth Estimation, 3D Reconstruction",,,See all 1951 tasks,3D Depth Estimation2 benchmark,3D Depth Estimation2 benchmark
Relative_Human,Relative Human Dataset,"Relative Human (RH) contains multi-person in-the-wild RGB images with rich human annotations, including:

Depth layers: relative depth relationship/ordering between all people in the image.
Age group classfication: adults, teenagers, kids, babies.
Others: Genders, Bounding box, 2D pose.",https://production-media.paperswithcode.com/datasets/43ba50a0-415e-4515-a24d-a2a323ec7923.png,EditUnknown,"3D, Image",,,,,,,,"3D Multi-Person Mesh Recovery, 2D Human Pose Estimation, 3D Multi-Person Pose Estimation (absolute), 3D Absolute Human Pose Estimation, 3D Depth Estimation","3d-depth-estimation-on-relative-human, 3d-multi-person-mesh-recovery-on-relative",,See all 1951 tasks,3D Depth Estimation2 benchmark,3D Depth Estimation2 benchmark
WinSyn,WinSyn Dataset,75k photos of windows + 21k synthetic renders of building windows.,https://production-media.paperswithcode.com/datasets/0b3a485e-bdfc-46f5-8b99-8c2c2301e63f.jpg,EditCC BY-NC-ND 3.0,"3D, Image, Text",English,,,,,,,"2D Semantic Segmentation, 3D Depth Estimation, Synthetic-to-Real Translation",,,See all 1951 tasks,3D Depth Estimation2 benchmark,3D Depth Estimation2 benchmark
EgoBody,EgoBody Dataset,"EgoBody dataset is a novel large-scale dataset for egocentric 3D human pose, shape and motions under interactions in complex 3D scenes.",https://production-media.paperswithcode.com/datasets/bbee2067-b6bf-4fd2-9e88-0438cee69ea2.jpg,EditUnknown,"3D, Image",,,,,,,,3D human pose and shape estimation,3d-human-pose-and-shape-estimation-on-egobody,,See all 1951 tasks,3D human pose and shape estima,3D human pose and shape estima
AGORA,AGORA Dataset,"AGORA is a synthetic human dataset with high realism and accurate ground truth. It consists of around 14K training and 3K test images by rendering between 5 and 15 people per image using either image-based lighting or rendered 3D environments, taking care to make the images physically plausible and photoreal. In total, AGORA contains 173K individual person crops.
AGORA provides (1) SMPL/SMPL-X parameters and (2) segmentation masks for each subject in images.",https://production-media.paperswithcode.com/datasets/image3_Zn6mbZv.png,EditCustom (non-commercial),"3D, Image",,,,,,training and 3K test images,,"3D Human Shape Estimation, 3D Multi-Person Pose Estimation, 2D Human Pose Estimation, 3D Multi-Person Mesh Recovery, 3D Human Reconstruction, 3D Human Pose Estimation, Monocular 3D Human Pose Estimation, 3D Hand Pose Estimation","3d-multi-person-pose-estimation-on-agora, 3d-multi-person-mesh-recovery-on-agora, 3d-human-reconstruction-on-agora-1, 3d-human-pose-estimation-on-agora",,See all 1951 tasks,3D Human Pose Estimation58 ben,3D Human Pose Estimation58 ben
Human3.6M,Human3.6M Dataset,"The Human3.6M dataset is one of the largest motion capture datasets, which consists of 3.6 million human poses and corresponding images captured by a high-speed motion capture system. There are 4 high-resolution progressive scan cameras to acquire video data at 50 Hz. The dataset contains activities by 11 professional actors in 17 scenarios: discussion, smoking, taking photo, talking on the phone, etc., as well as provides accurate 3D joint positions and high-resolution videos.",https://arxiv.org/abs/1601.01006,"EditCustom (research-only, attribution)","3D, Image, Text, Time Series, Video",English,,,,,,,"2D Pose Estimation, Unsupervised Human Pose Estimation, 3D Human Pose Estimation in Limited Data, Human Part Segmentation, 3D Pose Estimation, Video Prediction, Weakly-supervised 3D Human Pose Estimation, Pose Retrieval, 3D Absolute Human Pose Estimation, 3D Human Pose Estimation, Multi-Hypotheses 3D Human Pose Estimation, Monocular 3D Human Pose Estimation, Human Pose Forecasting, Unsupervised 3D Human Pose Estimation, Human action generation, Root Joint Localization","3d-human-pose-estimation-on-human36m, monocular-3d-human-pose-estimation-on-human3, human-action-generation-on-human3-6m, human-part-segmentation-on-human3-6m, unsupervised-human-pose-estimation-on-human3, video-prediction-on-human36m, 2d-pose-estimation-on-human3-6m, multi-hypotheses-3d-human-pose-estimation-on, human-pose-forecasting-on-human36m, 3d-pose-estimation-on-human3-6m, unsupervised-3d-human-pose-estimation-on, 3d-absolute-human-pose-estimation-on-human36m, root-joint-localization-on-human3-6m, 3d-human-pose-estimation-in-limited-data-on, weakly-supervised-3d-human-pose-estimation-on, pose-retrieval-on-human3-6m",,See all 1951 tasks,3D Human Pose Estimation58 ben,3D Human Pose Estimation58 ben
LSP,LSP Dataset,"The Leeds Sports Pose (LSP) dataset is widely used as the benchmark for human pose estimation. The original LSP dataset contains 2,000 images of sportspersons gathered from Flickr, 1000 for training and 1000 for testing. Each image is annotated with 14 joint locations, where left and right joints are consistently labelled from a person-centric viewpoint. The extended LSP dataset contains additional 10,000 images labeled for training.

Image: Sumer et al",https://arxiv.org/abs/1605.01014,EditCustom,"3D, Image",,,,,000 images,"training and 1000 for testing. Each image is annotated with 14 joint locations, where left and right joints are consistently labelled from a person-centric viewpoint. The extended LSP dataset contains additional 10,000 images",,"3D Human Pose Estimation, Pose Estimation, 3D Pose Estimation",pose-estimation-on-leeds-sports-poses,,See all 1951 tasks,3D Human Pose Estimation58 ben,3D Human Pose Estimation58 ben
MPI-INF-3DHP,MPI-INF-3DHP Dataset,MPI-INF-3DHP is a 3D human body pose estimation dataset consisting of both constrained indoor and complex outdoor scenes. It records 8 actors performing 8 activities from 14 camera views. It consists on >1.3M frames captured from the 14 cameras.,https://arxiv.org/abs/2002.10322,EditCustom (non-commercial),"3D, Image",,,,,,,,"Weakly-supervised 3D Human Pose Estimation, Multi-view 3D Human Pose Estimation, Pose Retrieval, Cross-domain 3D Human Pose Estimation, 3D Human Pose Estimation, Multi-Hypotheses 3D Human Pose Estimation, Unsupervised 3D Human Pose Estimation","pose-retrieval-on-mpi-inf-3dhp, multi-view-3d-human-pose-estimation-on-mpi, multi-hypotheses-3d-human-pose-estimation-on-1, 3d-human-pose-estimation-on-mpi-inf-3dhp, cross-domain-3d-human-pose-estimation-on-mpi, weakly-supervised-3d-human-pose-estimation-on-1, unsupervised-3d-human-pose-estimation-on-mpi",,See all 1951 tasks,3D Human Pose Estimation58 ben,3D Human Pose Estimation58 ben
Panoptic,Panoptic Dataset,"CMU Panoptic is a large scale dataset providing 3D pose annotations (1.5 millions) for multiple people engaging social activities. It contains 65 videos (5.5 hours) with multi-view annotations, but only 17 of them are in multi-person scenario and have the camera parameters.

Massively Multiview System


480 VGA camera views
30+ HD views
10 RGB-D sensors
Hardware-based sync
Calibration
Interesting Scenes with Labels

Multiple people


Socially interacting groups
3D body pose
3D facial landmarks
Transcripts + speaker ID

Hardware setup


480 VGA cameras, 640 x 480 resolution, 25 fps, synchronized among themselves using a hardware clock
31 HD cameras, 1920 x 1080 resolution, 30 fps, synchronized among themselves using a hardware clock, timing aligned with VGA cameras
10 Kinect Ⅱ Sensors. 1920 x 1080 (RGB), 512 x 424 (depth), 30 fps, timing aligned among themselves and other sensors
5 DLP Projectors. synchronized with HD cameras",https://arxiv.org/abs/1908.09220,EditCustom (non-commercial),"3D, Image, Video",,1920,,,,,,"3D Human Pose Estimation, 3D Human Pose Tracking, Head Pose Estimation, 3D Multi-Person Pose Estimation","3d-multi-person-pose-estimation-on-cmu, 3d-human-pose-tracking-on-cmu-panoptic, head-pose-estimation-on-panoptic, 3d-human-pose-estimation-on-cmu-panoptic",,See all 1951 tasks,3D Human Pose Estimation58 ben,3D Human Pose Estimation58 ben
TotalCapture,TotalCapture Dataset,"The TotalCapture dataset consists of 5 subjects performing several activities such as walking, acting, a range of motion sequence (ROM) and freestyle motions, which are recorded using 8 calibrated, static HD RGB cameras and 13 IMUs attached to head, sternum, waist, upper arms, lower arms, upper legs, lower legs and feet, however the IMU data is not required for our experiments. The dataset has publicly released foreground mattes and RGB images. Ground-truth poses are obtained using a marker-based motion capture system, with the markers are <5mm in size. All data is synchronised and operates at a framerate of 60Hz, providing ground truth poses as joint positions.",https://arxiv.org/abs/1908.03030,"EditCustom (research-only, non-commercial, attribution)","3D, Image",,,,,,,,"3D Absolute Human Pose Estimation, 3D Human Pose Estimation, Pose Estimation","3d-absolute-human-pose-estimation-on-total-1, 3d-human-pose-estimation-on-total-capture",,See all 1951 tasks,3D Human Pose Estimation58 ben,3D Human Pose Estimation58 ben
CREMI,CREMI Dataset,"MICCAI Challenge on Circuit Reconstruction from Electron Microscopy Images.

About
The goal of this challenge is to evaluate algorithms for automatic reconstruction of neurons and neuronal connectivity from serial section electron microscopy data. The comparison is performed not only by evaluating the quality of neuron segmentations, but also by assessing the accuracy of detecting synapses and identifying synaptic partners. The challenge is carried out on three large and diverse datasets from adult Drosophila melanogaster brain tissue, comprising neuron segmentation ground truth and annotations for synaptic connections. A successful solution would demonstrate its efficiency and generalizability, and carry great potential to reduce the time spent on manual reconstruction of neural circuits in electron microscopy volumes.

Description
We provide three datasets, each consisting of two (5 μm)3 volumes (training and testing, each 1250 px × 1250 px × 125 px) of serial section EM of the adult fly brain. Each volume has neuron and synapse labelings and annotations for pre- and post-synaptic partners.",https://production-media.paperswithcode.com/datasets/ab32fb40-b443-4d91-95e7-e421f724bc38.png,EditUnknown,"3D, Image",,,,,,,,"3D Instance Segmentation, Brain Image Segmentation",brain-image-segmentation-on-cremi,,See all 1951 tasks,3D Instance Segmentation10 ben,3D Instance Segmentation10 ben
FOR-instance,FOR-instance Dataset,"The challenge of accurately segmenting individual trees from laser scanning data hinders the assessment of crucial tree parameters necessary for effective forest management, impacting many downstream applications. While dense laser scanning offers detailed 3D representations, automating the segmentation of trees and their structures from point clouds remains difficult. The lack of suitable benchmark datasets and reliance on small datasets have limited method development. The emergence of deep learning models exacerbates the need for standardized benchmarks. Addressing these gaps, the FOR-instance data represent a novel benchmarking dataset to enhance forest measurement using dense airborne laser scanning data, aiding researchers in advancing segmentation methods for forested 3D scenes.

In this repository, users will find forest laser scanning point clouds from unamnned aerial vehicle (using Riegl sensors) that are manually segmented according to the individual trees (1130 trees) and semantic classes. The point clouds are subdivided into five data collections representing different forests in Norway, the Czech Republic, Austria, New Zealand, and Australia. 

These data are meant to be used either for developement of new methods (using the dev data) or for testing of exisitng methods (test data). The data splits are provided in the data_split_metadata.csv file.

A full description of the FOR-instance data can be found at http://arxiv.org/abs/2309.01279",https://production-media.paperswithcode.com/datasets/b2e6d2ed-a112-4478-bdc0-1b8380552112.png,EditCreative Commons Attribution 4.0 International,"3D, Image",,,,,,,,"3D Instance Segmentation, 3D Semantic Segmentation",,,See all 1951 tasks,3D Instance Segmentation10 ben,3D Instance Segmentation10 ben
MultiScan,MultiScan Dataset,"We introduce MultiScan, a scalable RGBD dataset construction pipeline leveraging commodity mobile devices to scan indoor scenes with articulated objects and web-based semantic annotation interfaces to efficiently annotate object and part semantics and part mobility parameters. We use this pipeline to collect 273 scans of 117 indoor scenes containing 10957 objects and 5129 parts. The resulting MultiScan dataset provides RGBD streams with per-frame camera poses, textured 3D surface meshes, richly annotated part-level and object-level semantic labels, and part mobility parameters. We validate our dataset on instance segmentation and part mobility estimation tasks and benchmark methods for these tasks from prior work. Our experiments show that part segmentation and mobility estimation in real 3D scenes remain challenging despite recent progress in 3D object segmentation.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,"3D, Image",,,,,,,,"3D Instance Segmentation, 3D Object Detection",3d-object-detection-on-multiscan,,See all 1951 tasks,3D Instance Segmentation10 ben,3D Instance Segmentation10 ben
ScanNet200,ScanNet200 Dataset,"The ScanNet200 benchmark studies 200-class 3D semantic segmentation - an order of magnitude more class categories than previous 3D scene understanding benchmarks. The source of scene data is identical to ScanNet, but parses a larger vocabulary for semantic and instance segmentation",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"3D, Image",,,,,,,,"3D Open-Vocabulary Instance Segmentation, 3D Instance Segmentation, 3D Semantic Segmentation","3d-open-vocabulary-instance-segmentation-on, 3d-semantic-segmentation-on-scannet200, 3d-instance-segmentation-on-scannet200",,See all 1951 tasks,3D Instance Segmentation10 ben,3D Instance Segmentation10 ben
SceneNN,SceneNN Dataset,"SceneNN is an RGB-D scene dataset consisting of more than 100 indoor scenes. The scenes are captured at various places, e.g., offices, dormitory, classrooms, pantry, etc., from University of Massachusetts Boston and Singapore University of Technology and Design.
All scenes are reconstructed into triangle meshes and have per-vertex and per-pixel annotation. The dataset is additionally enriched with fine-grained information such as axis-aligned bounding boxes, oriented bounding boxes, and object poses.",http://103.24.77.34/scenenn/home/,EditUnknown,"3D, Image",,,,,,,,3D Instance Segmentation,3d-instance-segmentation-on-scenenn-1,,See all 1951 tasks,3D Instance Segmentation10 ben,3D Instance Segmentation10 ben
STPLS3D,STPLS3D Dataset,"Our project (STPLS3D) aims to provide a large-scale aerial photogrammetry dataset with synthetic and real annotated 3D point clouds for semantic and instance segmentation tasks.

Although various 3D datasets with different functions and scales have been proposed recently, it remains challenging for individuals to complete the whole pipeline of large-scale data collection, sanitization, and annotation (e.g., semantic and instance labels). Moreover, the created datasets usually suffer from extremely imbalanced class distribution or partial low-quality data samples. Motivated by this, we explore the procedurally synthetic 3D data generation paradigm to equip individuals with the full capability of creating large-scale annotated photogrammetry point clouds. Specifically, we introduce a synthetic aerial photogrammetry point clouds generation pipeline that takes full advantage of open geospatial data sources and off-the-shelf commercial packages. Unlike generating synthetic data in virtual games, where the simulated data usually have limited gaming environments created by artists, the proposed pipeline simulates the reconstruction process of the real environment by following the same UAV flight pattern on a wide variety of synthetic terrain shapes and building densities, which ensure similar quality, noise pattern, and diversity with real data. In addition, the precise semantic and instance annotations can be generated fully automatically, avoiding the expensive and time-consuming manual annotation process.  Based on the proposed pipeline, we present a richly-annotated synthetic 3D aerial photogrammetry point cloud dataset, termed STPLS3D, with more than 16 km^2 of landscapes and up to 18 fine-grained semantic categories. For verification purposes, we also provide a parallel dataset collected from four areas in the real environment.",https://github.com/meidachen/STPLS3D,EditUnknown,"3D, Image",,,,,,,,"3D Open-Vocabulary Instance Segmentation, Instance Segmentation, 3D Instance Segmentation, 3D Semantic Segmentation, Semantic Segmentation","3d-semantic-segmentation-on-stpls3d, 3d-open-vocabulary-instance-segmentation-on-3, 3d-instance-segmentation-on-stpls3d",,See all 1951 tasks,3D Instance Segmentation10 ben,3D Instance Segmentation10 ben
Campus___Shelf,Campus & Shelf Dataset,"The Campus and Shelf datasets were presented in the paper 3D Pictorial Structures for Multiple Human Pose Estimation.
The first dataset shows persons walking and talking in front of a building, and the second up to four persons assembling a shelf.",https://production-media.paperswithcode.com/datasets/02e340df-7914-49fa-8537-64d8380019d9.jpg,EditUnknown,"3D, Image",,,,,,,,"3D Human Pose Estimation, 3D Multi-Person Pose Estimation","3d-multi-person-pose-estimation-on-campus, 3d-multi-person-pose-estimation-on-shelf",,See all 1951 tasks,3D Multi-Person Pose Estimatio,3D Multi-Person Pose Estimatio
MuPoTS-3D,MuPoTS-3D Dataset,"MuPoTs-3D (Multi-person Pose estimation Test Set in 3D) is a dataset for pose estimation composed of more than 8,000 frames from 20 real-world scenes with up to three subjects. The poses are annotated with a 14-point skeleton model.",https://arxiv.org/abs/2008.09457,EditUnknown,"3D, Image",,,,,,,,"3D Multi-Person Pose Estimation (root-relative), 3D Multi-Person Pose Estimation (absolute), Unsupervised 3D Multi-Person Pose Estimation, 3D Multi-Person Pose Estimation","3d-multi-person-pose-estimation-root-relative, 3d-multi-person-human-pose-estimation-on, 3d-multi-person-pose-estimation-absolute-on, unsupervised-3d-multi-person-pose-estimation",,See all 1951 tasks,3D Multi-Person Pose Estimatio,3D Multi-Person Pose Estimatio
Store_dataset,Store dataset Dataset,"The Store Dataset is a dataset for estimating 3D poses of multiple humans in real-time. It is captured inside two kinds of simulated stores with 12 and 28 cameras, respectively.",https://arxiv.org/abs/2003.03972,EditUnknown,"3D, Image",,,,,,,,"Pose Estimation, 3D Pose Estimation, 3D Multi-Person Pose Estimation",,,See all 1951 tasks,3D Multi-Person Pose Estimatio,3D Multi-Person Pose Estimatio
Argoverse,Argoverse Dataset,"Argoverse is a tracking benchmark with over 30K scenarios collected in Pittsburgh and Miami. Each scenario is a sequence of frames sampled at 10 HZ. Each sequence has an interesting object called “agent”, and the task is to predict the future locations of agents in a 3 seconds future horizon. The sequences are split into training, validation and test sets, which have 205,942, 39,472 and 78,143 sequences respectively. These splits have no geographical overlap.",https://arxiv.org/abs/2007.13732,EditCustom,"3D, Image, Text, Time Series, Video",English,,,,,,,"3D Object Detection, Monocular Cross-View Road Scene Parsing(Vehicle), Motion Forecasting, 3D Object Tracking, Monocular Cross-View Road Scene Parsing(Road), Trajectory Prediction","motion-forecasting-on-argoverse-cvpr-2020, 3d-object-tracking-on-argoverse-cvpr-2020, monocular-cross-view-road-scene-parsing-1, monocular-cross-view-road-scene-parsing-road-2, 3d-object-detection-on-argoverse, trajectory-prediction-on-argoverse",,See all 1951 tasks,3D Object Detection90 benchmar,3D Object Detection90 benchmar
Argoverse_2,Argoverse 2 Dataset,"Argoverse 2 (AV2) is a collection of three datasets for perception and forecasting research in the self-driving domain. The annotated Sensor Dataset contains 1,000 sequences of multimodal data, encompassing high-resolution imagery from seven ring cameras, and two stereo cameras in addition to lidar point clouds, and 6-DOF map-aligned pose. Sequences contain 3D cuboid annotations for 26 object categories, all of which are sufficiently-sampled to support training and evaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point clouds and map-aligned pose. This dataset is the largest ever collection of lidar sensor data and supports self-supervised learning and the emerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset contains 250,000 scenarios mined for interesting and challenging interactions be- tween the autonomous vehicle and other actors in each local scene. Models are tasked with the prediction of future motion for “scored actors"" in each scenario and are provided with track histories that capture object location, heading, velocity, and category. In all three datasets, each scenario contains its own HD Map with 3D lane and crosswalk geometry — sourced from data captured in six distinct cities. We believe these datasets will support new and existing machine learning research problems in ways that existing datasets do not. All datasets are released under the CC BY-NC-SA 4.0 license.",https://production-media.paperswithcode.com/datasets/f7e47824-be40-4836-95bf-3ce0794fec93.jpg,EditCC BY-NC-SA 4.0,"3D, Image",,,,,,,,"Scene Flow Estimation, Dynamic Point Removal, Self-supervised Scene Flow Estimation, 3D Object Detection","dynamic-point-removal-on-argoverse-2, self-supervised-scene-flow-estimation-on-1, scene-flow-estimation-on-argoverse-2",,See all 1951 tasks,3D Object Detection90 benchmar,3D Object Detection90 benchmar
3D-POP,3D-POP Dataset,"The dataset is designed specifically to solve a range of computer vision problems (2D-3D tracking, posture) faced by biologists while designing behavior studies with animals.

Typically, datasets for animal-specific vision tasks are created using open-source video material. This might be effective for an initial start, but these methods are not deployment ready for the behavior community. Therefore, we designed a semi-automated method for biologists to create well-curated datasets at a large scale for the ML and Vision community.

3D-POP is the first dataset with 3D ground truth for multi-animal, multi-view tracking problems.

Highlight: The dataset is captured with the intention of using it for various vision problems and with different levels of complexity (no of cameras, no of individuals)

Video explanation: Link to YouTube video

Video teaser: Link to YouTube video

Dataset Features:
Marker-based videos:

6 hours+ of annotations of 18 individuals (groups of 1, 2, 5, 10).
Bounding box
Trajectories (2D and 3D)
Posture (2D and 3D) with 9 key points
Identities
Total of 57 sequences (4K) with 4 views.
Dataset customization* (Users can modify the dataset and add key points to the dataset)

Markerless:

1Hr+ videos of 18 individuals in groups of 1, 2, 5, 11. The birds have no markers. This data is provided as test cases and unsupervised approaches.

Problems:
2D domain:

Position, Posture of birds (different group sizes n = 1, 2, 5, 10) with Single/Multiview.
Tracking with single - multiview

3D domain:

Position, Posture of birds (different group sizes n = 1, 2, 5, 10) with Single/Multiview.
Tracking with single - multiview

Fine-grained recognition:

Identity tracking with ground truth.

Unsupervised learning:

2D or 3D posture problems

Idea:
The dataset is created with a motion capture system, using the 6-DOF tracking ability. Assumptions are that head and body act as rigid bodies when birds walk and forage (proved with experiment). Therefore, we get the 3D position of key points by tracking head/body orientation.",https://production-media.paperswithcode.com/datasets/11a64d35-dda3-44cf-aac7-3ca47c16fd99.png,EditCC-BY-4.0,"3D, Image, Video",,,,,,,,"2D Pose Estimation, Animal Pose Estimation, 3D Pose Estimation, 3D Object Detection From Stereo Images, Multi-Object Tracking, Multi-Animal Tracking with identification, 3D Shape Reconstruction, 3D Object Tracking, Object Tracking, Occlusion Handling, 3D Object Detection From Monocular Images, 3D Object Reconstruction From A Single Image",,,See all 1951 tasks,3D Object Detection From Stere,3D Object Detection From Stere
3D-ZeF,3D-ZeF Dataset,"3D-ZeF dataset consists of eight sequences with a duration between 15-120 seconds and 1-10 free moving zebrafish. The videos have been annotated with a total of 86,400 points and bounding boxes.",/paper/3d-zef-a-3d-zebrafish-tracking-benchmark-1,EditUnknown,"3D, Image, Video",,,,,,,,"3D Multi-Object Tracking, 3D Object Detection From Stereo Images, Multi-Object Tracking",,,See all 1951 tasks,3D Object Detection From Stere,3D Object Detection From Stere
Corn_Seeds_Dataset,Corn Seeds Dataset Dataset,"This dataset is the images of corn seeds considering the top and bottom view independently (two images for one corn seed: top and bottom). There are four classes of the corn seed (Broken-B, Discolored-D, Silkcut-S, and Pure-P) 17802 images are labeled by the experts at the AdTech Corp. and 26K images were unlabeled out of which 9k images were labeled using the Active Learning (BatchBALD)

We have created three different dataset: (1). Primary dataset: contains the 17802 images labeled by the experts. Top-view(8901) and Bottom-view(8901).

(2). Dataset with fake images: We generated fake images using Conditional GAN (BigGAN) as follows: broken-2937, discolored-5823, pure-2937, silkcut-5823 instances and added them into the train set to balance the data set.

(3). Balanced dataset: In this case of adding newly captured images labeled using the Batch Active Learning method, new 9000 labeled images are added into the primary dataset. This new dataset contains 26,802 images split into train and validation set 80: 20, respectively. Contains the 17802 images and the 9K images labeled by the Active Learning (BatchBALD).",https://production-media.paperswithcode.com/datasets/7728bc0b-3483-44fb-99cf-a1da8c2c5709.jpg,EditUnknown,"3D, Audio, Image, Text",English,,,,17802 images,"split into train and validation set 80: 20, respectively. Contains the 17802 images",,"2D Semantic Segmentation, Image Classification with Label Noise, Fake Image Detection, Fine-Grained Image Classification, 3D Object Reconstruction, 3D Classification, Image Generation, Semantic Segmentation, Multi-Label Learning, Image Classification, 3D Object Recognition, Zero-Shot Image Classification",,,See all 1951 tasks,3D Object Recognition4 benchma,3D Object Recognition4 benchma
Cube__,Cube++ Dataset,"Cube++ is a novel dataset for the color constancy problem that continues on the Cube+ dataset. It includes 4890 images of different scenes under various conditions. For calculating the ground truth illumination, a calibration object with known surface colors was placed in every scene.",https://github.com/Visillect/CubePlusPlus,EditUnknown,"3D, Image",,,,,4890 images,,,"3D Object Recognition, Color Constancy",3d-object-recognition-on-cube-engraving,,See all 1951 tasks,3D Object Recognition4 benchma,3D Object Recognition4 benchma
CY101_Dataset,CY101 Dataset Dataset,"In this dataset an uppertorso humanoid robot with 7-DOF arm explored 100 different objects belonging to 20 different categories using 10 behaviors: Look, Crush, Grasp, Hold, Lift, Drop, Poke, Push, Shake and Tap.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"3D, Image",,,,,,,,"2D Object Detection, 3D Object Classification, 3D Object Recognition, 3D Object Detection",,,See all 1951 tasks,3D Object Recognition4 benchma,3D Object Recognition4 benchma
DOORS,DOORS Dataset,"DOORS  is a dataset designed for boulders recognition, centroid regression, segmentation, and navigation applications.  The dataset is divided into two sets:



Regression: Contains images, masks, and labels for 4 splits of single boulders positioned on the surface of a spherical mesh. It can be used to perform navigation, boulder recognition, segmentation, and centroid regression.



Segmentation: Contain images, masks, and labels of 2 datasets: DS1 and DS2. DS1 is made of the same images of the Regression dataset but is specifically designed for segmentation. DS2 is made of images with multiple instances of boulders appearing on the surface of the Didymos asteroid model",https://arxiv.org/pdf/2210.16253v1.pdf,EditCreative Commons Attribution 4.0 International,"3D, Image",,,,,,,,"3D Object Recognition, Semantic Segmentation",,,See all 1951 tasks,3D Object Recognition4 benchma,3D Object Recognition4 benchma
ModelNet,ModelNet Dataset,"The ModelNet40 dataset contains synthetic object point clouds. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular because of its various categories, clean shapes, well-constructed dataset, etc. The original ModelNet40 consists of 12,311 CAD-generated meshes in 40 categories (such as airplane, car, plant, lamp), of which 9,843 are used for training while the rest 2,468 are reserved for testing. The corresponding point cloud data points are uniformly sampled from the mesh surfaces, and then further preprocessed by moving to the origin and scaling into a unit sphere.",https://arxiv.org/abs/1911.12885,EditUnknown,"3D, Image",,,,,,,40,"Zero-Shot Transfer 3D Point Cloud Classification, Zero-shot 3D Point Cloud Classification, Few-Shot Point Cloud Classification, Training-free 3D Point Cloud Classification, 3D Point Cloud Data Augmentation, 3D Point Cloud Classification, Generative 3D Object Classification, 3D Parameter-Efficient Fine-Tuning for Classification, 3D Object Retrieval, 3D Object Classification, 3D Point Cloud Linear Classification, Few-Shot 3D Point Cloud Classification, 3D Object Recognition","zero-shot-transfer-3d-point-cloud-1, 3d-object-retrieval-on-modelnet40, few-shot-3d-point-cloud-classification-on-1, few-shot-3d-point-cloud-classification-on-3, zero-shot-transfer-3d-point-cloud, few-shot-point-cloud-classification-on, 3d-object-classification-on-modelnet10, zero-shot-3d-point-cloud-classification-on, 3d-point-cloud-linear-classification-on, generative-3d-object-classification-on-2, training-free-3d-point-cloud-classification, 3d-point-cloud-data-augmentation-on, 3d-object-classification-on-modelnet40, 3d-parameter-efficient-fine-tuning-for-1, few-shot-3d-point-cloud-classification-on-4, few-shot-3d-point-cloud-classification-on-2, 3d-object-recognition-on-modelnet40, 3d-point-cloud-classification-on-modelnet40",,See all 1951 tasks,3D Object Recognition4 benchma,3D Object Recognition4 benchma
SHREC,SHREC Dataset,"The SHREC dataset contains 14 dynamic gestures performed by 28 participants (all participants are right handed) and captured by the Intel RealSense short range depth camera. Each gesture is performed between 1 and 10 times by each participant in two way: using one finger and the whole hand. Therefore, the dataset is composed by 2800 sequences captured. The depth image, with a resolution of 640x480, and the coordinates of 22 joints (both in the 2D depth image space and in the 3D world space) are saved for each frame of each sequence in the dataset.",https://arxiv.org/abs/1803.10435,EditUnknown,"3D, Image, Video",,,,,,,,"Gesture Recognition, Skeleton Based Action Recognition, Point Cloud Super Resolution, 3D Object Recognition, Hand Gesture Recognition","point-cloud-super-resolution-on-shrec15, hand-gesture-recognition-on-dhg-14, 3d-object-recognition-on-shrec11-split16-4, hand-gesture-recognition-on-shrec-2017, gesture-recognition-on-shrec-2017-track-on-3d, hand-gesture-recognition-on-dhg-28, hand-gesture-recognition-on-shrec-2017-track, skeleton-based-action-recognition-on-shrec",,See all 1951 tasks,3D Object Recognition4 benchma,3D Object Recognition4 benchma
The_RBO_Dataset_of_Articulated_Objects_and_Interac,The RBO Dataset of Articulated Objects and Interactions Dataset,"The RBO dataset of articulated objects and interactions is a collection of 358 RGB-D video sequences (67:18 minutes) of humans manipulating 14 articulated objects under varying conditions (light, perspective, background, interaction). All sequences are annotated with ground truth of the poses of the rigid parts and the kinematic state of the articulated object (joint states) obtained with a motion capture system. We also provide complete kinematic models of these objects (kinematic structure and three-dimensional textured shape models). In 78 sequences the contact wrenches during the manipulation are also provided.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY 4.0,"3D, Image, Video",,,,,,,,"3D Object Tracking, 3D Object Retrieval, 3D Object Recognition, 3D Object Reconstruction",,,See all 1951 tasks,3D Object Recognition4 benchma,3D Object Recognition4 benchma
Washington_RGB-D,Washington RGB-D Dataset,"Washington RGB-D is a widely used testbed in the robotic community, consisting of 41,877 RGB-D images organized into 300 instances divided in 51 classes of common indoor objects (e.g. scissors, cereal box, keyboard etc). Each object instance was positioned on a turntable and captured from three different viewpoints while rotating.",https://arxiv.org/abs/1702.08513,EditUnknown,"3D, Image",,,,,300 instances,"testbed in the robotic community, consisting of 41,877 RGB-D images",51,"Object Recognition, Object Detection, 3D Object Recognition",,,See all 1951 tasks,3D Object Recognition4 benchma,3D Object Recognition4 benchma
Synthetic_OD_Data,Synthetic OD Data Dataset,Synthetic OD data to mimic data showed in the application of the paper.,https://production-media.paperswithcode.com/datasets/6bbe7ec3-e5f2-4af5-9a4e-6b761d8eb6ab.png,EditUnknown,3D,,,,,,,,3D Point Cloud Reinforcement Learning,3d-point-cloud-reinforcement-learning-on,,See all 1951 tasks,3D Point Cloud Reinforcement L,3D Point Cloud Reinforcement L
HO-3D_v2,HO-3D v2 Dataset,"A hand-object interaction dataset with 3D pose annotations of hand and object. The dataset contains 66,034 training images and 11,524 test images from a total of 68 sequences. The sequences are captured in multi-camera and single-camera setups and contain 10 different subjects manipulating 10 different objects from YCB dataset. The annotations are automatically obtained using an optimization algorithm. The hand pose annotations for the test set are withheld and the accuracy of the algorithms on the test set can be evaluated with standard metrics using the CodaLab challenge submission(see project page). The object pose annotations for the test and train set are provided along with the dataset.",https://production-media.paperswithcode.com/datasets/logo_mRPd4nq.png,EditUnknown,"3D, Image",,,,,,"training images and 11,524 test images",,"hand-object pose, 3D Canonical Hand Pose Estimation, 3D Pose Estimation, 3D Hand Pose Estimation","3d-hand-pose-estimation-on-ho-3d, hand-object-pose-on-ho-3d",,See all 1951 tasks,3D Pose Estimation7 benchmarks,3D Pose Estimation7 benchmarks
MuCo-3DHP,MuCo-3DHP Dataset,MuCo-3DHP is a large scale training data set showing real images of sophisticated multi-person interactions and occlusions.,https://arxiv.org/pdf/1712.03453v3.pdf,EditCustom (non-commercial),"3D, Image",,,,,,,,"3D Human Pose Estimation, Pose Estimation, 3D Pose Estimation",,,See all 1951 tasks,3D Pose Estimation7 benchmarks,3D Pose Estimation7 benchmarks
MVOR,MVOR Dataset,"Multi-View Operating Room (MVOR) is a dataset recorded during real clinical interventions. It consists of 732 synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR. It also includes the visual challenges present in such environments, such as occlusions and clutter.",https://github.com/CAMMA-public/mvor,EditCC-BY-NC-SA 4.0,"3D, Image",,,,,,,,"3D Human Pose Estimation, Pose Estimation, 3D Pose Estimation",,,See all 1951 tasks,3D Pose Estimation7 benchmarks,3D Pose Estimation7 benchmarks
PartNet-Mobility,PartNet-Mobility Dataset,"Dataset produced for the SAPIEN simulation environment. From the website: ""PartNet-Mobility dataset is a collection of 2K articulated objects with motion annotations and rendernig material. The dataset powers research for generalizable computer vision and manipulation. The dataset is a continuation of ShapeNet and PartNet. """,https://production-media.paperswithcode.com/datasets/38fffadb-2c28-476a-8aa7-af52f60448a5.jpg,EditMIT,"3D, Image",,,,,,,,"3D Feature Matching, 3D Shape Modeling, Articulated Object modelling, 3D Pose Estimation",,,See all 1951 tasks,3D Pose Estimation7 benchmarks,3D Pose Estimation7 benchmarks
SVIRO,SVIRO Dataset,"Contains bounding boxes for object detection, instance segmentation masks, keypoints for pose estimation and depth images for each synthetic scenery as well as images for each individual seat for classification.",/paper/sviro-synthetic-vehicle-interior-rear-seat,EditUnknown,"3D, Image",,,,,,,,"Intent Detection, Pose Estimation, 3D Pose Estimation",,,See all 1951 tasks,3D Pose Estimation7 benchmarks,3D Pose Estimation7 benchmarks
UnrealEgo,UnrealEgo Dataset,"UnrealEgo is a dataset that provides in-the-wild stereo images with a large variety of motions for 3D human pose estimation. The in-the-wild stereo images are stereo fisheye images and depth maps with a resolution of 1024×1024 pixels each with 25 frames per second and a total of 450k  (900k images) are captured for the dataset. Metadata is provided for each frame, including 3D joint positions, camera positions, and 2D coordinates of reprojected joint positions in the fisheye views.",https://production-media.paperswithcode.com/datasets/05ac387a-718c-4596-8408-60846c90d32c.png,EditUnknown,"3D, Image",,,,,900k images,,,"Egocentric Pose Estimation, Pose Estimation, 3D Pose Estimation",egocentric-pose-estimation-on-unrealego,,See all 1951 tasks,3D Pose Estimation7 benchmarks,3D Pose Estimation7 benchmarks
ABC_Dataset,ABC Dataset Dataset,"The ABC Dataset is a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms.",https://production-media.paperswithcode.com/datasets/abc_jxmEMzg.jpg,EditCustom,"3D, Image",,,,,,,,"Object Detection, Physical Simulations, Region Proposal, Single-View 3D Reconstruction, 3D Reconstruction",,,See all 1951 tasks,3D Reconstruction46 benchmarks,3D Reconstruction46 benchmarks
BlendedMVS,BlendedMVS Dataset,"BlendedMVS is a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. The dataset was created by applying a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, these mesh models were rendered to color images and depth maps.",/paper/blendedmvs-a-large-scale-dataset-for,EditCreative Commons Attribution 4.0 International License,3D,,,,,,training ground truth for learning-based MVS. The dataset was created by applying a 3D reconstruction pipeline to recover high-quality textured meshes from images,,"3D Reconstruction, Meta-Learning, Novel View Synthesis",,,See all 1951 tasks,3D Reconstruction46 benchmarks,3D Reconstruction46 benchmarks
DTU,DTU Dataset,"DTU MVS 2014 is a multi-view stereo dataset, which is an order of magnitude larger in number of scenes and with a significant increase in diversity. Specifically, it contains 80 scenes of large variability. Each scene consists of 49 or 64 accurate camera positions and reference structured light scans, all acquired by a 6-axis industrial robot.",https://production-media.paperswithcode.com/datasets/house_small.png,EditFree,3D,,2014,,,,,,"Point Clouds, 3D Reconstruction","3d-reconstruction-on-dtu, point-clouds-on-dtu",,See all 1951 tasks,3D Reconstruction46 benchmarks,3D Reconstruction46 benchmarks
ETH3D,ETH3D Dataset,"ETHD is a multi-view stereo benchmark / 3D reconstruction benchmark that covers a variety of indoor and outdoor scenes.
Ground truth geometry has been obtained using a high-precision laser scanner.
A DSLR camera as well as a synchronized multi-camera rig with varying field-of-view was used to capture images.",/paper/a-multi-view-stereo-benchmark-with-high,EditCC BY-NC-SA 4.0,3D,,,,,,,,"Monocular Depth Estimation, Dense Pixel Correspondence Estimation, Depth Estimation, Stereo Matching, Multi-View 3D Reconstruction, 3D Reconstruction","dense-pixel-correspondence-estimation-on-3, monocular-depth-estimation-on-eth3d, multi-view-3d-reconstruction-on-eth3d",,See all 1951 tasks,3D Reconstruction46 benchmarks,3D Reconstruction46 benchmarks
Hypersim,Hypersim Dataset,"For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. Hypersim is a photorealistic synthetic dataset for holistic indoor scene understanding. It contains 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry.",https://github.com/apple/ml-hypersim,EditCustom,"3D, Image",,,,,400 images,,,"Multi-Task Learning, Monocular Depth Estimation, 3D Shape Recognition, 3D Pose Estimation, Inverse Rendering, Instance Segmentation, Depth Estimation, Panoptic Segmentation, 3D Panoptic Segmentation, 3D Semantic Segmentation, 3D Shape Reconstruction, 2D Object Detection, Intrinsic Image Decomposition, Semantic Segmentation, Single-View 3D Reconstruction, 3D Reconstruction, 3D Object Detection","panoptic-segmentation-on-hypersim, 3d-semantic-segmentation-on-hypersim, semantic-segmentation-on-hypersim, 3d-panoptic-segmentation-on-hypersim, monocular-depth-estimation-on-hypersim",,See all 1951 tasks,3D Reconstruction46 benchmarks,3D Reconstruction46 benchmarks
MegaDepth,MegaDepth Dataset,The MegaDepth dataset is a dataset for single-view depth prediction that includes 196 different locations reconstructed from COLMAP SfM/MVS.,/paper/megadepth-learning-single-view-depth,EditUnknown,"3D, Video",,,,,,,,"Optical Flow Estimation, 3D Reconstruction, Depth Estimation",,,See all 1951 tasks,3D Reconstruction46 benchmarks,3D Reconstruction46 benchmarks
ShapeNet,ShapeNet Dataset,"ShapeNet is a large scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA. The repository contains over 300M models with 220,000 classified into 3,135 classes arranged using WordNet hypernym-hyponym relationships. ShapeNet Parts subset contains 31,693 meshes categorised into 16 common object classes (i.e. table, chair, plane etc.). Each shapes ground truth contains 2-5 parts (with a total of 50 part classes).",https://arxiv.org/abs/1907.04444,EditCustom (non-commerical),"3D, Image, Text",English,,,,,,135,"Point Cloud Generation, 3D Object Reconstruction, Novel View Synthesis, 3D Part Segmentation, Point Cloud Completion, Single-View 3D Reconstruction, Semantic Segmentation, Training-free 3D Part Segmentation, 3D Reconstruction","point-cloud-generation-on-shapenet-airplane, point-cloud-completion-on-shapenet, semantic-segmentation-on-shapenet, training-free-3d-part-segmentation-on, point-cloud-generation-on-shapenet-chair, 3d-part-segmentation-on-shapenet-part, single-view-3d-reconstruction-on-shapenet, 3d-reconstruction-on-shapenet, novel-view-synthesis-on-shapenet-chair, novel-view-synthesis-on-shapenet-car, point-cloud-generation-on-shapenet-car, 3d-object-reconstruction-on-shapenet, point-cloud-generation-on-shapenet",,See all 1951 tasks,3D Reconstruction46 benchmarks,3D Reconstruction46 benchmarks
ShapeNetCore,ShapeNetCore Dataset,"ShapeNetCore is a subset of the full ShapeNet dataset with single clean 3D models and manually verified category and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object categories of PASCAL 3D+, a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore.",https://production-media.paperswithcode.com/datasets/0f5ee5ed-128d-4265-9020-dc823f947036.png,EditCustom (non-commercial),"3D, Image",,,,,,,,"3D Reconstruction, Single-View 3D Reconstruction, 3D Classification",single-view-3d-reconstruction-on-shapenetcore,,See all 1951 tasks,3D Reconstruction46 benchmarks,3D Reconstruction46 benchmarks
PanoContext,PanoContext Dataset,The PanoContext dataset contains 500 annotated cuboid layouts of indoor environments such as bedrooms and living rooms.,https://arxiv.org/abs/1803.08999,EditUnknown,3D,,,,,,,,3D Room Layouts From A Single RGB Panorama,3d-room-layouts-from-a-single-rgb-panorama-on,,See all 1951 tasks,3D Room Layouts From A Single ,3D Room Layouts From A Single 
Rent3D,Rent3D Dataset,A dataset which contains over 200 apartments.,/paper/rent3d-floor-plan-priors-for-monocular-layout,EditUnknown,"3D, Image",,,,,,,,"3D Room Layouts From A Single RGB Panorama, Semantic Segmentation, Room Layout Estimation",,,See all 1951 tasks,3D Room Layouts From A Single ,3D Room Layouts From A Single 
AMOS,AMOS Dataset,"Despite the considerable progress in automatic abdominal multi-organ segmentation from CT/MRI scans in recent years, a comprehensive evaluation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples, which still limits the power of modern deep models and makes it difficult to provide a fully comprehensive and fair estimate of various methods. To mitigate the limitations, we present AMOS, a large-scale, diverse, clinical dataset for abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease patients, each with voxel-level annotations of 15 abdominal organs, providing challenging examples and test-bed for studying robust segmentation algorithms under diverse targets and scenarios. We further benchmark several state-of-the-art medical segmentation models to evaluate the status of the existing methods on this new challenging dataset. We have made our datasets, benchmark servers, and baselines publicly available, and hope to inspire future research. Information can be found at https://amos22. grand-challenge. org.",https://production-media.paperswithcode.com/datasets/1b0ca5c4-4b61-4cf7-9d9c-48a2aa5bba3f.png,EditUnknown,"3D, Image",,,,,,"valuation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples",,"Medical Image Segmentation, 3D Semantic Segmentation",medical-image-segmentation-on-amos,,See all 1951 tasks,3D Semantic Segmentation23 ben,3D Semantic Segmentation23 ben
KITTI-360,KITTI-360 Dataset,"KITTI-360 is a large-scale dataset that contains rich sensory information and full annotations. It is the successor of the popular KITTI dataset,  providing more comprehensive semantic/instance labels in 2D and 3D, richer 360 degree sensory information (fisheye images and pushbroom laser scans), very accurate and geo-localized vehicle and camera poses, and a series of new challenging benchmarks.",https://production-media.paperswithcode.com/datasets/89bce839-f270-49f4-8f76-7d20eb99a1f2.jpg,"EditAll datasets and benchmarks on this page are copyright by us and published under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License. This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license. Per GDPR requirements, to download and to use the data you need to register and specify the i","3D, Image",English,,,,,,,"2D Semantic Segmentation, 3D Semantic Scene Completion, Semantic SLAM, Instance Segmentation, Weakly Supervised 3D Detection, Panoptic Segmentation, Novel View Synthesis, 3D Instance Segmentation, 3D Semantic Segmentation, 3D Semantic Scene Completion from a single RGB image, Semantic Segmentation, 3D Object Detection From Monocular Images","3d-semantic-scene-completion-on-kitti-360, weakly-supervised-3d-detection-on-kitti-360, panoptic-segmentation-on-kitti-360, 3d-semantic-scene-completion-from-a-single-2, 3d-object-detection-from-monocular-images-on-7, 3d-semantic-segmentation-on-kitti-360, semantic-segmentation-on-kitti-360",,See all 1951 tasks,3D Semantic Segmentation23 ben,3D Semantic Segmentation23 ben
PartNet,PartNet Dataset,"PartNet is a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. The dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others.",/paper/partnet-a-large-scale-benchmark-for-fine,EditUnknown,"3D, Image",,,,,,,,"3D Instance Segmentation, Instance Segmentation, Semantic Segmentation, 3D Semantic Segmentation","3d-instance-segmentation-on-partnet, instance-segmentation-on-partnet, 3d-semantic-segmentation-on-partnet",,See all 1951 tasks,3D Semantic Segmentation23 ben,3D Semantic Segmentation23 ben
RELLIS-3D,RELLIS-3D Dataset,"RELLIS-3D is a multi-modal dataset for off-road robotics. It was collected in an off-road environment containing annotations for 13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis Campus of Texas A&M University and presents challenges to existing algorithms related to class imbalance and environmental topography. The dataset also provides full-stack sensor data in ROS bag format, including RGB camera images, LiDAR point clouds, a pair of stereo images, high-precision GPS measurement, and IMU data.",https://github.com/unmannedlab/RELLIS-3D,EditUnknown,"3D, Image",,,,,235 images,,,"Autonomous Navigation, Scene Understanding, Semantic Segmentation, 3D Semantic Segmentation","semantic-segmentation-on-rellis-3d-dataset, 3d-semantic-segmentation-on-rellis-3d-dataset",,See all 1951 tasks,3D Semantic Segmentation23 ben,3D Semantic Segmentation23 ben
SemanticPOSS,SemanticPOSS Dataset,The SemanticPOSS dataset for 3D semantic segmentation contains 2988 various and complicated LiDAR scans with large quantity of dynamic instances. The data is collected in Peking University and uses the same data format as SemanticKITTI.,/paper/semanticposs-a-point-cloud-dataset-with-large,EditUnknown,"3D, Image",,,,,,,,"Weakly supervised Semantic Segmentation, Autonomous Driving, Semantic Segmentation, 3D Semantic Segmentation","semantic-segmentation-on-semanticposs, weakly-supervised-semantic-segmentation-on-8",,See all 1951 tasks,3D Semantic Segmentation23 ben,3D Semantic Segmentation23 ben
CORSMAL,CORSMAL Dataset,"CORSMAL is a dataset for estimating the position and orientation in 3D (or 6D pose) of an object from a single view. The dataset consists of 138,240 images of rendered hands and forearms holding 48 synthetic objects, split into 3 grasp categories over 30 real backgrounds.",https://arxiv.org/pdf/2211.10470v1.pdf,EditUnknown,"3D, Image",,,,,240 images,,,"6D Pose Estimation, Pose Estimation",,,See all 1951 tasks,6D Pose Estimation8 benchmarks,6D Pose Estimation8 benchmarks
HomebrewedDB,HomebrewedDB Dataset,"HomebrewedDB is a dataset for 6D pose estimation mainly targeting training from 3D models (both textured and textureless), scalability, occlusions, and changes in light conditions and object appearance. The dataset features 33 objects (17 toy, 8 household and 8 industry-relevant objects) over 13 scenes of various difficulty. It also consists of a set of benchmarks to test various desired detector properties, particularly focusing on scalability with respect to the number of objects and resistance to changing light conditions, occlusions and clutter.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-04-30_at_09.32.01.png,EditCC0 1.0 Universal,"3D, Image",,,,,,,,6D Pose Estimation,,,See all 1951 tasks,6D Pose Estimation8 benchmarks,6D Pose Estimation8 benchmarks
LM,LM Dataset,"The LM (Linemod) dataset is a valuable resource introduced by Stefan Hinterstoisser and colleagues in their research on model-based training, detection, and pose estimation of texture-less 3D objects in heavily cluttered scenes¹. Let's delve into the details:


Purpose and Context:
The primary goal of the LM dataset is to facilitate the development and evaluation of methods for detecting and estimating the 6 degrees-of-freedom pose of texture-less 3D objects.

It specifically targets scenarios where objects lack distinctive textures and are embedded in complex backgrounds with occlusions.



Methodology:


The dataset builds upon the LINEMOD approach, which combines depth and color information to create templates representing different views of an object.
LINEMOD templates are learned from 3D models and serve as a basis for object detection.

The initial LINEMOD method had limitations, including online template learning and approximate pose estimation.



Improvements and Contributions:


Hinterstoisser et al. enhance LINEMOD by incorporating accurate 3D models of objects.
Their approach leverages the 3D model to address the shortcomings of the original LINEMOD.
Notable improvements include better pose estimation and reduced false positives.

The proposed framework is suitable for robotics applications, such as object manipulation.



Dataset Details:


The LM dataset consists of 15 registered video sequences, each containing over 1100 frames.
These sequences feature 15 different texture-less household objects.
Objects in the dataset exhibit discriminative color, shape, and size characteristics.
Researchers can use this dataset to evaluate and compare their methods for object detection and pose estimation.



In summary, the LM dataset provides a valuable benchmark for advancing the field of 6D object pose estimation, especially in scenarios where texture information is limited¹². Researchers can access this dataset to test and refine their algorithms, ultimately contributing to advancements in robotics and machine vision.

(1) Model Based Training, Detection and Pose ... - Stefan HINTERSTOISSER. http://stefan-hinterstoisser.com/papers/hinterstoisser2012accv.pdf.
(2) Datasets - BOP: Benchmark for 6D Object Pose Estimation. https://bop.felk.cvut.cz/datasets/.
(3) paroj/linemod_dataset: Hinterstoisser et al. ACCV12 dataset - GitHub. https://github.com/paroj/linemod_dataset.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"3D, Image",,,,,,,,"6D Pose Estimation, 6D Pose Estimation using RGB, 6D Pose Estimation using RGBD, Domain Adaptation","6d-pose-estimation-on-linemod-2, 6d-pose-estimation-using-rgbd-on-linemod, 6d-pose-estimation-using-rgb-on-occlusion, domain-adaptation-on-synth-objects-to-linemod, 6d-pose-estimation-on-linemod",,See all 1951 tasks,6D Pose Estimation8 benchmarks,6D Pose Estimation8 benchmarks
NERDS_360,NERDS 360 Dataset,"We present a large-scale dataset for 3D urban scene understanding. Compared to existing datasets, our dataset consists of 75 outdoor urban scenes with diverse backgrounds, encompassing over 15,000 images. These scenes offer 360◦ hemispherical views, capturing diverse foreground objects illuminated under various lighting conditions. Additionally, our dataset encompasses scenes that are not limited to forward-driving views, addressing the limitations of previous datasets such as limited overlap and coverage between camera
views. The closest pre-existing dataset for generalizable evaluation is DTU [2] (80 scenes) which comprises mostly indoor objects and does not provide multiple foreground objects or background scenes.

We use the Parallel Domain synthetic data generation to render high-fidelity 360◦ scenes. We select 3 different maps i.e. SF 6thAndMission, SF GrantAndCalifornia and SF VanNessAveAndTurkSt and sample 75 different scenes across all 3 maps as our backgrounds (All 75 scenes across 3 maps are significantly different road scenes from each other, captured at different viewpoints in the city). We select 20 different cars in 50 different textures for training and randomly sample from 1 to 4 cars to render in a scene. We refer to this dataset as NeRDS 360: NeRF for Reconstruction, Decomposition and Scene Synthesis of 360◦ outdoor scenes. In total, we generate 15k renderings by sampling 200 cameras in a 
 hemispherical dome at a fixed distance from the center of cars. We held out 5 scenes with 4 different cars and different backgrounds for testing, comprising 100 cameras distributed uniformly sampled in the upper hemisphere, different from the camera distributions used for training. We use the diverse validation camera distribution to test our approach’s ability to generalize to unseen viewpoints as well as unseen scenes during training. Our dataset and the corresponding task is extremely challenging due to occlusions, diversity of backgrounds, and rendered objects with various lightning and shadows. 

Our task entails reconstructing 360◦ hemispherical views of complete scenes using a handful of observations i.e. 1 to 5 as shown by red cameras whereas evaluating using all 100 hemispherical views, hence our task requires strong priors for novel view synthesis of outdoor scenes. 

Tasks our dataset support:


Generaliazable Novel view synthesis (Few shot evaluation)
Novel view synthesis (Overfitting evaluation)
6D pose estimation
Object editing 
Depth estimation
Semantic Segmentation
Instance Segmentation",https://production-media.paperswithcode.com/datasets/330dbc66-3c5f-4361-8612-220aa089bc17.jpg,EditUnknown,"3D, Image",,,,,000 images,,,"Generalizable Novel View Synthesis, Instance Segmentation, Depth Estimation, Novel View Synthesis, 6D Pose Estimation, Semantic Segmentation",generalizable-novel-view-synthesis-on-nerds,,See all 1951 tasks,6D Pose Estimation8 benchmarks,6D Pose Estimation8 benchmarks
OPT,OPT Dataset,"Accurately tracking the six degree-of-freedom pose of an object in real scenes is an important task in computer vision and augmented reality with numerous applications. Although a variety of algorithms for this task have been proposed, it remains difficult to evaluate existing methods in the literature as oftentimes different sequences are used and no large benchmark datasets close to real-world scenarios are available. In this paper, we present a large object pose tracking benchmark dataset consisting of RGB-D video sequences of 2D and 3D targets with ground-truth information. The videos are recorded under various lighting conditions, different motion patterns and speeds with the help of a programmable robotic arm. We present extensive quantitative evaluation results of the state-of-the-art methods on this benchmark dataset and discuss the potential research directions in this field.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"3D, Image",,,,,,,,6D Pose Estimation,6d-pose-estimation-on-opt,,See all 1951 tasks,6D Pose Estimation8 benchmarks,6D Pose Estimation8 benchmarks
REAL275,REAL275 Dataset,"REAL275 is a benchmark for category-level pose estimation. It contains 4300 training frames, 950 validation and 2750 for testing across 18 different real scenes.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"3D, Image",,,,,,,,"6D Pose Estimation, 6D Pose Estimation using RGBD",6d-pose-estimation-using-rgbd-on-real275,,See all 1951 tasks,6D Pose Estimation8 benchmarks,6D Pose Estimation8 benchmarks
UW_Indoor_Scenes__UW-IS__Occluded_dataset,UW Indoor Scenes (UW-IS) Occluded dataset Dataset,"UW Indoor Scenes (UW-IS) Occluded dataset is curated using commodity hardware (Intel RealSense D435) to reflect real world robotics scenarios. It consists of two completely different indoor environments. The first environment is a lounge where the objects are placed on a tabletop. The second environment is a mock warehouse setup where the objects are placed on a shelf. For each of these environments, we have RGB-D images from 36 videos comprising five to seven objects each, taken from distances up to approximately 2m. The videos cover two different lighting conditions, three different levels of object separation for three different object categories (i.e., kitchen objects, food items, and tools/miscellaneous). The first level of object separation is such that there is no object occlusion. The second level of object separation is such that some occlusion occurs, while the third level is where the objects are placed extremely close together. Overall, the dataset considers 20 object classes and consists of 8,456 images, which have a total of 42,902 object instances. We also provide instance segmentation masks and 6D pose annotations for all the images generated using LabelFusion (Marion et al., 2018)",https://production-media.paperswithcode.com/datasets/759d8cc5-71d9-42a8-bf0b-3f544316dfc2.png,EditCC BY 4.0,"3D, Image",,2018,,,456 images,,,"Object Recognition, Instance Segmentation, 6D Pose Estimation using RGBD, 6D Pose Estimation, 6D Pose Estimation using RGB",,,See all 1951 tasks,6D Pose Estimation8 benchmarks,6D Pose Estimation8 benchmarks
YCB-Video,YCB-Video Dataset,"The YCB-Video dataset is a large-scale video dataset for 6D object pose estimation. provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames.",https://rse-lab.cs.washington.edu/projects/posecnn/,EditUnknown,"3D, Image",,,,,,,,"Pose Estimation, 6D Pose Estimation using RGBD, 6D Pose Estimation, Symmetry Detection, 6D Pose Estimation using RGB, Occluded 3D Object Symmetry Detection","6d-pose-estimation-on-ycb-video, occluded-3d-object-symmetry-detection-on-ycb, 6d-pose-estimation-on-ycb-video-2, symmetry-detection-on-ycb-video, 6d-pose-estimation-using-rgbd-on-ycb-video",,See all 1951 tasks,6D Pose Estimation8 benchmarks,6D Pose Estimation8 benchmarks
YCBInEOAT_Dataset,YCBInEOAT Dataset Dataset,A new dataset with significant occlusions related to object manipulation.,/paper/se-3-tracknet-data-driven-6d-pose-tracking-by,EditUnknown,"3D, Image, Video",,,,,,,,"6D Pose Estimation, Pose Estimation, Pose Tracking",,,See all 1951 tasks,6D Pose Estimation8 benchmarks,6D Pose Estimation8 benchmarks
Kinetics_400,Kinetics 400 Dataset,"The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands.",https://arxiv.org/abs/1705.06950,EditCreative Commons Attribution 4.0 International License,"Image, Video",,,,,,,,"Boundary Detection, Skeleton Based Action Recognition, Self-Supervised Action Recognition Linear, Action Classification, Event Segmentation, Action Recognition In Videos, Self-Supervised Action Recognition","self-supervised-action-recognition-on-1, action-classification-on-kinetics-400, self-supervised-action-recognition-linear-on-3, boundary-detection-on-kinetics-400, action-recognition-in-videos-on-kinetics-400-1, skeleton-based-action-recognition-on-kinetics-2, event-segmentation-on-kinetics-400",,See all 1951 tasks,Action Classification27 benchm,Action Classification27 benchm
Something-Something_V2,Something-Something V2 Dataset,"The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels.

Source

Image Source",https://production-media.paperswithcode.com/datasets/Something-Something_V2-0000001166-9ef83fff_cu8iX0I.jpg,EditCustom,"Image, Text, Time Series, Video",English,,,,,,174,"General Action Video Anomaly Detection, Video Prediction, Video Classification, Early Action Prediction, Text-to-Video Generation, Action Classification, Action Recognition In Videos, Action Recognition","early-action-prediction-on-something-1, action-recognition-in-videos-on-something-3, general-action-video-anomaly-detection-on, action-classification-on-something-something-2, video-prediction-on-something-something-v2, action-recognition-in-videos-on-something, text-to-video-generation-on-something, video-classification-on-something-something-1",,See all 1951 tasks,Action Classification27 benchm,Action Classification27 benchm
YouCook2,YouCook2 Dataset,"YouCook2 is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.",http://youcook2.eecs.umich.edu/,EditCustom,"Audio, Image, Text, Video",English,2000,,,,,,"Zero-Shot Video Retrieval, Zero-Shot Video-Audio Retrieval, Action Classification, Dense Video Captioning, Zero-shot dense video captioning, Video Captioning, Video Retrieval, Long Video Retrieval (Background Removed)","dense-video-captioning-on-youcook2, video-retrieval-on-youcook2, action-classification-on-youcook2, zero-shot-dense-video-captioning-on-youcook2, long-video-retrieval-background-removed-on, zero-shot-video-audio-retrieval-on-youcook2, zero-shot-video-retrieval-on-youcook2, video-captioning-on-youcook2",,See all 1951 tasks,Action Classification27 benchm,Action Classification27 benchm
MultiTHUMOS,MultiTHUMOS Dataset,"The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.",http://ai.stanford.edu/~syyeung/everymoment.html,EditCC BY 4.0,"Image, Time Series, Video",,,,,,,5,"Temporal Action Localization, Action Recognition, Action Detection","action-detection-on-multi-thumos, action-detection-on-multithumos-1, temporal-action-localization-on-multithumos-1",,See all 1951 tasks,Action Detection62 benchmarks2,Action Detection62 benchmarks2
Okutama-Action,Okutama-Action Dataset,"A new video dataset for aerial view concurrent human action detection. It consists of 43 minute-long fully-annotated sequences with 12 action classes. Okutama-Action features many challenges missing in current datasets, including dynamic transition of actions, significant changes in scale and aspect ratio, abrupt camera movement, as well as multi-labeled actors.",/paper/okutama-action-an-aerial-view-video-dataset,EditUnknown,"Image, Video",,,,,,,,"Action Recognition, Object Tracking, Action Detection",action-recognition-on-okutama-action,,See all 1951 tasks,Action Detection62 benchmarks2,Action Detection62 benchmarks2
Toyota_Smarthome_Dataset,Toyota Smarthome Dataset Dataset,A large scale dataset with daily-living activities performed in a natural manner.,/paper/toyota-smarthome-untrimmed-real-world,EditUnknown,"Image, Video",,,,,,,,"Action Classification, Action Detection, Activity Detection","action-classification-on-toyota-smarthome, action-detection-on-tsu",,See all 1951 tasks,Action Detection62 benchmarks2,Action Detection62 benchmarks2
TVSeries,TVSeries Dataset,"A realistic dataset composed of 27 episodes from 6 popular TV series. The dataset spans over 16 hours of footage annotated with 30 action classes, totaling 6,231 action instances.",/paper/online-action-detection,EditUnknown,"Image, Video",,,,,,,,"Action Anticipation, Action Detection, Online Action Detection",online-action-detection-on-tvseries,,See all 1951 tasks,Action Detection62 benchmarks2,Action Detection62 benchmarks2
CVB,CVB Dataset,"Existing image/video datasets for cattle behavior recognition are mostly small, lack well-defined labels, or are collected in unrealistic controlled environments. This limits the utility of machine learning (ML) models learned from them. Therefore, we introduce a new dataset, called Cattle Visual Behaviors (CVB), that consists of 502 video clips, each fifteen seconds long, captured in natural lighting conditions, and annotated with eleven visually perceptible behaviors of grazing cattle. By creating and sharing CVB, our aim is to develop improved models capable of recognizing all important cattle behaviors accurately and to assist other researchers and practitioners in developing and evaluating new ML models for cattle behavior classification using video data.
The dataset is presented in the form of following three sub-directories.
1. raw_frames: contains 450 frames in each sub folder representing a 15 second video taken at a frame rate of 30 FPS.
2. annotations: contains the json files corresponding to the raw_frames folder. There is one json file for each video, that contains the bounding-box annotations for each cattle in the video and its associated behavior, and
3. CVB_in_AVA_format: contains the CVB data in the AVA dataset format.",https://production-media.paperswithcode.com/datasets/47fa3f55-7ea0-455e-b495-352164320687.png,EditCreative Commons Attribution Noncommercial-Share Alike 4.0,"3D, Image, Video",,,,,,,,"3D Multi-Object Tracking, Animal Action Recognition, 3D Classification, Multi-Animal Tracking with identification, Action Localization, 2D Object Detection, Action Recognition In Videos, Action Recognition",,,See all 1951 tasks,Action Localization112 benchma,Action Localization112 benchma
GraSP,GraSP Dataset,"Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity requirements of each task, and establish TAPIS's superiority over previously proposed baselines and conventional CNN-based models. Additionally, we validate the robustness of our method across multiple public benchmarks, confirming the reliability and applicability of our dataset. This work represents a significant step forward in Endoscopic Vision, offering a novel and comprehensive framework for future research towards a holistic understanding of surgical procedures.",https://production-media.paperswithcode.com/datasets/11f80193-0a13-4aac-9b47-d3f8510cac5c.png,EditUnknown,"Image, Video",,,,,,,,"Action Localization, Instance Segmentation, Surgical phase recognition",surgical-phase-recognition-on-grasp,,See all 1951 tasks,Action Localization112 benchma,Action Localization112 benchma
HACS,HACS Dataset,"HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.

Authors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled
from 492K, 6K and 6K videos, respectively.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-02_at_10.58.11_AM.png,EditBSD 3-Clause,"Image, Time Series, Video",,,,,,,,"Action Localization, Action Recognition, Temporal Action Localization","action-recognition-on-hacs, temporal-action-localization-on-hacs",,See all 1951 tasks,Action Localization112 benchma,Action Localization112 benchma
HowTo100M,HowTo100M Dataset,"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:


136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)
23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness

Each video is associated with a narration available as subtitles automatically downloaded from Youtube.",https://www.di.ens.fr/willow/research/howto100m/,EditCustom,"Image, Text, Video",English,,,,,,,"Action Recognition, Video Captioning, Video Retrieval, Video Question Answering",video-question-answering-on-howto100m-qa,,See all 1951 tasks,Action Recognition171 benchmar,Action Recognition171 benchmar
NTU_RGB_D,NTU RGB+D Dataset,"NTU RGB+D is a large-scale dataset for RGB-D human action recognition. It involves 56,880 samples of 60 action classes collected from 40 subjects. The actions can be generally divided into three categories: 40 daily actions (e.g., drinking, eating, reading), nine health-related actions (e.g., sneezing, staggering, falling down), and 11 mutual actions (e.g., punching, kicking, hugging). These actions take place under 17 different scene conditions corresponding to 17 video sequences (i.e., S001–S017). The actions were captured using three cameras with different horizontal imaging viewpoints, namely, −45∘,0∘, and +45∘. Multi-modality information is provided for action characterization, including depth maps, 3D skeleton joint position, RGB frames, and infrared sequences. The performance evaluation is performed by a cross-subject test that split the 40 subjects into training and test groups, and by a cross-view test that employed one camera (+45∘) for testing, and the other two cameras for training.",https://arxiv.org/abs/1806.11269,"EditCustom (research-only, non-commercial, attribution)","3D, Image, Text, Time Series, Video",English,,,,880 samples,,,"Zero Shot Skeletal Action Recognition, Skeleton Based Action Recognition, Generalized Zero Shot skeletal action recognition, Human Interaction Recognition, Early Action Prediction, 3D Action Recognition, Self-supervised Skeleton-based Action Recognition, Action Recognition In Videos, Action Recognition, Unsupervised Skeleton Based Action Recognition, Human action generation, Pose Prediction","3d-action-recognition-on-ntu-rgb-d-1, action-recognition-in-videos-on-ntu-rgb-d, early-action-prediction-on-ntu-rgb-d, generalized-zero-shot-skeletal-action, self-supervised-skeleton-based-action, pose-prediction-on-filtered-ntu-rgbd, skeleton-based-action-recognition-on-ntu-rgbd, zero-shot-skeletal-action-recognition-on-ntu, human-action-generation-on-ntu-rgb-d, unsupervised-skeleton-based-action, human-interaction-recognition-on-ntu-rgb-d, action-recognition-in-videos-on-ntu-rgbd",,See all 1951 tasks,Action Recognition171 benchmar,Action Recognition171 benchmar
Sports-1M,Sports-1M Dataset,"The Sports-1M dataset consists of over a million videos from YouTube. The videos in the dataset can be obtained through the YouTube URL specified by the authors. Approximately 7% (as of 2016) of the videos have been removed by the YouTube uploaders since the dataset was compiled. However, there are still over a million videos in the dataset with 487 sports-related categories with 1,000 to 3,000 videos per category. The videos are automatically labelled with 487 sports classes using the YouTube Topics API by analyzing the text metadata associated with the videos (e.g. tags, descriptions). Approximately 5% of the videos are annotated with more than one class.",https://arxiv.org/abs/1610.06906,EditCC BY 3.0,"Image, Video",,2016,,,,,,"Action Recognition, Action Recognition In Videos","action-recognition-in-videos-on-sports-1m, action-recognition-in-videos-on-sports-1m-1",,See all 1951 tasks,Action Recognition In Videos25,Action Recognition In Videos25
50_Salads,50 Salads Dataset,"Activity recognition research has shifted focus from distinguishing full-body motion patterns to recognizing complex interactions of multiple entities. Manipulative gestures – characterized by interactions between hands, tools, and manipulable objects – frequently occur in food preparation, manufacturing, and assembly tasks, and have a variety of applications including situational support, automated supervision, and skill assessment. With the aim to stimulate research on recognizing manipulative gestures we introduce the 50 Salads dataset. It captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation.

The dataset includes

RGB video data 640×480 pixels at 30 Hz
Depth maps 640×480 pixels at 30 Hz
3-axis accelerometer data at 50 Hz of devices attached to a knife, a mixing spoon, a small spoon, a peeler, a glass, an oil bottle, and a pepper dispenser.
Synchronization parameters for temporal alignment of video and accelerometer data
Annotations as temporal intervals of pre- core- and post-phases of activities corresponding to steps in a recipe",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,"Image, Video",,,,,,,,"Unsupervised Action Segmentation, Action Segmentation","action-segmentation-on-50-salads-1, unsupervised-action-segmentation-on-50-salads",,See all 1951 tasks,Action Segmentation15 benchmar,Action Segmentation15 benchmar
Breakfast,Breakfast Dataset,"The Breakfast Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded “in the wild” as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.",https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/,EditCC BY 4.0,"Image, Video",,,,,,,,"Weakly Supervised Action Segmentation (Transcript), Action Segmentation, Video Classification, Unsupervised Action Segmentation, Long-video Activity Recognition, Weakly Supervised Action Segmentation (Action Set))","weakly-supervised-action-segmentation, video-classification-on-breakfast, weakly-supervised-action-segmentation-action, long-video-activity-recognition-on-breakfast, unsupervised-action-segmentation-on-breakfast, action-segmentation-on-breakfast-1",,See all 1951 tasks,Action Segmentation15 benchmar,Action Segmentation15 benchmar
EgoProceL,EgoProceL Dataset,"EgoProceL is a large-scale dataset for procedure learning. It consists of 62 hours of egocentric videos recorded by 130 subjects performing 16 tasks for procedure learning. EgoProceL contains videos and key-step annotations for multiple tasks from CMU-MMAC, EGTEA Gaze+, and individual tasks like toy-bike assembly, tent assembly, PC assembly, and PC disassembly. EgoProceL overcomes the limitations of third-person videos. As, using third-person videos makes the manipulated object small in appearance and often occluded by the actor, leading to significant errors. In contrast, we observe that videos obtained from first-person (egocentric) wearable cameras provide an unobstructed and clear view of the action.",https://production-media.paperswithcode.com/datasets/d66430d5-5d40-4232-bf56-fb6dcd0fca13.png,EditUnknown,"Image, Video",,,,,,,,"Weakly Supervised Action Segmentation (Transcript), Action Detection, Event Segmentation, Video Segmentation, Action Segmentation",,,See all 1951 tasks,Action Segmentation15 benchmar,Action Segmentation15 benchmar
GTEA,GTEA Dataset,"The Georgia Tech Egocentric Activities (GTEA) dataset contains seven types of daily activities such as making sandwich, tea, or coffee. Each activity is performed by four different people, thus totally 28 videos. For each video, there are about 20 fine-grained action instances such as take bread, pour ketchup, in approximately one minute.",https://arxiv.org/abs/1705.07818,EditUnknown,"Image, Video",,,,,,,,"Fine-Grained Action Detection, Weakly Supervised Action Localization, Action Segmentation","weakly-supervised-action-localization-on-gtea, action-segmentation-on-gtea-1",,See all 1951 tasks,Action Segmentation15 benchmar,Action Segmentation15 benchmar
HOI4D,HOI4D Dataset,"A large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M RGB-D egOCentric video frames over 4000 sequences collected by 4 participants interacting with 800 different object instances from 16 categories over 610 different indoor rooms.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY-NC 4.0,"3D, Image, Video",,,,,,,16,"Panoptic Segmentation, Motion Segmentation, Pose Tracking, Semantic Segmentation, Human-Object Interaction Detection, Action Segmentation",,,See all 1951 tasks,Action Segmentation15 benchmar,Action Segmentation15 benchmar
JIGSAWS,JIGSAWS Dataset,"The JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) is a surgical activity dataset for human motion modeling. The data was collected through a collaboration between The Johns Hopkins University (JHU) and Intuitive Surgical, Inc. (Sunnyvale, CA. ISI) within an IRB-approved study. The release of this dataset has been approved by the Johns Hopkins University IRB.   The dataset was captured using the da Vinci Surgical System from eight surgeons with different levels of skill performing five repetitions of three elementary surgical tasks on a bench-top model: suturing, knot-tying and needle-passing, which are standard components of most surgical skills training curricula. The JIGSAWS dataset consists of three components:


kinematic data: Cartesian positions, orientations, velocities, angular velocities and gripper angle describing the motion of the manipulators.
video data: stereo video captured from the endoscopic camera. Sample videos of the JIGSAWS tasks can be downloaded from the official webpage.
manual annotations including:
gesture (atomic surgical activity segment labels).
skill (global rating score using modified objective structured assessments of technical skills).
experimental setup: a standardized cross-validation experimental setup that can be used to evaluate automatic surgical gesture recognition and skill assessment methods.",https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release,EditCustom,"Image, Video",,,,,,,,"Surgical Skills Evaluation, Action Quality Assessment, Action Segmentation","surgical-skills-evaluation-on-jigsaws, action-quality-assessment-on-jigsaws, action-segmentation-on-jigsaws",,See all 1951 tasks,Action Segmentation15 benchmar,Action Segmentation15 benchmar
TUM_Kitchen,TUM Kitchen Dataset,"The TUM Kitchen dataset is an action recognition dataset that contains 20 video sequences captured by 4 cameras with overlapping views. The camera network captures the scene from four viewpoints with 25 fps, and every RGB frame is of the resolution 384×288 by pixels. The action labels are frame-wise, and provided for the left arm, the right arm and the torso separately.",https://arxiv.org/abs/1803.05790,EditUnknown,"Image, Time Series, Video",,,,,,,,"Temporal Action Localization, Action Recognition, Action Segmentation",,,See all 1951 tasks,Action Segmentation15 benchmar,Action Segmentation15 benchmar
Watch-n-Patch,Watch-n-Patch Dataset,"The Watch-n-Patch dataset was created with the focus on modeling human activities, comprising multiple actions in a completely unsupervised setting. It is collected with Microsoft Kinect One sensor for a total length of about 230 minutes, divided in 458 videos. 7 subjects perform human daily activities in 8 offices and 5 kitchens with complex backgrounds. Moreover, skeleton data are provided as ground truth annotations.",https://arxiv.org/abs/1707.06786,EditCustom,"Image, Video",,,,,,,,"Action Recognition, Human-Object Interaction Detection, Action Segmentation",,,See all 1951 tasks,Action Segmentation15 benchmar,Action Segmentation15 benchmar
DeepWeeds,DeepWeeds Dataset,"The DeepWeeds dataset consists of 17,509 images capturing eight different weed species native to Australia in situ with neighbouring flora.",https://github.com/AlexOlsen/DeepWeeds,EditUnknown,Image,,,,,509 images,,,"Active Learning, Semantic Segmentation, Robust classification",,,See all 1951 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
DialoGLUE,DialoGLUE Dataset,"DialoGLUE is a natural language understanding benchmark for task-oriented dialogue designed to encourage dialogue research in representation-based transfer, domain adaptation, and sample-efficient task learning. It consisting of 7 task-oriented dialogue datasets covering 4 distinct natural language understanding tasks.",/paper/dialoglue-a-natural-language-understanding,EditUnknown,Text,English,,,,,,,"Active Learning, Dialogue Management, Natural Language Understanding","natural-language-understanding-on-dialoglue-1, natural-language-understanding-on-dialoglue",,See all 1951 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
FDST,FDST Dataset,The Fudan-ShanghaiTech dataset (FDST) is a dataset for video crowd counting. It contains 15K frames with about 394K annotated heads captured from 13 different scenes,https://arxiv.org/abs/1907.07911,EditUnknown,,,,,,,,,"Active Learning, Binarization, Crowd Counting",,,See all 1951 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
Groove,Groove Dataset,"The Groove MIDI Dataset (GMD) is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming. The dataset contains 1,150 MIDI files and over 22,000 measures of drumming.",https://www.tensorflow.org/datasets/catalog/groove,EditUnknown,"Audio, Image, Text, Video",English,,,,,,,"Music Generation, Beat Tracking, Quantization, Active Learning, Downbeat Tracking","beat-tracking-on-groove, downbeat-tracking-on-groove",,See all 1951 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
HJDataset,HJDataset Dataset,"HJDataset is a large dataset of Historical Japanese Documents with Complex Layouts. It contains over 250,000 layout element annotations of seven types. In addition to bounding boxes and masks of the content regions, it also includes the hierarchical structures and reading orders for layout elements. The dataset is constructed using a combination of human and machine efforts.",/paper/a-large-dataset-of-historical-japanese,EditUnknown,"Image, Text",English,,,,,,,"Object Detection, Document Layout Analysis, Active Learning",,,See all 1951 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
Illness-dataset,Illness-dataset Dataset,"A dataset for evaluating text classification, domain adaptation, and active learning models. The dataset consists of 22,660 documents (tweets) collected in 2018 and 2019. It spans across four domains: Alzheimer's, Parkinson's, Cancer, and Diabetes.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,"Image, Text",English,2018,,,660 documents,,,"Active Learning, Text Classification, Domain Adaptation",,,See all 1951 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
Industrial_Benchmark,Industrial Benchmark Dataset,"A benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github.",/paper/a-benchmark-environment-motivated-by,EditUnknown,,,,,,,,,"Active Learning, Decision Making, OpenAI Gym",,,See all 1951 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
MNIST-8M,MNIST-8M Dataset,MNIST8M is derived from the MNIST dataset by applying random deformations and translations to the dataset.,https://arxiv.org/abs/1602.08194,EditUnknown,,,,,,,,,"Stochastic Optimization, Distributed Computing, Active Learning",,,See all 1951 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
SYNTHIA-AL,SYNTHIA-AL Dataset,Specially designed to evaluate active learning for video object detection in road scenes.,/paper/temporal-coherence-for-active-learning-in,EditUnknown,"Image, Video",,,,,,,,"Video Object Detection, Object Detection, Active Learning",,,See all 1951 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
First-Person_Hand_Action_Benchmark,First-Person Hand Action Benchmark Dataset,"First-Person Hand Action Benchmark is a collection of RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations.",https://arxiv.org/pdf/1704.02463v2.pdf,"EditCustom (research-only, non-commercial)","3D, Image, Video",,,,,,,,"Skeleton Based Action Recognition, Pose Estimation, Activity Recognition, Hand Pose Estimation, 3D Hand Pose Estimation","skeleton-based-action-recognition-on-first, activity-recognition-on-first-person-hand",,See all 1951 tasks,Activity Recognition207 benchm,Activity Recognition207 benchm
GazeFollow,GazeFollow Dataset,"GazeFollow is a large-scale dataset annotated with the location of where people in images are looking. It uses several major datasets that contain people as a source of images: 1, 548 images from SUN, 33, 790 images from MS COCO, 9, 135 images from Actions 40, 7, 791 images from PASCAL, 508 images from the ImageNet detection challenge and 198, 097 images from the Places dataset. This concatenation results in a challenging and large image collection of people performing diverse activities in many everyday scenarios.",http://gazefollow.csail.mit.edu/index.html,EditCustom (research-only),"3D, Image, Video",,,,,548 images,,,"Activity Recognition, Gaze Target Estimation, Pose Estimation, Decision Making",gaze-target-estimation-on-gazefollow,,See all 1951 tasks,Activity Recognition207 benchm,Activity Recognition207 benchm
Home_Action_Genome,Home Action Genome Dataset,"Home Action Genome is a large-scale multi-view video database of indoor daily activities. Every activity is captured by synchronized multi-view cameras, including an egocentric view.
There are 30 hours of vides with 70 classes of daily activities and 453 classes of atomic actions.",https://production-media.paperswithcode.com/datasets/homage.jpg,"EditCustom (research, non-commercial)","Graph, Image, Text, Time Series, Video",English,,,,,,70,"Action Parsing, Activity Prediction, Activity Recognition, Video Classification, Multimodal Activity Recognition, Activity Detection, Multiview Detection, Action Recognition, Scene Graph Detection, Image Generation from Scene Graphs","video-classification-on-home-action-genome, image-generation-from-scene-graphs-on-home",,See all 1951 tasks,Activity Recognition207 benchm,Activity Recognition207 benchm
MEVA,MEVA Dataset,"Large-scale dataset for human activity recognition. Existing security datasets either focus on activity counts by aggregating public video disseminated due to its content, which typically excludes same-scene background video, or they achieve persistence by observing public areas and thus cannot control for activity content. The dataset is over 9300 hours of untrimmed, continuous video, scripted to include diverse, simultaneous activities, along with spontaneous background activity.",/paper/meva-a-large-scale-multiview-multimodal-video,EditUnknown,"Image, Video",,,,,,,,"Activity Recognition, Action Detection, Activity Detection",,,See all 1951 tasks,Activity Recognition207 benchm,Activity Recognition207 benchm
MoVi,MoVi Dataset,"Contains 60 female and 30 male actors performing a collection of 20 predefined everyday actions and sports movements, and one self-chosen movement.",/paper/movi-a-large-multipurpose-motion-and-video,EditFor non-commercial and scientific research purposes,"3D, Image, Video",,,,,,,,"3D Human Shape Estimation, Pose Estimation, Human Mesh Recovery, Activity Recognition, 3D Human Reconstruction, 3D Human Pose Estimation, Virtual Try-on",3d-human-shape-estimation-on-movi,,See all 1951 tasks,Activity Recognition207 benchm,Activity Recognition207 benchm
OPERAnet,OPERAnet Dataset,"OPERAnet is a multimodal activity recognition dataset acquired from radio frequency and vision-based sensors. Approximately 8 hours of annotated measurements are provided, which are collected across two different rooms from 6 participants performing 6 activities, namely, sitting down on a chair, standing from sit, lying down on the ground, standing from the floor, walking and body rotating. The dataset has been acquired from four synchronized modalities for the purpose of passive Human Activity Recognition (HAR) as well as localization and crowd counting.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,,,,Activity Recognition,,,See all 1951 tasks,Activity Recognition207 benchm,Activity Recognition207 benchm
TACoS_Multi-Level_Corpus,TACoS Multi-Level Corpus Dataset,Augments the video-description dataset TACoS with short and single sentence descriptions.,/paper/coherent-multi-sentence-video-description,EditUnknown,"Image, Text, Video",English,,,,,,,"Activity Recognition, Natural Language Moment Retrieval, Video Description",natural-language-moment-retrieval-on-tacos,,See all 1951 tasks,Activity Recognition207 benchm,Activity Recognition207 benchm
AdvSuffixes,AdvSuffixes Dataset,"AdvSuffixes - Information
AdvSuffixes is a curated dataset of adversarial prompts and suffixes designed to evaluate and enhance the robustness of large language models (LLMs) against adversarial attacks. By appending these suffixes to standard prompts, researchers and developers can explore and analyze how LLMs respond to potentially harmful input scenarios. This dataset is heavily inspired by AdvBench.

Dataset Structure
The dataset is organized as follows:
data/
│
├── advsuffixes/
│   ├── advsuffixes.csv      # Adversarial suffixes and their respective prompts
│   ├── advsuffixes_eval.txt # 100 additional evaluation prompts that are out-of-distribution
│
├── ...


advsuffixes.csv: The primary dataset containing pairs of adversarial suffixes and corresponding prompts.  
advsuffixes_eval.txt: A set of 100 additional evaluation prompts designed to test model robustness, that are out-of-distribution from the original 519 prompts in advsuffixes.csv. 

Details about the dataset generation have been provided in Appendix B in the supplementary materials of the paper. There are 11763 listed suffixes overall, averaging 22.6 suffixes per prompt.


License
This dataset is distributed under the GNU General Public License v3.0.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditGPL-3.0 License,,,,,,,,,"Adversarial Attack, Adversarial Robustness",,,See all 1951 tasks,Adversarial Attack3 benchmarks,Adversarial Attack3 benchmarks
Cifar10Mnist,Cifar10Mnist Dataset,"The Cifar10Mnist dataset is created using CIFAR-10 and MNIST data sources. 
Since the CIFAR-10 training set consists of 50000 images and the MNIST training set contains 60000 digits, the first 50000 digits from MNIST are padded on top of the CIFAR-10 images after making them slightly translucent. A first training dataset is then obtained (50000 images). Furthermore, the remaining 10000 MNIST digits are padded on top of 10000 random CIFAR10 images (with a fixed seed). This gives the possibility of having a second training dataset of  60000 images. 
For the test set, the 10000 CIFAR-10 images are padded over the 10000 MNIST digits.",https://production-media.paperswithcode.com/datasets/30df180b-b8f3-4296-a667-7449d9bc50d7.png,EditMIT License,,,,,,50000 images,training set consists of 50000 images,,"Adversarial Attack, Multi-Task Learning",,,See all 1951 tasks,Adversarial Attack3 benchmarks,Adversarial Attack3 benchmarks
comma_2k19,comma 2k19 Dataset,"comma 2k19 is a dataset of over 33 hours of commute in California's 280 highway. This means 2019 segments, 1 minute long each, on a 20km section of highway driving between California's San Jose and San Francisco. The dataset was collected using comma EONs that have sensors similar to those of any modern smartphone including a road-facing camera, phone GPS, thermometers and a 9-axis IMU.",/paper/a-commute-in-data-the-comma2k19-dataset,EditUnknown,Image,,2019,,,,,,"Adversarial Attack, Autonomous Vehicles, Lane Detection",,,See all 1951 tasks,Adversarial Attack3 benchmarks,Adversarial Attack3 benchmarks
ImageNet-P,ImageNet-P Dataset,"ImageNet-P consists of noise, blur, weather, and digital distortions. The dataset has validation perturbations; has difficulty levels; has CIFAR-10, Tiny ImageNet, ImageNet 64 × 64, standard, and Inception-sized editions; and has been designed for benchmarking not training networks. ImageNet-P departs from ImageNet-C by having perturbation sequences generated from each ImageNet validation image. Each sequence contains more than 30 frames, so to counteract an increase in dataset size and evaluation time only 10 common perturbations are used.",https://arxiv.org/pdf/1903.12261.pdf,EditUnknown,Image,English,,,,,,,"Adversarial Attack, Image Classification, Domain Generalization",image-classification-on-imagenet-p,,See all 1951 tasks,Adversarial Attack3 benchmarks,Adversarial Attack3 benchmarks
NAS-Bench-1Shot1,NAS-Bench-1Shot1 Dataset,NAS-Bench-1Shot1 draws on the recent large-scale tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS methods.,https://arxiv.org/pdf/2001.10422v2.pdf,EditUnknown,,,,,,,,,"Adversarial Attack, AutoML, Neural Architecture Search",,,See all 1951 tasks,Adversarial Attack3 benchmarks,Adversarial Attack3 benchmarks
PointDenoisingBenchmark,PointDenoisingBenchmark Dataset,"The PointDenoisingBenchmark dataset features 28 different shapes, split into 18 training shapes and 10 test shapes.


PointDenoisingBenchmark for outliers removal: contains noisy point clouds with different levels of gaussian noise and the corresponding clean ground truths.
PointDenoisingBenchmark for denoising: contains noisy point clouds with different levels of noise and density of outliers and the corresponding clean ground truths.",/paper/pointcleannet-learning-to-denoise-and-remove,EditUnknown,Text,English,,,,,,,"Adversarial Attack, Abusive Language, Denoising",,,See all 1951 tasks,Adversarial Attack3 benchmarks,Adversarial Attack3 benchmarks
TCAB,TCAB Dataset,"Text Classification Attack Benchmark (TCAB) is a dataset for analyzing, understanding, detecting, and labeling adversarial attacks against text classifiers. TCAB includes 1.5 million attack instances, generated by twelve adversarial attack targeting three classifiers trained on six source datasets for sentiment analysis and abuse detection in English. The process of generating attacks is automated, so that TCAB can easily be extended to incorporate new text attacks and better classifiers as they are developed.",https://arxiv.org/pdf/2210.12233v1.pdf,EditApache-2.0 license,"Image, Text",English,,,,,,,"Adversarial Attack, Text Classification, Sentiment Analysis",,,See all 1951 tasks,Adversarial Attack3 benchmarks,Adversarial Attack3 benchmarks
WSJ0-2mix,WSJ0-2mix Dataset,WSJ0-2mix is a speech recognition corpus of speech mixtures using utterances from the Wall Street Journal (WSJ0) corpus.,/paper/deep-clustering-discriminative-embeddings-for,EditUnknown,Audio,,,,,,,,"Adversarial Attack, Speech Separation, Audio Source Separation","adversarial-attack-on-wsj0-2mix, speech-separation-on-wsj0-2mix-16k, speech-separation-on-wsj0-2mix",,See all 1951 tasks,Adversarial Attack3 benchmarks,Adversarial Attack3 benchmarks
Adversarial_Attack_Detection15_papers_with_code_Da,Adversarial Attack Detection15 papers with code Dataset,,https://paperswithcode.com/dataset/adversarial-attack-detection,,,,,,,,,,,,,See all 1951 tasks,Adversarial Attack Detection15,Adversarial Attack Detection15
HarmfulTasks,HarmfulTasks Dataset,"This dataset consists of 225 malicious tasks, which were integrated into ten distinct jailbreaking prompts. The malicious tasks were divided into five categories, namely, 


Misinformation and Disinformation
Security Threats and Cybercrimes
Unlawful Behaviors and Activities 
Hate Speech and Discrimination 
Substance Abuse and Dangerous Practices.

The jailbreaking prompts were carefully selected to cover a diverse range of scenarios. These scenarios included role-playing, simulations, attention-shifting, and privileged execution, and the placement of the malicious task within the jailbreaking prompts was also varied.

List of malicious tasks only: https://github.com/CrystalEye42/eval-safety/blob/main/malicious_tasks_dataset.yaml

Malicious tasks with jailbreaking prompts: https://github.com/CrystalEye42/eval-safety/blob/main/integrated.yaml",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,Text,English,,,,,,,Adversarial Text,,,See all 1951 tasks,Adversarial Text48 papers with,Adversarial Text48 papers with
Texygen_Platform,Texygen Platform Dataset,"Texygen is a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and facilitate the sharing of fine-tuned open-source implementations among researchers for their work. As a consequence, this would help in improving the reproductivity and reliability of future research work in text generation.",https://github.com/geek-ai/Texygen,EditUnknown,Text,English,,,,,,,"Text Generation, Adversarial Text, Imitation Learning",,,See all 1951 tasks,Adversarial Text48 papers with,Adversarial Text48 papers with
RPCD,RPCD Dataset,"The Reddit Photo Critique Dataset (RPCD) contains tuples of image and photo critiques. RPCD consists of 74K images and 220K comments and is collected from a Reddit community used by hobbyists and professional photographers to improve their photography skills by leveraging constructive community feedback.

The proposed dataset differs from previous aesthetics datasets mainly in three aspects, namely


the large scale of the dataset and the extension of the comments criticizing different aspects of the image;
it contains mostly UltraHD images;
it can easily be extended to new data as it is collected through an automatic pipeline.",https://production-media.paperswithcode.com/datasets/bfab366a-dd90-4234-b74c-04b92b88f198.jpg,EditUnknown,"Image, Text",English,,,,74K images,,,"Aesthetics Quality Assessment, Aesthetic Image Captioning, Image Captioning",,,See all 1951 tasks,Aesthetic Image Captioning2 pa,Aesthetic Image Captioning2 pa
MVTecAD,MVTecAD Dataset,"MVTec AD is a dataset for benchmarking anomaly detection methods with a focus on industrial inspection. It contains over 5000 high-resolution images divided into fifteen different object and texture categories. Each category comprises a set of defect-free training images and a test set of images with various kinds of defects as well as images without defects.

There are two common metrics: Detection AUROC and Segmentation (or pixelwise) AUROC

Detection (or, classification) methods output single float (anomaly score) per input test image. 

Segmentation methods output anomaly probability for each pixel. 
""To assess segmentation performance, we evaluate the relative per-region overlap of the segmentation with the ground truth. To get an additional performance measure that is independent of the determined threshold, we compute the area under the receiver operating characteristic curve (ROC AUC). We define the true positive rate as the percentage of pixels that were correctly classified as anomalous"" [1]
Later segmentation metric was improved to balance regions with small and large area, see PRO-AUC and other in [2]

[1] Paul Bergmann et al, ""MVTec AD — A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection""
[2] Bergmann, P., Batzner, K., Fauser, M. et al. The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. Int J Comput Vis (2021). https://doi.org/10.1007/s11263-020-01400-4",https://www.mvtec.com/company/research/datasets/mvtec-ad/,EditCC BY-NC-SA 4.0,Image,,2021,,,,,,"Outlier Detection, zero-shot anomaly detection, Unsupervised Anomaly Detection, Multi-class Anomaly Detection, Anomaly Detection, Supervised Anomaly Detection","supervised-anomaly-detection-on-mvtec-ad, multi-class-anomaly-detection-on-mvtec-ad, anomaly-detection-on-mvtec-ad, zero-shot-anomaly-detection-on-mvtec-ad-1",,See all 1951 tasks,Anomaly Detection156 benchmark,Anomaly Detection156 benchmark
ShanghaiTech,ShanghaiTech Dataset,"The Shanghaitech dataset is a large-scale crowd counting dataset. It consists of 1198 annotated crowd images. The dataset is divided into two parts, Part-A containing 482 images and Part-B containing 716 images. Part-A is split into train and test subsets consisting of 300 and 182 images, respectively. Part-B is split into train and test subsets consisting of 400 and 316 images. Each person in a crowd image is annotated with one point close to the center of the head. In total, the dataset consists of 330,165 annotated people. Images from Part-A were collected from the Internet, while images from Part-B were collected on the busy streets of Shanghai.",https://arxiv.org/abs/1807.09959,EditUnknown,"Image, Video",,,,,482 images,split into train and test subsets consisting of 300 and 182 images,,"Crowd Counting, Abnormal Event Detection In Video, Video Anomaly Detection, Anomaly Detection In Surveillance Videos, Cross-Part Crowd Counting, Anomaly Detection","video-anomaly-detection-on-shanghaitech-4, crowd-counting-on-shanghaitech-a, anomaly-detection-on-shanghaitech, cross-part-crowd-counting-on-shanghaitech-a, cross-part-crowd-counting-on-shanghaitech-b, anomaly-detection-in-surveillance-videos-on-8, crowd-counting-on-shanghaitech-b",,See all 1951 tasks,Anomaly Detection156 benchmark,Anomaly Detection156 benchmark
ShanghaiTech_Campus,ShanghaiTech Campus Dataset,"The ShanghaiTech Campus dataset has 13 scenes with complex light conditions and camera angles. It contains 130 abnormal events and over 270, 000 training frames. Moreover, both the frame-level and pixel-level ground truth of abnormal events are annotated in this dataset.",https://production-media.paperswithcode.com/datasets/ab299ace-c9f7-4232-aa56-78b70ac61163.png,EditMIT,"Image, Video",,,,,,,,"Abnormal Event Detection In Video, Weakly-supervised Anomaly Detection, Video Understanding, Video Anomaly Detection, Anomaly Detection In Surveillance Videos, Anomaly Detection, Weakly-supervised Video Anomaly Detection","anomaly-detection-in-surveillance-videos-on-8, weakly-supervised-video-anomaly-detection-on, video-anomaly-detection-on-shanghaitech-4, anomaly-detection-on-shanghaitech, video-anomaly-detection-on-shanghaitech, video-anomaly-detection-on-hr-shanghaitech, anomaly-detection-in-surveillance-videos-on-1, anomaly-detection-on-shanghaitech-campus-2",,See all 1951 tasks,Anomaly Detection156 benchmark,Anomaly Detection156 benchmark
ACOS,ACOS Dataset,"Most of the aspect based sentiment analysis research aims at identifying the sentiment polarities toward some explicit aspect terms while ignores implicit aspects in text. To capture both explicit and implicit aspects, we focus on aspect-category based sentiment analysis, which involves joint aspect category detection and category-oriented sentiment classification. However, currently only a few simple studies have focused on this problem. The shortcomings in the way they defined the task make their approaches difficult to effectively learn the inner-relations between categories and the inter-relations between categories and sentiments. In this work, we re-formalize the task as a category-sentiment hierarchy prediction problem, which contains a hierarchy output structure to first identify multiple aspect categories in a piece of text, and then predict the sentiment for each of the identified categories. Specifically, we propose a Hierarchical Graph Convolutional Network (Hier-GCN), where a lower-level GCN is to model the inner-relations among multiple categories, and the higher-level GCN is to capture the inter-relations between aspect categories and sentiments. Extensive evaluations demonstrate that our hierarchy output structure is superior over existing ones, and the Hier-GCN model can consistently achieve the best results on four benchmarks.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-absa-on-acos,,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
ASQP,ASQP Dataset,"Aspect-based sentiment analysis (ABSA) typically focuses on extracting aspects and predicting their sentiments on individual sentences such as customer reviews. Recently, another kind of opinion sharing platform, namely question answering (QA) forum, has received increasing popularity, which accumulates a large number of user opinions towards various aspects. This motivates us to investigate the task of ABSA on QA forums (ABSA-QA), aiming to jointly detect the discussed aspects and their sentiment polarities for a given QA pair. Unlike review sentences, a QA pair is composed of two parallel sentences, which requires interaction modeling to align the aspect mentioned in the question and the associated opinion clues in the answer. To this end, we propose a model with a specific design of cross-sentence aspect-opinion interaction modeling to address this task. The proposed method is evaluated on three real-world datasets and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-absa-on-asqp,,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
ASTE,ASTE Dataset,"Target-based sentiment analysis or aspect-based sentiment analysis (ABSA) refers to addressing various sentiment analysis tasks at a fine-grained level, which includes but is not limited to aspect extraction, aspect sentiment classification, and opinion extraction. There exist many solvers of the above individual subtasks or a combination of two subtasks, and they can work together to tell a complete story, i.e. the discussed aspect, the sentiment on it, and the cause of the sentiment. However, no previous ABSA research tried to provide a complete solution in one shot. In this paper, we introduce a new subtask under ABSA, named aspect sentiment triplet extraction (ASTE). Particularly, a solver of this task needs to extract triplets (What, How, Why) from the inputs, which show WHAT the targeted aspects are, HOW their sentiment polarities are and WHY they have such polarities (i.e. opinion reasons). For instance, one triplet from “Waiters are very friendly and the pasta is simply average” could be (‘Waiters’, positive, ‘friendly’). We propose a two-stage framework to address this task. The first stage predicts what, how and why in a unified model, and then the second stage pairs up the predicted what (how) and why from the first stage to output triplets. In the experiments, our framework has set a benchmark performance in this novel triplet extraction task. Meanwhile, it outperforms a few strong baselines adapted from state-of-the-art related methods.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-absa-on-aste,,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
CAIL2019-SCM,CAIL2019-SCM Dataset,"Chinese AI and Law 2019 Similar Case Matching dataset. CAIL2019-SCM contains 8,964 triplets of cases published by the Supreme People's Court of China. CAIL2019-SCM focuses on detecting similar cases, and the participants are required to check which two cases are more similar in the triplets.",/paper/cail2019-scm-a-dataset-of-similar-case,EditUnknown,Text,English,2019,,,,,,"Aspect-Based Sentiment Analysis (ABSA), Semantic Text Matching, Sentiment Analysis","semantic-text-matching-on-cail2019-scm-val, semantic-text-matching-on-cail2019-scm-test",,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
Laptop-ACOS,Laptop-ACOS Dataset,"Laptop-ACOS is a brand new Laptop dataset collected from the Amazon platform in the years 2017 and 2018 (covering ten types of laptops under six brands such as ASUS, Acer, Samsung, Lenovo, MBP, MSI, and so on). It contains 4,076 review sentences, much larger than the SemEval Laptop datasets.
For Laptop-ACOS, we annotate the four elements and their corresponding quadruples all by ourselves. We employ the aspect categories defined in the SemEval 2016 Laptop dataset. The Laptop-ACOS dataset contains 4076 sentences with 5758 quadruples. As we have mentioned, a large percentage of the quadruples contain implicit aspects or implicit opinions .  By comparing two datasets, it can be observed that Laptop-ACOS has a higher percentage of implicit opinions than Restaurant-ACOS . It is worth noting that the Laptop-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,2017,,,4076 sentences,,,"Aspect-Based Sentiment Analysis (ABSA), Aspect-Category-Opinion-Sentiment Quadruple Extraction, Sentiment Analysis",aspect-category-opinion-sentiment-quadruple-1,,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
MAMS,MAMS Dataset,"MAMS is a challenge dataset for aspect-based sentiment analysis (ABSA), in which each sentences contain at least two aspects with different sentiment polarities. MAMS dataset contains two versions: one for aspect-term sentiment analysis (ATSA) and one for aspect-category sentiment analysis (ACSA).",https://github.com/siat-nlp/MAMS-for-ABSA,EditUnknown,Text,English,,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-on-mams,,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
Restaurant-ACOS,Restaurant-ACOS Dataset,"The Restaurant-ACOS dataset is constructed based on the SemEval 2016 Restaurant dataset (Pontiki et al., 2016) and its expansion datasets (Fan et al., 2019; Xu et al., 2020).
The SemEval 2016 Restaurant dataset (Pontiki et al., 2016) was annotated with explicit and implicit aspects, categories, and sentiment. (Fan et al., 2019; Xu et al., 2020) further added the opinion annotations. We integrate their annotations to construct aspect-category-opinion-sentiment quadruples and further annotate the implicit opinions. The Restaurant-ACOS dataset contains 2286 sentences with 3658 quadruples.
It is worth noting that the Restaurant-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,2016,,,2286 sentences,,,"Aspect-Based Sentiment Analysis (ABSA), Aspect-Category-Opinion-Sentiment Quadruple Extraction, Sentiment Analysis",aspect-category-opinion-sentiment-quadruple,,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
SemEval-2014_Task-4,SemEval-2014 Task-4 Dataset,"Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided.

Subtask 2: Aspect term polarity

For a given set of aspect terms within a sentence, determine whether the polarity of each aspect term is positive, negative, neutral or conflict (i.e., both positive and negative).

For example:

“I loved their fajitas” → {fajitas: positive}
“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive}
“The fajitas are their first plate” → {fajitas: neutral}
“The fajitas were great to taste, but not to see” → {fajitas: conflict}",https://huggingface.co/datasets/Charitarth/SemEval2014-Task4,EditUnknown,"Image, Text",English,,,,,,,"Aspect-oriented  Opinion Extraction, Aspect-Based Sentiment Analysis, Aspect Category Detection, Aspect Extraction, Aspect-Based Sentiment Analysis (ABSA)","aspect-extraction-on-semeval-2014-task-4-sub-1, aspect-category-detection-on-semeval-2014-1, aspect-oriented-opinion-extraction-on-semeval, aspect-based-sentiment-analysis-on-semeval-10, aspect-based-sentiment-analysis-on-semeval",,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
TASD,TASD Dataset,"Aspect-based sentiment analysis (ABSA) aims to detect the targets (which are composed by continuous words), aspects and sentiment polarities in text. Published datasets from SemEval-2015 and SemEval-2016 reveal that a sentiment polarity depends on both the target and the aspect. However, most of the existing methods consider predicting sentiment polarities from either targets or aspects but not from both, thus they easily make wrong predictions on sentiment polarities. In particular, where the target is implicit, i.e., it does not appear in the given text, the methods predicting sentiment polarities from targets do not work. To tackle these limitations in ABSA, this paper proposes a novel method for target-aspect-sentiment joint detection. It relies on a pre-trained language model and can capture the dependence on both targets and aspects for sentiment prediction. Experimental results on the SemEval-2015 and SemEval-2016 restaurant datasets show that the proposed method achieves a high performance in detecting target-aspect-sentiment triples even for the implicit target cases; moreover, it even outperforms the state-of-the-art methods for those subtasks of target-aspect-sentiment detection that they are competent to.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,2015,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-absa-on-tasd,,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
Twitter_Sentiment_Analysis,Twitter Sentiment Analysis Dataset,"This is an entity-level Twitter Sentiment Analysis dataset. For each message, the task is to judge the sentiment of the entire sentence towards a given entity. For example, A outperforms B is positive for entity A but negative for entity B. The dataset contains ~70K labeled training messages and 1K labeled validation messages. It is available online for free on Kaggle.",https://production-media.paperswithcode.com/datasets/dataset-cover.jpeg,EditCC0: Public Domain,"Image, Text",English,,,,,,,"Aspect-Based Sentiment Analysis (ABSA), Text Classification, Twitter Sentiment Analysis",text-classification-on-twitter-sentiment-1,,See all 1951 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
ASTE-Data-V2,ASTE-Data-V2 Dataset,"A benchmark dataset for the Aspect Sentiment Triplet Extraction, an updated version of ASTE-Data-V1.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Aspect Sentiment Triplet Extraction,aspect-sentiment-triplet-extraction-on-aste,,See all 1951 tasks,Aspect Sentiment Triplet Extra,Aspect Sentiment Triplet Extra
MuseASTE,MuseASTE Dataset,"•A new benchmark dataset for Aspect Sentiment Triplet Extraction.
•First Aspect Sentiment Triplet Extraction (ASTE) Dataset in Automotive Domain.
•Largest ASTE Dataset to date with annotations for over 28,295 sentences.
•Dataset includes complex aspects not verbatim present in the sentence.
•Domain: Aspect-based sentiment analysis, ASTE, Opinion Mining, Recommender System.
•Four baseline SOTA models implemented on the dataset",https://production-media.paperswithcode.com/datasets/5a159f4f-ad7d-47e6-b795-69678b4d0f4e.jpeg,EditCreative Commons Licenses 4.0,Text,English,,,,295 sentences,,,"Aspect-Based Sentiment Analysis (ABSA), Aspect Sentiment Triplet Extraction",aspect-sentiment-triplet-extraction-on-1,,See all 1951 tasks,Aspect Sentiment Triplet Extra,Aspect Sentiment Triplet Extra
AVA-ActiveSpeaker,AVA-ActiveSpeaker Dataset,"Contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio.",/paper/ava-activespeaker-an-audio-visual-dataset-for,EditUnknown,"Audio, Image",,,,,,,,"Audio-Visual Active Speaker Detection, Self-Supervised Learning, Speaker Diarization, Speech Enhancement",audio-visual-active-speaker-detection-on-ava,,See all 1951 tasks,Audio-Visual Active Speaker De,Audio-Visual Active Speaker De
VPCD,VPCD Dataset,"VPCD contains multi-modal annotations (face, body and voice) for all primary and secondary characters from a range of diverse TV-shows and movies. It is used for evaluating multi-modal person-clustering. It contains body-tracks for each annotated character, face-tracks when visible, and voice-tracks when speaking, with their associated features.

It consists of more than 30,000 face and body tracks of 300+ characters, from over 23 hours of video.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-05-21_at_17.09.01_1.jpg,EditCC BY 4.0,"Audio, Image",,,,,,,,Audio-Visual Active Speaker Detection,audio-visual-active-speaker-detection-on-vpcd,,See all 1951 tasks,Audio-Visual Active Speaker De,Audio-Visual Active Speaker De
AudioSet,AudioSet Dataset,"Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.",https://arxiv.org/abs/2001.09414,EditCC BY 4.0,"Audio, Image",,,,,,,,"Zero-shot Audio Classification, Audio Classification, Audio Tagging, Multi-modal Classification, Target Sound Extraction, Audio Source Separation","audio-classification-on-audioset, audio-source-separation-on-audioset, zero-shot-audio-classification-on-audioset, target-sound-extraction-on-audioset, multi-modal-classification-on-audioset, audio-tagging-on-audioset",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
Common_Voice,Common Voice Dataset,"Common Voice is an audio dataset that consists of a unique MP3 and corresponding text file. There are 9,283 recorded hours in the dataset. The dataset also includes demographic metadata like age, sex, and accent. The dataset consists of 7,335 validated hours in 60 languages.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-07_at_8.52.43_PM.png,EditCC0,"Audio, Image, Text",English,,,,,,,"Audio Classification, Language Identification, Few-Shot Audio Classification, Speech Recognition, Automatic Speech Recognition, Cross-Lingual ASR","speech-recognition-on-common-voice-8-0-29, speech-recognition-on-common-voice-dutch, automatic-speech-recognition-on-mozilla-71, automatic-speech-recognition-on-mozilla-63, audio-classification-on-common-voice-16-1, speech-recognition-on-common-voice-8-0-38, speech-recognition-on-common-voice-8-0-10, speech-recognition-on-common-voice-turkish, speech-recognition-on-common-voice-8-0-39, speech-recognition-on-common-voice-indonesian, speech-recognition-on-common-voice-8-0-uzbek, speech-recognition-on-common-voice-7-0-arabic, speech-recognition-on-common-voice-8-0-tatar, speech-recognition-on-common-voice-8-0-odia, speech-recognition-on-common-voice-8-0-erzya, speech-recognition-on-common-voice-8-0-kazakh, speech-recognition-on-common-voice-8-0-french, speech-recognition-on-common-voice-czech, speech-recognition-on-common-voice-8-0-41, speech-recognition-on-common-voice-frisian, speech-recognition-on-common-voice-arabic, speech-recognition-on-common-voice-7-0-29, automatic-speech-recognition-on-common-voice-18, speech-recognition-on-common-voice-7-0-german, speech-recognition-on-common-voice-8-0-german, speech-recognition-on-common-voice-8-0-9, speech-recognition-on-common-voice-7-0-abkhaz, speech-recognition-on-common-voice-8-0-33, automatic-speech-recognition-on-mozilla-84, speech-recognition-on-common-voice-welsh, automatic-speech-recognition-on-mozilla-66, speech-recognition-on-common-voice-2, automatic-speech-recognition-on-mozilla-108, few-shot-audio-classification-on-common-voice, automatic-speech-recognition-on-mozilla-64, speech-recognition-on-common-voice-8-0-20, speech-recognition-on-common-voice-8-0-11, speech-recognition-on-common-voice-8-0-hausa, automatic-speech-recognition-on-common-voice-17, speech-recognition-on-common-voice-maltese, speech-recognition-on-common-voice-swedish, speech-recognition-on-common-voice-8-0-breton, speech-recognition-on-common-voice-breton, speech-recognition-on-common-voice-persian, automatic-speech-recognition-on-mozilla-96, speech-recognition-on-common-voice-vi, speech-recognition-on-common-voice-8-0-basaa, speech-recognition-on-common-voice-russian, speech-recognition-on-common-voice-japanese, speech-recognition-on-common-voice-odia, speech-recognition-on-common-voice-vietnamese, speech-recognition-on-common-voice-7-0-1, speech-recognition-on-common-voice-8-0-dutch, speech-recognition-on-common-voice-8-0-24, speech-recognition-on-common-voice-7-0-hindi, automatic-speech-recognition-on-mozilla-114, speech-recognition-on-common-voice-spanish, speech-recognition-on-common-voice-8-0-3, speech-recognition-on-common-voice-8-0-37, automatic-speech-recognition-on-mozilla-126, speech-recognition-on-common-voice-8-0-30, speech-recognition-on-common-voice-8-0-2, speech-recognition-on-common-voice-lithuanian, speech-recognition-on-mozilla-common-voice-16, cross-lingual-asr-on-common-voice, speech-recognition-on-common-voice-polish, speech-recognition-on-common-voice-portuguese, speech-recognition-on-common-voice-georgian, automatic-speech-recognition-on-commonvoice-8, automatic-speech-recognition-on-mcv17, speech-recognition-on-common-voice-8-0-1, speech-recognition-on-common-voice-french, speech-recognition-on-mozilla-common-voice-9, automatic-speech-recognition-on-mozilla-127, speech-recognition-on-common-voice-8-0-kabyle, speech-recognition-on-common-voice-8-0-32, speech-recognition-on-common-voice-german, speech-recognition-on-common-voice-italian, speech-recognition-on-common-voice-8-0-votic, speech-recognition-on-common-voice-chinese-2, speech-recognition-on-common-voice-hindi, speech-recognition-on-common-voice-8-0-7, automatic-speech-recognition-on-commonvoice-4, speech-recognition-on-common-voice-7-0-votic, speech-recognition-on-common-voice-8-0-hindi, automatic-speech-recognition-on-mozilla-125, speech-recognition-on-common-voice-8-0-8, speech-recognition-on-common-voice-8-0-40, speech-recognition-on-common-voice-7-0-odia, speech-recognition-on-common-voice-english, speech-recognition-on-mozilla-common-voice-15, speech-recognition-on-common-voice-tamil, speech-recognition-on-common-voice-8-0-22",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
CREMA-D,CREMA-D Dataset,"CREMA-D is an emotional multimodal actor data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified).

Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).

Participants rated the emotion and emotion levels based on the combined audiovisual presentation, the video alone, and the audio alone. Due to the large number of ratings needed, this effort was crowd-sourced and a total of 2443 participants each rated 90 unique clips, 30 audio, 30 visual, and 30 audio-visual. 95% of the clips have more than 7 ratings.",https://www.tensorflow.org/datasets/catalog/crema_d,EditUnknown,"Audio, Image, Text, Video",English,,,,12 sentences,,,"Talking Face Generation, Audio Classification, Speech Emotion Recognition, Video Emotion Recognition, Facial Expression Recognition (FER), Few-Shot Audio Classification, Self-Supervised Learning","talking-face-generation-on-crema-d, video-emotion-recognition-on-crema-d, few-shot-audio-classification-on-crema-d, facial-expression-recognition-on-crema-d, self-supervised-learning-on-crema-d, speech-emotion-recognition-on-crema-d, audio-classification-on-crema-d",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
EPIC-KITCHENS-100,EPIC-KITCHENS-100 Dataset,"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the ""test of time"" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit ""two years on"".
The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.",https://production-media.paperswithcode.com/datasets/EPIC-Kitchens-0000000300-2485dab7_8WDhYkq_AkL1bgW.jpg,EditCC BY NC 4.0,"Audio, Image, Time Series, Video",,2018,,,,,,"Audio Classification, Open Vocabulary Action Recognition, Temporal Action Localization, Action Anticipation, Multi-Instance Retrieval, Action Recognition, Unsupervised Domain Adaptation","audio-classification-on-epic-kitchens-100, action-recognition-on-epic-kitchens-100, unsupervised-domain-adaptation-on-epic, action-anticipation-on-epic-kitchens-100, temporal-action-localization-on-epic-kitchens, multi-instance-retrieval-on-epic-kitchens-100, open-vocabulary-action-recognition-on-epic",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
ESC-50,ESC-50 Dataset,"The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org.",https://arxiv.org/abs/1902.08314,EditCC BY-NC 3.0,"Audio, Image",,2000,,,,,,"Zero-shot Audio Classification, Self-Supervised Audio Classification, Environment Sound Classification, Audio Classification, Data Augmentation, Environmental Sound Classification, Few-Shot Audio Classification, Image Classification, Zero-Shot Environment Sound Classification","audio-classification-on-esc-50, zero-shot-environment-sound-classification-on-1, zero-shot-audio-classification-on-esc-50, few-shot-audio-classification-on-esc-50, environmental-sound-classification-on-esc-50, environment-sound-classification-on-esc-50, self-supervised-audio-classification-on-esc, image-classification-on-esc-50",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
FSD50K,FSD50K Dataset,"Freesound Dataset 50k (or FSD50K for short) is an open dataset of human-labeled sound events containing 51,197 Freesound clips unequally distributed in 200 classes drawn from the AudioSet Ontology. FSD50K has been created at the Music Technology Group of Universitat Pompeu Fabra. It consists mainly of sound events produced by physical sound sources and production mechanisms, including human sounds, sounds of things, animals, natural sounds, musical instruments and more.",https://zenodo.org/record/4060432,EditOther (Attribution),"Audio, Image",,,,,,,200,"Contrastive Learning, Audio Classification, Environmental Sound Classification, Domain Adaptation","environmental-sound-classification-on-fsd50k, audio-classification-on-fsd50k",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
Speech_Commands,Speech Commands Dataset,Speech Commands is an audio dataset of spoken words designed to help train and evaluate keyword spotting systems .,https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.12.27_PM.png,EditCC BY,"Audio, Image, Text, Time Series",English,,,,,,,"Audio Classification, Keyword Spotting, Time Series Analysis, Speech Recognition, Federated Learning","time-series-on-speech-commands, audio-classification-on-speech-commands-1, keyword-spotting-on-google-speech-commands, speech-recognition-on-speech-commands-2",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
UCR_Time_Series_Classification_Archive,UCR Time Series Classification Archive Dataset,"The UCR Time Series Archive - introduced in 2002,
has become an important resource in the time series data mining
community, with at least one thousand published papers making
use of at least one data set from the archive. The original
incarnation of the archive had sixteen data sets but since that
time, it has gone through periodic expansions. The last expansion
took place in the summer of 2015 when the archive grew from
45 to 85 data sets. This paper introduces and will focus on the
new data expansion from 85 to 128 data sets. Beyond expanding
this valuable resource, this paper offers pragmatic advice to
anyone who may wish to evaluate a new algorithm on the archive.
Finally, this paper makes a novel and yet actionable claim: of the
hundreds of papers that show an improvement over the standard
baseline (1-nearest neighbor classification), a large fraction may
be misattributing the reasons for their improvement. Moreover,
they may have been able to achieve the same improvement with
a much simpler modification, requiring just a single line of code.",https://github.com/yuezhihan/ts2vec/blob/main/datasets/preprocess_electricity.py,EditUnknown,"Audio, Image, Time Series",,2002,,,,,,"Time Series Averaging, Audio Classification, Time Series Classification, Time Series Clustering, ECG Classification","ecg-classification-on-ucr-time-series, audio-classification-on-ucr-time-series",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
UrbanSound8K,UrbanSound8K Dataset,"Urban Sound 8K is an audio dataset that contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. All excerpts are taken from field recordings uploaded to www.freesound.org.",https://zenodo.org/record/401395,EditCC BY-NC 3.0,"Audio, Image",,,,,,,10,"Zero-shot Audio Classification, Environment Sound Classification, Audio Classification, Data Augmentation, Environmental Sound Classification","environmental-sound-classification-on, zero-shot-audio-classification-on, environment-sound-classification-on",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
VGG-Sound,VGG-Sound Dataset,Consists of more than 210k videos for 310 audio classes.,/paper/vggsound-a-large-scale-audio-visual-dataset,EditCC-BY 4.0,"Audio, Image, Text, Video",English,,,,,,,"Zero-shot Audio Classification, Video-to-Sound Generation, Audio Classification, Multi-modal Classification, Speaker Recognition, Image Classification, Speaker Verification","multi-modal-classification-on-vgg-sound, audio-classification-on-vggsound, zero-shot-audio-classification-on-vgg-sound, video-to-sound-generation-on-vgg-sound",,See all 1951 tasks,Audio Classification28 benchma,Audio Classification28 benchma
AirSim,AirSim Dataset,"AirSim is a simulator for drones, cars and more, built on Unreal Engine. It is open-source, cross platform, and supports software-in-the-loop simulation with popular flight controllers such as PX4 & ArduPilot and hardware-in-loop with PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin that can simply be dropped into any Unreal environment. Similarly, there exists an experimental version for a Unity plugin.",https://arxiv.org/pdf/1705.05065v2.pdf,EditMIT,,,,,,,,,"Autonomous Driving, Autonomous Vehicles, Imitation Learning",,,See all 1951 tasks,Autonomous Driving10 benchmark,Autonomous Driving10 benchmark
CARLA,CARLA Dataset,"CARLA (CAR Learning to Act) is an open simulator for urban driving, developed as an open-source layer over Unreal Engine 4. Technically, it operates similarly to, as an open source layer over Unreal Engine 4 that provides sensors in the form of RGB cameras (with customizable positions), ground truth depth maps, ground truth semantic segmentation maps with 12 semantic classes designed for driving (road, lane marking, traffic sign, sidewalk and so on), bounding boxes for dynamic objects in the environment, and measurements of the agent itself (vehicle location and orientation).",https://arxiv.org/abs/1909.11512,EditCC-BY / MIT,,,,,,,,,"Autonomous Vehicles, Imitation Learning, Autonomous Driving, CARLA MAP Leaderboard, CARLA Leaderboard 2.0, CARLA longest6","autonomous-driving-on-carla-leaderboard, carla-map-leaderboard-on-carla, carla-leaderboard-2-0-on-carla, carla-longest6-on-carla",,See all 1951 tasks,Autonomous Driving10 benchmark,Autonomous Driving10 benchmark
IDD,IDD Dataset,"IDD is a dataset for road scene understanding in unstructured environments used for semantic segmentation and object detection for autonomous driving. It consists of 10,004 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads.",/paper/idd-a-dataset-for-exploring-problems-of,EditUnknown,Image,,,,,004 images,,34,"Autonomous Driving, Semantic Segmentation, Domain Adaptation",,,See all 1951 tasks,Autonomous Driving10 benchmark,Autonomous Driving10 benchmark
INTERACTION_Dataset,INTERACTION Dataset Dataset,"The INTERACTION dataset contains naturalistic motions of various traffic participants in a variety of highly interactive driving scenarios from different countries. The dataset can serve for many behavior-related research areas, such as 


1) intention/behavior/motion prediction, 
2) behavior cloning and imitation learning,
3) behavior analysis and modeling,
4) motion pattern and representation learning,
5) interactive behavior extraction and categorization,
6) social and human-like behavior generation,
7) decision-making and planning algorithm development and verification,
8) driving scenario/case generation, etc.",https://interaction-dataset.com/,EditCustom (non-commercial),Time Series,,,,,,,,"Imitation Learning, Autonomous Driving, Autonomous Vehicles, Trajectory Prediction",trajectory-prediction-on-interaction-dataset-2,,See all 1951 tasks,Autonomous Driving10 benchmark,Autonomous Driving10 benchmark
TORCS,TORCS Dataset,"TORCS (The Open Racing Car Simulator) is a driving simulator. It is capable of simulating the essential elements of vehicular dynamics such as mass, rotational inertia, collision, mechanics of suspensions, links and differentials, friction and aerodynamics. Physics simulation is simplified and is carried out through Euler integration of differential equations at a temporal discretization level of 0.002 seconds. The rendering pipeline is lightweight and based on OpenGL that can be turned off for faster training. TORCS offers a large variety of tracks and cars as free assets. It also provides a number of programmed robot cars with different levels of performance that can be used to benchmark the performance of human players and software driving agents. TORCS was built with the goal of developing Artificial Intelligence for vehicular control and has been used extensively by the machine learning community ever since its inception.",https://arxiv.org/abs/2010.00993,EditGPLv2,,,,,,,,,"Decision Making, Autonomous Driving, Imitation Learning",,,See all 1951 tasks,Autonomous Driving10 benchmark,Autonomous Driving10 benchmark
Virtual_KITTI,Virtual KITTI Dataset,"Virtual KITTI is a photo-realistic synthetic video dataset designed to learn and evaluate computer vision models for several video understanding tasks: object detection and multi-object tracking, scene-level and instance-level semantic segmentation, optical flow, and depth estimation.

Virtual KITTI contains 50 high-resolution monocular videos (21,260 frames) generated from five different virtual worlds in urban settings under different imaging and weather conditions. These worlds were created using the Unity game engine and a novel real-to-virtual cloning method. These photo-realistic synthetic videos are automatically, exactly, and fully annotated for 2D and 3D multi-object tracking and at the pixel level with category, instance, flow, and depth labels (cf. below for download links).",https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/,EditCreative Commons Attribution-NonCommercial-ShareAlike 3.0,"3D, Image, Video",English,,,,,,,"Optical Flow Estimation, Monocular Depth Estimation, Visual Odometry, Monocular 3D Object Detection, Depth Estimation, Multi-Object Tracking, Autonomous Driving, Object Tracking, Stereo Matching, Semantic Segmentation, Simultaneous Localization and Mapping","monocular-depth-estimation-on-virtual-kitti-2, monocular-3d-object-detection-on-virtual",,See all 1951 tasks,Autonomous Driving10 benchmark,Autonomous Driving10 benchmark
WoodScape,WoodScape Dataset,"Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of its prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images.
WoodScape is an extensive fisheye automotive dataset named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images.",https://github.com/valeoai/WoodScape,EditUnknown,Image,,1906,,,000 images,"valence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images",40,"Object Detection, Autonomous Driving, Semi-Supervised Semantic Segmentation, Semantic Segmentation",semi-supervised-semantic-segmentation-on-45,,See all 1951 tasks,Autonomous Driving10 benchmark,Autonomous Driving10 benchmark
Hyper_Drive,Hyper Drive Dataset,"Towards automated analysis of large environments, hyperspectral sensors must be adapted into a format where they can be operated from mobile robots. In this dataset, we highlight hyperspectral datacubes collected from the Hyper-Drive  imaging system. Our system collects and registers datacubes spanning the visible to shortwave infrared (660-1700 nm) in 33 wavelength channels. The system also simultaneously captures the ambient solar spectrum reflected off a white reference tile. The dataset consists of 500 labeled datacubes from on-road and off-road terrain compliant with the ATLAS.

This work first appeared at WHISPERS 2023 In Athens, Greece and is a product of the  Robotics and Intelligent Vehicles Research Laboratory (RIVeR) at Northeastern University.

In addition to the 500 labeled hyperspectral datacubes, raw ROS bagfiles generated of each of the sensor feeds at a higher frame rate are available here. These files are provided as an additional resource and do not contain semantic labels, but contain ~10,000 additional hyperspectral datacubes of in-between frames from the labeled dataset. It also contains additional datatypes for terrain analysis such as inertial measurement unit (IMU) data. To the best of the authors knowledge, it is the largest vehicle-centric hyperspectral dataset currently available!

This research was sponsored by the United States Army Core of Engineers (USACE) Engineer Research and Development Center (ERDC) Geospatial Research Laboratory (GRL) and was accomplished under Cooperative Agreement Federal Award Identification Number (FAIN) W9132V-22-2-0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of USACE EDRC GRL or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.",https://production-media.paperswithcode.com/datasets/9fa50186-768f-48ec-897e-11b04f6efb1f.png,EditMIT,Image,,2023,,,,,,"Robot Navigation, Autonomous Vehicles, Autonomous Navigation, Hyperspectral Unmixing, Hyperspectral Image Classification, Autonomous Driving, Hyperspectral Image Segmentation, Hyperspectral Image Denoising, Hyperspectral image analysis, Classification Of Hyperspectral Images, Self-Driving Cars",,,See all 1951 tasks,Autonomous Navigation1 benchma,Autonomous Navigation1 benchma
IN2LAAMA,IN2LAAMA Dataset,IN2LAAMA is a set of lidar-inertial datasets collected with a Velodyne VLP-16 lidar and a Xsens MTi-3 IMU.,/paper/in2laama-inertial-lidar-localisation,EditUnknown,Image,,,,,,,,"Autonomous Navigation, Autonomous Vehicles, Simultaneous Localization and Mapping",,,See all 1951 tasks,Autonomous Navigation1 benchma,Autonomous Navigation1 benchma
JRDB,JRDB Dataset,"A novel egocentric dataset collected from social mobile manipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal sensor data including stereo cylindrical 360 degrees RGB video at 15 fps, 3D point clouds from two Velodyne 16 Lidars, line 3D point clouds from two Sick Lidars, audio signal, RGB-D video at 30 fps, 360 degrees spherical image from a fisheye camera and encoder values from the robot's wheels.",/paper/jrdb-a-dataset-and-benchmark-for-visual,EditUnknown,"Image, Video",,,,,,,,"Human Detection, Autonomous Navigation, Multi-Object Tracking",multi-object-tracking-on-jrdb,,See all 1951 tasks,Autonomous Navigation1 benchma,Autonomous Navigation1 benchma
LDDRS,LDDRS Dataset,"The LWIR DoFP Dataset of Road Scene (LDDRS) is a road detection dataset with 2,113 annotated images. It contains both day and night scenes, with multiple cars and pedestrians per image.",https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700460.pdf,EditUnknown,,,,,,,,,Autonomous Navigation,,,See all 1951 tasks,Autonomous Navigation1 benchma,Autonomous Navigation1 benchma
MIDGARD,MIDGARD Dataset,"MIDGARD is an open-source simulator for autonomous robot navigation in outdoor unstructured environments.
It is designed to enable the training of autonomous agents (e.g., unmanned ground vehicles) in photorealistic 3D environments, and support the generalization skills of learning-based agents thanks to the variability in training scenarios.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Robot Navigation, Autonomous Navigation, Visual Navigation, Reinforcement Learning (RL), PointGoal Navigation",,,See all 1951 tasks,Autonomous Navigation1 benchma,Autonomous Navigation1 benchma
ApolloCar3D,ApolloCar3D Dataset,"ApolloCar3DT is a dataset that contains 5,277 driving images and over 60K car instances, where each car is fitted with an industry-grade 3D CAD model with absolute model size and semantically labelled keypoints. This dataset is above 20 times larger than PASCAL3D+ and KITTI, the current state-of-the-art.",https://arxiv.org/pdf/1811.12222v2.pdf,EditCustom (non-commercial),"3D, Image",,,,,,,,"Pose Estimation, Autonomous Vehicles, Keypoint Detection, 3D Pose Estimation, Car Pose Estimation, Vehicle Key-Point and Orientation Estimation, 3D Car Instance Understanding, 3D Shape Reconstruction, 6D Pose Estimation, Autonomous Driving, 3D Shape Reconstruction From A Single 2D Image, 6D Pose Estimation using RGB, 3D Reconstruction, Vehicle Pose Estimation","3d-shape-reconstruction-from-a-single-2d, 3d-pose-estimation-on-apollocar3d, 3d-reconstruction-on-apollocar3d, autonomous-vehicles-on-apollocar3d, keypoint-detection-on-apollocar3d, 3d-car-instance-understanding-on-apollocar3d, autonomous-driving-on-apollocar3d, 3d-shape-reconstruction-on-apollocar3d, 6d-pose-estimation-using-rgb-on-apollocar3d, vehicle-pose-estimation-on-apollocar3d, 6d-pose-estimation-on-apollocar3d, car-pose-estimation-on-apollocar3d, pose-estimation-on-apollocar3d, vehicle-key-point-and-orientation-estimation",,See all 1951 tasks,Autonomous Vehicles57 benchmar,Autonomous Vehicles57 benchmar
Argoverse_2_Motion_Forecasting,Argoverse 2 Motion Forecasting Dataset,"The Argoverse 2 Motion Forecasting Dataset is a curated collection of 250,000 scenarios for training and validation. Each scenario is 11 seconds long and contains the 2D, birds-eye-view centroid and heading of each tracked object sampled at 10 Hz.

To curate this collection, we sifted through thousands of hours of driving data from our fleet of self-driving test vehicles to find the most challenging segments. We place special emphasis on kinematically and socially unusual behavior, especially when exhibited by actors relevant to the ego-vehicle’s decision-making process. Some examples of interactions captured within our dataset include: buses navigating through multi-lane intersections, vehicles yielding to pedestrians at crosswalks, and cyclists sharing dense city streets.

Spanning 2,000+ km over six geographically diverse cities, Argoverse 2 covers a large geographic area. Argoverse 2 also contains a large object taxonomy with 10 non-overlapping classes that encompass a broad range of actors, both static and dynamic. In comparison to the Argoverse 1 Motion Forecasting Dataset, the scenarios in this dataset are approximately twice as long and more diverse.

Together, these changes incentivize methods that perform well on extended forecast horizons, handle multiple types of dynamic objects, and ensure safety in long tail scenarios.",https://production-media.paperswithcode.com/datasets/b6e8d1b3-46c1-407a-b870-0f3f48703cb5.gif,EditCC BY-NC-SA 4.0,"Time Series, Video",,,,,,"test vehicles to find the most challenging segments. We place special emphasis on kinematically and socially unusual behavior, especially when exhibited by actors relevant to the ego-vehicle’s decision-making process. Some examples",,"Autonomous Vehicles, motion prediction, Motion Forecasting, Autonomous Driving, Self-Driving Cars, Trajectory Prediction",,,See all 1951 tasks,Autonomous Vehicles57 benchmar,Autonomous Vehicles57 benchmar
Drive_Act,Drive&Act Dataset,"The Drive&Act dataset is a state of the art multi modal benchmark for driver behavior recognition. The dataset includes 3D skeletons in addition to frame-wise hierarchical labels of 9.6 Million frames captured by 6 different views and 3 modalities (RGB, IR and depth).

It offers following key features:


12h of video data in 29 long sequences
Calibrated multi view camera system with 5 views
Multi modal videos: NIR, Depth and Color data
Markerless motion capture: 3D Body Pose and Head Pose
Model of the static interior of the car
83 manually annotated hierarchical activity labels:
Level 1: Long running tasks (12)
Level 2: Semantic actions (34)
Level 3: Object Interaction tripplets [action|object|location] (6|17|14)",/paper/driveact-a-multi-modal-dataset-for-fine,EditUnknown,"Image, Video",,,,,,,,"Skeleton Based Action Recognition, Activity Recognition, Action Recognition, Autonomous Vehicles",skeleton-based-action-recognition-on-drive,,See all 1951 tasks,Autonomous Vehicles57 benchmar,Autonomous Vehicles57 benchmar
RadarScenes,RadarScenes Dataset,"RadarScenes is a real-world radar point cloud dataset for automotive applications.

It consists of measurements and point-wise annotations from more than four hours of driving collected by four series radar sensors mounted on one test vehicle. Individual detections of dynamic objects were manually grouped to clusters and labeled afterwards. The purpose of this data set is to enable the development of novel (machine learning-based) radar perception algorithms with the focus on moving road users. Images of the recorded sequences were captured using a documentary camera.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-04-08_at_10.18.01.jpg,EditAttribution-NonCommercial-ShareAlike 4.0 International,,,,,,,,,Autonomous Vehicles,,,See all 1951 tasks,Autonomous Vehicles57 benchmar,Autonomous Vehicles57 benchmar
ROAD,ROAD Dataset,"ROAD is designed to test an autonomous vehicle's ability to detect road events, defined as triplets composed by an active agent, the action(s) it performs and the corresponding scene locations. ROAD comprises videos originally from the Oxford RobotCar Dataset, annotated with bounding boxes showing the location in the image plane of each road event.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY-NC 4.0,"Image, Video",,,,,,,,"Autonomous Vehicles, Action Detection, Autonomous Driving, Event Detection, Activity Detection, Decision Making, Continual Learning",,,See all 1951 tasks,Autonomous Vehicles57 benchmar,Autonomous Vehicles57 benchmar
Talk2Car,Talk2Car Dataset,"The Talk2Car dataset finds itself at the intersection of various research domains, promoting the development of cross-disciplinary solutions for improving the state-of-the-art in grounding natural language into visual space. The annotations were gathered with the following aspects in mind:
Free-form high quality natural language commands, that stimulate the development of solutions that can operate in the wild.
A realistic task setting. Specifically, the authors consider an autonomous driving setting, where a passenger can control the actions of an Autonomous Vehicle by giving commands in natural language.
The Talk2Car dataset was build on top of the nuScenes dataset to include an extensive suite of sensor modalities, i.e. semantic maps, GPS, LIDAR, RADAR and 360-degree RGB images annotated with 3D bounding boxes. Such variety of input modalities sets the object referral task on the Talk2Car dataset apart from related challenges, where additional sensor modalities are generally missing.",https://talk2car.github.io/,EditUnknown,,,,,,,,,"Self-Driving Cars, Autonomous Driving, Autonomous Vehicles, Referring Expression Comprehension",referring-expression-comprehension-on-2,,See all 1951 tasks,Autonomous Vehicles57 benchmar,Autonomous Vehicles57 benchmar
TITAN,TITAN Dataset,"TITAN consists of 700 labeled video-clips (with odometry) captured from a moving vehicle on highly interactive urban traffic scenes in Tokyo. The dataset includes 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes that are organized hierarchically corresponding to atomic, simple/complex-contextual, transportive, and communicative actions.",/paper/titan-future-forecast-using-action-priors,EditCustom,Time Series,,,,,,,50,"Autonomous Driving, Autonomous Vehicles, Trajectory Prediction",,,See all 1951 tasks,Autonomous Vehicles57 benchmar,Autonomous Vehicles57 benchmar
Backdoor_Attack205_papers_with_code_Dataset,Backdoor Attack205 papers with code Dataset,,https://paperswithcode.com/dataset/backdoor-attack,,,,,,,,,,,,,See all 1951 tasks,Backdoor Attack205 papers with,Backdoor Attack205 papers with
BSD100,BSD100 Dataset,,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Image Super-Resolution, Joint Demosaicing and Denoising, Blind Super-Resolution","image-super-resolution-on-bsd100-3x-upscaling, image-super-resolution-on-bsd100-2x-upscaling, image-super-resolution-on-bsd100-8x-upscaling, image-super-resolution-on-bsd100-16x, blind-super-resolution-on-bsd100-3x-upscaling, image-super-resolution-on-bsd100-4x-upscaling, blind-super-resolution-on-bsd100-4x-upscaling, joint-demosaicing-and-denoising-on-bsd100, blind-super-resolution-on-bsd100-2x-upscaling",,See all 1951 tasks,Blind Super-Resolution18 bench,Blind Super-Resolution18 bench
DIV2KRK,DIV2KRK Dataset,"Using the validation set (100 images) from the widely used DIV2K dataset, we blurred and subsampled each image with a different, randomly generated kernel. Kernels were 11x11 anisotropic gaussians with random lengths λ1, λ2∼U(0.6, 5) independently distributed for each axis, rotated by a random angle θ∼U[−π, π].",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,,,,100 images,validation set (100 images,,Blind Super-Resolution,"blind-super-resolution-on-div2krk-2x, blind-super-resolution-on-div2krk-4x",,See all 1951 tasks,Blind Super-Resolution18 bench,Blind Super-Resolution18 bench
KID-F,KID-F Dataset,"Description
K-pop Idol Dataset - Female (KID-F) is the first dataset of K-pop idol high quality face images. It consists of about 6,000 high quality face images at 512x512 resolution and identity labels for each image.

We collected about 90,000 K-pop female idol images and crop the face from each image. And we classified high quality face images. As a result, there are about 6,000 high quality face images in this dataset.

There are 300 test datasets for a benchmark. There are no duplicate images between test and train images. Some identities in test images are not duplicated with train images. (It means some test images is new identity to the trained model) Each test images have its degraded pair. You can use these degraded test images for testing face super resolution performance.

We also provide identity labels for each image. You can download the csv file from our github

Download
You can download dataset from here.
Google Drive

Agreement

The use of this software is RESTRICTED to non-commercial research and educational purposes.
All images of the KID-F dataset are obtained from the internet which are not property of EDA(PCEO-AI-CLUB). EDA is not responsible for the content nor the meaning of these images.
You agree not to reproduce, duplicate, copy, sell, trade, resell or exploit for any commercial purposes, any portion of the images and any portion of derived data.
You agree not to further copy, publish or distribute any portion of the KID-F dataset. Except, for internal use at a single site within the same organization it is allowed to make copies of the dataset.
EDA reserves the right to terminate your access to the CelebA dataset at any time.",https://production-media.paperswithcode.com/datasets/8005aa40-9d46-4e21-81f5-3e8da0a5c868.png,EditUnknown,Image,,,,,,,,"Super-Resolution, Blind Super-Resolution, Face Hallucination",,,See all 1951 tasks,Blind Super-Resolution18 bench,Blind Super-Resolution18 bench
Set5,Set5 Dataset,"The Set5 dataset is a dataset consisting of 5 images (“baby”, “bird”, “butterfly”, “head”, “woman”) commonly used for testing performance of Image Super-Resolution models.",https://production-media.paperswithcode.com/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg,EditUnknown,Image,,,,,5 images,,,"Image Super-Resolution, Blind Super-Resolution, Compressive Sensing, Image Rescaling","blind-super-resolution-on-set5-3x-upscaling, image-super-resolution-on-set5-3x-upscaling, compressive-sensing-on-set5, image-super-resolution-on-set5-8x-upscaling, blind-super-resolution-on-set5-4x-upscaling, image-super-resolution-on-set5-2x-upscaling, image-super-resolution-on-set5-4x-upscaling, image-rescaling-on-set5-2x, blind-super-resolution-on-set5-2x-upscaling",,See all 1951 tasks,Blind Super-Resolution18 bench,Blind Super-Resolution18 bench
BraTS-Africa,BraTS-Africa Dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Brain Tumor Segmentation,brain-tumor-segmentation-on-brats-africa,,See all 1951 tasks,Brain Tumor Segmentation11 ben,Brain Tumor Segmentation11 ben
BraTS_2013,BraTS 2013 Dataset,"BRATS 2013 is a brain tumor segmentation dataset consists of synthetic and real images, where each of them is further divided into high-grade gliomas (HG) and low-grade gliomas (LG). There are 25 patients with both synthetic HG and LG images and 20 patients with real HG and 10 patients with real LG images. For each patient, FLAIR, T1, T2, and post-Gadolinium T1 magnetic resonance (MR) image sequences are available.",https://arxiv.org/abs/1908.06965,EditCustom (non-commercial),Image,,2013,,,,,,Brain Tumor Segmentation,"brain-tumor-segmentation-on-brats-2013-1, brain-tumor-segmentation-on-brats-2013",,See all 1951 tasks,Brain Tumor Segmentation11 ben,Brain Tumor Segmentation11 ben
BraTS_2017,BraTS 2017 Dataset,"The BRATS2017 dataset. It contains 285 brain tumor MRI scans, with four MRI modalities as T1, T1ce, T2, and Flair for each scan. The dataset also provides full masks for brain tumors, with labels for ED, ET, NET/NCR. The segmentation evaluation is based on three tasks: WT, TC and ET segmentation.",https://arxiv.org/abs/1911.02014,EditCustom (attribution),Image,,,,,,,,Brain Tumor Segmentation,brain-tumor-segmentation-on-brats-2017-val,,See all 1951 tasks,Brain Tumor Segmentation11 ben,Brain Tumor Segmentation11 ben
BraTs_Peds_2024,BraTs Peds 2024 Dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"MRI segmentation, Brain Tumor Segmentation",brain-tumor-segmentation-on-brats-peds-2024,,See all 1951 tasks,Brain Tumor Segmentation11 ben,Brain Tumor Segmentation11 ben
EVICAN,EVICAN Dataset,"Deep learning use for quantitative image analysis is exponentially increasing. However, training accurate, widely deployable deep learning algorithms requires a plethora of annotated (ground truth) data. Image collections must contain not only thousands of images to provide sufficient example objects (i.e. cells), but also contain an adequate degree of image heterogeneity. We present a new dataset, EVICAN-Expert visual cell annotation, comprising partially annotated grayscale images of 30 different cell lines from multiple microscopes, contrast mechanisms and magnifications that is readily usable as training data for computer vision applications. With 4600 images and ∼26 000 segmented cells, our collection offers an unparalleled heterogeneous training dataset for cell biology deep learning application development. The dataset is freely available (https://edmond.mpdl.mpg.de/imeji/collection/l45s16atmi6Aa4sI?q=).",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,4600 images,"training accurate, widely deployable deep learning algorithms requires a plethora of annotated (ground truth) data. Image collections must contain not only thousands of images",,Cell Segmentation,cell-segmentation-on-evican,,See all 1951 tasks,Cell Segmentation9 benchmarks8,Cell Segmentation9 benchmarks8
Fluo-C3DL-MDA231,Fluo-C3DL-MDA231 Dataset,"MDA231 human breast carcinoma cells infected with a pMSCV vector including the GFP sequence, embedded in a collagen matrix

Dr. R. Kamm. Dept. of Biological Engineering, Massachusetts Institute of Technology, Cambridge MA (USA)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Cell Segmentation,cell-segmentation-on-fluo-c3dl-mda231,,See all 1951 tasks,Cell Segmentation9 benchmarks8,Cell Segmentation9 benchmarks8
Fluo-N2DH-GOWT1,Fluo-N2DH-GOWT1 Dataset,"GFP-GOWT1 mouse stem cells

Dr. E. Bártová. Institute of Biophysics, Academy of Sciences of the Czech Republic, Brno, Czech Republic",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Cell Segmentation, Cell Detection","cell-segmentation-on-fluo-n2dh-gowt1, cell-detection-on-fluo-n2dh-gowt1",,See all 1951 tasks,Cell Segmentation9 benchmarks8,Cell Segmentation9 benchmarks8
Fluo-N2DL-HeLa,Fluo-N2DL-HeLa Dataset,"HeLa cells stably expressing H2b-GFP

Mitocheck Consortium",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Cell Segmentation, Cell Detection","cell-detection-on-fluo-n2dl-hela, cell-segmentation-on-fluo-n2dl-hela",,See all 1951 tasks,Cell Segmentation9 benchmarks8,Cell Segmentation9 benchmarks8
LIVECell,LIVECell Dataset,"The LIVECell (Label-free In Vitro image Examples of Cells) dataset is a large-scale microscopic image dataset for instance-segmentation of individual cells in 2D cell cultures.

LIVECell consists of 5,239 manually annotated, expert-validated, Incucyte HD phase-contrast microscopy images with a total of 1,686,352 individual cells annotated from eight different cell types (average 313 cells per image). The  LIVECell images have predefined splits into training (3188), validation (539) and test (1512) sets. Each split is also further subdivided into each of the eight cell types. The training set also has splits of different sizes (2, 4, 5, 25, 50%) to allow dataset size experimentation.",https://production-media.paperswithcode.com/datasets/cell-example.png,EditCC-BY-NC 4.0,Image,,,,,,"validated, Incucyte HD phase-contrast microscopy images",,"Instance Segmentation, Cell Segmentation",cell-segmentation-on-livecell,,See all 1951 tasks,Cell Segmentation9 benchmarks8,Cell Segmentation9 benchmarks8
NIH3T3,NIH3T3 Dataset,The archive contains original images from NIH3T3 cells stained with Hoechst 33342 as PNG files. It also contains images (as Photoshop and GIMP files) showing hand-segmentation of the Hoechst images into regions containing single nuclei.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Cell Segmentation,,,See all 1951 tasks,Cell Segmentation9 benchmarks8,Cell Segmentation9 benchmarks8
PhC-C2DH-U373,PhC-C2DH-U373 Dataset,"Glioblastoma-astrocytoma U373 cells on a polyacrylamide substrate

Dr. S. Kumar. Department of Bioengineering, University of California at Berkeley, Berkeley CA (USA)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Cell Segmentation, Cell Detection","cell-segmentation-on-phc-c2dh-u373, cell-detection-on-phc-c2dh-u373",,See all 1951 tasks,Cell Segmentation9 benchmarks8,Cell Segmentation9 benchmarks8
STARE,STARE Dataset,"The STARE (Structured Analysis of the Retina) dataset is a dataset for retinal vessel segmentation. It contains 20 equal-sized (700×605) color fundus images. For each image, two groups of annotations are provided..",https://arxiv.org/abs/2009.12053,EditUnknown,"Image, Video",,,,,,,,"Dichotomous Image Segmentation, Retinal Vessel Segmentation, Optic Disc Segmentation, Colorectal Gland Segmentation:, Semantic Segmentation, Cell Segmentation, Video Polyp Segmentation","retinal-vessel-segmentation-on-stare, optic-disc-segmentation-on-stare, colorectal-gland-segmentation-on-stare, cell-segmentation-on-stare, dichotomous-image-segmentation-on-stare, video-polyp-segmentation-on-stare, semantic-segmentation-on-stare",,See all 1951 tasks,Cell Segmentation9 benchmarks8,Cell Segmentation9 benchmarks8
U2OS,U2OS Dataset,The archive contains original images from U2OS cells stained with Hoechst 33342 as PNG files. It also contains images (as Photoshop and GIMP files) showing hand-segmentation of the Hoechst images into regions containing single nuclei.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Cell Segmentation,,,See all 1951 tasks,Cell Segmentation9 benchmarks8,Cell Segmentation9 benchmarks8
ChartQA,ChartQA Dataset,"Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Chart Question Answering,chart-question-answering-on-chartqa,,See all 1951 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
DVQA,DVQA Dataset,DVQA is a synthetic question-answering dataset on images of bar-charts.,https://production-media.paperswithcode.com/datasets/dvqa.jpg,EditAttribution-NonCommercial 4.0 International,"Image, Text",English,,,,,,,"Object Detection, Visual Question Answering (VQA), Chart Question Answering, Question Answering",visual-question-answering-vqa-on-dvqa-test,,See all 1951 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
FigureQA,FigureQA Dataset,"FigureQA is a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts.",/paper/figureqa-an-annotated-figure-dataset-for,EditCustom,"Image, Text",English,,,,000 images,,,"Visual Question Answering (VQA), Chart Question Answering, Visual Reasoning, Question Answering",visual-question-answering-on-figureqa-test-1,,See all 1951 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
LEAF-QA,LEAF-QA Dataset,"LEAF-QA, a comprehensive dataset of 250,000 densely annotated figures/charts, constructed from real-world open data sources, along with ~2 million question-answer (QA) pairs querying the structure and semantics of these charts. LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community. Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz. FigureQA and DVQA, which present only limited variations in chart data. LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering.",https://arxiv.org/pdf/1907.12861,EditUnknown,"Image, Text",English,,,,,,,"Visual Question Answering (VQA), Chart Question Answering, Question Answering",,,See all 1951 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
MMVP,MMVP Dataset,"The MMVP (Multimodal Visual Patterns) Benchmark focuses on identifying ""CLIP-blind pairs"" – images that appear similar to the CLIP model despite having clear visual differences. These patterns highlight the challenges these systems face in answering straightforward questions, often leading to incorrect responses and hallucinated explanations.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Text",English,,,,,,,"Music Question Answering, Chart Question Answering",,,See all 1951 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
PlotQA,PlotQA Dataset,"PlotQA is a VQA dataset with 28.9 million question-answer pairs grounded over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates.
Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not contain variability in data labels, real-valued data, or complex reasoning questions. Consequently, proposed models for these datasets do not fully address the challenge of reasoning over plots. In particular, they assume that the answer comes either from a small fixed size vocabulary or from a bounding box within the image. However, in practice this is an unrealistic assumption because many questions require reasoning and thus have real valued answers which appear neither in a small fixed size vocabulary nor in the image. In this work, we aim to bridge this gap between existing datasets and real world plots by introducing PlotQA. Further, 80.76% of the out-of-vocabulary (OOV) questions in PlotQA have answers that are not in a fixed vocabulary.",https://production-media.paperswithcode.com/datasets/0c7a7156-5529-470c-87cd-09ee847f0f03.png,Editpublic,"Image, Text",English,,,,,,,"Visual Question Answering, Visual Question Answering (VQA), Chart Question Answering, Question Answering","visual-question-answering-on-plotqa-d2, chart-question-answering-on-plotqa, visual-question-answering-on-plotqa-d2-1, visual-question-answering-on-plotqa-d1, visual-question-answering-on-plotqa-d1-1",,See all 1951 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
RealCQA,RealCQA Dataset,"RealCQA
Scientific Chart Question Answering as a Test-bed for First-Order Logic

check on huggingface : https://huggingface.co/datasets/sal4ahm/RealCQA",https://production-media.paperswithcode.com/datasets/a726c05f-49ef-4eea-bdf1-20147163aa1b.png,"EditGNU GENERAL PUBLIC LICENSE                        Version 3, 29 June 2007",Text,English,,,,,,,Chart Question Answering,chart-question-answering-on-realcqa,,See all 1951 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
SBS_Figures,SBS Figures Dataset,"Building a large-scale figure QA dataset requires a considerable amount of work, from gathering and selecting figures to extracting attributes like text, numbers, and colors, and generating QAs. Although recent developments in LLMs have led to efforts to synthesize figures, most of these focus primarily on QA generation. Additionally, creating figures directly using LLMs often encounters issues such as code errors, similar-looking figures, and repetitive content in figures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic Figures), a dataset for pre-training figure QA. Our proposed pipeline enables the creation of chart figures with complete annotations of the visualized data and dense QA annotations without any manual annotation process. Our stage-by-stage pipeline makes it possible to create diverse topic and appearance figures efficiently while minimizing code errors. Our SBSFigures demonstrate a strong pre-training effect, making it possible to achieve efficient training with a limited amount of real-world chart data starting from our pre-trained weights.",https://production-media.paperswithcode.com/datasets/e21ad6a4-a241-4b1d-ae3f-b9f8fa1a7969.png,EditUnknown,Text,English,,,,,,,"Chart Understanding, Chart Question Answering",,,See all 1951 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
Vega-Lite_Chart_Collection,Vega-Lite Chart Collection Dataset,"We present a new collection of 1,981 Vega-Lite specifications, which is used to demonstrate the generalizability and viability of our NL generation framework. This collection is the largest set of human-generated charts obtained from GitHub to date. It covers varying levels of complexity from a simple line chart without any interaction to a chart with four plots where data points are linked with selection interactions. Compared to the benchmarks, our dataset shows the highest average pairwise edit distance between specifications, which proves that the charts are highly diverse from one another. Moreover, it contains the largest number of charts with composite views, interactions (e.g., tooltips, panning & zooming, and linking), and diverse chart types (e.g., map, grid & matrix, diagram, etc.).",https://production-media.paperswithcode.com/datasets/67fb2170-2976-4c57-80d7-922d7402e038.png,EditMIT,Text,English,,,,,,,Chart Question Answering,,,See all 1951 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
CIFAR-10,CIFAR-10 Dataset,"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.",https://production-media.paperswithcode.com/datasets/4fdf2b82-2bc3-4f97-ba51-400322b228b1.png,EditUnknown,"Audio, Graph, Image, Text",English,,,,6000 images,training images and 10000 test images,10,"Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly, Image Classification with Label Noise, Density Estimation, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, ROLSSL-Reversed, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Personalized Federated Learning, Classification, Conditional Image Generation, Anomaly Detection, Dataset Distillation - 1IPC, Continual Learning, Active Learning, Open-World Semi-Supervised Learning, Network Pruning, Graph Classification, Image Clustering, Zero-Shot Learning, Binarization, Novel Class Discovery, Out-of-Distribution Detection, Image Compression, Small Data Image Classification, Image Classification, Nature-Inspired Optimization Algorithm, ROLSSL-Consistent, Sparse Learning and binarization, Semi-Supervised Image Classification (Cold Start), Learning with noisy labels, Stochastic Optimization, Contrastive Learning, Classification with Binary Neural Network, Image Retrieval, Clean-label Backdoor Attack (0.05%), Online Clustering, Supervised Image Retrieval, Adversarial Robustness, ROLSSL-Uniform, Semi-Supervised Image Classification, Unsupervised Image Classification, Object Recognition, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Neural Network Compression, Partial Label Learning, Adversarial Defense, Classification with Binary Weight Network, Model Poisoning, Hard-label Attack, Image Classification with Human Noise, Quantization, Data Augmentation, Robust classification, Sequential Image Classification, Image Generation, Long-tail Learning on CIFAR-10-LT (ρ=100), Neural Architecture Search, Adversarial Attack, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, Self-Supervised Learning, Long-tail Learning, Provable Adversarial Defense, Transductive Zero-Shot Classification, Domain-IL Continual Learning","architecture-search-on-cifar-10-image, image-classification-with-label-noise-on-5, image-clustering-on-cifar-10, semi-supervised-image-classification-cold, neural-architecture-search-on-nats-bench-size-1, anomaly-detection-on-cifar-10, conditional-image-generation-on-cifar-10, robust-classification-on-cifar-10, density-estimation-on-cifar-10-conditional, small-data-image-classification-on-cifar-10-3, image-classification-with-human-noise-on, image-classification-with-label-noise-on-7, data-augmentation-on-cifar-10, nature-inspired-optimization-algorithm-on-1, partial-label-learning-on-cifar-10-partial-2, image-generation-on-cifar-10, long-tail-learning-on-cifar-10-lt-r-100, semi-supervised-image-classification-on-cifar-12, small-data-image-classification-on-cifar10-10, semi-supervised-image-classification-on-cifar-27, image-generation-on-cifar-10-20-data, classification-with-binary-weight-network-on, out-of-distribution-detection-on-cifar-10, continual-learning-on-split-cifar-10-5-tasks, semi-supervised-image-classification-on-cifar-17, binarization-on-cifar-10, unsupervised-anomaly-detection-with-specified-21, anomaly-detection-on-one-class-cifar-10, image-classification-on-cifar-10-image, neural-architecture-search-on-cifar-10, image-classification-with-label-noise-on-8, image-classification-on-cifar-104000, provable-adversarial-defense-on-cifar-10, image-classification-on-cifar10-1, open-world-semi-supervised-learning-on-cifar, personalized-federated-learning-on-cifar-10, image-classification-on-cifar-10-60-symmetric, dataset-distillation-1ipc-on-cifar-10, density-estimation-on-cifar-10, adversarial-robustness-on-cifar-10, partial-label-learning-on-cifar-10-partial-1, self-supervised-learning-on-cifar10, small-data-image-classification-on-cifar-10-1, zero-shot-learning-on-cifar-10, out-of-distribution-detection-on-cifar10-1, semi-supervised-image-classification-on-cifar-6, active-learning-on-cifar10-10000, image-compression-on-cifar-10, long-tail-learning-on-cifar-10-lt-r-10, stochastic-optimization-on-cifar-10-resnet-18, sparse-learning-and-binarization-on-cifar-10, small-data-image-classification-on-cifar-10-2, semi-supervised-image-classification-cold-2, semi-supervised-image-classification-on-cifar-11, model-poisoning-on-cifar-10, stochastic-optimization-on-cifar-10, classification-with-binary-neural-network-on, unsupervised-image-classification-on-cifar-10, rolssl-uniform-on-cifar-10, image-retrieval-on-cifar-10, image-generation-on-cifar-10-10-data, anomaly-detection-on-leave-one-class-out, sequential-image-classification-on-noise, rolssl-consistent-on-cifar-10, transductive-zero-shot-classification-on-7, quantization-on-cifar-10, supervised-image-retrieval-on-cifar-10, image-classification-with-label-noise-on-3, semi-supervised-image-classification-on-cifar-28, unsupervised-anomaly-detection-with-specified-9, adversarial-attack-on-cifar-10, learning-with-noisy-labels-on-cifar-10, self-supervised-learning-on-cifar-10, online-clustering-on-cifar10, clean-label-backdoor-attack-0-05-on-cifar-10, small-data-image-classification-on-cifar-10, image-classification-with-label-noise-on-14, novel-class-discovery-on-cifar10, semi-supervised-image-classification-on-cifar-10, image-classification-with-label-noise-on-4, semi-supervised-image-classification-on-cifar-16, graph-classification-on-cifar-10, unsupervised-anomaly-detection-with-specified-6, semi-supervised-image-classification-on-cifar-15, semi-supervised-image-classification-cold-8, network-pruning-on-cifar-10, graph-classification-on-cifar10-100k, domain-il-continual-learning-on-cifar10-5, semi-supervised-image-classification-on-cifar-7, neural-network-compression-on-cifar-10, contrastive-learning-on-cifar-10, image-classification-on-cifar-10, semi-supervised-image-classification-on-cifar, rolssl-reversed-on-cifar-10, image-classification-with-label-noise-on-6, stochastic-optimization-on-cifar-10-wrn-28-10, hard-label-attack-on-cifar-10, long-tail-learning-on-cifar-10-lt-r-100-on, image-classification-with-label-noise-on-9, adversarial-defense-on-cifar-10, image-classification-on-cifar-10-40-symmetric, semi-supervised-image-classification-on-3, out-of-distribution-detection-on-cifar-10-vs, image-classification-on-cifar-10-with-noisy, partial-label-learning-on-cifar-10-partial, classification-on-cifar10-1, unsupervised-anomaly-detection-with-specified-16, unsupervised-anomaly-detection-with-specified-7",,See all 1951 tasks,Classification370 benchmarks37,Classification370 benchmarks37
CIFAR-100,CIFAR-100 Dataset,"The CIFAR-100 dataset (Canadian Institute for Advanced Research, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 images per class. Each image comes with a ""fine"" label (the class to which it belongs) and a ""coarse"" label (the superclass to which it belongs). There are 500 training images and 100 testing images per class.

The criteria for deciding whether an image belongs to a class were as follows:


The class name should be high on the list of likely answers to the question “What is in this picture?”
The image should be photo-realistic. Labelers were instructed to reject line drawings.
The image should contain only one prominent instance of the object to which the class refers.
The object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.",https://www.cs.toronto.edu/~kriz/cifar.html,EditUnknown,"Audio, Graph, Image, Text",English,,,,600 images,training images and 100 testing images,100,"Image Classification with Label Noise, Personalized Federated Learning, Classification, Conditional Image Generation, Knowledge Distillation, Anomaly Detection, Dataset Distillation - 1IPC, Continual Learning, Class Incremental Learning, Open-World Semi-Supervised Learning, Few-Shot Class-Incremental Learning, Network Pruning, Non-exemplar-based Class Incremental Learning, Image Clustering, Variational Inference, Zero-Shot Learning, Binarization, Learning with coarse labels, Novel Class Discovery, Out-of-Distribution Detection, Small Data Image Classification, Image Classification, Classifier calibration, Sparse Learning and binarization, Data Free Quantization, Learning with noisy labels, Stochastic Optimization, Classification with Binary Neural Network, Bayesian Inference, Incremental Learning, class-incremental learning, Adversarial Robustness, Semi-Supervised Image Classification, Adversarial Defense, Classification with Binary Weight Network, Few-Shot Image Classification, Image Generation, Neural Architecture Search, Adversarial Attack, Self-Supervised Learning, Long-tail Learning, Provable Adversarial Defense, Transductive Zero-Shot Classification","semi-supervised-image-classification-on-cifar-2, provable-adversarial-defense-on-cifar-100, few-shot-image-classification-on-cifar100-5, out-of-distribution-detection-on-cifar-100, image-classification-on-cifar100, incremental-learning-on-cifar-100-50-classes-3, adversarial-defense-on-cifar-100, few-shot-class-incremental-learning-on-cifar, class-incremental-learning-on-cifar100, novel-class-discovery-on-cifar100, image-generation-on-cifar-100, network-pruning-on-cifar-100, class-incremental-learning-on-cifar-100-50-1, semi-supervised-image-classification-on-cifar-8, learning-with-noisy-labels-on-cifar-100, anomaly-detection-on-unlabeled-cifar-10-vs, data-free-quantization-on-cifar-100, classification-with-binary-neural-network-on-2, incremental-learning-on-cifar-100-50-classes, classifier-calibration-on-cifar-100, sparse-learning-and-binarization-on-cifar-100, image-classification-with-label-noise-on-15, stochastic-optimization-on-cifar-100, learning-with-coarse-labels-on-cifar100, personalized-federated-learning-on-cifar-100, self-supervised-learning-on-cifar-100, classification-on-cifar-100, semi-supervised-image-classification-on-cifar-4, out-of-distribution-detection-on-cifar100, open-world-semi-supervised-learning-on-cifar-1, long-tail-learning-on-cifar-100-lt-r-10, incremental-learning-on-cifar-100-50-classes-2, semi-supervised-image-classification-on-cifar-9, classification-with-binary-weight-network-on-2, dataset-distillation-1ipc-on-cifar-100, conditional-image-generation-on-cifar-100, binarization-on-cifar-100, non-exemplar-based-class-incremental-learning, anomaly-detection-on-one-class-cifar-100, continual-learning-on-cifar100-20-tasks, semi-supervised-image-classification-on-cifar-3, classification-on-cifar100, small-data-on-cifar-100-1000-labels-1, class-incremental-learning-on-cifar-100-50-2, adversarial-robustness-on-cifar-100, long-tail-learning-on-cifar-100-lt-r-100, class-incremental-learning-on-cifar100-1, incremental-learning-on-cifar-100-50-classes-1, knowledge-distillation-on-cifar-100, variational-inference-on-cifar100, image-classification-on-cifar-100, image-clustering-on-cifar-100, self-supervised-learning-on-cifar100, bayesian-inference-on-cifar100, transductive-zero-shot-classification-on-8, zero-shot-learning-on-cifar-100, neural-architecture-search-on-cifar-100-1, adversarial-attack-on-cifar-100",,See all 1951 tasks,Classification370 benchmarks37,Classification370 benchmarks37
DTD,DTD Dataset,The Describable Textures Dataset (DTD) contains 5640 texture images in the wild. They are annotated with human-centric attributes inspired by the perceptual properties of textures.,https://arxiv.org/abs/1911.02274,EditCustom (research-only),Image,,,,,,,,"Image Clustering, Prompt Engineering, Few-Shot Learning, Zero-Shot Learning, Classification, Neural Architecture Search, Image Classification, Transductive Zero-Shot Classification","prompt-engineering-on-dtd, neural-architecture-search-on-dtd, classification-on-dtd, zero-shot-learning-on-dtd, image-clustering-on-dtd, few-shot-learning-on-dtd, image-classification-on-dtd, transductive-zero-shot-classification-on-dtd",,See all 1951 tasks,Classification370 benchmarks37,Classification370 benchmarks37
Food-101,Food-101 Dataset,"The  Food-101 dataset consists of 101 food categories with 750 training and 250 test images per category, making a total of 101k images. The labels for the test images have been manually cleaned, while the training set contains some noise.",https://arxiv.org/abs/1712.08730,EditUnknown,"Image, Text",English,,,,101k images,training and 250 test images,,"Image Clustering, Fine-Grained Image Classification, Learning with noisy labels, Zero-Shot Transfer Image Classification, Prompt Engineering, Multimodal Text and Image Classification, Few-Shot Learning, Document Text Classification, Zero-Shot Learning, Classification, Multi-Modal Document Classification, Neural Architecture Search, Image Compression, Image Classification, Transductive Zero-Shot Classification","multi-modal-document-classification-on-food, zero-shot-transfer-image-classification-on-17, transductive-zero-shot-classification-on-food, image-compression-on-food-101, fine-grained-image-classification-on-food-101, neural-architecture-search-on-food-101, few-shot-learning-on-food101, multimodal-text-and-image-classification-on-1, zero-shot-learning-on-food-101, learning-with-noisy-labels-on-food-101, classification-on-food101, prompt-engineering-on-food-101, image-clustering-on-food-101, image-classification-on-food-101-1, document-text-classification-on-food-101",,See all 1951 tasks,Classification370 benchmarks37,Classification370 benchmarks37
GLUE,GLUE Dataset,"General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.",https://arxiv.org/abs/1908.06725,EditCustom (various),"Image, Text",English,,,,,,,"Linguistic Acceptability, Semantic Textual Similarity within Bi-Encoder, Stochastic Optimization, Model Compression, Few-Shot Learning, Semantic Textual Similarity, Natural Language Understanding, Data-free Knowledge Distillation, Natural Language Inference, Classification, QQP, Text Classification","few-shot-learning-on-glue-qqp, text-classification-on-glue-rte, data-free-knowledge-distillation-on-qnli, natural-language-inference-on-mrpc, natural-language-inference-on-qnli, text-classification-on-glue-mrpc, text-classification-on-sst-2, text-classification-on-glue-sst2, natural-language-inference-on-wnli, semantic-textual-similarity-on-mrpc, stochastic-optimization-on-cola, semantic-textual-similarity-within-bi-encoder, text-classification-on-glue-cola, few-shot-learning-on-mrpc, text-classification-on-glue-qqp, natural-language-understanding-on-glue, classification-on-sst-2, qqp-on-qqp, text-classification-on-glue, model-compression-on-qnli, natural-language-inference-on-glue, text-classification-on-glue-stsb, linguistic-acceptability-on-cola, classification-on-rte, natural-language-inference-on-rte",,See all 1951 tasks,Classification370 benchmarks37,Classification370 benchmarks37
ImageNet,ImageNet Dataset,"The ImageNet dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection.
The publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld.
ILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”.
The ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.


Total number of non-empty WordNet synsets: 21841
Total number of images: 14197122
Number of images with bounding box annotations: 1,034,908
Number of synsets with SIFT features: 1000
Number of images with SIFT features: 1.2 million",https://arxiv.org/abs/1409.0575,"EditCustom (research, non-commercial)","3D, Graph, Image, Text",English,2010,,,,,,"Sparse Learning, Weakly Supervised Object Detection, Classification, Knowledge Distillation, Network Pruning, Image Super-Resolution, Image Compressed Sensing, Image Clustering, Feature Upsampling, Prompt Engineering, Zero-Shot Composed Image Retrieval (ZS-CIR), Medical Image Classification, Zero-Shot Learning, Binarization, Visual Question Answering (VQA), Image Classification, Image Reconstruction, Zero-Shot Transfer Image Classification, Weakly-Supervised Object Localization, Contrastive Learning, Model Compression, Few-Shot Learning, Classification with Binary Neural Network, Image Classification with Differential Privacy, Adversarial Robustness, Semi-Supervised Image Classification, Unsupervised Image Classification, Image Deblurring, Object Recognition, Image Colorization, Adversarial Defense, Image Inpainting, Few-Shot Image Classification, Quantization, Data Augmentation, Self-Supervised Image Classification, Zero-Shot Transfer Image Classification (CN), Color Image Denoising, Image Generation, Neural Architecture Search, Transductive Zero-Shot Classification, JPEG Decompression","weakly-supervised-object-localization-on-2, zero-shot-learning-on-imagenet, image-classification-with-dp-on-imagenet, quantization-on-imagenet, visual-question-answering-vqa-on-imagenet, transductive-zero-shot-classification-on, image-colorization-on-imagenet, zero-shot-composed-image-retrieval-zs-cir-on-5, few-shot-image-classification-on-imagenet-5, zero-shot-transfer-image-classification-on-1, semi-supervised-image-classification-on-2, self-supervised-image-classification-on-1, binarization-on-imagenet, sparse-learning-on-imagenet, color-image-denoising-on-imagenet-sigma100, color-image-denoising-on-imagenet-sigma200, classification-on-imagenet-1k, zero-shot-transfer-image-classification-on-3, semi-supervised-image-classification-on-1, prompt-engineering-on-imagenet-v2, medical-image-classification-on-imagenet, neural-architecture-search-on-imagenet, image-classification-on-imagenet-v2, unsupervised-image-classification-on-imagenet, contrastive-learning-on-imagenet-1k, semi-supervised-image-classification-on-16, data-augmentation-on-imagenet, image-super-resolution-on-imagenet, network-pruning-on-imagenet, jpeg-decompression-on-imagenet, adversarial-defense-on-imagenet, color-image-denoising-on-imagenet-sigma250, weakly-supervised-object-detection-on, few-shot-image-classification-on-imagenet-10, knowledge-distillation-on-imagenet, zero-shot-transfer-image-classification-cn-on, classification-with-binary-neural-network-on-1, image-clustering-on-imagenet, color-image-denoising-on-imagenet-sigma50, color-image-denoising-on-imagenet-sigma150, few-shot-image-classification-on-imagenet-1-1, image-deblurring-on-imagenet, image-inpainting-on-imagenet, model-compression-on-imagenet, prompt-engineering-on-imagenet, image-classification-on-imagenet-1k, self-supervised-image-classification-on, image-classification-on-imagenet, feature-upsampling-on-imagenet, adversarial-robustness-on-imagenet, image-compressed-sensing-on-imagenet, image-reconstruction-on-imagenet, image-clustering-on-imagenet-1k, few-shot-image-classification-on-imagenet-0",,See all 1951 tasks,Classification370 benchmarks37,Classification370 benchmarks37
SST-2,SST-2 Dataset,"The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.

Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,2005,,,,,,"Few-Shot Learning, Explanation Fidelity Evaluation, Classification, Text Classification, Sentiment Analysis","text-classification-on-sst2, few-shot-learning-on-sst-2-binary, text-classification-on-sst-2, sentiment-analysis-on-sst-2-binary, explanation-fidelity-evaluation-on-sst2, classification-on-sst-2",,See all 1951 tasks,Classification370 benchmarks37,Classification370 benchmarks37
SST,SST Dataset,"The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser and includes a total of 215,154 unique phrases
from those parse trees, each annotated by 3 human judges.

Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive.
The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.",https://production-media.paperswithcode.com/datasets/sst.jpg,EditUnknown,"Image, Text",English,2005,,,,,5,"Few-Shot Text Classification, Few-Shot Learning, Explanation Fidelity Evaluation, Classification, Out-of-Distribution Detection, Text Classification, Sentiment Analysis","out-of-distribution-detection-on-sst, sentiment-analysis-on-sst-5-fine-grained, explanation-fidelity-evaluation-on-sst-5, text-classification-on-sst2, few-shot-text-classification-on-sst-5, few-shot-learning-on-sst-2-binary, text-classification-on-sst-2, sentiment-analysis-on-sst-2-binary, explanation-fidelity-evaluation-on-sst2, classification-on-sst-2",,See all 1951 tasks,Classification370 benchmarks37,Classification370 benchmarks37
CAT__Context_Adjustment_Training,CAT: Context Adjustment Training Dataset,"CAT is a specialized dataset for co-saliency detection. This dataset is intended for both helping to assess the performance of vision algorithms and supporting research that aims to exploit large volumes of annotated data, e.g., for training deep neural networks.

Scale & Features
- A total number of 33500 image samples.
- 280 semantic groups affiliated to 15 superclasses.
- High-quality mask annotations.
- Diverse visual context with multiple foreground objects.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,Image,,,,,,,,Co-Salient Object Detection,,,See all 1951 tasks,Co-Salient Object Detection4 b,Co-Salient Object Detection4 b
CoSal2015,CoSal2015 Dataset,"Cosal2015 is a large-scale dataset for co-saliency detection which consists of 2,015 images of 50 categories, and each group suffers from various challenging factors such as complex environments, occlusion issues, target appearance variations and background clutters, etc. All these increase the difficulty for accurate co-saliency detection.",https://arxiv.org/abs/2003.06167,EditUnknown,Image,,,,,015 images,,50,Co-Salient Object Detection,co-salient-object-detection-on-cosal2015,,See all 1951 tasks,Co-Salient Object Detection4 b,Co-Salient Object Detection4 b
Cell,Cell Dataset,The CELL benchmark is made of fluorescence microscopy images of cell.,/paper/multi-domain-adversarial-learning-1,EditUnknown,Image,,,,,,,,"Medical Image Segmentation, Nuclear Segmentation, Color Image Denoising","color-image-denoising-on-cellnet, nuclear-segmentation-on-cell17, medical-image-segmentation-on-cell",,See all 1951 tasks,Color Image Denoising80 benchm,Color Image Denoising80 benchm
Darmstadt_Noise_Dataset,Darmstadt Noise Dataset Dataset,the dataset contains data about hydrogen storage in metal hydrides,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Color Image Denoising, Denoising","color-image-denoising-on-darmstadt-noise, denoising-on-darmstadt-noise-dataset",,See all 1951 tasks,Color Image Denoising80 benchm,Color Image Denoising80 benchm
McMaster,McMaster Dataset,"The McMaster dataset is a dataset for color demosaicing, which contains 18 cropped images of size 500×500.",https://arxiv.org/abs/1710.04026,EditUnknown,Image,,,,,,,,"Color Image Denoising, Joint Demosaicing and Denoising","joint-demosaicing-and-denoising-on-mcmaster, color-image-denoising-on-mcmaster-sigma35, color-image-denoising-on-mcmaster-sigma75, color-image-denoising-on-mcmaster-sigma15, color-image-denoising-on-mcmaster-sigma50, color-image-denoising-on-mcmaster-sigma25",,See all 1951 tasks,Color Image Denoising80 benchm,Color Image Denoising80 benchm
RENOIR,RENOIR Dataset,"A dataset of color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes.",https://production-media.paperswithcode.com/datasets/23ade962-4173-494e-88d4-79324f249e2b.jpg,EditFree,Image,,,,,,,,"Color Image Denoising, Image Denoising",color-image-denoising-on-renoir,,See all 1951 tasks,Color Image Denoising80 benchm,Color Image Denoising80 benchm
CIRCO,CIRCO Dataset,"CIRCO (Composed Image Retrieval on Common Objects in context) is an open-domain benchmarking dataset for Composed Image Retrieval (CIR) based on real-world images from COCO 2017 unlabeled set. It is the first CIR dataset with multiple ground truths and aims to address the problem of false negatives in existing datasets. CIRCO comprises a total of 1020 queries, randomly divided into 220 and 800 for the validation and test set, respectively, with an average of 4.53 ground truths per query.",https://arxiv.org/pdf/2303.15247v1.pdf,EditCreative Commons BY-NC 4.0,Image,,2017,,,,,,"Zero-Shot Composed Image Retrieval (ZS-CIR), Image Retrieval, Composed Image Retrieval (CoIR)",zero-shot-composed-image-retrieval-zs-cir-on,,See all 1951 tasks,Composed Image Retrieval  CoIR,Composed Image Retrieval  CoIR
CIRR,CIRR Dataset,"Composed Image Retrieval (or, Image Retreival conditioned on Language Feedback) is a relatively new retrieval task, where an input query consists of an image and short textual description of how to modify the image. 

For humans, the advantage of a bi-modal query is clear: some concepts and attributes are more succinctly described visually, others through language. By cross-referencing the two modalities, a reference image can capture the general gist of a scene, while the text can specify finer details.

We identify a major challenge of this task as the inherent ambiguity in knowing what information is important (typically one object of interest in the scene) and what can be ignored (e.g., the background and other irrelevant objects).

We release the first dataset of open-domain, real-life images with human-generated modification sentences, which support research on one-shot composed image retrieval, dialogue systems, fine-grained visiolinguistic reasoning, and more.",https://production-media.paperswithcode.com/datasets/repository-open-graph-template_copy.png,EditMIT License,Image,,,,,,,,"Zero-Shot Composed Image Retrieval (ZS-CIR), Image Retrieval, Composed Image Retrieval (CoIR)","composed-image-retrieval-coir-on-cirr-1, image-retrieval-on-cirr, zero-shot-composed-image-retrieval-zs-cir-on-1",,See all 1951 tasks,Composed Image Retrieval  CoIR,Composed Image Retrieval  CoIR
Fashion_IQ,Fashion IQ Dataset,Fashion IQ support and advance research on interactive fashion image retrieval. Fashion IQ is the first fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual attribute labels for these images.,/paper/the-fashion-iq-dataset-retrieving-images-by,EditUnknown,Image,,,,,,,,"Image Retrieval with Multi-Modal Query, Zero-Shot Composed Image Retrieval (ZS-CIR), Image Retrieval, Virtual Try-on, Composed Image Retrieval (CoIR)","composed-image-retrieval-coir-on-fashion-iq, virtual-try-on-on-fashioniq, image-retrieval-with-multi-modal-query-on-1, image-retrieval-on-fashion-iq, zero-shot-composed-image-retrieval-zs-cir-on-2, zero-shot-composed-image-retrieval-zs-cir-on-3",,See all 1951 tasks,Composed Image Retrieval  CoIR,Composed Image Retrieval  CoIR
LaSCo,LaSCo Dataset,"Large Scale Composed Image Retrieval (LaSCo) is a new dataset for Composed Image Retrieval (CoIR), x10 times larger than current ones.",https://production-media.paperswithcode.com/datasets/b141347d-5111-4e03-84f7-ce9dd93ac3fe.png,EditBY-NC-ND,Image,,,,,,,,"Image Retrieval, Composed Image Retrieval (CoIR)",image-retrieval-on-lasco,,See all 1951 tasks,Composed Image Retrieval  CoIR,Composed Image Retrieval  CoIR
PatternCom,PatternCom Dataset,"PatternCom is a composed image retrieval benchmark based on PatternNet. PatternNet is a large-scale high-resolution remote sensing image retrieval dataset. There are 38 classes and each class has 800 images of size 256×256 pixels. In PatternCom, we select some classes to be depicted in query images, and add a query text that defines an attribute relevant to that class. For instance, query images of “swimming pools” are combined with text queries defining “shape” as “rectangular”, “oval”, and “kidney-shaped”. In total, PatternCom includes six attributes consisted of up to four different classes each. Each attribute can be associated with two to five values per class. The number of positives ranges from 2 to 1345 and there are more than 21k queries in total.",https://production-media.paperswithcode.com/datasets/f68b83a3-7a63-4732-8174-88672a825ac2.png,EditUnknown,Image,,,,,800 images,val benchmark based on PatternNet. PatternNet is a large-scale high-resolution remote sensing image retrieval dataset. There are 38 classes and each class has 800 images,38,"Zero-Shot Composed Image Retrieval (ZS-CIR), Composed Image Retrieval (CoIR)",zero-shot-composed-image-retrieval-zs-cir-on-10,,See all 1951 tasks,Composed Image Retrieval  CoIR,Composed Image Retrieval  CoIR
WebVid-CoVR,WebVid-CoVR Dataset,"The WebVid-CoVR dataset is a collection of video-text-video triplets that can be used for the task of composed video retrieval (CoVR). CoVR is a task that involves searching for videos that match both a query image and a query text. The text typically specifies the desired modification to the query image. 

The WebVid-CoVR dataset is automatically generated from web-scraped video-caption pairs, using a language model to generate the modification text. The dataset contains 1.6 million triplets, with diverse content and variations. The dataset also includes a manually annotated test set of 2.5K triplets, which can be used to evaluate CoVR models.",https://production-media.paperswithcode.com/datasets/abffcd90-bfc8-4e9e-9d81-f4b4d6d55fa6.gif,EditUnknown,"Image, Video",,,,,,,,"Zero-Shot Composed Image Retrieval (ZS-CIR), Composed Video Retrieval (CoVR), Composed Image Retrieval (CoIR)",composed-video-retrieval-covr-on-covr,,See all 1951 tasks,Composed Image Retrieval  CoIR,Composed Image Retrieval  CoIR
AO-CLEVr,AO-CLEVr Dataset,"AO-CLEVr is a new synthetic-images dataset containing images of ""easy"" Attribute-Object categories, based on the CLEVr. AO-CLEVr has attribute-object pairs created from 8 attributes: { red, purple, yellow, blue, green, cyan, gray, brown } and 3 object shapes {sphere, cube, cylinder}, yielding 24 attribute-object pairs. Each pair consists of 7500 images. Each image has a single object that consists of the attribute-object pair. The object is randomly assigned one of two sizes (small/large), one of two materials (rubber/metallic), a random position, and random lightning according to CLEVr defaults.",https://github.com/nv-research-israel/causal_comp,EditUnknown,,,,,,7500 images,,,"Compositional Zero-Shot Learning, Zero-Shot Learning",,,See all 1951 tasks,Compositional Zero-Shot Learni,Compositional Zero-Shot Learni
C-GQA,C-GQA Dataset,"We propose a split built on top of Stanford GQA dataset originally proposed for VQA and name it Compositional GQA (C-GQA) dataset (see supplementary for the details). CGQA contains over 9.5k compositional labels making it the most extensive dataset for CZSL. With cleaner labels and a larger label space, our hope is that this dataset will inspire further research on the topic.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Compositional Zero-Shot Learning,,,See all 1951 tasks,Compositional Zero-Shot Learni,Compositional Zero-Shot Learni
MIT-States,MIT-States Dataset,"The MIT-States dataset has 245 object classes, 115 attribute classes and ∼53K images. There is a wide range of objects (e.g., fish, persimmon, room) and attributes (e.g., mossy, deflated, dirty). On average, each object instance is modified by one of the 9 attributes it affords.",https://arxiv.org/abs/1803.09851,EditUnknown,Image,,,,,53K images,,,"Image Retrieval with Multi-Modal Query, Compositional Zero-Shot Learning, Zero-Shot Learning","compositional-zero-shot-learning-on-mit-2, image-retrieval-with-multi-modal-query-on-mit, compositional-zero-shot-learning-on-mit-3, zero-shot-learning-on-mit-states-1",,See all 1951 tasks,Compositional Zero-Shot Learni,Compositional Zero-Shot Learni
ReaSCAN,ReaSCAN Dataset,ReaSCAN is a synthetic navigation task that requires models to reason about surroundings over syntactically difficult languages.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution 4.0 International License,Text,English,,,,,,,"Compositional Zero-Shot Learning, Vision-Language Navigation, Compositional Generalization (AVG)",compositional-generalization-avg-on-reascan,,See all 1951 tasks,Compositional Zero-Shot Learni,Compositional Zero-Shot Learni
UT-Zappos50K,UT-Zappos50K Dataset,"UT Zappos50K (UT-Zap50K) is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories — shoes, sandals, slippers, and boots — followed by functional types and individual brands. The shoes are centered on a white background and pictured in the same orientation for convenient analysis.
This dataset is created in the context of an online shopping task, where users pay special attentions to fine-grained visual differences. For instance, it is more likely that a shopper is deciding between two pairs of similar men's running shoes instead of between a woman's high heel and a man's slipper. GIST and LAB color features are provided. In addition, each image has 8 associated meta-data (gender, materials, etc.) labels that are used to filter the shoes on Zappos.com.
We introduced this dataset in the context of a pairwise comparison task, where the goal is to predict which of two images more strongly exhibits a visual attribute. When given a novel image pair, we want to answer the question, “Does Image A contain more or less of an attribute than Image B?” Both training and evaluation are performed using pairwise labels.
However, the usefulness of this dataset extends beyond the comparison task that we’ve demonstrated. The meta-data labels and the large size of the dataset makes it suitable for other tasks as well, such as:

category/brand classification
fine-grained attribute learning with rationales
gender-specific style matching
zero-shot learning",https://production-media.paperswithcode.com/datasets/55e2cfef-a972-43ec-8e7a-87822b115d21.jpg,EditUnknown,,,,,,,,,Compositional Zero-Shot Learning,,,See all 1951 tasks,Compositional Zero-Shot Learni,Compositional Zero-Shot Learni
UT_Zappos50K,UT Zappos50K Dataset,"UT Zappos50K is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories — shoes, sandals, slippers, and boots — followed by functional types and individual brands. The shoes are centered on a white background and pictured in the same orientation for convenient analysis.",http://vision.cs.utexas.edu/projects/finegrained/utzap50k/,"EditCustom (academic, non-commercial)","Image, Text",English,,,,,,,"Image-to-Image Translation, Few-Shot Image Classification, Image Captioning, Compositional Zero-Shot Learning, Image Generation","few-shot-image-classification-on-ut-zappos50k, compositional-zero-shot-learning-on-ut",,See all 1951 tasks,Compositional Zero-Shot Learni,Compositional Zero-Shot Learni
Conformal_Prediction242_papers_with_code_Dataset,Conformal Prediction242 papers with code Dataset,,https://paperswithcode.com/dataset/conformal-prediction,,,,,,,,,,,,,See all 1951 tasks,Conformal Prediction242 papers,Conformal Prediction242 papers
European_Flood_2013_Dataset,European Flood 2013 Dataset Dataset,"This dataset consists of 3,710 flood images, annotated by domain experts regarding their relevance with respect to three tasks (determining the flooded area, inundation depth, water pollution).",https://github.com/cvjena/eu-flood-dataset,EditUnknown,Image,,,,,,,,"Content-Based Image Retrieval, Image Retrieval",,,See all 1951 tasks,Content-Based Image Retrieval3,Content-Based Image Retrieval3
GPR1200,GPR1200 Dataset,"Most publications that aim to optimize neural networks for CBIR, train and test their models on domain specific datasets. It is therefore unclear, if those networks can be used as a general-purpose image feature extractor. After analyzing popular image retrieval test sets we decided to manually curate GPR1200, an easy to use and accessible but challenging benchmark dataset with 1200 categories and 10 class examples. Classes and images were manually selected from six publicly available datasets of different image areas, ensuring high class diversity and clean class boundaries.

This dataset can therefore be used for benchmarking image descriptor systems on their generalizability.",https://production-media.paperswithcode.com/datasets/98336806-4a94-445f-804a-e339c246ce75.jpg,EditCustom,Image,,,,,,"train and test their models on domain specific datasets. It is therefore unclear, if those networks can be used as a general-purpose image feature extractor. After analyzing popular image retrieval test sets we decided to manually curate GPR1200, an easy to use and accessible but challenging benchmark dataset with 1200 categories and 10 class examples",1200,"Content-Based Image Retrieval, Image Retrieval",,,See all 1951 tasks,Content-Based Image Retrieval3,Content-Based Image Retrieval3
INRIA_Holidays_Dataset,INRIA Holidays Dataset Dataset,"The Holidays dataset is a set of images which mainly contains some of the authors' personal holidays photos. The remaining ones were taken on purpose to test the robustness to various attacks: rotations, viewpoint and illumination changes, blurring, etc. The dataset includes a very large variety of scene types (natural, man-made, water and fire effects, etc) and images are in high resolution. The dataset contains 500 image groups, each of which represents a distinct scene or object. The first image of each group is the query image and the correct retrieval results are the other images of the group.",http://lear.inrialpes.fr/~jegou/data.php#holidays,EditUnknown,Image,,,,,,"test the robustness to various attacks: rotations, viewpoint and illumination changes, blurring, etc. The dataset includes a very large variety of scene types (natural, man-made, water and fire effects, etc) and images",,Content-Based Image Retrieval,content-based-image-retrieval-on-inria-1,,See all 1951 tasks,Content-Based Image Retrieval3,Content-Based Image Retrieval3
Oxford5k,Oxford5k Dataset,"Oxford5K is the Oxford Buildings Dataset, which contains 5062 images collected from Flickr. It offers a set of 55 queries for 11 landmark buildings, five for each landmark.",https://arxiv.org/abs/1907.05793,EditCustom,Image,,,,,5062 images,,,"Image Classification, Content-Based Image Retrieval, Image Retrieval",image-retrieval-on-oxford5k,,See all 1951 tasks,Content-Based Image Retrieval3,Content-Based Image Retrieval3
PS-Battles,PS-Battles Dataset,"The PS-Battles dataset is gathered from a large community of image manipulation enthusiasts and provides a basis for media derivation and manipulation detection in the visual domain. The dataset consists of 102'028 images grouped into 11'142 subsets, each containing the original image as well as a varying number of manipulated derivatives.",/paper/180404866,EditUnknown,Image,,,,,028 images,,,"Object Detection, Image Classification, Content-Based Image Retrieval",,,See all 1951 tasks,Content-Based Image Retrieval3,Content-Based Image Retrieval3
CORe50,CORe50 Dataset,CORe50 is a dataset designed for assessing Continual Learning techniques in an Object Recognition context.,/paper/core50-a-new-dataset-and-benchmark-for,EditCC BY 4.0,Image,,,,,,,,"Object Recognition, Continual Learning, Incremental Learning",,,See all 1951 tasks,Continual Learning61 benchmark,Continual Learning61 benchmark
Permuted_MNIST,Permuted MNIST Dataset,"Permuted MNIST is an MNIST variant that consists of 70,000 images of handwritten digits from 0 to 9, where 60,000 images are used for training, and 10,000 images for test. The difference of this dataset from the original MNIST is that each of the ten tasks is the multi-class classification of a different random permutation of the input pixels.",https://arxiv.org/abs/1708.01547,EditUnknown,,,,,,000 images,"training, and 10,000 images",,"Continual Learning, Incremental Learning, Domain-IL Continual Learning","continual-learning-on-permuted-mnist, domain-il-continual-learning-on-permuted",,See all 1951 tasks,Continual Learning61 benchmark,Continual Learning61 benchmark
Sketch,Sketch Dataset,"The Sketch dataset contains over 20,000 sketches evenly distributed over 250 object categories.",http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/,EditCC BY 4.0,,,,,,,,,Continual Learning,continual-learning-on-sketch-fine-grained-6,,See all 1951 tasks,Continual Learning61 benchmark,Continual Learning61 benchmark
Continual_Named_Entity_Recognition3_benchmarks3_pa,Continual Named Entity Recognition3 benchmarks3 papers with code Dataset,,https://paperswithcode.com/dataset/continual-named-entity-recognition,,,,,,,,,,,,,See all 1951 tasks,Continual Named Entity Recogni,Continual Named Entity Recogni
Continuous_Object_Recognition2_papers_with_code_Da,Continuous Object Recognition2 papers with code Dataset,,https://paperswithcode.com/dataset/continuous-object-recognition,,,,,,,,,,,,,See all 1951 tasks,Continuous Object Recognition2,Continuous Object Recognition2
DAD,DAD Dataset,"Contains normal driving videos together with a set of anomalous actions in its training set. In the test set of the DAD dataset, there are unseen anomalous actions that still need to be winnowed out from normal driving.",/paper/driver-anomaly-detection-a-dataset-and,EditUnknown,Image,,,,,,,,"Anomaly Detection, Contrastive Learning, Open Set Learning",,,See all 1951 tasks,Contrastive Learning4 benchmar,Contrastive Learning4 benchmar
FIVR-200K,FIVR-200K Dataset,"The FIVR-200K dataset has been collected to simulate the problem of Fine-grained Incident Video Retrieval (FIVR). The dataset comprises 225,960 videos associated with 4,687 Wikipedia events and 100 selected video queries.",https://production-media.paperswithcode.com/datasets/ca77f64b-c8a2-488c-a782-0f1320fa7d06.png,EditUnknown,Video,,,,,,,,"Contrastive Learning, Video Retrieval",video-retrieval-on-fivr-200k,,See all 1951 tasks,Contrastive Learning4 benchmar,Contrastive Learning4 benchmar
GuitarSet,GuitarSet Dataset,"GuitarSet is a dataset of high-quality guitar recordings and rich annotations. It contains 360 excerpts 30 seconds in length. The 360 excerpts are the result of the following combinations:


6 players,
2 versions: comping and soloing,
5 styles: Rock, Singer-Songwriter, Bossa Nova, Jazz, and Funk,
3 progressions: 12 Bar Blues, Autumn Leaves, and Pachelbel Canon,
2 tempi: slow and fast.

Each excerpt is annotated with 6 pitch contour and midi note annotations (one per string), 2 chord annotations (instructed and performed), beat and tempo annotations.",https://guitarset.weebly.com/,EditUnknown,"Audio, Image, Video",,,,,,,,"Contrastive Learning, Beat Tracking, Music Information Retrieval, Information Retrieval, Downbeat Tracking","beat-tracking-on-guitarset, downbeat-tracking-on-guitarset",,See all 1951 tasks,Contrastive Learning4 benchmar,Contrastive Learning4 benchmar
INRIA_Aerial_Image_Labeling,INRIA Aerial Image Labeling Dataset,The INRIA Aerial Image Labeling dataset is comprised of 360 RGB tiles of 5000×5000px with a spatial resolution of 30cm/px on 10 cities across the globe. Half of the cities are used for training and are associated to a public ground truth of building footprints. The rest of the dataset is used only for evaluation with a hidden ground truth. The dataset was constructed by combining public domain imagery and public domain official building footprints.,https://arxiv.org/abs/1909.01671,EditUnknown,Image,,,,,,,,"Self-Supervised Learning, Contrastive Learning, Semantic Segmentation",semantic-segmentation-on-inria-aerial-image,,See all 1951 tasks,Contrastive Learning4 benchmar,Contrastive Learning4 benchmar
OLIVES_Dataset,OLIVES Dataset Dataset,"Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. While the clinical labels, fundus images and OCT scans are instrumental measurements, the vectorized biomarkers are interpreted attributes from the other measurements. Clinical practitioners use all these data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between these relevant data modalities. Existing datasets are limited in that: (i) they view the problem as disease prediction without assessing biomarkers, and (ii) they do not consider the explicit relationship among all four data modalities over the treatment period. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitations. This is the first OCT and fundus dataset that includes clinical labels, biomarker labels, and time-series patient treatment information from associated clinical trials. The dataset consists of 1268 fundus eye images each with 49 OCT scans, and 16 biomarkers, along with 3 clinical labels and a disease diagnosis of DR or DME. In total, there are 96 eyes' data averaged over a period of at least two years with each eye treated for an average of 66 weeks and 7 injections. OLIVES dataset has advantages in other fields of machine learning research including self-supervised learning as it provides alternate augmentation schemes that are medically grounded.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Multimodal Deep Learning, Contrastive Learning, Semi-supervised Medical Image Classification",,,See all 1951 tasks,Contrastive Learning4 benchmar,Contrastive Learning4 benchmar
controllable_image_captioning8_papers_with_code_Da,controllable image captioning8 papers with code Dataset,,https://paperswithcode.com/dataset/controllable-image-captioning,,,,,,,,,,,,,See all 1951 tasks,controllable image captioning8,controllable image captioning8
Earth_on_Canvas,Earth on Canvas Dataset,"A Zero-Shot Sketch-based Inter-Modal Object Retrieval Scheme for Remote Sensing Images

WITH the advancement in sensor technology, huge amounts of data are being collected from various satellites. Hence, the task of target-based data retrieval and acquisition has become exceedingly challenging. Existing satellites essentially scan a vast overlapping region of the Earth using various sensing techniques, like multi-spectral, hyperspectral, Synthetic Aperture Radar (SAR), video, and compressed sensing, to name a few. With increasing complexity and different sensing techniques at our disposal, it has become our primary interest to design efficient algorithms to retrieve data from multiple data modalities, given the complementary information that is captured by different sensors. This type of problem is referred to as inter-modal data retrieval. In remote sensing (RS), there are primarily two important types of problems, i.e., land-cover classification and object detection. In this work, we focus on the target-based object retrieval part, which falls under the realm of object detection in RS. Object retrieval essentially requires high-resolution imagery for objects to be distinctly visible in the image. The main challenge with the conventional retrieval approach using large-scale databases is that, quite often, we do not have any query image sample of the target class at our disposal. The target of interest solely exists as a perception to the user in the form of an imprecise sketch. In such situations where a photo query is absent, it can be immensely useful if we can promptly make a quick hand-made sketch of the target. Sketches are a highly symbolic and hieroglyphic representation of data. One can exploit the notion of this minimalistic representative of sketch queries for sketch-based image retrieval (SBIR) framework. While dealing with satellite images, it is imperative to collect as many samples of images as possible for each object class for object recognition with a high success rate. However, in general, there exists a considerable number of classes for which we seldom have any training data samples. Therefore, for such classes, we can use the zero-shot learning (ZSL) strategy. The ZSL approach aims to solve a task without receiving any example of that task during the training phase. This makes the network capable of handling an unseen class (new class) sample obtained during the inference phase upon deployment of the network. Hence, we propose the aerial sketch-image dataset, namely Earth on Canvas dataset.

Classes in this dataset:
Airplane, Baseball Diamond, Buildings, Freeway, Golf Course, Harbor, Intersection, Mobile home park, Overpass, Parking lot,  River, Runway, Storage tank, Tennis court.",https://production-media.paperswithcode.com/datasets/9352c765-33b5-44f4-b377-3165dfb4a0dd.png,Editcreative commons,,,,,,,"val and acquisition has become exceedingly challenging. Existing satellites essentially scan a vast overlapping region of the Earth using various sensing techniques, like multi-spectral, hyperspectral, Synthetic Aperture Radar (SAR), video, and compressed sensing, to name a few. With increasing complexity and different sensing techniques at our disposal, it has become our primary interest to design efficient algorithms to retrieve data from multiple data modalities, given the complementary information that is captured by different sensors. This type of problem is referred to as inter-modal data retrieval. In remote sensing (RS), there are primarily two important types of problems, i.e., land-cover classification and object detection. In this work, we focus on the target-based object retrieval part, which falls under the realm of object detection in RS. Object retrieval essentially requires high-resolution imagery for objects to be distinctly visible in the image. The main challenge with the conventional retrieval approach using large-scale databases is that, quite often, we do not have any query image sample of the target class at our disposal. The target of interest solely exists as a perception to the user in the form of an imprecise sketch. In such situations where a photo query is absent, it can be immensely useful if we can promptly make a quick hand-made sketch of the target. Sketches are a highly symbolic and hieroglyphic representation of data. One can exploit the notion of this minimalistic representative of sketch queries for sketch-based image retrieval (SBIR) framework. While dealing with satellite images",,"Cross-Modal Retrieval, Cross-Domain Few-Shot, Zero-Shot Cross-Modal Retrieval",,,See all 1951 tasks,Cross-Domain Few-Shot10 benchm,Cross-Domain Few-Shot10 benchm
Places205,Places205 Dataset,"The Places205 dataset is a large-scale scene-centric dataset with 205 common scene categories. The training dataset contains around 2,500,000 images from these categories. In the training set, each scene category has the minimum 5,000 and maximum 15,000 images. The validation set contains 100 images per category (a total of 20,500 images), and the testing set includes 200 images per category (a total of 41,000 images).",https://arxiv.org/abs/1610.01119,EditCC BY,Image,,,,,000 images,"training dataset contains around 2,500,000 images",,"Scene Recognition, Image Classification, Cross-Domain Few-Shot","image-classification-on-places205, cross-domain-few-shot-on-places",,See all 1951 tasks,Cross-Domain Few-Shot10 benchm,Cross-Domain Few-Shot10 benchm
Clotho,Clotho Dataset,"Clotho is an audio captioning dataset, consisting of 4981 audio samples, and each audio sample has five captions (a total of 24 905 captions). Audio samples are of 15 to 30 s duration and captions are eight to 20 words long.",https://zenodo.org/record/3490684,EditOther (Attribution),"Audio, Image, Text",English,,,,,,,"Multi-Task Learning, Zero-shot Text to Audio Retrieval, Zero-shot Audio Captioning, Data Augmentation, Language Modelling, Audio captioning, Audio to Text Retrieval, Text to Audio Retrieval","zero-shot-text-to-audio-retrieval-on-clotho, audio-captioning-on-clotho, zero-shot-audio-captioning-on-clotho, audio-to-text-retrieval-on-clotho, text-to-audio-retrieval-on-clotho",,See all 1951 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
IAM,IAM Dataset,"The IAM database contains 13,353 images of handwritten lines of text created by 657 writers. The texts those writers transcribed are from the Lancaster-Oslo/Bergen Corpus of British English. It includes contributions from 657 writers making a total of 1,539 handwritten pages comprising of 115,320 words and is categorized as part of modern collection. The database is labeled at the sentence, line, and word levels.",https://arxiv.org/abs/1904.03734,"EditCustom (research-only, non-commercial, attribution)","Image, Text",English,,,,353 images,,,"HTR, Data Augmentation, Handwriting Recognition, Handwritten Text Recognition, Optical Character Recognition (OCR)","handwritten-text-recognition-on-iam, htr-on-iam",,See all 1951 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
MathQA,MathQA Dataset,MathQA significantly enhances the AQuA dataset with fully-specified operational programs.,/paper/mathqa-towards-interpretable-math-word,EditCustom,Text,English,,,,,,,"Math Word Problem Solving, Data Augmentation, Question Answering",math-word-problem-solving-on-mathqa,,See all 1951 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
MuST-C,MuST-C Dataset,"MuST-C currently represents the largest publicly available multilingual corpus (one-to-many) for speech translation. It covers eight language directions, from English to German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian. The corpus consists of audio, transcriptions and translations of English TED talks, and it comes with a predefined training, validation and test split.",https://arxiv.org/abs/1910.03320,EditCC BY-NC-ND 4.0,"Audio, Image, Text",English,,,,,,,"Data Augmentation, Speech-to-Text Translation, Speech Recognition","speech-to-text-translation-on-must-c-1, speech-to-text-translation-on-must-c-en-de",,See all 1951 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
PAWS,PAWS Dataset,"Paraphrase Adversaries from Word Scrambling (PAWS) is a dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. The dataset has two subsets, one based on Wikipedia and the other one based on the Quora Question Pairs (QQP) dataset.",https://github.com/google-research-datasets/paws,EditCustom,Text,English,,,,,,,"Data Augmentation, Natural Language Inference, Paraphrase Identification",,,See all 1951 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
De-aliasing9_papers_with_code_Dataset,De-aliasing9 papers with code Dataset,,https://paperswithcode.com/dataset/de-aliasing,,,,,,,,,,,,,See all 1951 tasks,De-aliasing9 papers with code,De-aliasing9 papers with code
Deblur-NeRF,Deblur-NeRF Dataset,"This dataset focus on two blur types: camera motion blur and defocus blur. For each type of blur we synthesize $5$ scenes using Blender. We manually place multi-view cameras to mimic real data capture. To render images with camera motion blur, we randomly perturb the camera pose, and then linearly interpolate poses between the original and perturbed poses for each view. We render images from interpolated poses and blend them in linear RGB space to generate the final blurry images. For defocus blur, we use the built-in functionality to render depth-of-field images. We fix the aperture and randomly choose a focus plane between the nearest and furthest depth.

We also captured $20$ real world scenes with $10$ scenes for each blur type for a qualitative study. The camera used was a Canon EOS RP with manual exposure mode. We captured the camera motion blur images by manually shaking the camera during exposure, while the reference images are taken using a tripod. To capture defocus images, we choose a large aperture. We compute the camera poses of blurry and reference images in the real world scenes using the COLMAP.",https://production-media.paperswithcode.com/datasets/31403458-a405-48e1-94ff-0b0fe9ee13e6.png,EditMIT,,,,,,,,,"Deblurring, Novel View Synthesis",,,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
Dialogue_State_Tracking_Challenge,Dialogue State Tracking Challenge Dataset,"The Dialog State Tracking Challenges 2 & 3 (DSTC2&3) were research challenge focused on improving the state of the art in tracking the state of spoken dialog systems. State tracking, sometimes called belief tracking, refers to accurately estimating the user's goal as a dialog progresses. Accurate state tracking is desirable because it provides robustness to errors in speech recognition, and helps reduce ambiguity inherent in language within a temporal process like dialog.
In these challenges, participants were given labelled corpora of dialogs to develop state tracking algorithms. The trackers were then evaluated on a common set of held-out dialogs, which were released, un-labelled, during a one week period.

The corpus was collected using Amazon Mechanical Turk, and consists of dialogs in two domains: restaurant information, and tourist information. Tourist information subsumes restaurant information, and includes bars, cafés etc. as well as multiple new slots. There were two rounds of evaluation using this data:

DSTC 2 released a large number of training dialogs related to restaurant search. Compared to DSTC (which was in the bus timetables domain), DSTC 2 introduces changing user goals, tracking 'requested slots' as well as the new restaurants domain. Results from DSTC 2 were presented at SIGDIAL 2014.
DSTC 3 addressed the problem of adaption to a new domain - tourist information. DSTC 3 releases a small amount of labelled data in the tourist information domain; participants will use this data plus the restaurant data from DSTC 2 for training.
Dialogs used for training are fully labelled; user transcriptions, user dialog-act semantics and dialog state are all annotated. (This corpus therefore is also suitable for studies in Spoken Language Understanding.)",https://github.com/matthen/dstc,EditUnknown,"Image, Text, Video",English,2014,,,,,,"Spoken Dialogue Systems, Slot Filling, Dialogue State Tracking, Spoken Language Understanding, Deblurring, Intent Detection, domain classification","domain-classification-on-dialogue-state, dialogue-state-tracking-on-second-dialogue, slot-filling-on-dialogue-state-tracking, intent-detection-on-dialogue-state-tracking, deblurring-on-second-dialogue-state-tracking",,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
GoPro,GoPro Dataset,"The GoPro dataset for deblurring consists of 3,214 blurred images with the size of 1,280×720 that are divided into 2,103 training images and 1,111 test images. The dataset consists of pairs of a realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera.",https://arxiv.org/abs/1903.10157,EditUnknown,"Image, Video",,,,,,"training images and 1,111 test images",,"Video Frame Interpolation, Unified Image Restoration, Deblurring, Image Deblurring","image-deblurring-on-gopro, unified-image-restoration-on-gopro, video-frame-interpolation-on-gopro, deblurring-on-gopro",,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
HIDE,HIDE Dataset,"Consists of 8,422 blurry and sharp image pairs with 65,784 densely annotated FG human bounding boxes.",/paper/human-aware-motion-deblurring-1,EditUnknown,Image,,,,,,,,"Image Restoration, Image Super-Resolution, Deblurring, Image Deblurring","deblurring-on-hide, image-deblurring-on-hide, image-deblurring-on-hide-trained-on-gopro, deblurring-on-hide-trained-on-gopro",,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
MSU_BASED,MSU BASED Dataset,"Qualitative dataset with real blurred videos, created by using beam-splitter setup in lab environment",https://production-media.paperswithcode.com/datasets/1e4ff292-3a0d-454e-9408-73265eaff1f8.png,EditUnknown,,,,,,,,,Deblurring,deblurring-on-based,,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
QMUL-SurvFace,QMUL-SurvFace Dataset,"QMUL-SurvFace is a surveillance face recognition benchmark that contains 463,507 face images of 15,573 distinct identities captured in real-world uncooperative surveillance scenes over wide space and time.",/paper/surveillance-face-recognition-challenge,EditUnknown,Image,,,,,,,,"Super-Resolution, Face Recognition, Deblurring, Face Verification",face-verification-on-qmul-survface,,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
Real_Blur_Dataset,Real Blur Dataset Dataset,"The dataset consists of 4,738 pairs of images of 232 different scenes including reference pairs. All images were captured both in the camera raw and JPEG formats, hence generating two datasets: RealBlur-R from the raw images, and RealBlur-J from the JPEG images. Each training set consists of 3,758 image pairs, while each test set consists of 980 image pairs.

The deblurring result is first aligned to its ground truth sharp image using a homography estimated by the enhanced correlation coefficients method, and PSNR or SSIM is computed in sRGB color space.",https://production-media.paperswithcode.com/datasets/qualatitive_result_web.png,EditUnknown,Image,,,,,,,,"Deblurring, Image Deblurring","deblurring-on-realblur-j-trained-on-gopro, deblurring-on-realblur-j-1, image-deblurring-on-realblur-r, deblurring-on-realblur-r-trained-on-gopro, image-deblurring-on-realblur-j, deblurring-on-realblur-r",,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
REDS,REDS Dataset,"The realistic and dynamic scenes (REDS) dataset was proposed in the NTIRE19 Challenge. The dataset is composed of 300 video sequences with resolution of 720×1,280, and each video has 100 frames, where the training set, the validation set and the testing set have 240, 30 and 30 videos, respectively",https://arxiv.org/abs/2007.12928,EditUnknown,,,,,,,,,"Joint Demosaicing and Denoising, Deblurring","deblurring-on-reds, joint-demosaicing-and-denoising-on-reds",,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
Rendered_WB_dataset,Rendered WB dataset Dataset,"A dataset of over 65,000 pairs of incorrectly white-balanced images and their corresponding correctly white-balanced images.",/paper/when-color-constancy-goes-wrong-correcting,EditUnknown,Image,,,,,,,,"Image Dehazing, Deblurring, Color Constancy",,,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
RSBlur,RSBlur Dataset,"The RSBlur dataset provides pairs of real and synthetic blurred images with ground truth sharp images. The dataset enables the evaluation of deblurring methods and blur synthesis methods on real-world blurred images.  Training, validation, and test sets consist of 8,878, 1,120, and 3,360 blurred images, respectively.",https://production-media.paperswithcode.com/datasets/bd573da2-a4c8-4c19-8eeb-9ef1dfcbfd47.png,EditUnknown,,,,,,,"valuation of deblurring methods and blur synthesis methods on real-world blurred images.  Training, validation, and test sets consist of 8,878, 1,120, and 3,360 blurred images",,Deblurring,"deblurring-on-rsblur, deblurring-on-rsblur-trained-on-synthetic",,See all 1951 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
Deep_Hashing57_papers_with_code_Dataset,Deep Hashing57 papers with code Dataset,,https://paperswithcode.com/dataset/deep-hashing,,,,,,,,,,,,,See all 1951 tasks,Deep Hashing57 papers with cod,Deep Hashing57 papers with cod
Deep_Learning2603_papers_with_code_Dataset,Deep Learning2603 papers with code Dataset,,https://paperswithcode.com/dataset/deep-learning,,,,,,,,,,,,,See all 1951 tasks,Deep Learning2603 papers with ,Deep Learning2603 papers with 
PixelShift200,PixelShift200 Dataset,"Advanced pixel shift technology is employed to perform a full color sampling of the image. Pixel shift technology takes four samples of the same image at nearly the same time, and physically controls the camera sensor to move one pixel horizontally or vertically at each sampling to capture all color information at each pixel. The pixel shift technology ensures that the sampled images follow the distribution of natural images sampled by the camera, and the full information of the color (R, Gr, Gb, B channel) is completely obtained without any need of interpolation. In this way, the collected RGB images are artifacts-free, which leads to better training results for demosaicing related tasks.

PixelShift200 Dataset contains 210 high quality 4K images.


Training: 200 images
Testing: 10 images
Key Features: fully colored, demosiacing artifacts free
Camera: SONY α7R III",https://guochengqian.github.io/project/pixelshift200/#overview,EditUnknown,,,,,,4K images,Training: 200 images,,"Super-Resolution, Demosaicking, Denoising",,,See all 1951 tasks,Demosaicking62 papers with cod,Demosaicking62 papers with cod
BirdSong,BirdSong Dataset,"The BirdSong dataset consists of audio recordings of bird songs at the H. J. Andrews (HJA) Experimental Forest, using unattended microphones. The goal of the dataset is to provide data to automatically identify the species of bird responsible for each utterance in these recordings. The dataset contains 548 10-seconds audio recordings.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Graph,,,,,,,,"Multi-Label Learning, Graph Matching, Denoising",,,See all 1951 tasks,Denoising153 benchmarks2605 pa,Denoising153 benchmarks2605 pa
CRVD,CRVD Dataset,The CRVD dataset consists of 55 groups of noisy-clean videos with ISO values ranging from 1600 to 25600.,/paper/supervised-raw-video-denoising-with-a,EditUnknown,"Image, Video",,,,,,,,"Image Denoising, Video Denoising, Denoising",video-denoising-on-crvd-1,,See all 1951 tasks,Denoising153 benchmarks2605 pa,Denoising153 benchmarks2605 pa
DND,DND Dataset,"Benchmarking Denoising Algorithms with Real Photographs

This dataset consists of 50 pairs of noisy and (nearly) noise-free images captured with four consumer cameras. Since the images are of very high-resolution, the providers extract 20 crops of size 512 × 512 from each image, thus yielding a total of 1000 patches.",https://production-media.paperswithcode.com/datasets/download_9zgFyQL.png,EditUnknown,Image,,,,,,,,"Image Denoising, Denoising","denoising-on-dnd-1, image-denoising-on-dnd",,See all 1951 tasks,Denoising153 benchmarks2605 pa,Denoising153 benchmarks2605 pa
FMD,FMD Dataset,"The Fluorescence Microscopy Denoising (FMD) dataset is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. Image averaging is used to effectively obtain ground truth images and 60,000 noisy images with different noise levels.",https://arxiv.org/abs/1812.10366,EditUnknown,Image,,,,,,,,"Image Denoising, intensity image denoising, Dictionary Learning, Denoising","intensity-image-denoising-on-fmd, image-denoising-on-fmd",,See all 1951 tasks,Denoising153 benchmarks2605 pa,Denoising153 benchmarks2605 pa
iris,iris Dataset,"The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula ""all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus"".",https://production-media.paperswithcode.com/datasets/Large53.jpeg,EditUnknown,"Image, Text",English,1936,,,,,,"Clustering Algorithms Evaluation, Quantum Machine Learning, Incremental Constrained Clustering, Denoising, Image/Document Clustering, Reinforcement Learning, Feature Importance, General Classification","clustering-algorithms-evaluation-on-iris, image-document-clustering-on-iris, denoising-on-iris, incremental-constrained-clustering-on-iris, general-classification-on-iris, reinforcement-learning-on-iris, feature-importance-on-iris, quantum-machine-learning-on-iris",,See all 1951 tasks,Denoising153 benchmarks2605 pa,Denoising153 benchmarks2605 pa
PolyU,PolyU Dataset,"PolyU Dataset is a large dataset of real-world noisy images with reasonably obtained corresponding “ground truth” images. The basic idea is to capture the same and unchanged scene for many (e.g., 500) times and compute their mean image, which can be roughly taken as the “ground truth” image for the real-world noisy images. The rational of this strategy is that for each pixel, the noise is generated randomly larger or smaller than 0. Sampling the same pixel many times and computing the average value will approximate the truth pixel value and alleviate significantly the noise.",https://arxiv.org/pdf/1804.02603.pdf,EditUnknown,Image,,,,,,,,"Image Denoising, Image Enhancement, Denoising",image-denoising-on-polyu-dataset,,See all 1951 tasks,Denoising153 benchmarks2605 pa,Denoising153 benchmarks2605 pa
SID,SID Dataset,"The See-in-the-Dark (SID) dataset contains 5094 raw short-exposure images, each with a corresponding long-exposure reference image.
Images were captured using two cameras: Sony α7SII and Fujifilm X-T2.",https://production-media.paperswithcode.com/datasets/6f160d9d-c3b1-4f63-ae58-ffc7eaffda19.png,EditUnknown,Image,,,,,,,,"Image Denoising, Low-Light Image Enhancement, Denoising","low-light-image-enhancement-on-sid, image-denoising-on-sid-x100, image-denoising-on-sid-x300",,See all 1951 tasks,Denoising153 benchmarks2605 pa,Denoising153 benchmarks2605 pa
Depiction_Invariant_Object_Recognition1_benchmark1,Depiction Invariant Object Recognition1 benchmark1 papers with code Dataset,,https://paperswithcode.com/dataset/depiction-invariant-object-recognition,,,,,,,,,,,,,See all 1951 tasks,Depiction Invariant Object Rec,Depiction Invariant Object Rec
CamlessVideosFromTheWild,CamlessVideosFromTheWild Dataset,"57 stock videos from Pexels, predominantly covering road scenes which involve minimal distortion.

They involve different camera setups, also with varying camera heights, obstacles present throughout some videos (for e.g. car hood), highly varying image resolutions, and even weather conditions (day, rain, snow, night etc.).

For more details, check the paper 
CamLessMonoDepth: Monocular Depth Estimation with Unknown Camera Parameters
 https://arxiv.org/pdf/2110.14347v1.pdf",https://production-media.paperswithcode.com/datasets/657f6f34-bb4f-49c8-9847-43d4b31575c2.png,Edithttps://www.pexels.com/license/,"3D, Video",,,,,,,,"Unsupervised Monocular Depth Estimation, Depth And Camera Motion",,,See all 1951 tasks,Depth And Camera Motion8 bench,Depth And Camera Motion8 bench
2D-3D-S,2D-3D-S Dataset,"The 2D-3D-S dataset provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. It covers over 6,000 m2 collected in 6 large-scale indoor areas that originate from 3 different buildings. It contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360° equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces.",https://github.com/alexsax/2D-3D-Semantics,EditCustom,"3D, Image",,,,,,,,"Semi-Supervised Semantic Segmentation, Depth Estimation, Visual Navigation, Semantic Segmentation, Self-Supervised Learning, Semi-Supervised RGBD Semantic Segmentation, 3D Room Layouts From A Single RGB Panorama, Robust Semi-Supervised RGBD Semantic Segmentation","semantic-segmentation-on-stanford2d3d-rgbd, semantic-segmentation-on-stanford2d3d-1, robust-semi-supervised-rgbd-semantic, semi-supervised-semantic-segmentation-on-2d, semi-supervised-rgbd-semantic-segmentation-on, semantic-segmentation-on-stanford2d3d-2, 3d-room-layouts-from-a-single-rgb-panorama-on-3, depth-estimation-on-stanford2d3d-panoramic",,See all 1951 tasks,Depth Estimation57 benchmarks9,Depth Estimation57 benchmarks9
Cityscapes,Cityscapes Dataset,"Cityscapes is a large-scale database which focuses on semantic understanding of urban street scenes. It provides semantic, instance-wise, and dense pixel annotations for 30 classes grouped into 8 categories (flat surfaces, humans, vehicles, constructions, objects, nature, sky, and void). The dataset consists of around 5000 fine annotated images and 20000 coarse annotated ones. Data was captured in 50 cities during several months, daytimes, and good weather conditions. It was originally recorded as video so the frames were manually selected to have the following features: large number of dynamic objects, varying scene layout, and varying background.",https://arxiv.org/abs/1704.06857,EditCustom,"3D, Image, Text, Time Series, Video",English,,,,,,30,"Instance Segmentation, Video Semantic Segmentation, Domain 11-1, Scene Parsing, Overlapped 14-1, Multi-Task Learning, Open Vocabulary Semantic Segmentation, Interactive Segmentation, Domain 11-5, Real-Time Semantic Segmentation, Domain Generalization, Semantic Segmentation, 2D Semantic Segmentation, Real-time Instance Segmentation, Robust Object Detection, Image-to-Image Translation, Semi-Supervised Semantic Segmentation, Monocular Depth Estimation, Domain 1-1, Unsupervised Semantic Segmentation with Language-image Pre-training, Depth Estimation, Edge Detection, Unsupervised Semantic Segmentation, Video Prediction, Panoptic Segmentation, Weakly-Supervised Semantic Segmentation, Overlapped 10-1, Unsupervised Monocular Depth Estimation, Image Generation, Federated Learning","panoptic-segmentation-on-cityscapes-val, overlapped-10-1-on-cityscapes, multi-task-learning-on-cityscapes, interactive-segmentation-on-cityscapes-val, panoptic-segmentation-on-cityscapes-test, monocular-depth-estimation-on-cityscapes, image-to-image-translation-on-cityscapes, video-semantic-segmentation-on-cityscapes-val, domain-11-5-on-cityscapes-val, semi-supervised-semantic-segmentation-on-40, video-prediction-on-cityscapes-128x128, semantic-segmentation-on-cityscapes-val, scene-parsing-on-cityscapes-test, instance-segmentation-on-cityscapes-val, depth-estimation-on-cityscapes-test, video-prediction-on-cityscapes-1, federated-learning-on-cityscapes, semi-supervised-semantic-segmentation-on-35, image-generation-on-cityscapes, semantic-segmentation-on-cityscapes, semi-supervised-semantic-segmentation-on-2, domain-11-1-on-cityscapes-val, image-to-image-translation-on-cityscapes-1, domain-1-1-on-cityscapes, domain-11-1-on-cityscapes, semi-supervised-semantic-segmentation-on-43, unsupervised-semantic-segmentation-on, semi-supervised-semantic-segmentation-on-3, open-vocabulary-semantic-segmentation-on, unsupervised-monocular-depth-estimation-on, robust-object-detection-on-cityscapes, weakly-supervised-semantic-segmentation-on-17, image-generation-on-cityscapes-25k-256x512, semi-supervised-semantic-segmentation-on-22, real-time-semantic-segmentation-on-cityscapes-3, robust-object-detection-on-cityscapes-1, unsupervised-semantic-segmentation-with-3, unsupervised-semantic-segmentation-on-1, domain-1-1-on-cityscapes-val, semi-supervised-semantic-segmentation-on-19, image-generation-on-cityscapes-5k-256x512, overlapped-14-1-on-cityscapes, semantic-segmentation-on-cityscapes-2, weakly-supervised-semantic-segmentation-on-16, semi-supervised-semantic-segmentation-on-18, instance-segmentation-on-cityscapes, 2d-semantic-segmentation-on-cityscapes-val, real-time-semantic-segmentation-on-cityscapes-1, real-time-instance-segmentation-on-cityscapes, edge-detection-on-cityscapes, semi-supervised-semantic-segmentation-on-1, semi-supervised-semantic-segmentation-on-8, real-time-semantic-segmentation-on-cityscapes, domain-11-5-on-cityscapes",,See all 1951 tasks,Depth Estimation57 benchmarks9,Depth Estimation57 benchmarks9
Matterport3D,Matterport3D Dataset,"The Matterport3D dataset is a large RGB-D dataset for scene understanding in indoor environments. It contains 10,800 panoramic views inside 90 real building-scale scenes, constructed from 194,400 RGB-D images. Each scene is a residential building consisting of multiple rooms and floor levels, and is annotated with surface construction, camera poses, and semantic segmentation.",https://arxiv.org/abs/1812.04155,EditCustom (non-commercial),"3D, Image, Time Series",,,,,,,,"Depth Completion, Monocular Depth Estimation, Depth Prediction, Depth Estimation, Semantic Segmentation","semantic-segmentation-on-matterport3d, depth-completion-on-matterport3d, depth-prediction-on-matterport3d, depth-estimation-on-matterport3d, monocular-depth-estimation-on-matterport3d",,See all 1951 tasks,Depth Estimation57 benchmarks9,Depth Estimation57 benchmarks9
Middlebury,Middlebury Dataset,The Middlebury Stereo dataset consists of high-resolution stereo sequences with complex geometry and pixel-accurate ground-truth disparity data. The ground-truth disparities are acquired using a novel technique that employs structured lighting and does not require the calibration of the light projectors.,https://vision.middlebury.edu/stereo/data/,EditCustom,"3D, Image, Video",,,,,,,,"Video Frame Interpolation, Image Super-Resolution, Depth Estimation, Stereo Image Super-Resolution","stereo-image-super-resolution-on-middlebury, video-frame-interpolation-on-middlebury, stereo-image-super-resolution-on-middlebury-1, image-super-resolution-on-middlebury-2x, image-super-resolution-on-middlebury-4x",,See all 1951 tasks,Depth Estimation57 benchmarks9,Depth Estimation57 benchmarks9
NYUv2,NYUv2 Dataset,"The NYU-Depth V2 data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect. It features:


1449 densely labeled pairs of aligned RGB and depth images
464 new scenes taken from 3 cities
407,024 new unlabeled frames
Each object is labeled with a class and an instance number.
The dataset has several components:
Labeled: A subset of the video data accompanied by dense multi-class labels. This data has also been preprocessed to fill in missing depth labels.
Raw: The raw RGB, depth and accelerometer data as provided by the Kinect.
Toolbox: Useful functions for manipulating the data and labels.",https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,EditUnknown,"3D, Image",,,,,,,,"Boundary Detection, Depth Completion, Multi-Task Learning, Monocular Depth Estimation, 3D Semantic Scene Completion, Scene Segmentation, Surface Normal Estimation, Scene Classification (unified classes), Instance Segmentation, Depth Estimation, Panoptic Segmentation, Surface Normals Estimation, 3D Semantic Scene Completion from a single RGB image, Real-Time Semantic Segmentation, Semantic Segmentation, Plane Instance Segmentation, Zero-shot Scene Classification (unified classes), 3D Object Detection","zero-shot-scene-classification-unified, 3d-semantic-scene-completion-on-nyuv2, surface-normal-estimation-on-nyu-depth-v2, depth-estimation-on-nyu-depth-v2, panoptic-segmentation-on-nyu-depth-v2, real-time-semantic-segmentation-on-nyu-depth-1, surface-normals-estimation-on-nyu-depth-v2-1, 3d-semantic-scene-completion-from-a-single, scene-classification-unified-classes-on-nyu, surface-normals-estimation-on-nyu-depth-v2, 3d-object-detection-on-nyu-depth-v2, monocular-depth-estimation-on-nyu-depth-v2, multi-task-learning-on-nyuv2, depth-completion-on-nyu-depth-v2, monocular-depth-estimation-on-nyu-depth-v2-4, boundary-detection-on-nyu-depth-v2, semantic-segmentation-on-nyu-depth-v2, scene-segmentation-on-nyu-depth-v2, instance-segmentation-on-nyu-depth-v2, plane-instance-segmentation-on-nyu-depth-v2",,See all 1951 tasks,Depth Estimation57 benchmarks9,Depth Estimation57 benchmarks9
ScanNet,ScanNet Dataset,"ScanNet is an instance-level indoor RGB-D dataset that includes both 2D and 3D data. It is a collection of labeled voxels rather than points or objects. Up to now, ScanNet v2, the newest version of ScanNet, has collected 1513 annotated scans with an approximate 90% surface coverage. In the semantic segmentation task, this dataset is marked in 20 classes of annotated 3D voxelized objects.",https://arxiv.org/abs/1908.08854,EditCustom,"3D, Image",,,,,,,20,"2D Panoptic Segmentation, Zero-shot 3D Point Cloud Classification, 3D Semantic Instance Segmentation, Scene Recognition, Scene Segmentation, Continual Semantic Segmentation, Interactive 3D Instance Segmentation, Interactive 3D Instance Segmentation -Trained on Scannet40 - Evaluated on Scannet40, Depth Estimation, Panoptic Segmentation, 3D Instance Segmentation, Generalized Zero-Shot Learning, Surface Normals Estimation, Semantic Segmentation, Unsupervised 3D Semantic Segmentation, 3D Reconstruction, 3D Object Detection","panoptic-segmentation-on-scannetv2, depth-estimation-on-scannetv2, interactive-3d-instance-segmentation-on, 3d-reconstruction-on-scannet, generalized-zero-shot-learning-on-scannet, surface-normals-estimation-on-scannetv2, 3d-semantic-instance-segmentation-on, scene-segmentation-on-scannet, zero-shot-3d-point-cloud-classification-on-1, interactive-3d-instance-segmentation-trained, semantic-segmentation-on-scannetv2, depth-estimation-on-scannet, 2d-panoptic-segmentation-on-scannetv2, 3d-instance-segmentation-on-scannetv2, semantic-segmentation-on-scannet, 3d-instance-segmentation-on-scannet, 3d-semantic-instance-segmentation-on-1, 3d-object-detection-on-scannetv2, panoptic-segmentation-on-scannet, unsupervised-3d-semantic-segmentation-on, continual-semantic-segmentation-on-scannet, scene-recognition-on-scannet",,See all 1951 tasks,Depth Estimation57 benchmarks9,Depth Estimation57 benchmarks9
SUNCG,SUNCG Dataset,"SUNCG is a large-scale dataset of synthetic 3D scenes with dense volumetric annotations.

The dataset is currently not available.",https://sscnet.cs.princeton.edu/,EditUnknown,"3D, Image",,,,,,,,"Visual Navigation, Depth Estimation, Semantic Segmentation",,,See all 1951 tasks,Depth Estimation57 benchmarks9,Depth Estimation57 benchmarks9
Taskonomy,Taskonomy Dataset,"Taskonomy provides a large and high-quality dataset of varied indoor scenes.


Complete pixel-level geometric information via aligned meshes.
Semantic information via knowledge distillation from ImageNet, MS COCO, and MIT Places.
Globally consistent camera poses. Complete camera intrinsics.
High-definition images.
3x times big as ImageNet.",http://taskonomy.stanford.edu/,EditUnknown,3D,,,,,,,,"Depth Estimation, Surface Normals Estimation","surface-normals-estimation-on-taskonomy, depth-estimation-on-taskonomy",,See all 1951 tasks,Depth Estimation57 benchmarks9,Depth Estimation57 benchmarks9
TUM_RGB-D,TUM RGB-D Dataset,TUM RGB-D is an RGB-D dataset. It contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640x480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz).,https://vision.in.tum.de/data/datasets/rgbd-dataset,EditCC BY 4.0,"3D, Image",,,,,,,,"Depth Estimation, Simultaneous Localization and Mapping, Visual Odometry",,,See all 1951 tasks,Depth Estimation57 benchmarks9,Depth Estimation57 benchmarks9
AtariARI,AtariARI Dataset,"The AtariARI (Atari Annotated RAM Interface) is an environment for representation learning. The Atari Arcade Learning Environment (ALE) does not explicitly expose any ground truth state information. However, ALE does expose the RAM state (128 bytes per timestep) which are used by the game programmer to store important state information such as the location of sprites, the state of the clock, or the current room the agent is in. To extract these variables, the dataset creators consulted commented disassemblies (or source code) of Atari 2600 games which were made available by Engelhardt and Jentzsch and CPUWIZ. The dataset creators were able to find and verify important state variables for a total of 22 games. Once this information was acquired, combining it with the ALE interface produced a wrapper that can automatically output a state label for every example frame generated from the game. The dataset creators make this available with an easy-to-use gym wrapper, which returns this information with no change to existing code using gym interfaces.",https://arxiv.org/pdf/1906.08226.pdf,EditUnknown,,,,,,,,,"Dimensionality Reduction, Atari Games",,,See all 1951 tasks,Dimensionality Reduction2 benc,Dimensionality Reduction2 benc
CN-CELEB,CN-CELEB Dataset,"CN-Celeb is a large-scale speaker recognition dataset collected `in the wild'. This dataset contains more than 130,000 utterances from 1,000 Chinese celebrities, and covers 11 different genres in real world.",/paper/cn-celeb-a-challenging-chinese-speaker,EditUnknown,"Audio, Image",,,,,,,,"Dimensionality Reduction, Speaker Verification, Speaker Recognition",speaker-verification-on-cn-celeb,,See all 1951 tasks,Dimensionality Reduction2 benc,Dimensionality Reduction2 benc
Deep_Fakes_Dataset,Deep Fakes Dataset Dataset,"The Deep Fakes Dataset is a collection of ""in the wild"" portrait videos for deepfake detection. The videos in the dataset are diverse real-world samples in terms of the source generative model, resolution, compression, illumination, aspect-ratio, frame rate, motion, pose, cosmetics, occlusion, content, and context. They originate from various sources such as news articles, forums, apps, and research presentations; totalling up to 142 videos, 32 minutes, and 17 GBs. Synthetic videos are matched with their original counterparts when possible.",http://cs.binghamton.edu/~ncilsal2/DeepFakesDataset/,EditUnknown,"Text, Video",English,,,,,,,"Video Generation, Dimensionality Reduction, Video Compression",,,See all 1951 tasks,Dimensionality Reduction2 benc,Dimensionality Reduction2 benc
GoodSounds,GoodSounds Dataset,"GoodSounds dataset contains around 28 hours of recordings of single notes and scales played by 15 different professional musicians, all of them holding a music degree and having some expertise in teaching. 12 different instruments (flute, cello, clarinet, trumpet, violin, alto sax alto, tenor sax, baritone sax, soprano sax, oboe, piccolo and bass) were recorded using one or up to 4 different microphones. For all the instruments the whole set of playable semitones in the instrument is recorded several times with different tonal characteristics. Each note is recorded into a separate monophonic audio file of 48kHz and 32 bits. Rich annotations of the recordings are available, including details on recording environment and rating on tonal qualities of the sound (“good-sound”, “bad”, “scale-good”, “scale-bad”).",http://mtg.upf.edu/node/3197,EditUnknown,Audio,,,,,,,,"Music Information Retrieval, Dimensionality Reduction, Information Retrieval",,,See all 1951 tasks,Dimensionality Reduction2 benc,Dimensionality Reduction2 benc
HolStep,HolStep Dataset,"HolStep is a dataset based on higher-order logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies.",/paper/holstep-a-machine-learning-dataset-for-higher,EditBSD-3-Clause,,,,,,,,,"Mathematical Proofs, Automated Theorem Proving, Dimensionality Reduction","automated-theorem-proving-on-holstep, automated-theorem-proving-on-holstep-1",,See all 1951 tasks,Dimensionality Reduction2 benc,Dimensionality Reduction2 benc
Oxford-Affine,Oxford-Affine Dataset,The Oxford-Affine dataset is a small dataset containing 8 scenes with sequence of 6 images per scene. The images in a sequence are related by homographies.,https://arxiv.org/abs/1801.01466,EditUnknown,"3D, Image",,,,,6 images,,,"3D Reconstruction, Dimensionality Reduction, Image Retrieval",,,See all 1951 tasks,Dimensionality Reduction2 benc,Dimensionality Reduction2 benc
Oxford105k,Oxford105k Dataset,Oxford105k is the combination of the Oxford5k dataset and 99782 negative images crawled from Flickr using 145 most popular tags. This dataset is used to evaluate search performance for object retrieval (reported as mAP) on a large scale.,https://arxiv.org/abs/1504.03285,EditCustom,Image,,,,,,,,"Instance Search, Dimensionality Reduction, Image Retrieval",,,See all 1951 tasks,Dimensionality Reduction2 benc,Dimensionality Reduction2 benc
SoF,SoF Dataset,"The Specs on Faces (SoF) dataset, a collection of 42,592 (2,662×16) images for 112 persons (66 males and 46 females) who wear glasses under different illumination conditions. The dataset is FREE for reasonable academic fair use. The dataset presents a new challenge regarding face detection and recognition. It is focused on two challenges: harsh illumination environments and face occlusions, which highly affect face detection, recognition, and classification. The glasses are the common natural occlusion in all images of the dataset. However, there are two more synthetic occlusions (nose and mouth) added to each image. Moreover, three image filters, that may evade face detectors and facial recognition systems, were applied to each image. All generated images are categorized into three levels of difficulty (easy, medium, and hard). That enlarges the number of images to be 42,592 images (26,112 male images and 16,480 female images). There is metadata for each image that contains many information such as: the subject ID, facial landmarks, face and glasses rectangles, gender and age labels, year that the photo was taken, facial emotion, glasses type, and more.",https://sites.google.com/view/sof-dataset,EditUnknown,,,,,,592 images,,,Dimensionality Reduction,,,See all 1951 tasks,Dimensionality Reduction2 benc,Dimensionality Reduction2 benc
STRING,STRING Dataset,STRING is a collection of protein-protein interaction (PPI) networks.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY 4.0,Time Series,,,,,,,,"Link Prediction, Dimensionality Reduction",,,See all 1951 tasks,Dimensionality Reduction2 benc,Dimensionality Reduction2 benc
3DIdent,3DIdent Dataset,"Novel benchmark which features aspects of natural scenes, e.g. a complex 3D object and different lighting conditions, while still providing access to the continuous ground-truth factors.

We use the Blender rendering engine to create visually complex 3D images. Each image in the dataset shows a colored 3D object which is located and rotated above a colored ground in a 3D space. Additionally, each scene contains a colored spotlight which is focused on the object and located on a half-circle around the scene. The observations are encoded with an RGB color space, and the spatial resolution is 224x224 pixels.

The images are rendered based on a 10-dimensional latent, where: (1) three dimensions describe the XYZ position, (2) three dimensions describe the rotation of the object in Euler angles, (3) two dimensions describe the color of the object and the ground of the scene, respectively, and (4) two dimensions describe the position and color of the spotlight. We use the HSV color space to describe the color of the object and the ground with only one latent each by having the latent factor control the hue value.

The training set and test set contain 250,000 and 25,000 observation-latent pairs, respectively, whereby the latents are uniformly sampled from the unit hyperrectangle.",https://production-media.paperswithcode.com/datasets/1aca66a6-66f2-443a-b9b9-7d49cc810182.png,EditCreative Commons Attribution 4.0 International,,,,,,,,,Disentanglement,disentanglement-on-3dident,,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
3D_Cars,3D Cars Dataset,"Car CAD models from ""3d object detection and viewpoint estimation with a deformable
3d cuboid model"" were used to generate the dataset. For each of the 199 car models, the authors generated $64\times64$ color renderings from 24 rotation angles each offset by 15 degrees, as well as from 4 different camera elevations.",https://production-media.paperswithcode.com/datasets/ffa55f37-436f-4f60-8815-f620cc24261f.gif,EditUnknown,,,,,,,,,Disentanglement,,,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
3D_Shapes_Dataset,3D Shapes Dataset Dataset,"3dshapes is a dataset of 3D shapes procedurally generated from 6 ground truth independent latent factors. These factors are floor colour, wall colour, object colour, scale, shape and orientation.",https://github.com/deepmind/3d-shapes,EditUnknown,,,,,,,,,Disentanglement,,,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
Causal3DIdent,Causal3DIdent Dataset,"Update on 3DIdent, where we introduce six additional object classes (Hare, Dragon, Cow, Armadillo, Horse, and Head), and impose a causal graph over the latent variables. For further details, see Appendix B in the associated paper (https://arxiv.org/abs/2106.04619).",https://production-media.paperswithcode.com/datasets/1e720be9-fc37-4d1d-9260-d0543b2b6179.png,EditCreative Commons Attribution 4.0 International,Image,,,,,,,,"Image Classification, Disentanglement",image-classification-on-causal3dident,,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
dSprites,dSprites Dataset,"dSprites is a dataset of 2D shapes procedurally generated from 6 ground truth independent latent factors. These factors are color, shape, scale, rotation, x and y positions of a sprite.

All possible combinations of these latents are present exactly once, generating N = 737280 total images.",https://github.com/deepmind/dsprites-dataset,EditUnknown,,,,,,,,,Disentanglement,,,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
Fitness-AQA,Fitness-AQA Dataset,"Largest, first-of-its-kind, in-the-wild, fine-grained workout/exercise posture analysis dataset, covering three different exercises: BackSquat, Barbell Row, and Overhead Press. Seven different types of exercise errors are covered. Unlabeled data is also provided to facilitate self-supervised learning.",https://production-media.paperswithcode.com/datasets/b05790c3-c044-477e-a88d-a415732aaf5d.png,EditUnknown,"3D, Image, Video",,,,,,,,"Pose Estimation, Action Quality Assessment, Pose Contrastive Learning, 3D Pose Estimation, 2D Human Pose Estimation, Video Understanding, Action Understanding, 3D Action Recognition, Disentanglement, Motion Disentanglement, Action Recognition",,,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
KITTI-Masks,KITTI-Masks Dataset,"This Dataset consists of 2120 sequences of binary masks of pedestrians. The sequence length varies between 2-710. For details, we refer to our paper. It is based on the original KITTI Segmentation challenge which can be found at https://www.vision.rwth-aachen.de/page/mots 

A detailed description can be found at: https://openreview.net/pdf?id=EbIDjBynYJ8

An example dataloader can be found at: 
https://github.com/bethgelab/slow_disentanglement/",https://production-media.paperswithcode.com/datasets/2bbcb796-5e70-4cf4-9bab-df0d8c8f69d3.png,EditCreative Commons Attribution 4.0 International,"Image, 3D",English,,,,,,,Disentanglement,disentanglement-on-kitti-masks,,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
MPI3D_Disentanglement,MPI3D Disentanglement Dataset,"A data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position.",/paper/on-the-transfer-of-inductive-bias-from,EditUnknown,,,,,,,,,Disentanglement,,,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
smallNORB,smallNORB Dataset,"The smallNORB dataset is a datset for 3D object recognition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees).
The training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).",https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/,"EditCustom (research-only, non-commercial, attribution)",Image,,,,,5 instances,,,"Image Classification, Disentanglement",image-classification-on-smallnorb,,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
Sprites,Sprites Dataset,"The Sprites dataset contains 60 pixel color images of animated characters (sprites). There are 672 sprites, 500 for training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images, so the full dataset has 120K images in total. There are many changes in the appearance of the sprites, they differ in their body shape, gender, hair, armor, arm type, greaves, and weapon.",https://arxiv.org/abs/1711.02245,EditUnknown,"Time Series, Video",,,,,178 images,"training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images",,"Video Prediction, Imputation, Disentanglement","imputation-on-sprites, video-prediction-on-sprites, video-prediction-on-colored-dsprites",,See all 1951 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
DomainNet,DomainNet Dataset,"DomainNet is a dataset of common objects in six different domain. All domains include 345 categories (classes) of objects such as Bracelet, plane, bird and cello. The domains include clipart: collection of clipart images; real: photos and real world images; sketch: sketches of specific objects; infograph: infographic images with specific object; painting artistic depictions of objects in the form of paintings and quickdraw: drawings of the worldwide players of game “Quick Draw!”.",https://arxiv.org/abs/2008.11687,"EditCustom (research-only, non-commercial)",,,,,,,,345,"Partial Domain Adaptation, Multi-target Domain Adaptation, Blended-target Domain Adaptation, Universal Domain Adaptation, Zero-Shot Learning + Domain Generalization, Domain Adaptation, Domain Generalization, Multi-Source Unsupervised Domain Adaptation, Unsupervised Domain Adaptation, Unsupervised Continual Domain Shift Learning","domain-generalization-on-domainnet, unsupervised-domain-adaptation-on-domainnet-1, universal-domain-adaptation-on-domainnet, zero-shot-learning-domain-generalization-on, blended-target-domain-adaptation-on-domainnet, unsupervised-continual-domain-shift-learning-2, partial-domain-adaptation-on-domainnet, multi-target-domain-adaptation-on-domainnet, multi-source-unsupervised-domain-adaptation, domain-adaptation-on-domainnet-1",,See all 1951 tasks,Domain Adaptation158 benchmark,Domain Adaptation158 benchmark
Office-31,Office-31 Dataset,"The Office dataset contains 31 object categories in three domains: Amazon, DSLR and Webcam. The 31 categories in the dataset consist of objects commonly encountered in office settings, such as keyboards, file cabinets, and laptops. The Amazon domain contains on average 90 images per class and 2817 images in total. As these images were captured from a website of online merchants, they are captured against clean background and at a unified scale. The DSLR domain contains 498 low-noise high resolution images (4288×2848). There are 5 objects per category. Each object was captured from different viewpoints on average 3 times. For Webcam, the 795 images of low resolution (640×480) exhibit significant noise and color as well as white balance artifacts.",https://arxiv.org/abs/1611.08195,EditUnknown,,,,,,90 images,,31,"Partial Domain Adaptation, Multi-target Domain Adaptation, Blended-target Domain Adaptation, Universal Domain Adaptation, Open-Set Multi-Target Domain Adaptation, Domain Adaptation, Unsupervised Domain Adaptationn, Multi-Source Unsupervised Domain Adaptation, Unsupervised Domain Adaptation","universal-domain-adaptation-on-office-31, unsupervised-domain-adaptationn-on-office-31, open-set-multi-target-domain-adaptation-on, multi-source-unsupervised-domain-adaptation-5, blended-target-domain-adaptation-on-office-31, multi-target-domain-adaptation-on-office-31, partial-domain-adaptation-on-office-31, unsupervised-domain-adaptation-on-office-31, domain-adaptation-on-office-31",,See all 1951 tasks,Domain Adaptation158 benchmark,Domain Adaptation158 benchmark
Office-Home,Office-Home Dataset,"Office-Home is a benchmark dataset for domain adaptation which contains 4 domains where each domain consists of 65 categories. The four domains are: Art – artistic images in the form of sketches, paintings, ornamentation, etc.; Clipart – collection of clipart images; Product – images of objects without a background and Real-World – images of objects captured with a regular camera. It contains 15,500 images, with an average of around 70 images per class and a maximum of 99 images in a class.",https://arxiv.org/abs/1812.08974,EditCustom (non-commercial research and educational purposes),,,,,,500 images,,65,"Partial Domain Adaptation, Multi-target Domain Adaptation, Blended-target Domain Adaptation, Universal Domain Adaptation, Open-Set Multi-Target Domain Adaptation, Domain Adaptation, Domain Generalization, cross-domain few-shot learning, Transfer Learning, Multi-Source Unsupervised Domain Adaptation, Unsupervised Domain Adaptation","unsupervised-domain-adaptation-on-office-home-1, cross-domain-few-shot-learning-on-office-home, unsupervised-domain-adaptation-on-office-home, multi-source-unsupervised-domain-adaptation-2, blended-target-domain-adaptation-on-office, transfer-learning-on-office-home, open-set-multi-target-domain-adaptation-on-1, universal-domain-adaptation-on-office-home, partial-domain-adaptation-on-office-home, domain-adaptation-on-office-home, domain-generalization-on-office-home, multi-target-domain-adaptation-on-office-home",,See all 1951 tasks,Domain Adaptation158 benchmark,Domain Adaptation158 benchmark
PACS,PACS Dataset,"PACS is an image dataset for domain generalization. It consists of four domains, namely Photo (1,670 images), Art Painting (2,048 images), Cartoon (2,344 images) and Sketch (3,929 images). Each domain contains seven categories.",https://arxiv.org/abs/2003.06054,EditUnknown,Image,,,,,670 images,,,"Photo to Rest Generalization, Image to sketch recognition, Single-Source Domain Generalization, Domain Adaptation, Annotated Code Search, Domain Generalization, Unsupervised Domain Adaptation, Unsupervised Continual Domain Shift Learning","annotated-code-search-on-pacs-staqc-py, single-source-domain-generalization-on-pacs, unsupervised-domain-adaptation-on-pacs, domain-adaptation-on-pacs, unsupervised-continual-domain-shift-learning-1, photo-to-rest-generalization-on-pacs, annotated-code-search-on-pacs-conala, annotated-code-search-on-pacs-so-ds, image-to-sketch-recognition-on-pacs, domain-generalization-on-pacs-2",,See all 1951 tasks,Domain Adaptation158 benchmark,Domain Adaptation158 benchmark
USPS,USPS Dataset,"USPS is a digit dataset automatically scanned from envelopes by the U.S. Postal Service containing a total of 9,298 16×16 pixel grayscale samples; the images are centered, normalized and show a broad range of font styles.",https://arxiv.org/abs/1808.01102,EditUnknown,Image,,,,,,,,"Image Clustering, Deep Clustering, Domain Adaptation","domain-adaptation-on-usps-to-mnist, deep-clustering-on-usps, image-clustering-on-usps, domain-adaptation-on-mnist-to-usps",,See all 1951 tasks,Domain Adaptation158 benchmark,Domain Adaptation158 benchmark
CIFAR-10C,CIFAR-10C Dataset,Common corruptions dataset for CIFAR10,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache License 2.0,Image,,,,,,,,"Classification, Domain Generalization","domain-generalization-on-cifar-10c, classification-on-cifar-10c",,See all 1951 tasks,Domain Generalization24 benchm,Domain Generalization24 benchm
ImageNet-Sketch,ImageNet-Sketch Dataset,"ImageNet-Sketch data set consists of 50,889 images,  approximately 50 images for each of the 1000 ImageNet classes. The data set is constructed with Google Image queries ""sketch of "", where  is the standard class name. Only within the ""black and white"" color scheme is searched. 100 images are initially queried for every class, and the pulled images are cleaned by deleting the irrelevant images and images that are for similar but different classes. For some classes, there are less than 50 images after manually cleaning, and then the data set is augmented by flipping and rotating the images.",https://github.com/HaohanWang/ImageNet-Sketch,EditUnknown,Image,English,,,,889 images,,,"Zero-Shot Transfer Image Classification, Data Augmentation, Domain Adaptation, Domain Generalization, Image Classification","image-classification-on-imagenet-sketch, domain-generalization-on-imagenet-sketch, zero-shot-transfer-image-classification-on-8",,See all 1951 tasks,Domain Generalization24 benchm,Domain Generalization24 benchm
BIPED,BIPED Dataset,"Details
It contains 250 outdoor images of 1280$\times$720 pixels each. These images have been carefully annotated by experts on the computer vision field, hence no redundancy has been considered. In spite of that, all results have been cross-checked several times in order to correct possible mistakes or wrong edges by just one subject. This dataset is publicly available as a benchmark for evaluating edge detection algorithms. The generation of this dataset is motivated by the lack of edge detection datasets, actually, there is just one dataset publicly available for the edge detection task published in 2016 (MDBD: Multicue Dataset for Boundary Detection—the subset for edge detection). The level of details of the edge level annotations in the BIPED’s images can be appreciated looking at the GT, see Figs above.

BIPED dataset has 250 images in high definition. Thoses images are already split up for training and testing. 200 for training and 50 for testing.

Version
The current version is the second one.",https://production-media.paperswithcode.com/datasets/5367ead7-86b1-4b84-97ea-77ccd616d17b.png,EditUnknown,Image,,2016,,,250 images,"valuating edge detection algorithms. The generation of this dataset is motivated by the lack of edge detection datasets, actually, there is just one dataset publicly available for the edge detection task published in 2016 (MDBD: Multicue Dataset for Boundary Detection—the subset for edge detection). The level of details of the edge level annotations in the BIPED’s images",,Edge Detection,edge-detection-on-biped-1,,See all 1951 tasks,Edge Detection8 benchmarks145 ,Edge Detection8 benchmarks145 
BRIND,BRIND Dataset,"BRIND is a short name of BSDS-RIND is the first public benchmark that dedicated to studying simultaneously the four edge types, namely Reflectance Edge (RE), Illumination Edge (IE), Normal Edge (NE) and Depth Edge (DE)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Edge Detection,edge-detection-on-brind,,See all 1951 tasks,Edge Detection8 benchmarks145 ,Edge Detection8 benchmarks145 
BSDS500,BSDS500 Dataset,"Berkeley Segmentation Data Set 500 (BSDS500) is a standard benchmark for contour detection. This dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images with carefully annotated boundaries collected from multiple users. The dataset is divided into three parts: 200 for training, 100 for validation and the rest 200 for test.",https://arxiv.org/abs/1603.04530,EditUnknown,Image,,,,,,valuating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images,,"Edge Detection, JPEG Artifact Correction, Image Compression","jpeg-artifact-correction-on-bsds500-quality-3, jpeg-artifact-correction-on-bsds500-quality-2, jpeg-artifact-correction-on-bsds500-quality-1, edge-detection-on-bsds500-1, image-compression-on-bsds500, jpeg-artifact-correction-on-bsds500-quality, jpeg-artifact-correction-on-bsds500-quality-4, jpeg-artifact-correction-on-bsds500-quality-5",,See all 1951 tasks,Edge Detection8 benchmarks145 ,Edge Detection8 benchmarks145 
CID,CID Dataset,"The CID (Campus Image Dataset) is a dataset captured in low-light env with the help of Android programming. Its basic unit is group, which is named by capture time and contains 8 exposure-time-varying raw image shot in a burst.",https://github.com/505030475/ExtremeLowLight,EditUnknown,Image,,,,,,,,Edge Detection,edge-detection-on-cid,,See all 1951 tasks,Edge Detection8 benchmarks145 ,Edge Detection8 benchmarks145 
INRIA-Horse,INRIA-Horse Dataset,"The INRIA-Horse dataset consists of 170 horse images and 170 images without horses. All horses in all images are annotated with a bounding-box. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The horses are mostly unoccluded, taken from approximately the side viewpoint, and face the same direction.",https://arxiv.org/abs/1502.00741,EditUnknown,Image,,,,,170 images,,,"Object Detection, Edge Detection",,,See all 1951 tasks,Edge Detection8 benchmarks145 ,Edge Detection8 benchmarks145 
MDBD,MDBD Dataset,"In order to study the interaction of several early visual cues (luminance, color, stereo, motion) during boundary detection in challenging natural scenes, we have built a multi-cue video dataset composed of short binocular video sequences of natural scenes using a consumer-grade Fujifilm stereo camera (Mély, Kim, McGill, Guo and Serre, 2016). We considered a variety of places (from university campuses to street scenes and parks) and seasons to minimize possible biases. We attempted to capture more challenging scenes for boundary detection by framing a few dominant objects in each shot under a variety of appearances. Representative sample keyframes are shown on the figure below. The dataset contains 100 scenes, each consisting of a left and right view short (10-frame) color sequence. Each sequence was sampled at a rate of 30 frames per second. Each frame has a resolution of 1280 by 720 pixels.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2016,,,,,,"Boundary Detection, Edge Detection",edge-detection-on-mdbd,,See all 1951 tasks,Edge Detection8 benchmarks145 ,Edge Detection8 benchmarks145 
SBD,SBD Dataset,"The Semantic Boundaries Dataset (SBD) is a dataset for predicting pixels on the boundary of the object (as opposed to the inside of the object with semantic segmentation). The dataset consists of 11318 images from the trainval set of the PASCAL VOC2011 challenge, divided into 8498 training and 2820 test images. This dataset has object instance boundaries with accurate figure/ground masks that are also labeled with one of 20 Pascal VOC classes.",https://arxiv.org/abs/1511.07803,EditUnknown,"Image, Time Series",,,,,11318 images,"trainval set of the PASCAL VOC2011 challenge, divided into 8498 training and 2820 test images",,"Edge Detection, Semantic Contour Prediction, Interactive Segmentation","semantic-contour-prediction-on-sbd-val, interactive-segmentation-on-sbd, edge-detection-on-sbd",,See all 1951 tasks,Edge Detection8 benchmarks145 ,Edge Detection8 benchmarks145 
UDED,UDED Dataset,"This dataset is a collection of 1, 2, or 3 images from: BIPED, BSDS500, BSDS300, DIV2K, WIRE-FRAME, CID, CITYSCAPES, ADE20K, MDBD, NYUD, THANGKA, PASCAL-Context, SET14, URBAN10, and the camera-man image. The image selection process consists on computing the Inter-Quartile Range (IQR) intensity value on all the images, images larger than 720×720 pixels were not considered. In dataset whose images are in HR, they were cut. We thank all the datasets owners to make them public. This dataset is just for Edge Detection not contour nor Boundary tasks.",https://production-media.paperswithcode.com/datasets/cb921d3d-0c52-4bcf-ac52-3b60f6e88b1a.png,EditUnknown,Image,,,,,3 images,"value on all the images, images",,Edge Detection,edge-detection-on-uded,,See all 1951 tasks,Edge Detection8 benchmarks145 ,Edge Detection8 benchmarks145 
DECADE,DECADE Dataset,DECADE is a large-scale dataset of ego-centric videos from a dog's perspective as well as her corresponding movements.,/paper/who-let-the-dogs-out-modeling-dog-behavior,EditUnknown,"Image, Time Series, Video",,,,,,,,"Temporal Action Localization, Egocentric Activity Recognition",,,See all 1951 tasks,Egocentric Activity Recognitio,Egocentric Activity Recognitio
EGOK360,EGOK360 Dataset,"Contains annotations of human activity with different sub-actions, e.g., activity Ping-Pong with four sub-actions which are pickup-ball, hit, bounce-ball and serve.",/paper/egok360-a-360-egocentric-kinetic-human,EditUnknown,"Image, Video",,,,,,,,"Activity Recognition, Video Understanding, Egocentric Activity Recognition",,,See all 1951 tasks,Egocentric Activity Recognitio,Egocentric Activity Recognitio
EGTEA,EGTEA Dataset,"Extended GTEA Gaze+
EGTEA Gaze+ is a large-scale dataset for FPV actions and gaze. It subsumes GTEA Gaze+ and comes with HD videos (1280x960), audios, gaze tracking data, frame-level action annotations, and pixel-level hand masks at sampled frames.
Specifically, EGTEA Gaze+ contains 28 hours (de-identified) of cooking activities from 86 unique sessions of 32 subjects. These videos come with audios and gaze tracking (30Hz). We have further provided human annotations of actions (human-object interactions) and hand masks.

The action annotations include 10325 instances of fine-grained actions, such as ""Cut bell pepper"" or ""Pour condiment (from) condiment container into salad"".

The hand annotations consist of 15,176 hand masks from 13,847 frames from the videos.",http://cbs.ic.gatech.edu/fpv/,EditUnknown,"Image, Video",,,,,10325 instances,,,"Egocentric Activity Recognition, Action Anticipation, Long-tail Learning","egocentric-activity-recognition-on-egtea-1, long-tail-learning-on-egtea, action-anticipation-on-egtea",,See all 1951 tasks,Egocentric Activity Recognitio,Egocentric Activity Recognitio
Aff-Wild,Aff-Wild Dataset,"Aff-Wild is a large-scale in-the-wild dataset for valence-arousal estimation from videos with a variety of head poses, illumination conditions and occlusions.",/paper/deep-affect-prediction-in-the-wild-aff-wild,EditCustom (non-commercial),"Image, Video",,,,,,,,"Gesture Recognition, Emotion Recognition, Action Unit Detection",,,See all 1951 tasks,Emotion Recognition67 benchmar,Emotion Recognition67 benchmar
CARER,CARER Dataset,"CARER is an emotion dataset collected through noisy labels, annotated via distant supervision as in (Go et al., 2009). 

The subset of data provided here corresponds to the six emotions variant described in the paper. The six emotions are anger, fear, joy, love, sadness, and surprise.",/paper/carer-contextualized-affect-representations,EditUnknown,"Image, Text",English,2009,,,,,,"Emotion Recognition, Text Classification, Multi Class Text Classification, Semantic Textual Similarity","text-classification-on-emotion, multi-class-text-classification-on-emotion",,See all 1951 tasks,Emotion Recognition67 benchmar,Emotion Recognition67 benchmar
EmotionLines,EmotionLines Dataset,"EmotionLines contains a total of 29245 labeled utterances from 2000 dialogues. Each utterance in dialogues is labeled with one of seven emotions, six Ekman’s basic emotions plus the neutral emotion. Each labeling was accomplished by 5 workers, and for each utterance in a label, the emotion category with the highest votes was set as the label of the utterance. Those utterances voted as more than two different emotions were put into the non-neutral category. Therefore the dataset has a total of 8 types of emotion labels, anger, disgust, fear, happiness, sadness, surprise, neutral, and non-neutral.",https://arxiv.org/abs/1905.11240,EditCC BY-NC-ND,"Image, Text",English,2000,,,,,,"Language Modelling, Emotion Recognition in Conversation, Emotion Recognition, Emotion Classification",emotion-recognition-in-conversation-on-5,,See all 1951 tasks,Emotion Recognition67 benchmar,Emotion Recognition67 benchmar
Hateful_Memes,Hateful Memes Dataset,"The Hateful Memes data set is a multimodal dataset for hateful meme detection (image + text) that contains 10,000+ new multimodal examples created by Facebook AI. Images were licensed from Getty Images so that researchers can use the data set to support their work.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-07_at_8.40.37_PM.png,EditUnknown,"Image, Text",English,,,,,,,"Image Clustering, Misinformation, Emotion Recognition, Image Captioning, Meme Classification, Hateful Meme Classification","image-clustering-on-hateful-memes, meme-classification-on-hateful-memes, hateful-meme-classification-on-hateful-memes-1",,See all 1951 tasks,Emotion Recognition67 benchmar,Emotion Recognition67 benchmar
Hateful_Memes_Challenge,Hateful Memes Challenge Dataset,"A new challenge set for multimodal classification, focusing on detecting hate speech in multimodal memes.",/paper/the-hateful-memes-challenge-detecting-hate,EditUnknown,"Image, Text",English,,,,,,,"Misinformation, Emotion Recognition, Image Captioning",,,See all 1951 tasks,Emotion Recognition67 benchmar,Emotion Recognition67 benchmar
ISEAR,ISEAR Dataset,"Over a period of many years during the 1990s, a large group of psychologists all over the world collected data in the ISEAR project, directed by Klaus R. Scherer and Harald Wallbott. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of 7 major emotions (joy, fear, anger, sadness, disgust, shame, and guilt). In each case, the questions covered the way they had appraised the situation and how they reacted. The final data set thus contained reports on seven emotions each by close to 3000 respondents in 37 countries on all 5 continents.",https://www.unige.ch/cisa/research/materials-and-online-research/research-material/,EditCC BY-NC-SA 3.0,Image,,,,,,,,"Emotion Recognition, Emotion Classification, Word Embeddings",,,See all 1951 tasks,Emotion Recognition67 benchmar,Emotion Recognition67 benchmar
MSP-IMPROV,MSP-IMPROV Dataset,"We present the MSP-IMPROV corpus, a multimodal emotional database, where the goal is to have control over lexical content and emotion while also promoting naturalness in the recordings. Studies on emotion perception often require stimuli with fixed lexical content, but that convey different emotions. These stimuli can also serve as an instrument to understand how emotion modulates speech at the phoneme level, in a manner that controls for coarticulation. Such audiovisual data are not easily available from natural recordings. A common solution is to record actors reading sentences that portray different emotions, which may not produce natural behaviors. We propose an alternative approach in which we define hypothetical scenarios for each sentence that are carefully designed to elicit a particular emotion. Two actors improvise these emotion-specific situations, leading them to utter contextualized, non-read renditions of sentences that have fixed lexical content and convey different emotions. We describe the context in which this corpus was recorded, the key features of the corpus, the areas in which this corpus can be useful, and the emotional content of the recordings. The paper also provides the performance for speech and facial emotion classifiers. The analysis brings novel classification evaluations where we study the performance in terms of inter-evaluator agreement and naturalness perception, leveraging the large size of the audiovisual database.",https://production-media.paperswithcode.com/datasets/d5144405-2dc4-4522-bd92-0bd0e5c3bd52.png,EditAcademic License,"Audio, Image",,,,,,,,"Speech Emotion Recognition, Valence Estimation, Emotion Recognition, Dominance Estimation, Arousal Estimation, Emotion Classification","speech-emotion-recognition-on-msp-improv, arousal-estimation-on-msp-improv",,See all 1951 tasks,Emotion Recognition67 benchmar,Emotion Recognition67 benchmar
SEED,SEED Dataset,"The SEED dataset contains subjects' EEG signals when they were watching films clips. The film clips are carefully selected so as to induce different types of emotion, which are positive, negative, and neutral ones.",http://bcmi.sjtu.edu.cn/home/seed/index.html,"EditCustom (research-only, non-commercial)",Image,,,,,,,,"Emotion Recognition, EEG Emotion Recognition, Electroencephalogram (EEG)","emotion-recognition-on-seed, eeg-on-seed-iv, eeg-emotion-recognition-on-seed-iv, eeg-on-seed",,See all 1951 tasks,Emotion Recognition67 benchmar,Emotion Recognition67 benchmar
CPED,CPED Dataset,"We construct a dataset named CPED from 40 Chinese TV shows. CPED consists of multisource knowledge related to empathy and personal characteristic. This knowledge covers 13 emotions, gender, Big Five personality traits, 19 dialogue acts and other knowledge. 


We build a multiturn Chinese Personalized and Emotional Dialogue dataset called CPED. To the best of our knowledge, CPED is the first Chinese personalized and emotional dialogue dataset. CPED contains 12K dialogues and 133K utterances with multi-modal context. Therefore, it can be used in both complicated dialogue understanding and human-like conversation generation.
CPED has been annotated with 3 character attributes (name, gender age), Big Five personality traits, 2 types of dynamic emotional information (sentiment and emotion) and DAs. The personality traits and emotions can be used as prior external knowledge for open-domain conversation generation, making the conversation system have a good command of personification capabilities.
We propose three tasks for CPED: personality recognition in conversations (PRC), emotion recognition in conversations (ERC), and personalized and emotional conversation (PEC). A set of experiments verify the importance of using personalities and emotions as prior external knowledge for conversation generation.",https://production-media.paperswithcode.com/datasets/b88f8229-0c4f-4e3e-89a9-9ed45bd21eba.png,EditApache-2.0,"Image, Text",English,,,,,,,"Dialogue Generation, Conversational Response Generation, Emotion Recognition in Conversation, Open-Domain Dialog, Emotion Recognition, Personalized and Emotional Conversation, Personality Trait Recognition, Dialog Act Classification, Emotional Dialogue Acts, Dialogue Act Classification, Personality Recognition in Conversation, Multimodal Emotion Recognition","personality-recognition-in-conversation-on-1, personalized-and-emotional-conversation-on, emotion-recognition-in-conversation-on-cped",,See all 1951 tasks,Emotion Recognition in Convers,Emotion Recognition in Convers
DailyDialog,DailyDialog Dataset,"DailyDialog is a high-quality multi-turn open-domain English dialog dataset. It contains 13,118 dialogues split into a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. On average there are around 8 speaker turns per dialogue with around 15 tokens per turn.",http://yanran.li/dailydialog,"EditCustom (research-only, non-commercial)","Image, Text",English,,,,,,,"Text Generation, Emotion Recognition in Conversation","emotion-recognition-in-conversation-on-3, text-generation-on-dailydialog",,See all 1951 tasks,Emotion Recognition in Convers,Emotion Recognition in Convers
EmoContext,EmoContext Dataset,"EmoContext consists of three-turn English Tweets. The emotion labels include happiness, sadness, anger and other.",https://production-media.paperswithcode.com/datasets/2-Table1-1_1.png,EditCustom,Image,,,,,,,,Emotion Recognition in Conversation,emotion-recognition-in-conversation-on-ec,,See all 1951 tasks,Emotion Recognition in Convers,Emotion Recognition in Convers
EmoryNLP,EmoryNLP Dataset,"EmoryNLP comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)’s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral.",https://production-media.paperswithcode.com/datasets/2-Table1-1.png,EditUnknown,Image,,1982,,,,,,Emotion Recognition in Conversation,emotion-recognition-in-conversation-on-4,,See all 1951 tasks,Emotion Recognition in Convers,Emotion Recognition in Convers
EmoWOZ,EmoWOZ Dataset,"EmoWOZ is the first large-scale open-source dataset for emotion recognition in task-oriented dialogues. It contains emotion annotations for user utterances in the entire MultiWOZ (10k+ human-human dialogues) and DialMAGE (1k human-machine dialogues collected from our human trial). Overall, there are 83k user utterances annotated. In addition, the emotion annotation scheme is tailored to task-oriented dialogues and considers the valence, the elicitor, and the conduct of the user emotion.",https://huggingface.co/datasets/hhu-dsml/emowoz,EditCreative Commons Attribution-NonCommercial 4.0 International Public License,Image,,,,,,,,"Emotion Recognition, Emotion Recognition in Conversation",emotion-recognition-in-conversation-on-emowoz-1,,See all 1951 tasks,Emotion Recognition in Convers,Emotion Recognition in Convers
SEMAINE,SEMAINE Dataset,"The SEMAINE videos dataset contains spontaneous data capturing the audiovisual interaction between a human and an operator undertaking the role of an avatar with four personalities: Poppy (happy), Obadiah (gloomy), Spike (angry) and Prudence (pragmatic). The audiovisual sequences have been recorded at a video rate of 25 fps (352 x 288 pixels). The dataset consists of audiovisual interaction between a human and an operator undertaking the role of an agent (Sensitive Artificial Agent). SEMAINE video clips have been annotated with couples of epistemic states such as agreement, interested, certain, concentration, and thoughtful with continuous rating (within the range [1,-1]) where -1 indicates most negative rating (i.e: No concentration at all) and +1 defines the highest (Most concentration). Twenty-four recording sessions are used in the Solid SAL scenario. Recordings are made of both the user and the operator, and there are usually four character interactions in each recording session, providing a total of 95 character interactions and 190 video clips.",https://arxiv.org/abs/2005.13982,EditUnknown,Image,,,,,,,,Emotion Recognition in Conversation,emotion-recognition-in-conversation-on-2,,See all 1951 tasks,Emotion Recognition in Convers,Emotion Recognition in Convers
3U-VQA,3U-VQA Dataset,"To tackle the challenge of obtaining out-of-distribution (OOD) data for LVQA models, we introduced a novel dataset named 3U-VQA dataset (Usual, Unusual and Unknown object scenarios for LVQA with difficulty scoring dataset). The dataset comprises question and image sets. Each instance in the questions set is associated with a set of features representing the question-related criteria set. The questions and their ground truth answers are written using placeholders for the objects and their features, which can be specified based on the user needs and requirements. When creating the questions, we avoided deliberately binary (Yes/No) questions to prevent potential bias caused by the question's type in the model response.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache-2.0 license,"Image, Text",English,,,,,,,"Explainable Artificial Intelligence (XAI), Visual Question Answering (VQA), Image Captioning",,,See all 1951 tasks,Explainable artificial intelli,Explainable Artificial Intelli
ADNI,ADNI Dataset,"Alzheimer's Disease Neuroimaging Initiative (ADNI) is a multisite study that aims to improve clinical trials for the prevention and treatment of Alzheimer’s disease (AD).[1] This cooperative study combines expertise and funding from the private and public sector to study subjects with AD, as well as those who may develop AD and controls with no signs of cognitive impairment.[2] Researchers at 63 sites in the US and Canada track the progression of AD in the human brain with neuroimaging, biochemical, and genetic biological markers.[2][3] This knowledge helps to find better clinical trials for the prevention and treatment of AD. ADNI has made a global impact,[4]
 firstly by developing a set of standardized protocols to allow the comparison of results from multiple centers,[4] and secondly by its data-sharing policy which makes available all at the data without embargo to qualified researchers worldwide.[5] To date, over 1000 scientific publications have used ADNI data.[6] A number of other initiatives related to AD and other diseases have been designed and implemented using ADNI as a model.[4] ADNI has been running since 2004 and is currently funded until 2021.[7]

Source: Wikipedia, https://en.wikipedia.org/wiki/Alzheimer%27s_Disease_Neuroimaging_Initiative",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Graph, Image",,2004,,,,,,"Alzheimer's Disease Detection, Graph Classification, Cubic splines Image Registration, Stable MCI vs Progressive MCI, Anomaly Detection, Explainable Artificial Intelligence (XAI)","stable-mci-vs-progressive-mci-on-adni, graph-classification-on-adni, anomaly-detection-on-adni, alzheimer-s-disease-detection-on-adni, explainable-artificial-intelligence-xai-on, cubic-splines-image-registration-on-adni",,See all 1951 tasks,Explainable artificial intelli,Explainable Artificial Intelli
InVar-100,InVar-100 Dataset,"The Industrial Objects in Varied Contexts (InVar) Dataset was internally produced by our team and contains 100 objects in 20800 total images (208 images per class). The objects consist of common automotive, machine and robotics lab parts. Each class contains 4 sub-categories (52 images each) with different attributes and visual complexities. 

White background (D_wh): The object is against a clean white background and the object is clear, centred and in focus. 

Stationary Setup (D_st): These images are also taken against a clean background using a stationary camera setup, with uncentered objects at a constant distance. The images have lower DPI resolution with occasional cropping. 

Handheld (D_ha): These images are taken with the user holding the objects, with occasional occluding. 

Cluttered background (D_cl): These images are taken with the object placed along with other objects from the lab in the background and no occlusion.",https://production-media.paperswithcode.com/datasets/9a1617d0-cb7f-4ce7-9c4f-20234c6ce9a1.png,EditCreative Commons Attribution 4.0,Image,,,,,208 images,,,"Explainable Artificial Intelligence (XAI), Image Classification, Continual Learning",,,See all 1951 tasks,Explainable artificial intelli,Explainable Artificial Intelli
SpanEX,SpanEX Dataset,"Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). We introduce SpanEx, a multi-annotator dataset of human-annotated span interaction explanations for two NLU tasks: NLI and FC.

SpanEx 
- consists of 7071 instances annotated for span interactions. 
- the first dataset with human phrase-level interaction explanations with explicit labels for interaction types.
- annotated by three annotators, which opens new avenues for studies of human explanation agreement -- an understudied area in the explainability literature.",https://production-media.paperswithcode.com/datasets/1580911c-4a94-465e-a8cf-0694eb66ac72.png,Editmit,Text,English,,,,7071 instances,,,"Explainable Artificial Intelligence (XAI), Explanation Generation",,,See all 1951 tasks,Explainable artificial intelli,Explainable Artificial Intelli
XImageNet-12,XImageNet-12 Dataset,"Enlarge the dataset to understand how image background effect the Computer Vision ML model. With the following topics: Blur Background / Segmented Background / AI generated Background/ Bias of tools during annotation/ Color in Background / Dependent Factor in Background/ LatenSpace Distance of Foreground/ Random Background with Real Environment!

We introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc.,

Our research builds upon the foundation laid by ""Noise or Signal: The Role of Image Backgrounds in Object Recognition"" (Xiao et al., ICLR 2022), ""Explainable AI: Object Recognition With Help From Background"" (Qiang et al., ICLR Workshop 2022), reinforced the notion that models trained solely on backgrounds can substantially improve accuracy. One noteworthy discovery highlighted in their studies is that more accurate models tend to rely less on backgrounds.",https://production-media.paperswithcode.com/datasets/2b15a92b-f7b1-44cb-a024-e48969cc3d07.png,Edithttp://creativecommons.org/licenses/by-nc-sa/4.0/,Image,English,2022,,,200K images,,12,"Explainable Artificial Intelligence (XAI), Classification, Semantic Segmentation",classification-on-ximagenet-12,,See all 1951 tasks,Explainable artificial intelli,Explainable Artificial Intelli
Exposure_Fairness7_papers_with_code_Dataset,Exposure Fairness7 papers with code Dataset,,https://paperswithcode.com/dataset/exposure-fairness,,,,,,,,,,,,,See all 1951 tasks,Exposure Fairness7 papers with,Exposure Fairness7 papers with
LSHTC,LSHTC Dataset,"LSHTC is a dataset for large-scale text classification. The data used in the LSHTC challenges originates from two popular sources: the DBpedia and the ODP (Open Directory Project) directory, also known as DMOZ. DBpedia instances were selected from the english, non-regional Extended Abstracts provided by the DBpedia site. The DMOZ instances consist
of either Content vectors, Description vectors or both. A Content vectors is obtained by directly indexing the web page using standard indexing chain (preprocessing, stemming/lemmatization, stop-word removal).",/paper/lshtc-a-benchmark-for-large-scale-text,EditUnknown,"Image, Text",English,,,,,,,"Multi-Label Classification, Extreme Multi-Label Classification, Text Classification",,,See all 1951 tasks,Extreme Multi-Label Classifica,Extreme Multi-Label Classifica
AFW,AFW Dataset,"AFW (Annotated Faces in the Wild) is a face detection dataset that contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box.",https://ieeexplore.ieee.org/document/6248014,EditUnknown,Image,,,,,205 images,,,"Face Alignment, Face Detection, Facial Landmark Detection",face-detection-on-annotated-faces-in-the-wild,,See all 1951 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
COCO-WholeBody,COCO-WholeBody Dataset,"COCO-WholeBody is an extension of COCO dataset with whole-body annotations. There are 4 types of bounding boxes (person box, face box, left-hand box, and right-hand box) and 133 keypoints (17 for body, 6 for feet, 68 for face and 42 for hands) annotations for each person in the image.",/paper/whole-body-human-pose-estimation-in-the-wild,EditCC-BY-NC 4.0  ( not for commercial purpose),"3D, Image",English,,,,,,,"Face Detection, Pose Estimation, Multi-Person Pose Estimation, 2D Human Pose Estimation, Foot keypoint detection, Hand Pose Estimation, Facial Landmark Detection","multi-person-pose-estimation-on-coco-1, hand-pose-estimation-on-coco-wholebody, facial-landmark-detection-on-coco-wholebody, face-detection-on-coco-wholebody, foot-keypoint-detection-on-coco-wholebody, 2d-human-pose-estimation-on-coco-wholebody-1",,See all 1951 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
FDDB,FDDB Dataset,"The Face Detection Dataset and Benchmark (FDDB) dataset is a collection of labeled faces from Faces in the Wild dataset. It contains a total of 5171 face annotations, where images are also of various resolution, e.g. 363x450 and 229x410. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Both greyscale and color images are included.",https://arxiv.org/abs/1809.03336,EditUnknown,Image,,,,,,,,Face Detection,face-detection-on-fddb,,See all 1951 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
MALF,MALF Dataset,"The MALF dataset is a large dataset with 5,250 images annotated with multiple facial attributes and it is specifically constructed for fine grained evaluation.",https://arxiv.org/abs/1804.10275,EditUnknown,Image,,,,,250 images,,,"Robust Face Recognition, Object Detection, Face Detection",,,See all 1951 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
MaskedFace-Net,MaskedFace-Net Dataset,"Proposes three types of masked face detection dataset; namely, the Correctly Masked Face Dataset (CMFD), the Incorrectly Masked Face Dataset (IMFD) and their combination for the global masked face detection (MaskedFace-Net).",/paper/maskedface-net-a-dataset-of-correctly,EditUnknown,Image,,,,,,,,Face Detection,,,See all 1951 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
PASCAL_Face,PASCAL Face Dataset,"The PASCAL FACE dataset is a dataset for face detection and face recognition. It has a total of 851 images which are a subset of the PASCAL VOC and has a total of 1,341 annotations. These datasets contain only a few hundreds of images and have limited variations in face appearance.",https://arxiv.org/abs/1804.10275,EditUnknown,Image,,,,,851 images,,,Face Detection,face-detection-on-pascal-face,,See all 1951 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
UMDFaces,UMDFaces Dataset,"UMDFaces is a face dataset divided into two parts:


Still Images - 367,888 face annotations for 8,277 subjects.
Video Frames - Over 3.7 million annotated video frames from over 22,000 videos of 3100 subjects.

Part 1 - Still Images

The dataset contains 367,888 face annotations for 8,277 subjects divided into 3 batches. The annotations contain human curated bounding boxes for faces and estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.

Part 2 - Video Frames

The second part contains 3,735,476 annotated video frames extracted from a total of 22,075 for 3,107 subjects. The annotations contain the estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_5.51.14_PM.png,EditUnknown,Image,,,,,,,,"Face Recognition, Face Verification, Face Detection",,,See all 1951 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
WIDER,WIDER Dataset,"WIDER is a dataset for complex event recognition from static images. As of v0.1, it contains 61 event categories and around 50574 images annotated with event class labels.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_4.05.24_PM.png,EditCustom (research-only),"Image, Text",English,,,,50574 images,,,"Face Detection, Multi-Task Learning, Blind Face Restoration, Image Captioning",blind-face-restoration-on-wider,,See all 1951 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
Face_dataset_by_Generated_Photos,Face dataset by Generated Photos Dataset,"The free Face dataset made for students and teachers. It contains 10,000 photos with equal distribution of race and gender parameters, along with metadata and facial landmarks. Free to use for research with citation Photos by Generated.Photos. 

Photos 

All the photos are 100% synthetic. Based on model-released photos. Royalty-free. Can be used for any research purpose except for the ones violating the law. Worldwide. No time limitations.
Quantity    10,000
Quality     256x256px
Diversity   Ethnicity, gender

Metadata

The JSON files contain the metadata for each image in a machine-readable format, including:
(1) FaceLandmarks: mouth, right_eyebrow, left_eyebrow, right_eye, left_eye, nose, jaw.
(2) FaceAttributes: headPose, gender, makeup, emotion, facialHair, hair (hairColor, hairLength, bald), occlusion, ethnicity, eye_color, smile, age",https://production-media.paperswithcode.com/datasets/716fccc6-cf49-4449-831c-2dd1bd0006e8.png,EditFree for non-commercial purposes with attribution,"Image, Text",English,,,,,,,"Face Detection, Face Generation",,,See all 1951 tasks,Face Generation12 benchmarks13,Face Generation12 benchmarks13
iFakeFaceDB,iFakeFaceDB Dataset,"iFakeFaceDB is a face image dataset for the study of synthetic face manipulation detection, comprising about 87,000 synthetic face images generated by the Style-GAN model and transformed with the GANprintR approach. All images were aligned and resized to the size of 224 x 224.",https://github.com/socialabubi/iFakeFaceDB,EditUnknown,"Image, Text",English,,,,,,,Face Generation,,,See all 1951 tasks,Face Generation12 benchmarks13,Face Generation12 benchmarks13
Adience,Adience Dataset,"The Adience dataset, published in 2014, contains 26,580 photos across 2,284 subjects with a binary gender label and one label from eight different age groups, partitioned into five splits. The key principle of the data set is to capture the images as close to real world conditions as possible, including all variations in appearance, pose, lighting condition and image quality, to name a few.",https://arxiv.org/abs/1708.07689,EditCustom,Image,,2014,,,,,,"Face Recognition, Face Quality Assessement, Age And Gender Classification, Age Estimation","age-and-gender-classification-on-adience, face-recognition-on-adience-online-open-set, face-quality-assessement-on-adience, age-and-gender-classification-on-adience-age, face-recognition-on-adience, age-estimation-on-adience-1",,See all 1951 tasks,Face Recognition44 benchmarks6,Face Recognition44 benchmarks6
CASIA-FASD,CASIA-FASD Dataset,CASIA-FASD is a small face anti-spoofing dataset  containing 50 subjects.,https://arxiv.org/abs/1901.05602,EditUnknown,Image,,,,,,,,"Anomaly Detection, Face Recognition, Face Anti-Spoofing",,,See all 1951 tasks,Face Recognition44 benchmarks6,Face Recognition44 benchmarks6
CASIA-WebFace,CASIA-WebFace Dataset,"The CASIA-WebFace dataset is used for face verification and face identification tasks. The dataset contains 494,414 face images of 10,575 real identities collected from the web.",https://arxiv.org/abs/1811.07104,"EditCustom (research-only, non-commercial)",Image,,,,,,,,"Image Super-Resolution, Face Verification, Metric Learning, Face Recognition, Facial Inpainting","facial-inpainting-on-webface, image-super-resolution-on-webface-8x",,See all 1951 tasks,Face Recognition44 benchmarks6,Face Recognition44 benchmarks6
Extended_Yale_B,Extended Yale B Dataset,The Extended Yale B database contains 2414 frontal-face images with size 192×168 over 38 subjects and about 64 images per subject. The images were captured under different lighting conditions and various facial expressions.,https://arxiv.org/abs/1210.1316,EditUnknown,Image,,,,,64 images,,,"Face Recognition, Image Classification, Image Clustering, Dictionary Learning",image-clustering-on-extended-yale-b,,See all 1951 tasks,Face Recognition44 benchmarks6,Face Recognition44 benchmarks6
MS-Celeb-1M,MS-Celeb-1M Dataset,"The MS-Celeb-1M dataset is a large-scale face recognition dataset consists of 100K identities, and each identity has about 100 facial images. The original identity labels are obtained automatically from webpages.

NOTE: This dataset is currently inactive.",https://arxiv.org/abs/1904.02749,EditUnknown,Image,,,,,,,,"Face Recognition, Face Verification, Face Identification",,,See all 1951 tasks,Face Recognition44 benchmarks6,Face Recognition44 benchmarks6
VGG_Face,VGG Face Dataset,"The VGG Face dataset is face identity recognition dataset that consists of 2,622 identities. It contains over 2.6 million images.",https://www.robots.ox.ac.uk/~vgg/data/vgg_face/,EditCC BY-NC 4.0,Image,,,,,,,,", Face Recognition, Face Verification, Domain Adaptation",on-vgg-face,,See all 1951 tasks,Face Recognition44 benchmarks6,Face Recognition44 benchmarks6
AFLW2000-3D,AFLW2000-3D Dataset,AFLW2000-3D is a dataset of 2000 images that have been annotated with image-level 68-point 3D facial landmarks. This dataset is used for evaluation of 3D facial landmark detection models. The head poses are very diverse and often hard to be detected by a CNN-based face detector.,https://www.tensorflow.org/datasets/catalog/aflw2k3d,EditUnknown,"3D, Image",,2000,,,2000 images,,,"3D Face Alignment, 3D Face Reconstruction, 3D Facial Landmark Localization, Face Alignment, Face Swapping, Head Pose Estimation, Facial Landmark Detection","facial-landmark-detection-on-aflw2000-3d, face-alignment-on-aflw2000-3d, face-swapping-on-aflw2000-3d, 3d-facial-landmark-localization-on-aflw2000, 3d-face-reconstruction-on-aflw2000-3d, 3d-face-alignment-on-aflw2000-3d, face-alignment-on-aflw2000, head-pose-estimation-on-aflw2000",,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
Celeb-DF,Celeb-DF Dataset,"Celeb-DF is a large-scale challenging dataset for deepfake forensics. It includes 590 original videos collected from YouTube with subjects of different ages, ethnic groups and genders, and 5639 corresponding DeepFake videos.",https://github.com/danmohaha/celeb-deepfakeforensics,EditCustom,Image,,,,,,,,"Face Swapping, Image Forensics, DeepFake Detection",,,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
DeeperForensics-1.0,DeeperForensics-1.0 Dataset,"DeeperForensics-1.0 represents the largest face forgery detection dataset by far, with 60,000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. The full dataset includes 48,475 source videos and 11,000 manipulated videos. The source videos are collected on 100 paid and consented actors from 26 countries, and the manipulated videos are generated by a newly proposed many-to-many end-to-end face swapping method, DF-VAE. 7 types of real-world perturbations at 5 intensity levels are employed to ensure a larger scale and higher diversity.",https://production-media.paperswithcode.com/datasets/deeper3.gif,EditUnknown,"Image, Video",,,,,,,,"Video Forensics, Face Swapping, DeepFake Detection",,,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
DFDC,DFDC Dataset,"The DFDC (Deepfake Detection Challenge) is a dataset for deepface detection consisting of more than 100,000 videos.

The DFDC dataset consists of two versions:


Preview dataset. with 5k videos. Featuring two facial modification algorithms.
Full dataset, with 124k videos. Featuring eight facial modification algorithms",/paper/the-deepfake-detection-challenge-dfdc-preview,EditCustom,Image,,,,,,,,"Misinformation, Face Swapping, DeepFake Detection",deepfake-detection-on-dfdc,,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
DFDM,DFDM Dataset,"We created a new dataset, named DFDM, with 6,450 Deepfake videos generated by different Autoencoder models. Specifically, five Autoencoder models with variations in encoder, decoder, intermediate layer, and input resolution, respectively, have been selected to generate Deepfakes based on the same input. We have  observed the visible but subtle visual differences among different Deepfakes, demonstrating the evidence of model attribution artifacts.",https://production-media.paperswithcode.com/datasets/b11b4645-20f6-4796-b584-bc53754b4a61.jpg,EditUnknown,"Image, Text",English,,,,,,,"Face Generation, Face Swapping, DeepFake Detection",,,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
FaceForensics__,FaceForensics++ Dataset,"FaceForensics++ is a forensics dataset consisting of 1000 original video sequences that have been manipulated with four automated face manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The data has been sourced from 977 youtube videos and all videos contain a trackable mostly frontal face without occlusions which enables automated tampering methods to generate realistic forgeries.",https://github.com/ondyari/FaceForensics,EditCustom,"Image, Text",English,,,,,,,"Face Swapping, Image Generation, DeepFake Detection","face-swapping-on-faceforensics, deepfake-detection-on-faceforensics-1",,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
HOD,HOD Dataset,"HOD is a dataset for 3D object reconstruction which contains 35 objects, divided into two subsets named Sculptures and Daily Objects. The Sculptures has five human sculptures with complex geometries and pure white textures. The Daily Objects consists of 30 daily objects with various shapes and appearances. All of the Sculptures and nine of the Daily Objects are paired with high-fidelity scanned meshes as ground truth geometries for evaluation.",https://arxiv.org/pdf/2211.16835v1.pdf,EditCreative Commons Attribution-ShareAlike 4.0 International License,"3D, Image",,,,,,,,"Face Swapping, 3D Reconstruction",face-swapping-on-hod,,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
VGGFace2_HQ,VGGFace2 HQ Dataset,"A high-resolution version of VGGFace2 for academic face editing purposes.
This project uses GFPGAN for image restoration and insightface for data preprocessing (crop and align).",https://production-media.paperswithcode.com/datasets/21b3e40b-fd61-4f7b-87d0-d384bd403cc6.png,EditAttribution-NonCommercial 4.0 International,"Image, Text",English,,,,,,,"Face Generation, Face Swapping, Face Age Editing, Image Generation, Face Transfer, Age Estimation",,,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
VideoForensicsHQ,VideoForensicsHQ Dataset,"VideoForensicsHQ is a benchmark dataset for face video forgery detection, providing high quality visual manipulations.  It is one of the first face video manipulation benchmark sets that also contains audio and thus complements existing datasets along a new challenging dimension. VideoForensicsHQ shows manipulations at much higher video quality and resolution, and shows manipulations that are provably much harder to detect by humans than videos in other datasets. 

VideoForensicsHQ contains 1,737 videos of speaking faces (44% male, 56% female), with 8 different emotions, most of them of “HD” resolution. The videos amount to 1,666,816 frames.",https://arxiv.org/abs/2005.10360,EditUnknown,Image,,,,,,,,"Face Swapping, Metric Learning",,,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
WildDeepfake,WildDeepfake Dataset,"WildDeepfake is a dataset for real-world deepfakes detection which consists of 7,314 face sequences extracted from 707 deepfake videos that are collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop more effective detectors against real-world deepfakes.",https://github.com/deepfakeinthewild/deepfake-in-the-wild,EditUnknown,Image,,,,,,,,"Face Swapping, Metric Learning, DeepFake Detection",,,See all 1951 tasks,Face Swapping3 benchmarks307 p,Face Swapping3 benchmarks307 p
CANDOR_Corpus,CANDOR Corpus Dataset,"The CANDOR corpus is a large, novel, multimodal corpus of 1,656 recorded conversations in spoken English. This 7+ million word, 850 hour corpus totals over 1TB of audio, video, and transcripts, with moment-to-moment measures of vocal, facial, and semantic expression, along with an extensive survey of speaker post conversation reflections.",https://production-media.paperswithcode.com/datasets/a7ed502e-c271-4ea1-8f5e-b2f38bb2f4e5.png,Editsee registration for details,"Audio, Image, Text, Time Series, Video",English,,,,,,,"Emotion Recognition in Conversation, Recognizing Emotion Cause in Conversations, Video Emotion Recognition, Facial Emotion Recognition, Natural Language Understanding, Prosody Prediction, Natural Language Inference, English Conversational Speech Recognition, Facial Action Unit Detection",,,See all 1951 tasks,Facial Emotion Recognition2 be,Facial Emotion Recognition2 be
HEADSET,HEADSET Dataset,"The volumetric representation of human interactions is one of the fundamental domains in the development of immersive media productions and telecommunication applications. Particularly in the context of the rapid advancement of Extended Reality (XR) applications, this volumetric data has proven to be an essential technology for future XR elaboration. In this work, we present a new multimodal database to help advance the development of immersive technologies. Our proposed database provides ethically compliant and diverse volumetric data, in particular 27 participants displaying posed facial expressions and subtle body movements while speaking, plus 11 participants wearing head-mounted displays (HMDs). The recording system consists of a volumetric capture (VoCap) studio, including 31 synchronized modules with 62 RGB cameras and 31 depth cameras. In addition to textured meshes, point clouds, and multi-view RGB-D data, we use one Lytro Illum camera for providing light field (LF) data simultaneously. Finally, we also provide an evaluation of our dataset employment with regard to the tasks of facial expression classification, HMDs removal, and point cloud reconstruction. The dataset can be helpful in the evaluation and performance testing of various XR algorithms, including but not limited to facial expression recognition and reconstruction, facial reenactment, and volumetric video. HEADSET and its all associated raw data and license agreement will be publicly available for research purposes.",https://production-media.paperswithcode.com/datasets/c52f5e51-fd58-4a48-b786-821fa771afcd.png,EditOther (Non-Commercial),"3D, Image",,,,,,,,"3D Face Reconstruction, 3D Facial Expression Recognition, 3D Human Reconstruction, Occluded Face Detection, Facial Emotion Recognition",,,See all 1951 tasks,Facial Emotion Recognition2 be,Facial Emotion Recognition2 be
MH-FED,MH-FED Dataset,This dataset provides a  collection of 162K images and 70 Videos of Meta-Humans. There are  10  Highly realistic Meta-Humans expressing 7 facial expressions.,https://production-media.paperswithcode.com/datasets/7f09ec54-79db-4352-b944-bb5da1cc94ff.png,EditFree For Non-Profit Research Usage With Citation,"Image, Text",English,,,,162K images,,,"Facial Emotion Recognition, Facial expression generation, Facial Expression Translation, Facial Expression Recognition (FER)",,,See all 1951 tasks,Facial Emotion Recognition2 be,Facial Emotion Recognition2 be
Thermal_Face_Database,Thermal Face Database Dataset,"High-resolution thermal infrared face database with extensive manual annotations, introduced by Kopaczka et al, 2018. Useful for training algoeithms for image processing tasks as well as facial expression recognition. The full database itself, all annotations and the complete source code are freely available from the authors for research purposes at https://github.com/marcinkopaczka/thermalfaceproject.

Please cite following papers for the dataset:
[1] M. Kopaczka, R. Kolk and D. Merhof, ""A fully annotated thermal face database and its application for thermal facial expression recognition,"" 2018 IEEE International Instrumentation and Measurement Technology Conference (I2MTC), 2018, pp. 1-6, doi: 10.1109/I2MTC.2018.8409768.
[2] Kopaczka, M., Kolk, R., Schock, J., Burkhard, F., & Merhof, D. (2018). A thermal infrared face database with facial landmarks and emotion labels. IEEE Transactions on Instrumentation and Measurement, 68(5), 1389-1401.",https://production-media.paperswithcode.com/datasets/45aa610e-492d-40d4-924b-e4314e22fcbd.gif,EditBSD-3-Clause license,"Image, Video",,2018,,,,,,"Facial Emotion Recognition, Facial Action Unit Detection, Semantic Segmentation, Facial Landmark Detection",,,See all 1951 tasks,Facial Emotion Recognition2 be,Facial Emotion Recognition2 be
Aff-Wild2,Aff-Wild2 Dataset,"Aff-Wild2 is a large-scale in-the-wild database and an extension of the Aff-Wild dataset for affect recognition. It approximately doubles the number of included video frames and the number of subjects; thus, improving the variability of the included behaviors and of the involved persons. It is the only existing in-the-wild database with annotations for all 3 main behaviour tasks.

The Aff-Wild2 is annotated in a per frame basis for the seven basic expressions (i.e., happiness, surprise, anger, disgust, fear, sadness and the neutral state), twelve action units (AUs 1,2,4,6,7,10,12,15,23,24,25, 26) and valence and arousal. In total Aff-Wild2 consists of 564 videos of around 2.8M frames with 554 subjects.  Aff-Wild2 displays a big diversity in terms of subjects' ages, ethnicities and nationalities; it has also great variations and diversities of environments.

Sources:
1)  Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface;
2)  The 6th affective behavior analysis in-the-wild (abaw) competition",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-02_at_10.47.24_AM.png,EditUnknown,Image,,,,,,,,"Emotion Recognition, Multi-Task Learning, Facial Expression Recognition, Facial Expression Recognition (FER)","facial-expression-recognition-on-aff-wild2, facial-expression-recognition-on-aff-wild2-1",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
AffectNet,AffectNet Dataset,"AffectNet is a large facial expression dataset with around 0.4 million images manually labeled for the presence of eight (neutral, happy, angry, sad, fear, surprise, disgust, contempt) facial expressions along with the intensity of valence and arousal.",https://arxiv.org/abs/2007.10298,EditCustom (non-commercial),Image,,,,,,,,"Arousal Estimation, Valence Estimation, Facial Expression Recognition (FER)","arousal-estimation-on-affectnet, facial-expression-recognition-on-affectnet, valence-estimation-on-affectnet",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
BP4D,BP4D Dataset,"The BP4D-Spontaneous dataset is a 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains using both person-specific and generic approaches.
The database includes forty-one participants (23 women, 18 men). They were 18 – 29 years of age; 11 were Asian, 6 were African-American, 4 were Hispanic, and 20 were Euro-American.  An emotion elicitation protocol was designed to elicit emotions of participants effectively. Eight tasks were covered with an interview process and a series of activities to elicit eight emotions.
The database is structured by participants. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos. As well, the Metadata include manually annotated action units (FACS AU), automatically tracked head pose, and 2D/3D facial landmarks.  The database is in the size of about 2.6TB (without compression).",http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html,EditCustom,"Image, Video",,,,,,,,"Facial Action Unit Detection, Action Unit Detection, Facial Expression Recognition (FER)","facial-action-unit-detection-on-bp4d, facial-expression-recognition-on-bp4d, action-unit-detection-on-bp4d",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
CK_,CK+ Dataset,"The Extended Cohn-Kanade (CK+) dataset contains 593 video sequences from a total of 123 different subjects, ranging from 18 to 50 years of age with a variety of genders and heritage. Each video shows a facial shift from the neutral expression to a targeted peak expression, recorded at 30 frames per second (FPS) with a resolution of either 640x490 or 640x480 pixels. Out of these videos, 327 are labelled with one of seven expression classes: anger, contempt, disgust, fear, happiness, sadness, and surprise. The CK+ database is widely regarded as the most extensively used laboratory-controlled facial expression classification database available, and is used in the majority of facial expression classification methods.",https://arxiv.org/abs/2006.15759,EditCustom (non-commercial),Image,,,,,,,,"Face Verification, Facial Expression Recognition (FER)","facial-expression-recognition-on-ck, face-verification-on-ck",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
DISFA,DISFA Dataset,"The Denver Intensity of Spontaneous Facial Action (DISFA) dataset consists of 27 videos of 4844 frames each, with 130,788 images in total. Action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all.",https://arxiv.org/abs/1602.00172,"EditCustom (research-only, attribution)","Image, Video",,,,,788 images,,,"Facial Action Unit Detection, Smile Recognition, Facial Expression Recognition (FER)","facial-expression-recognition-on-disfa, facial-action-unit-detection-on-disfa, smile-recognition-on-disfa",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
FER2013,FER2013 Dataset,"Fer2013 contains approximately 30,000 facial RGB images of different expressions with size restricted to 48×48, and the main labels of it can be divided into 7 types: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral. The Disgust expression has the minimal number of images – 600, while other labels have nearly 5,000 samples each.",https://arxiv.org/abs/1910.06044,EditUnknown,Image,,,,,000 samples,,,"Image Clustering, Emotion Recognition, Facial Expression Recognition (FER), Image Compression, Facial Expression Recognition","facial-expression-recognition-on-fer2013, image-clustering-on-fer2013, facial-expression-recognition-on-fer2013-1, emotion-recognition-on-fer2013-1, image-compression-on-fer2013",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
FER_,FER+ Dataset,"The FER+ dataset is an extension of the original FER dataset, where the images have been re-labelled into one of 8 emotion types: neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt.",https://github.com/Microsoft/FERPlus,EditCustom,Image,,,,,,,,"Facial Expression Recognition, Facial Expression Recognition (FER)","facial-expression-recognition-on-ferplus, facial-expression-recognition-on-fer-2, facial-expression-recognition-on-fer-1",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
JAFFE,JAFFE Dataset,The JAFFE dataset consists of 213 images of different facial expressions from 10 different Japanese female subjects. Each subject was asked to do 7 facial expressions (6 basic facial expressions and neutral) and the images were annotated with average semantic ratings on each facial expression by 60 annotators.,https://arxiv.org/abs/1411.6235,EditCC BY 4.0,"Image, Text",English,,,,213 images,,,"Clustering Algorithms Evaluation, Image/Document Clustering, Facial Emotion Recognition, Facial Expression Recognition (FER)","clustering-algorithms-evaluation-on-jaffe, facial-expression-recognition-on-jaffe, facial-emotion-recognition-on-jaffe, image-document-clustering-on-jaffe-1",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
Oulu-CASIA,Oulu-CASIA Dataset,"The Oulu-CASIA NIR&VIS facial expression database consists of six expressions (surprise, happiness, sadness, anger, fear and disgust) from 80 people between 23 and 58 years old. 73.8% of the subjects are males. The subjects were asked to sit on a chair in the observation room in a way that he/ she is in front of camera. Camera-face distance is about 60 cm. Subjects were asked to make a facial expression according to an expression example shown in picture sequences. The imaging hardware works at the rate of 25 frames per second and the image resolution is 320 × 240 pixels.",https://ieeexplore.ieee.org/abstract/document/4761697,EditCustom,Image,,,,,,,,"Face Verification, Facial Expression Recognition (FER)","face-verification-on-casia-nir-vis-20, facial-expression-recognition-on-oulu-casia, face-verification-on-oulu-casia, face-verification-on-oulu-casia-nir-vis",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
RAF-DB,RAF-DB Dataset,"The Real-world Affective Faces Database (RAF-DB) is a dataset for facial expression. It contains 29672 facial images tagged with basic or compound expressions by 40 independent taggers. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc.",https://arxiv.org/abs/2007.10298,EditCustom (non-commercial),Image,,,,,,,,"Facial Expression Recognition, Facial Expression Recognition (FER)","facial-expression-recognition-on-raf-db, facial-expression-recognition-on-real-world, facial-expression-recognition-on-raf-db-1",,See all 1951 tasks,Facial Expression Recognition ,Facial Expression Recognition 
DiveFace,DiveFace Dataset,A new face annotation dataset with balanced distribution between genders and ethnic origins.,/paper/sensitivenets-learning-agnostic,EditUnknown,Image,,,,,,,,"Face Recognition, Fairness, Facial Attribute Classification","facial-attribute-classification-on-diveface, fairness-on-diveface",,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
FairFace,FairFace Dataset,"FairFace is a face image dataset which is race balanced. It contains 108,501 images from 7 different race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups.",https://github.com/joojs/fairface,EditCC BY 4.0,Image,,,,,501 images,,,"Decision Making, Fairness, Facial Attribute Classification",facial-attribute-classification-on-fairface,,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
GVGAI,GVGAI Dataset,"The General Video Game AI (GVGAI) framework is widely used in research which features a corpus of over 100 single-player games and 60 two-player games. These are fairly small games, each focusing on specific mechanics or skills the players should be able to demonstrate, including clones of classic arcade games such as Space Invaders, puzzle games like Sokoban, adventure games like Zelda or game-theory problems such as the Iterative Prisoners Dilemma. All games are real-time and require players to make decisions in only 40ms at every game tick, although not all games explicitly reward or require fast reactions; in fact, some of the best game-playing approaches add up the time in the beginning of the game to run Breadth-First Search in puzzle games in order to find an accurate solution. However, given the large variety of games (many of which are stochastic and difficult to predict accurately), scoring systems and termination conditions, all unknown to the players, highly-adaptive general methods are needed to tackle the diverse challenges proposed.",https://arxiv.org/abs/2003.12331,EditUnknown,"Image, Text",English,,,,,,,"Fairness, Image Generation, Recommendation Systems",,,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
HELP,HELP Dataset,"The HELP dataset is an automatically created natural language inference (NLI) dataset that embodies the combination of lexical and logical inferences focusing on monotonicity (i.e., phrase replacement-based reasoning). The HELP (Ver.1.0) has 36K inference pairs consisting of upward monotone, downward monotone, non-monotone, conjunction, and disjunction.",https://github.com/verypluming/HELP,EditCustom,"Image, Text",English,,,,,,,"Data Augmentation, Fairness, Natural Language Inference, Visual Navigation",visual-navigation-on-help-anna-hanna-1,,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
MIAP,MIAP Dataset,"MIAP is a dataset created by obtaining a new set of annotations on a subset of the Open Images dataset, containing bounding boxes and attributes for all of the people visible in those images, as the original Open Images dataset annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image.

The MIAP dataset focuses on enabling ML Fairness research. It provides additional annotations for 100,000 (70k from training and 30k from validation/test) images that contain at least one person bounding box in the original annotations.

These additional annotations provide exhaustive bounding boxes for all people in an image. Person boxes are further annotated with attribute labels for fairness research. Annotated attributes include the human perceived gender presentation (predominantly feminine, predominantly masculine, and unknown) and perceived age range (young, middle, older, and unknown) of the localized person. This procedure adds nearly 100,000 new boxes that were not annotated under the original labeling pipeline.

Annotations on the exhaustive set enable research into the fairness properties of models trained on partial annotations and the pipelines that produce these annotations.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-05-07_at_09.43.31.png,EditUnknown,Image,,,,,,training and 30k from validation/test) images,,"Human Detection, Fairness",,,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
MORPH,MORPH Dataset,"MORPH is a facial age estimation dataset, which contains 55,134 facial images of 13,617 subjects ranging from 16 to 77 years old.",https://arxiv.org/abs/2008.03077,EditCommercial,Image,,,,,,,,"Few-shot Age Estimation, Facial Attribute Classification, Age-Invariant Face Recognition, Face Recognition, Fairness, Age Estimation","age-invariant-face-recognition-on-morph, age-estimation-on-morph-album2, few-shot-age-estimation-on-morph-album2, fairness-on-morph, facial-attribute-classification-on-morph, age-estimation-on-morph, age-estimation-on-morph-album2-caucasian, face-recognition-on-morph",,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
Netflix_Prize,Netflix Prize Dataset,"Netflix Prize consists of about 100,000,000 ratings for 17,770 movies given by 480,189 users. Each rating in the training dataset consists of four entries: user, movie, date of grade, grade. Users and movies are represented with integer IDs, while ratings range from 1 to 5.",https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf,EditCustom,,,,,,,,,"Fairness, Recommendation Systems, Matrix Completion",collaborative-filtering-on-netflix,,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
RFW,RFW Dataset,To validate the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms.,/paper/racial-faces-in-the-wild-reducing-racial-bias,"EditCustom (research-only, non-commercial)",Image,,,,,,,,"Face Recognition, Fairness, Face Verification",,,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
UTKFace,UTKFace Dataset,"The UTKFace dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc.",https://susanqq.github.io/UTKFace/,"EditCustom (research-only, non-commercial)",Image,,,,,,,,"Age/Bias-conflicting, Race/Bias-conflicting, Multi-Task Learning, Race/Unbiased, Age/Unbiased, Facial Attribute Classification, Fairness, Age Estimation","multi-task-learning-on-utkface, fairness-on-utkface, race-bias-conflicting-on-utkface, age-bias-conflicting-on-utkface, age-unbiased-on-utkface, facial-attribute-classification-on-utkface, race-unbiased-on-utkface, age-estimation-on-utkface",,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
WinoBias,WinoBias Dataset,"WinoBias contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenarios where entities could be interacting in plausible ways. Templates were selected to be challenging and designed to cover cases requiring semantics and syntax separately.",https://uclanlp.github.io/corefBias/overview,EditMIT,,,,,,160 sentences,,,"Word Embeddings, Fairness, Coreference Resolution",,,See all 1951 tasks,Fairness9 benchmarks1599 paper,Fairness9 benchmarks1599 paper
ArtiFact,ArtiFact Dataset,"The ArtiFact dataset is a large-scale image dataset that aims to include a diverse collection of real and synthetic images from multiple categories, including Human/Human Faces, Animal/Animal Faces, Places, Vehicles, Art, and many other real-life objects. The dataset comprises 8 sources that were carefully chosen to ensure diversity and includes images synthesized from 25 distinct methods, including 13 GANs, 7 Diffusion, and 5 other miscellaneous generators. The dataset contains 2,496,738 images, comprising 964,989 real images and 1,531,749 fake images.

To ensure diversity across different sources, the real images of the dataset are randomly sampled from source datasets containing numerous categories, whereas synthetic images are generated within the same categories as the real images. Captions and image masks from the COCO dataset are utilized to generate images for text2image and inpainting generators, while normally distributed noise with different random seeds is used for noise2image generators. The dataset is further processed to reflect real-world scenarios by applying random cropping, downscaling, and JPEG compression, in accordance with the IEEE VIP Cup 2022 standards.

The ArtiFact dataset is intended to serve as a benchmark for evaluating the performance of synthetic image detectors under real-world conditions. It includes a broad spectrum of diversity in terms of generators used and syntheticity, providing a challenging dataset for image detection tasks.


Total number of images: 2,496,738
Number of real images: 964,989
Number of fake images: 1,531,749
Number of generators used for fake images: 25 (including 13 GANs, 7 Diffusion, and 5 miscellaneous generators)
Number of sources used for real images: 8
Categories included in the dataset: Human/Human Faces, Animal/Animal Faces, Places, Vehicles, Art, and other real-life objects
Image Resolution: 200 x 200",https://production-media.paperswithcode.com/datasets/2dd44f07-adf8-446a-b434-97d2894123cf.png,EditUnknown,Image,,2022,,,738 images,,8,"Fake Image Attribution, Classification, Fake Image Detection",,,See all 1951 tasks,Fake Image Detection15 papers ,Fake Image Detection15 papers 
DF40,DF40 Dataset,"Forgery Diversity: DF40 comprises 40 distinct deepfake techniques (both representive and SOTA methods are included), facilitating the detection of nowadays' SOTA deepfakes and AIGCs. We provide 10 face-swapping methods, 13 face-reenactment methods, 12 entire face synthesis methods, and 5 face editing.

Forgery Realism: DF40 includes realistic deepfake data created by highly popular generation software and methods, e.g., HeyGen, MidJourney, DeepFaceLab, to simulate real-world deepfakes. We even include the just-released DiT, SiT, PixArt-$\alpha$, etc.

Forgery Scale: DF40 offers million-level deepfake data scale for both images and videos.

Data Alignment: DF40 provides alignment between fake methods and data domains. Most methods (31) are generated under the FF++ and CDF domains. Using our fake data, you can further expand your evaluation (training on FF++ and testing on CDF).",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Fake Image Detection, DeepFake Detection",,,See all 1951 tasks,Fake Image Detection15 papers ,Fake Image Detection15 papers 
TwinSynths,TwinSynths Dataset,"The TwinSynths dataset is a novel benchmark designed to overcome common limitations found in earlier synthetic image datasets, such as low image quality, inadequate content preservation, and limited class diversity. TwinSynths generates pairs of images where each synthetic image is visually identical to its real counterpart, ensuring that the essential content remains intact while showcasing the unique architectural features of the generative models used. TwinSynths comprises two subsets:

TwinSynths-GAN This subset uses a GAN generator architecture which trained from scratch on individual real images using a mean-squared error loss to ensure pixel-level fidelity. By fixing the latent vector input, the method produces synthetic images that closely mirror the original content. The GAN subset consists of 8,000 generated images spanning 80 classes selected from ImageNet.

TwinSynths-DM For the diffusion model-based subset, DDIM inversion is used to maintain the content integrity of the original images. By applying a noise-adding forward process followed by a text-conditioned denoising procedure (using class name prompts), this generates synthetic images that are highly similar to their real counterparts. The same set of ImageNet classes is used as in the GAN subset, allowing for a consistent evaluation across different generative methods.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,"trained from scratch on individual real images using a mean-squared error loss to ensure pixel-level fidelity. By fixing the latent vector input, the method produces synthetic images",80,"Fake Image Detection, Image Classification, Binary Classification, DeepFake Detection",,,See all 1951 tasks,Fake Image Detection15 papers ,Fake Image Detection15 papers 
AwA,AwA Dataset,"Animals with Attributes (AwA) was a dataset for benchmarking transfer-learning algorithms, in particular attribute base classification. It consisted of 30475 images of 50 animals classes with six pre-extracted feature representations for each image. The animals classes are aligned with Osherson's classical class/attribute matrix, thereby providing 85 numeric attribute values for each class. Using the shared attributes, it is possible to transfer information between different classes.
The Animals with Attributes dataset was suspended. Its images are not available anymore because of copyright restrictions. A drop-in replacement, Animals with Attributes 2, is available instead.",https://arxiv.org/abs/1501.04560,EditUnknown,Image,,,,,30475 images,,,"Concept-based Classification, Few-Shot Image Classification, Long-tail learning with class descriptors, Generalized Zero-Shot Learning, Zero-Shot Learning, Generalized Few-Shot Learning",,,See all 1951 tasks,Few-Shot Image Classification1,Few-Shot Image Classification1
Caltech-256,Caltech-256 Dataset,"Caltech-256 is an object recognition dataset containing 30,607 real-world images, of different sizes, spanning 257 classes (256 object classes and an additional clutter class). Each class is represented by at least 80 images. The dataset is a superset of the Caltech-101 dataset.",https://arxiv.org/abs/2005.14070,EditUnknown,Image,,,,,80 images,,257,"Image Classification, Few-Shot Image Classification, Semi-Supervised Image Classification","semi-supervised-image-classification-on-10, image-classification-on-caltech-256, semi-supervised-image-classification-on-11, few-shot-image-classification-on-caltech-256",,See all 1951 tasks,Few-Shot Image Classification1,Few-Shot Image Classification1
iNaturalist,iNaturalist Dataset,"The iNaturalist 2017 dataset (iNat) contains 675,170 training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images per category. For example, the largest super-category “Plantae (Plant)” has 196,613 images from 2,101 categories; whereas the smallest super-category “Protozoa” only has 381 images from 4 categories.",https://arxiv.org/abs/1806.06193,EditCustom (non-commercial),"Image, Text",English,2017,,,613 images,"training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images",101,"Test Agnostic Long-Tailed Learning, Fine-Grained Image Classification, Few-Shot Image Classification, Image Retrieval, Image Generation, Image Classification, Long-tail Learning","few-shot-image-classification-on-inaturalist-1, image-retrieval-on-inaturalist, image-classification-on-inat2021-mini, image-classification-on-inaturalist, test-agnostic-long-tailed-learning-on-1, long-tail-learning-on-inaturalist-2018, image-generation-on-inaturalist-2019, image-classification-on-inaturalist-2019, image-classification-on-inaturalist-2018, few-shot-image-classification-on-inaturalist, few-shot-image-classification-on-inaturalist-2, fine-grained-image-classification-on-3, few-shot-image-classification-on-inaturalist-3",,See all 1951 tasks,Few-Shot Image Classification1,Few-Shot Image Classification1
mini-Imagenet,mini-Imagenet Dataset,"mini-Imagenet is proposed by  Matching Networks for One Shot Learning
. In NeurIPS, 2016. This dataset consists of 50000 training images and 10000 testing images, evenly
distributed across 100 classes.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2016,,,,training images and 10000 testing images,100,"Few-Shot Image Classification, Few-Shot Learning, Unsupervised Few-Shot Image Classification, Cross-Domain Few-Shot, Continual Learning, Few-Shot Class-Incremental Learning","unsupervised-few-shot-image-classification-on, few-shot-image-classification-on-mini-9, few-shot-learning-on-mini-imagenet-1-shot-2, few-shot-learning-on-mini-imagenet-5-shot, few-shot-class-incremental-learning-on-mini, few-shot-image-classification-on-mini-4, continual-learning-on-miniimagenet, few-shot-image-classification-on-mini-6, cross-domain-few-shot-on-miniimagenet, few-shot-image-classification-on-mini-12, few-shot-image-classification-on-mini-13, few-shot-image-classification-on-mini-3, few-shot-image-classification-on-mini-8, few-shot-learning-on-mini-imagenet-5-way-1, few-shot-image-classification-on-mini-7, few-shot-image-classification-on-mini-1, few-shot-image-classification-on-mini-10, few-shot-image-classification-on-mini-2, few-shot-image-classification-on-mini-5, unsupervised-few-shot-image-classification-on-1",,See all 1951 tasks,Few-Shot Image Classification1,Few-Shot Image Classification1
Stanford_Cars,Stanford Cars Dataset,"The Stanford Cars dataset consists of 196 classes of cars with a total of 16,185 images, taken from the rear. The data is divided into almost a 50-50 train/test split with 8,144 training images and 8,041 testing images. Categories are typically at the level of Make, Model, Year. The images are 360×240.",https://arxiv.org/abs/1702.01721,EditCustom (non-commercial),"Image, Text",English,,,,185 images,"train/test split with 8,144 training images",196,"Image Clustering, Fine-Grained Image Classification, Prompt Engineering, Few-Shot Image Classification, Few-Shot Learning, Zero-Shot Learning, Continual Learning, Image Generation, Learning with coarse labels, Neural Architecture Search, Image Classification, Transductive Zero-Shot Classification","few-shot-image-classification-on-stanford-3, fine-grained-image-classification-on-stanford, image-generation-on-stanford-cars, few-shot-image-classification-on-stanford-2, transductive-zero-shot-classification-on-5, prompt-engineering-on-stanford-cars-1, neural-architecture-search-on-stanford-cars, zero-shot-learning-on-stanford-cars, few-shot-learning-on-stanford-cars, image-clustering-on-stanford-cars, learning-with-coarse-labels-on-stanford-cars, image-classification-on-stanford-cars, continual-learning-on-stanford-cars-fine",,See all 1951 tasks,Few-Shot Image Classification1,Few-Shot Image Classification1
tieredImageNet,tieredImageNet Dataset,"The tieredImageNet dataset is a larger subset of ILSVRC-12 with 608 classes (779,165 images) grouped into 34 higher-level nodes in the ImageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of training, validation, and testing nodes, and the corresponding classes form the respective meta-sets. As argued in Ren et al. (2018), this split near the root of the ImageNet hierarchy results in a more challenging, yet realistic regime with test classes that are less similar to training classes.",https://github.com/yaoyao-liu/tiered-imagenet-tools,EditCustom (non-commercial),Image,English,2018,,,165 images,,608,"Unsupervised Few-Shot Image Classification, Image Classification, Few-Shot Image Classification","few-shot-image-classification-on-tiered-3, image-classification-on-tiered-imagenet-5-way, few-shot-image-classification-on-tiered, few-shot-image-classification-on-tiered-1, unsupervised-few-shot-image-classification-on-2, few-shot-image-classification-on-tiered-2, unsupervised-few-shot-image-classification-on-3",,See all 1951 tasks,Few-Shot Image Classification1,Few-Shot Image Classification1
MRPC,MRPC Dataset,"Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases).",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313059/,EditUnknown,Text,English,,,,,,,"Natural Language Inference, Semantic Textual Similarity within Bi-Encoder, Few-Shot Learning, Semantic Textual Similarity","semantic-textual-similarity-within-bi-encoder, natural-language-inference-on-mrpc, semantic-textual-similarity-on-mrpc, few-shot-learning-on-mrpc, semantic-textual-similarity-on-mrpc-dev",,See all 1951 tasks,Few-Shot Learning66 benchmarks,Few-Shot Learning66 benchmarks
UCF101,UCF101 Dataset,"UCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 × 240.",https://arxiv.org/abs/1711.03273,EditMIT,"Image, Text, Time Series, Video",English,,,,,,101,"Human Activity Recognition, Open Set Action Recognition, Self-supervised Video Retrieval, Action Classification, Image Clustering, Self-Supervised Action Recognition Linear, Temporal Action Localization, Prompt Engineering, Early Action Prediction, Zero-Shot Learning, Few Shot Action Recognition, Zero-Shot Action Recognition, Few-Shot Learning, Text-to-Video Generation, Action Recognition, Self-Supervised Action Recognition, Skeleton Based Action Recognition, Video Generation, Action Recognition In Videos, Video Frame Interpolation, Transductive Zero-Shot Classification","self-supervised-action-recognition-on-ucf101-1, self-supervised-action-recognition-on-ucf101, action-recognition-in-videos-on-ucf-101, action-recognition-in-videos-on-ucf101-2, text-to-video-generation-on-ucf-101, video-generation-on-ucf-101-16-frames-128x128, self-supervised-action-recognition-linear-on, human-activity-recognition-on-ucf-101, video-generation-on-ucf-101, zero-shot-action-recognition-on-ucf101, transductive-zero-shot-classification-on-9, self-supervised-video-retrieval-on-ucf101, prompt-engineering-on-ucf101, video-generation-on-ucf-101-16-frames, early-action-prediction-on-ucf101, video-generation-on-ucf-101-16-frames-64x64, action-classification-on-ucf101, image-clustering-on-ucf101, skeleton-based-action-recognition-on-ucf101, few-shot-learning-on-ucf101, action-recognition-on-ucf-101, zero-shot-learning-on-ucf101, action-recognition-in-videos-on-ucf101, few-shot-action-recognition-on-ucf101, open-set-action-recognition-on-ucf101-mitv2, video-frame-interpolation-on-ucf101-1",,See all 1951 tasks,Few-Shot Learning66 benchmarks,Few-Shot Learning66 benchmarks
BankNote-Net,BankNote-Net Dataset,"Millions of people around the world have low or no vision. Assistive software applications have been developed for a variety of day-to-day tasks, including currency recognition. To aid with this task, we present BankNote-Net, an open dataset for assistive currency recognition. The dataset consists of a total of 24,816 embeddings of banknote images captured in a variety of assistive scenarios, spanning 17 currencies and 112 denominations. These compliant embeddings were learned using supervised contrastive learning and a MobileNetV2 architecture, and they can be used to train and test specialized downstream models for any currency, including those not covered by our dataset or for which only a few real images per denomination are available (few-shot learning). We deploy a variation of this model for public use in the last version of the Seeing AI app developed by Microsoft, which has over a 100 thousand monthly active users.",https://github.com/microsoft/banknote-net,EditCDLA-Permissive-2.0,Image,,,,,,"train and test specialized downstream models for any currency, including those not covered by our dataset or for which only a few real images",,"Image Classification, Contrastive Learning, Few-Shot Object Detection, Few-Shot Learning",,,See all 1951 tasks,Few-Shot Object Detection15 be,Few-Shot Object Detection15 be
ELEVATER,ELEVATER Dataset,"The ELEVATER benchmark is a collection of resources for training, evaluating, and analyzing language-image models on image classification and object detection. ELEVATER consists of:


Benchmark: A benchmark suite that consists of 20 image classification datasets and 35 object detection datasets, augmented with external knowledge
Toolkit: An automatic hyper-parameter tuning toolkit; Strong language-augmented efficient model adaptation methods.
Baseline: Pre-trained language-free and language-augmented visual models.
Knowledge: A platform to study the benefit of external knowledge for vision problems.
Evaluation Metrics: Sample-efficiency (zero-, few-, and full-shot) and Parameter-efficiency.
Leaderboard: A public leaderboard to track performance on the benchmark

The ultimate goal of ELEVATER is to drive research in the development of language-image models to tackle core computer vision problems in the wild.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Object Detection, Image Classification, Few-Shot Object Detection, Zero-Shot Object Detection","object-detection-on-odinw-full-shot-35-tasks, object-detection-on-elevater",,See all 1951 tasks,Few-Shot Object Detection15 be,Few-Shot Object Detection15 be
FSOD,FSOD Dataset,Few-Shot Object Detection Dataset (FSOD) is a high-diverse dataset specifically designed for few-shot object detection and intrinsically designed to evaluate thegenerality of a model on novel categories.,https://github.com/fanq15/Few-Shot-Object-Detection-Dataset,EditUnknown,Image,,,,,,,,"Object Detection, Few-Shot Object Detection, Object Counting",,,See all 1951 tasks,Few-Shot Object Detection15 be,Few-Shot Object Detection15 be
LOGO-Net,LOGO-Net Dataset,A large-scale logo image database for logo detection and brand recognition from real-world product images.,/paper/logo-net-large-scale-deep-logo-detection-and,EditUnknown,Image,,,,,,,,"Object Detection, Logo Recognition, Few-Shot Object Detection",,,See all 1951 tasks,Few-Shot Object Detection15 be,Few-Shot Object Detection15 be
VizWiz-FewShot,VizWiz-FewShot Dataset,"VizWiz-FewShot is a a few-shot localization dataset originating from photographers who authentically were trying to learn about the visual content in the images they took. It includes nearly 10,000 segmentations of 100 categories in over 4,500 images that were taken by people with visual impairments.",https://production-media.paperswithcode.com/datasets/3a933a06-21ff-49f9-9213-fe4f09c67e8a.png,EditCreative Common CC-BY 4.0,Image,,,,,500 images,,100,"Object Detection, Instance Segmentation, Few-Shot Object Detection, Semantic Segmentation",,,See all 1951 tasks,Few-Shot Object Detection15 be,Few-Shot Object Detection15 be
FewSOL,FewSOL Dataset,"The Few-Shot Object Learning (FewSOL) dataset can be used for object recognition with a few images per object. It contains 336 real-world objects with 9 RGB-D images per object from different views. Object segmentation masks, object poses and object attributes are provided. In addition, synthetic images generated using 330 3D object models are used to augment the dataset.  FewSOL dataset can be used to study a set of few-shot object recognition problems such as classification, detection and segmentation, shape reconstruction, pose estimation, keypoint correspondences and attribute recognition. 

Motivation: If robots can recognize objects from a few exemplar images, it is possible to scale up the number of objects a robot can recognize because collecting a few images per object is a much easier process compared to building a 3D model of an object. In addition, models trained in the meta-learning setting can generalize to new objects without re-training.",https://production-media.paperswithcode.com/datasets/f99cb513-7f27-4592-b02e-35141f22afb2.png,EditMIT,"3D, Image, Text",English,,,,,,,"Object Recognition, Pose Estimation, Few-Shot Image Classification, Few-Shot Learning, Few-Shot Semantic Segmentation, Image-to-Text Retrieval, 3D Shape Reconstruction, Zero-shot Text-to-Image Retrieval, Key Point Matching, Image-text Classification",,,See all 1951 tasks,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation
FSS-1000,FSS-1000 Dataset,"FSS-1000 is a 1000 class dataset for few-shot segmentation. The dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc.",https://github.com/HKUSTCV/FSS-1000,EditUnknown,Image,,,,,,,,Few-Shot Semantic Segmentation,few-shot-semantic-segmentation-on-fss-1000,,See all 1951 tasks,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation
GAS,GAS Dataset,"GAS (Grasp Area Segmentation) dataset consists of 10089 RGB images of cluttered scenes grouped into 1121 grasp-area segmentation tasks. For each RGB image we provide a binary segmentation map with the graspable and non-graspable regions for every object in the scene. The dataset can be used for meta-training part-based grasp area estimation networks.

For creating the GAS dataset we use the RGB images and corresponding ground truth segmentation masks from the GraspNet 1-Billion dataset.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Time Series",,,,,,,,"Grasp Contact Prediction, Few-Shot Semantic Segmentation",,,See all 1951 tasks,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation
PASCAL-5i,PASCAL-5i Dataset,PASCAL-5i is a dataset used to evaluate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples from 5 classes that are used for evaluating the few-shot learning method. The rest 15 classes are used for training.,https://arxiv.org/abs/1902.11123,EditUnknown,Image,,,,,,valuate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples,5,"Few-Shot Learning, Semantic Segmentation, Few-Shot Semantic Segmentation",few-shot-semantic-segmentation-on-pascal5i-1,,See all 1951 tasks,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation
CausalChaos_,CausalChaos! Dataset,"CausalChaos! is a dataset for causal video question answering. It is based on Tom and Jerry cartoons. It features longer causal chains embedded in dynamic visual scenes. It also features challenging incorrect options, especially, Causal Confusion set which contains causally confounding incorrect options. All these factors prove to be challenging for current VLMs and other traditional Video Question Answering models.",https://github.com/LUNAProject22/CausalChaos,EditUnknown,"Image, Text, Video",English,,,,,,,"Few-shot Video Question Answering, Video Question Answering, Commonsense Causal Reasoning, Grounded Video Question Answering, Visual Question Answering (VQA), Causal Discovery in Video Reasoning, Causal Discovery, Zeroshot Video Question Answer",,,See all 1951 tasks,Few-shot Video Question Answer,Few-shot Video Question Answer
Kinetics-100,Kinetics-100 Dataset,"Kinetics-100 is a dataset split created from the Kinetics dataset to evaluate the performance of few-shot action recognition models. 100 classes are randomly selected from a total of 400 categories, each composed of 100 examples. The 100 classes are further split into 64, 12, and 24 non-overlapping classes to use as the meta-training set, meta-validation set, and meta-testing set, respectively.  Link to the selected samples can be found here: https://github.com/ffmpbgrnn/CMN/tree/master/kinetics-100",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,100 examples,"split created from the Kinetics dataset to evaluate the performance of few-shot action recognition models. 100 classes are randomly selected from a total of 400 categories, each composed of 100 examples",100,Few Shot Action Recognition,few-shot-action-recognition-on-kinetics-100,,See all 1951 tasks,Few Shot Action Recognition5 b,Few Shot Action Recognition5 b
MOMA-LRG,MOMA-LRG Dataset,"A dataset dedicated to multi-object, multi-actor activity parsing.

The dataset contains
* Video-level labels (activities)
* Segment-level labels (sub-activities)
* Atomic actions (spatio-temporal scene graph)

The scene graph annotations contain object/actor classes and bounding boxes, relationship annotations, and object/actor attributes.",https://production-media.paperswithcode.com/datasets/00b82b76-30d5-40f8-80e1-95c4d521c5ec.jpg,EditCC BY-SA 4.0,"Graph, Image, Video",,,,,,,,"Video Segmentation, Video Classification, Scene Graph Detection, Few Shot Action Recognition",few-shot-action-recognition-on-moma-lrg,,See all 1951 tasks,Few Shot Action Recognition5 b,Few Shot Action Recognition5 b
Something-Something-100,Something-Something-100 Dataset,"Something-Something-100 is a dataset split created from Something-Something V2. A total of 100 classes are selected and each comprises 100 samples. The 100 classes were split into 64, 12, and 24 non-overlapping classes to use as the meta-training set, meta-validation set, and meta-testing set, respectively. Link to exactly selected samples can be found here: https://github.com/ffmpbgrnn/CMN/tree/master/smsm-100",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,100 samples,split created from Something-Something V2. A total of 100 classes are selected and each comprises 100 samples,100,Few Shot Action Recognition,few-shot-action-recognition-on-something,,See all 1951 tasks,Few Shot Action Recognition5 b,Few Shot Action Recognition5 b
Polarized_Film_Removal_Dataset,Polarized Film Removal Dataset Dataset,"The current industrial pipeline includes 315 dynamic industrial scenarios, which can be categorized into three types: QR codes, text, and products. To enhance the diversity, we have different films with diverse material properties, coverage areas, film thicknesses, and levels of wrinkling. The film exhibits significant variability across each scenario. On the other hand, to ensure the stability of the industrial imaging pipeline, we maintained a consistent intensity level for the industrial light source and fixed the distance between the camera and the object flow. This helps to minimize the influence of errors external to the industrial system.",https://production-media.paperswithcode.com/datasets/f65fa069-9d07-4801-8301-a87de918298d.jpg,EditUnknown,,,,,,,,,Film Removal,,,See all 1951 tasks,Film Removal1 papers with code,Film Removal1 papers with code
Caltech-101,Caltech-101 Dataset,"The Caltech101 dataset contains images from 101 object categories (e.g., “helicopter”, “elephant” and “chair” etc.) and a background category that contains the images not from the 101 object categories. For each object category, there are about 40 to 800 images, while most classes have about 50 images. The resolution of the image is roughly about 300×200 pixels.",https://arxiv.org/abs/1604.01518,EditUnknown,Image,,,,,800 images,,,"Density Estimation, Image Clustering, Fine-Grained Image Classification, Prompt Engineering, Unsupervised Anomaly Detection, Zero-Shot Learning, Semi-Supervised Image Classification, Semantic correspondence, Transductive Zero-Shot Classification","semantic-correspondence-on-caltech-101, zero-shot-learning-on-caltech-101, unsupervised-anomaly-detection-on-caltech-101-1, image-clustering-on-caltech-101, semi-supervised-image-classification-on-9, fine-grained-image-classification-on-caltech, transductive-zero-shot-classification-on-6, density-estimation-on-caltech-101, prompt-engineering-on-caltech-101, semi-supervised-image-classification-on-8",,See all 1951 tasks,Fine-Grained Image Classificat,Fine-Grained Image Classificat
EMNIST,EMNIST Dataset,EMNIST (extended MNIST) has 4 times more data than MNIST. It is a set of handwritten digits with a 28 x 28 format.,https://arxiv.org/abs/1901.10654,EditUnknown,"Image, Text",English,,,,,,,"Image Clustering, Dimensionality Reduction, Fine-Grained Image Classification, Image Generation, Image Classification","image-classification-on-emnist-bymerge, fine-grained-image-classification-on-emnist, image-generation-on-emnist-letters, image-clustering-on-emnist-balanced, dimensionality-reduction-on-emnist, image-classification-on-emnist-letters, image-classification-on-emnist-balanced, image-classification-on-emnist-digits, image-classification-on-emnist-byclass, fine-grained-image-classification-on-emnist-1",,See all 1951 tasks,Fine-Grained Image Classificat,Fine-Grained Image Classificat
FGVC-Aircraft,FGVC-Aircraft Dataset,"FGVC-Aircraft contains 10,200 images of aircraft, with 100 images for each of 102 different aircraft model variants, most of which are airplanes. The (main) aircraft in each image is annotated with a tight bounding box and a hierarchical airplane model label.
Aircraft models are organized in a four-levels hierarchy. The four levels, from finer to coarser, are:


Model, e.g. Boeing 737-76J. Since certain models are nearly visually indistinguishable, this level is not used in the evaluation.
Variant, e.g. Boeing 737-700. A variant collapses all the models that are visually indistinguishable into one class. The dataset comprises 102 different variants.
Family, e.g. Boeing 737. The dataset comprises 70 different families.
Manufacturer, e.g. Boeing. The dataset comprises 41 different manufacturers.
The data is divided into three equally-sized training, validation and test subsets.",https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/,EditCustom (non-commercial),Image,,,,,200 images,,,"Image Clustering, Fine-Grained Visual Recognition, Fine-Grained Image Classification, Prompt Engineering, Few-Shot Learning, Zero-Shot Learning, Neural Architecture Search, Image Classification, Transductive Zero-Shot Classification, Mitigating Contextual Bias","fine-grained-image-classification-on-fgvc, neural-architecture-search-on-fgvc-aircraft, image-classification-on-fgvc-aircraft, fine-grained-visual-recognition-on-fgvc-2, mitigating-contextual-bias-on-fgvc-aircraft, few-shot-learning-on-fgvc-aircraft-1, image-classification-on-fgvc-aircraft-1, prompt-engineering-on-fgvc-aircraft, image-clustering-on-fgvc-aircraft, zero-shot-learning-on-fgvc-aircraft, fine-grained-image-classification-on-fgvc-2, transductive-zero-shot-classification-on-fgvc",,See all 1951 tasks,Fine-Grained Image Classificat,Fine-Grained Image Classificat
STL-10,STL-10 Dataset,"The STL-10 is an image dataset derived from ImageNet and popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes (such as birds, cats, trucks), among which 5,000 images are partitioned for training while the remaining 8,000 images for testing. All the images are color images with 96×96 pixels in size.",https://arxiv.org/abs/1412.7259,EditCustom (attribution + ImageNet license),"Image, Text",English,,,,000 images,"valuate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images",,"Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly, Image Compression, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Image Clustering, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, Fine-Grained Image Classification, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, Contrastive Learning, Unsupervised Anomaly Detection, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Image Generation, Neural Architecture Search, Out-of-Distribution Detection, Anomaly Detection, Self-Supervised Learning, Image Classification, Unsupervised Image Classification, Semi-Supervised Image Classification","out-of-distribution-detection-on-stl-10, unsupervised-anomaly-detection-on-stl-10, unsupervised-anomaly-detection-with-specified, self-supervised-learning-on-stl-10, image-compression-on-stl-10, image-generation-on-stl-10, unsupervised-anomaly-detection-with-specified-20, unsupervised-anomaly-detection-with-specified-8, semi-supervised-image-classification-on-stl, image-clustering-on-stl-10, fine-grained-image-classification-on-stl-10, anomaly-detection-on-stl-10, neural-architecture-search-on-stl-10, contrastive-learning-on-stl-10, unsupervised-image-classification-on-stl-10, unsupervised-anomaly-detection-with-specified-5, unsupervised-anomaly-detection-with-specified-15, image-classification-on-stl-10",,See all 1951 tasks,Fine-Grained Image Classificat,Fine-Grained Image Classificat
Dafonts_Free,Dafonts Free Dataset,"This is a dataset of 18624 fonts labeled as 100% Free and Public domain / GPL / OFL on https://www.dafont.com/ with .ttf and .otf extensions.

Code used to create it can be found at: https://github.com/duskvirkus/dafonts-free",https://production-media.paperswithcode.com/datasets/006fbf63-172c-40f5-86cb-2894fada623a.png,EditCustom (MIT Based),"Image, Text",English,,,,,,,"Font Recognition, Font Generation, Font Style Transfer",,,See all 1951 tasks,Font Style Transfer5 papers wi,Font Style Transfer5 papers wi
ClonedPerson,ClonedPerson Dataset,"The ClonedPerson dataset is a large-scale synthetic person re-identification dataset introduced in the paper ""Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification"" in CVPR 2022. It is generated by MakeHuman and Unity3D. Characters in this dataset use an automatic approach to directly clone the whole outfits from real-world person images to virtual 3D characters, such that any virtual person thus created will appear very similar to its real-world counterpart. The dataset contains 887,766 synthesized person images of 5,621 identities.",https://production-media.paperswithcode.com/datasets/71843839-362d-4719-82f1-d4fa1cc8e455.png,EditUnknown,Image,,2022,,,,,,"Unsupervised Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification, Unsupervised Domain Adaptation","generalizable-person-re-identification-on-19, unsupervised-domain-adaptation-on, person-re-identification-on-clonedperson, unsupervised-person-re-identification-on-13",,See all 1951 tasks,Generalizable Person Re-identi,Generalizable Person Re-identi
CUHK03-C,CUHK03-C Dataset,"CUHK03-C is an evaluation set that consists of algorithmically generated corruptions applied to the CUHK03 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Person Re-Identification, Generalizable Person Re-identification",person-re-identification-on-cuhk03-c,,See all 1951 tasks,Generalizable Person Re-identi,Generalizable Person Re-identi
Market-1501-C,Market-1501-C Dataset,"Market-1501-C is an evaluation set that consists of algorithmically generated corruptions applied to the Market-1501 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",https://production-media.paperswithcode.com/datasets/4bac1738-87ee-4aff-a14c-d766574b9395.png,EditUnknown,Image,,,,,,,,"Person Re-Identification, Generalizable Person Re-identification",person-re-identification-on-market-1501-c,,See all 1951 tasks,Generalizable Person Re-identi,Generalizable Person Re-identi
MSMT17-C,MSMT17-C Dataset,"MSMT17-C is an evaluation set that consists of algorithmically generated corruptions applied to the MSMT17 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Person Re-Identification, Generalizable Person Re-identification",person-re-identification-on-msmt17-c,,See all 1951 tasks,Generalizable Person Re-identi,Generalizable Person Re-identi
RegDB-C,RegDB-C Dataset,"RegDB-C is an evaluation set that consists of algorithmically generated corruptions applied to the RegDB test-set (color images). These corruptions consist of Noise: Gaussian, shot, impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Cross-Modal Person Re-Identification, Cross-Modal  Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification",cross-modal-person-re-identification-on-regdb-1,,See all 1951 tasks,Generalizable Person Re-identi,Generalizable Person Re-identi
SYSU-MM01-C,SYSU-MM01-C Dataset,"SYSU-MM01-C is an evaluation set that consists of algorithmically generated corruptions applied to the SYSU-MM01 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Cross-Modal Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification",person-re-identification-on-sysu-mm01-c,,See all 1951 tasks,Generalizable Person Re-identi,Generalizable Person Re-identi
aPY,aPY Dataset,"aPY is a coarse-grained dataset composed of 15339 images from 3 broad categories (animals, objects and vehicles), further divided into a total of 32 subcategories (aeroplane, …, zebra).",https://arxiv.org/abs/1809.10120,EditUnknown,Image,,,,,15339 images,,,"Few-Shot Image Classification, Concept-based Classification, Generalized Zero-Shot Learning, Zero-Shot Learning","few-shot-image-classification-on-apy-0-shot, generalized-zero-shot-learning-on-apy, zero-shot-learning-on-apy-0-shot, generalized-zero-shot-learning-on-apy-0-shot, concept-based-classification-on-apy",,See all 1951 tasks,Generalized Zero-Shot Learning,Generalized Zero-Shot Learning
AwA2,AwA2 Dataset,"Animals with Attributes 2 (AwA2) is a dataset for benchmarking transfer-learning algorithms, such as attribute base classification and zero-shot learning. AwA2 is a drop-in replacement of original Animals with Attributes (AwA) dataset, with more images released for each category. Specifically, AwA2 consists of in total 37322 images distributed in 50 animal categories. The AwA2 also provides a category-attribute matrix, which contains an 85-dim attribute vector (e.g., color, stripe, furry, size, and habitat) for each category.",https://arxiv.org/abs/1803.03857,EditUnknown,Image,,,,,37322 images,,,"Concept-based Classification, Few-Shot Image Classification, Generalized Zero-Shot Learning, Zero-Shot Learning, Generalized Few-Shot Learning","concept-based-classification-on-awa2, few-shot-image-classification-on-awa2-0-shot, generalized-few-shot-learning-on-awa2, zero-shot-learning-on-awa2, generalized-zero-shot-learning-on-awa2",,See all 1951 tasks,Generalized Zero-Shot Learning,Generalized Zero-Shot Learning
OntoNotes_5.0,OntoNotes 5.0 Dataset,"OntoNotes 5.0 is a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).

OntoNotes Release 5.0 contains the content of earlier releases - and adds source data from and/or additional annotations for, newswire, broadcast news, broadcast conversation, telephone conversation and web data in English and Chinese and newswire data in Arabic.",https://catalog.ldc.upenn.edu/LDC2013T19,EditUnknown,"Image, Text",English,,,,,,,"FG-1-PG-1, Weakly-Supervised Named Entity Recognition, Named Entity Recognition (NER), Generalized Zero-Shot Learning, Entity Typing, Coreference Resolution, Chinese Named Entity Recognition, UIE, Semantic Role Labeling","entity-typing-on-ontonotes-v5-english, coreference-resolution-on-ontonotes, semantic-role-labeling-on-ontonotes, uie-on-ontonotes-5-0, chinese-named-entity-recognition-on-ontonotes-2, generalized-zero-shot-learning-on-ontonotes, named-entity-recognition-on-ontonotes-5-0, fg-1-pg-1-on-ontonotes-5-0, named-entity-recognition-ner-on-ontonotes-v5, entity-typing-on-ontonotes, weakly-supervised-named-entity-recognition-on-2, named-entity-recognition-on-ontonotes",,See all 1951 tasks,Generalized Zero-Shot Learning,Generalized Zero-Shot Learning
SUN_Attribute,SUN Attribute Dataset,"The SUN Attribute dataset consists of 14,340 images from 717 scene categories, and each category is annotated with a taxonomy of 102 discriminate attributes. The dataset can be used for high-level scene understanding and fine-grained scene recognition.",https://arxiv.org/abs/1606.09349,"EditCustom (research-only, non-commercial)",Image,,,,,340 images,,,"Object Recognition, Image Classification, Generalized Zero-Shot Learning, Zero-Shot Learning","generalized-zero-shot-learning-on-sun, zero-shot-learning-on-sun-attribute",,See all 1951 tasks,Generalized Zero-Shot Learning,Generalized Zero-Shot Learning
Graph_Anomaly_Detection48_papers_with_code_Dataset,Graph Anomaly Detection48 papers with code Dataset,,https://paperswithcode.com/dataset/graph-anomaly-detection,,,,,,,,,,,,,See all 1951 tasks,Graph Anomaly Detection48 pape,Graph Anomaly Detection48 pape
Citeseer,Citeseer Dataset,The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words.,https://linqs.soe.ucsc.edu/data,EditUnknown,"Graph, Image, Time Series",,,,,,,,"Node Clustering, Link Prediction, Graph Classification, Graph Clustering, Node Classification, Community Detection","node-classification-on-citeseer-1, community-detection-on-citeseer, link-prediction-on-citeseer-biased-evaluation, graph-classification-on-citeseer, node-clustering-on-citeseer, node-classification-on-citeseer-with-public-1, graph-clustering-on-citeseer, node-classification-on-citeseer-random, link-prediction-on-citeseer, node-classification-on-citeseer-05, node-classification-on-citeseer-with-public, node-classification-on-citeseer, node-classification-on-citeseer-full, link-prediction-on-citeseer-nonstandard",,See all 1951 tasks,Graph Classification76 benchma,Graph Classification76 benchma
Cora,Cora Dataset,The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.,https://relational.fit.cvut.cz/dataset/CORA,EditUnknown,"Graph, Image, Text, Time Series",English,,,,,,,"Node Clustering, Link Prediction, Graph Classification, Graph structure learning, Graph Clustering, Node Classification, Community Detection, Document Classification","30-trainning-unsupervised-with-linear, link-prediction-on-cora-nonstandard-variant, node-classification-on-cora-random-partition, node-classification-on-cora-fixed-5-node-per, link-prediction-on-cora, node-classification-on-cora, graph-classification-on-cora, graph-structure-learning-on-cora, link-prediction-on-cora-biased-evaluation, node-clustering-on-cora, node-classification-on-cora-1, node-classification-on-cora-full-supervised, node-classification-on-cora-fixed-10-node-per, node-classification-on-cora-05, graph-clustering-on-cora, node-classification-on-cora-3, node-classification-on-cora-with-public-split, document-classification-on-cora, node-classification-on-cora-fixed-20-node-per, community-detection-on-cora",,See all 1951 tasks,Graph Classification76 benchma,Graph Classification76 benchma
MNIST,MNIST Dataset,"The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.",http://yann.lecun.com/exdb/mnist/,EditUnknown,"Graph, Image, Text, Time Series, Video",English,,,,000 examples,"training set of 60,000 examples",,"Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly, Density Estimation, Handwritten Digit Recognition, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Domain Adaptation, Personalized Federated Learning, Anomaly Detection, Continuously Indexed Domain Adaptation, Continual Learning, Network Pruning, Malicious Detection, Graph Classification, Image Clustering, One-Shot Learning, Fine-Grained Image Classification, Unsupervised MNIST, Superpixel Image Classification, Rotated MNIST, Unsupervised Image-To-Image Translation, Image Classification, Core set discovery, Nature-Inspired Optimization Algorithm, Sparse Learning and binarization, Multiview Clustering, Stochastic Optimization, Unsupervised Anomaly Detection, Structured Prediction, Adversarial Defense against FGSM Attack, Unsupervised Image Classification, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Adversarial Defense, Classification with Binary Weight Network, Model Poisoning, Hard-label Attack, Video Prediction, Clustering Algorithms Evaluation, Sequential Image Classification, Image Generation, Deep Clustering, Neural Architecture Search, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, General Classification","image-clustering-on-mnist-test, image-classification-on-mnist, domain-adaptation-on-svnh-to-mnist, density-estimation-on-mnist, neural-architecture-search-on-mnist, image-clustering-on-mnist-full, continuously-indexed-domain-adaptation-on-3, network-pruning-on-mnist, unsupervised-image-classification-on-mnist, model-poisoning-on-mnist, unsupervised-anomaly-detection-with-specified-17, graph-classification-on-mnist, adversarial-defense-on-mnist, anomaly-detection-on-mnist-test, continual-learning-on-rotated-mnist, image-generation-on-mnist, clustering-algorithms-evaluation-on-mnist, image-classification-on-noisy-mnist-awgn, sparse-learning-and-binarization-on-mnist, core-set-discovery-on-mnist, stochastic-optimization-on-mnist, anomaly-detection-on-mnist, hard-label-attack-on-mnist, classification-with-binary-weight-network-on-3, superpixel-image-classification-on-75, image-classification-on-noisy-mnist-motion, domain-adaptation-on-usps-to-mnist, deep-clustering-on-mnist, unsupervised-mnist-on-mnist, handwritten-digit-recognition-on-mnist, unsupervised-anomaly-detection-with-specified-22, malicious-detection-on-mnist, domain-adaptation-on-rotating-mnist, fine-grained-image-classification-on-mnist, video-prediction-on-moving-mnist, unsupervised-anomaly-detection-with-specified-10, unsupervised-anomaly-detection-with-specified-23, image-clustering-on-mnist, one-shot-learning-on-mnist, unsupervised-anomaly-detection-on-mnist-1, domain-adaptation-on-mnist-to-usps, sequential-image-classification-on-sequential, structured-prediction-on-mnist, multiview-clustering-on-mnist, adversarial-defense-against-fgsm-attack-on, unsupervised-anomaly-detection-with-specified-13, image-classification-on-noisy-mnist-contrast, unsupervised-image-to-image-translation-on, personalized-federated-learning-on-mnist-1, general-classification-on-mnist, nature-inspired-optimization-algorithm-on, rotated-mnist-on-rotated-mnist-1",,See all 1951 tasks,Graph Classification76 benchma,Graph Classification76 benchma
MUTAG,MUTAG Dataset,"In particular, MUTAG is a collection of nitroaromatic compounds and the goal is to predict their mutagenicity on Salmonella typhimurium. Input graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms. It includes 188 samples of chemical compounds with 7 discrete node labels.",https://arxiv.org/abs/1911.08941,EditUnknown,"Graph, Image",,,,,188 samples,,,"Explanation Fidelity Evaluation, Graph Classification, Node Classification","graph-classification-on-mutag, node-classification-on-mutag, explanation-fidelity-evaluation-on-mutag",,See all 1951 tasks,Graph Classification76 benchma,Graph Classification76 benchma
OGB,OGB Dataset,"The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. OGB datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can be evaluated using the OGB Evaluator in a unified manner.
OGB is a community-driven initiative in active development.",https://ogb.stanford.edu/,EditUnknown,"Graph, Image, Time Series",,,,,,,,"Link Prediction, Graph Classification, Graph Property Prediction, Link Property Prediction, Node Property Prediction, Node Classification","graph-property-prediction-on-ogbg-molhiv, node-property-prediction-on-ogbn-arxiv, link-property-prediction-on-ogbl-wikikg2, node-property-prediction-on-ogbn-papers100m, node-property-prediction-on-ogbn-proteins, link-property-prediction-on-ogbl-collab, node-property-prediction-on-ogbn-products, link-property-prediction-on-ogbl-ppa, graph-property-prediction-on-ogbg-molpcba, link-property-prediction-on-ogbl-biokg, link-property-prediction-on-ogbl-citation2, link-property-prediction-on-ogbl-ddi, graph-property-prediction-on-ogbg-ppa, node-property-prediction-on-ogbn-mag, graph-property-prediction-on-ogbg-code2, link-prediction-on-ogbl-collab",,See all 1951 tasks,Graph Classification76 benchma,Graph Classification76 benchma
PROTEINS,PROTEINS Dataset,PROTEINS is a dataset of proteins that are classified as enzymes or non-enzymes. Nodes represent the amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart.,https://arxiv.org/abs/1911.08941,EditVarious,"Graph, Image",,,,,,,,Graph Classification,graph-classification-on-proteins,,See all 1951 tasks,Graph Classification76 benchma,Graph Classification76 benchma
Pubmed,Pubmed Dataset,The PubMed dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words.,https://linqs.soe.ucsc.edu/data,EditUnknown,"Graph, Image, Text, Time Series",English,,,,,,,"Node Clustering, Link Prediction, Text Summarization, Graph Classification, Node Classification on Non-Homophilic (Heterophilic) Graphs, Sentence Classification, Extended Summarization, Language Modelling, Graph Clustering, Unsupervised Extractive Summarization, Node Classification, Community Detection","extended-summarization-on-pubmed-long-val, node-classification-on-pubmed-random, graph-clustering-on-pubmed, graph-classification-on-pubmed, node-classification-on-pubmed-fixed-20-node, link-prediction-on-pubmed, node-clustering-on-pubmed, unsupervised-extractive-summarization-on-1, node-classification-on-pubmed-full-supervised, community-detection-on-pubmed, node-classification-on-pubmed-with-public, node-classification-on-pubmed-005, link-prediction-on-pubmed-nonstandard-variant, node-classification-on-pubmed-60-20-20-random, extended-summarization-on-pubmed-long-test, text-summarization-on-pubmed-1, node-classification-on-non-homophilic-16, node-classification-on-pubmed-48-32-20-fixed, node-classification-on-pubmed-01, link-prediction-on-pubmed-biased-evaluation, language-modelling-on-pubmed-central, node-classification-on-pubmed-003, node-classification-on-pubmed, sentence-classification-on-pubmed-20k-rct",,See all 1951 tasks,Graph Classification76 benchma,Graph Classification76 benchma
COMA,COMA Dataset,"CoMA contains 17,794 meshes of the human face in various expressions",https://arxiv.org/abs/1905.10290,EditCustom (non-commercial),Graph,,,,,,,,Graph Representation Learning,graph-representation-learning-on-coma,,See all 1951 tasks,Graph Representation Learning2,Graph Representation Learning2
IMDB-BINARY,IMDB-BINARY Dataset,"IMDB-BINARY is a movie collaboration dataset that consists of the ego-networks of 1,000 actors/actresses who played roles in movies in IMDB. In each graph, nodes represent actors/actress, and there is an edge between them if they appear in the same movie. These graphs are derived from the Action and Romance genres.",https://arxiv.org/abs/1811.03508,EditUnknown,"Graph, Image",,,,,,,,"Graph Representation Learning, Graph Classification","graph-classification-on-imdb-b, graph-classification-on-imdb-binary",,See all 1951 tasks,Graph Representation Learning2,Graph Representation Learning2
Myket_Android_Application_Install,Myket Android Application Install Dataset,"This dataset contains information on application install interactions of users in the Myket android application market. The dataset was created for the purpose of evaluating interaction prediction models, requiring user and item identifiers along with timestamps of the interactions. Hence, the dataset can be used for interaction prediction and building a recommendation system. Furthermore, the data forms a dynamic network of interactions, and we can also perform network representation learning on the nodes in the network, which are users and applications.

Data Creation
The dataset was initially generated by the Myket data team, and later cleaned and subsampled by Erfan Loghmani a master student at Sharif University of Technology at the time. The data team focused on a two-week period and randomly sampled 1/3 of the users with interactions during that period. They then selected install and update interactions for three months before and after the two-week period, resulting in interactions spanning about 6 months and two weeks.

We further subsampled and cleaned the data to focus on application download interactions. We identified the top 8000 most installed applications and selected interactions related to them. We retained users with more than 32 interactions, resulting in 280,391 users. From this group, we randomly selected 10,000 users, and the data was filtered to include only interactions for these users. The detailed procedure can be found in here.

Data Structure
The dataset has two main files.


myket.csv: This file contains the interaction information and follows the same format as the datasets used in the ""JODIE: Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks"" (ACM SIGKDD 2019) project. However, this data does not contain state labels and interaction features, resulting in associated columns being all zero.
app_info_sample.csv: This file comprises features associated with applications present in the sample. For each individual application, information such as the approximate number of installs, average rating, count of ratings, and category are included. These features provide insights into the applications present in the dataset.

Dataset Details

Total Instances: 694,121 install interaction instances
Instances Format: Triplets of user_id, app_name, timestamp
10,000 users and 7,988 android applications
Item features for 7,606 applications

For a detailed summary of the data's statistics, including information on users, applications, and interactions, please refer to the Python notebook available at summary-stats.ipynb. The notebook provides an overview of the dataset's characteristics and can be helpful for understanding the data's structure before using it for research or analysis.

Top 20 Most Installed Applications
| Package Name                       | Count of Interactions |
| ---------------------------------- | --------------------- |
| com.instagram.android              | 15292                 |
| ir.resaneh1.iptv                   | 12143                 |
| com.tencent.ig                     | 7919                  |
| com.ForgeGames.SpecialForcesGroup2 | 7797                  |
| ir.nomogame.ClutchGame             | 6193                  |
| com.dts.freefireth                 | 6041                  |
| com.whatsapp                       | 5876                  |
| com.supercell.clashofclans         | 5817                  |
| com.mojang.minecraftpe             | 5649                  |
| com.lenovo.anyshare.gps            | 5076                  |
| ir.medu.shad                       | 4673                  |
| com.firsttouchgames.dls3           | 4641                  |
| com.activision.callofduty.shooter  | 4357                  |
| com.tencent.iglite                 | 4126                  |
| com.aparat                         | 3598                  |
| com.kiloo.subwaysurf               | 3135                  |
| com.supercell.clashroyale          | 2793                  |
| co.palang.QuizOfKings              | 2589                  |
| com.nazdika.app                    | 2436                  |
| com.digikala                       | 2413                  |

Comparison with SNAP Datasets
The Myket dataset introduced in this repository exhibits distinct characteristics compared to the real-world datasets used by the project. The table below provides a comparative overview of the key dataset characteristics:

| Dataset         | #Users           | #Items          | #Interactions | Average Interactions per User | Average Unique Items per User |
| --------------- | ---------------- | --------------- | ------------- | ----------------------------- | ----------------------------- |
| Myket | 10,000 | 7,988 | 694,121       | 69.4                          | 54.6                          |
| LastFM          | 980              | 1,000           | 1,293,103     | 1,319.5                       | 158.2                         |
| Reddit          | 10,000 | 984             | 672,447       | 67.2                          | 7.9                           |
| Wikipedia       | 8,227            | 1,000           | 157,474       | 19.1                          | 2.2                           |
| MOOC            | 7,047            | 97              | 411,749       | 58.4                          | 25.3                          |

The Myket dataset stands out by having an ample number of both users and items, highlighting its relevance for real-world, large-scale applications. Unlike LastFM, Reddit, and Wikipedia datasets, where users exhibit repetitive item interactions, the Myket dataset contains a comparatively lower amount of repetitive interactions. This unique characteristic reflects the diverse nature of user behaviors in the Android application market environment.

Citation
If you use this dataset in your research, please cite the following preprint:

@misc{loghmani2023effect,
      title={Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks}, 
      author={Erfan Loghmani and MohammadAmin Fazli},
      year={2023},
      eprint={2308.06862},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}",https://production-media.paperswithcode.com/datasets/7d27d890-9462-4628-b0c3-8b0a2ae42d4e.png,EditMIT License,"Graph, Time Series",,2019,,,,,,"Graph Representation Learning, Link Prediction",,,See all 1951 tasks,Graph Representation Learning2,Graph Representation Learning2
REDDIT-BINARY,REDDIT-BINARY Dataset,"REDDIT-BINARY consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them respond to the other’s comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answer-based community or a discussion-based community.",https://arxiv.org/abs/1811.03508,EditUnknown,"Graph, Image",,,,,,,,"Graph Representation Learning, Graph Classification","graph-classification-on-reddit-b, graph-classification-on-reddit-binary",,See all 1951 tasks,Graph Representation Learning2,Graph Representation Learning2
Reddit,Reddit Dataset,"The Reddit dataset is a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or “subreddit”, that a post belongs to. 50 large communities have been sampled to build a post-to-post graph, connecting posts if the same user comments on both. In total this dataset contains 232,965 posts with an average degree of 492. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). For features, off-the-shelf 300-dimensional GloVe CommonCrawl word vectors are used.",https://arxiv.org/pdf/1706.02216.pdf,EditUnknown,"Graph, Image, Text, Time Series",English,2014,,,,,,"Dialogue Generation, Topic Models, Open-Domain Dialog, Classification, Text Summarization, Graph Classification, Topological Data Analysis, Dynamic Link Prediction, Graph Representation Learning, Text Classification, News Generation, Node Classification, Question Answering, News Recommendation, Dialogue Evaluation, Sarcasm Detection, Quantization, Generative Question Answering, Conversational Response Selection, Abstractive Text Summarization","graph-classification-on-reddit-multi-12k, sarcasm-detection-on-figlang-2020-reddit, graph-classification-on-reddit-multi-5k, dialogue-generation-on-reddit-multi-ref, conversational-response-selection-on-polyai, dynamic-link-prediction-on-reddit, classification-on-reddit-ideology-database, graph-classification-on-reddit-12k, graph-classification-on-reddit-binary, question-answering-on-squadshifts-reddit, node-classification-on-reddit, graph-classification-on-reddit-b, text-summarization-on-reddit-tifu",,See all 1951 tasks,Graph Representation Learning2,Graph Representation Learning2
WikiGraphs,WikiGraphs Dataset,"WikiGraphs is a dataset of Wikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning. Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data. 

WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark with a subgraph from the Freebase knowledge graph. This makes it easy to benchmark against other state-of-the-art text generative models that are capable of generating long paragraphs of coherent text. Both the graphs and the text data are of significantly larger scale compared to prior graph-text paired datasets.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-07-21_at_13.26.42.png,EditApache License 2.0,"Graph, Text",English,,,,,,,"Graph Representation Learning, KG-to-Text Generation, Conditional Text Generation, Graph Generation",kg-to-text-generation-on-wikigraphs,,See all 1951 tasks,Graph Representation Learning2,Graph Representation Learning2
Set12,Set12 Dataset,Set12 is a collection of 12 grayscale images of different scenes that are widely used for evaluation of image denoising methods. The size of each image is 256×256.,https://arxiv.org/abs/2007.03951,EditUnknown,Image,,,,,,,,Grayscale Image Denoising,"grayscale-image-denoising-on-set12-sigma15, grayscale-image-denoising-on-set12-sigma50, grayscale-image-denoising-on-set12-sigma70, grayscale-image-denoising-on-set12-sigma30, grayscale-image-denoising-on-set12-sigma25",,See all 1951 tasks,Grayscale Image Denoising41 be,Grayscale Image Denoising41 be
Collective_Activity,Collective Activity Dataset,"The Collective Activity Dataset contains 5 different collective activities: crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point.",http://vhosts.eecs.umich.edu/vision//activity-dataset.html,EditUnknown,"Image, Video",,,,,,,,Group Activity Recognition,group-activity-recognition-on-collective,,See all 1951 tasks,Group Activity Recognition2 be,Group Activity Recognition2 be
Volleyball,Volleyball Dataset,Volleyball is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.,https://github.com/mostafa-saad/deep-activity-rec#dataset,EditUnknown,"Image, Video",,,,,,,,"Action Recognition, Sports Ball Detection and Tracking, Group Activity Recognition","action-recognition-in-videos-on-volleyball, group-activity-recognition-on-volleyball, sports-ball-detection-and-tracking-on-1",,See all 1951 tasks,Group Activity Recognition2 be,Group Activity Recognition2 be
BanglaLekha-Isolated,BanglaLekha-Isolated Dataset,"This dataset contains Bangla handwritten numerals, basic characters and compound characters. This dataset was collected from multiple geographical location within Bangladesh and includes sample collected from a variety of aged groups. This dataset can also be used for other classification problems i.e: gender, age, district.",/paper/banglalekha-isolated-a-comprehensive-bangla,EditUnknown,Image,,,,,,,,"Handwriting Recognition, Multi-Label Classification, Transfer Learning, Optical Character Recognition (OCR)","handwriting-recognition-on-banglalekha, transfer-learning-on-banglalekha-isolated",,See all 1951 tasks,Handwriting Recognition6 bench,Handwriting Recognition6 bench
Bentham,Bentham Dataset,"Bentham manuscripts refers to a large set of documents that were written by the renowned English philosopher and reformer Jeremy Bentham (1748-1832). Volunteers of the Transcribe Bentham initiative transcribed this collection. Currently, >6 000 documents or > 25 000 pages have been transcribed using this public web platform.
For our experiments, we used the BenthamR0 dataset a part of the Bentham manuscripts.",https://production-media.paperswithcode.com/datasets/4e8b54da-2eb9-4cc6-8dab-dec60928af1c.jpg,Editcustom,"Image, Text",English,,,,000 documents,,,"Handwriting Recognition, Handwritten Text Recognition, Data Augmentation",handwritten-text-recognition-on-bentham,,See all 1951 tasks,Handwriting Recognition6 bench,Handwriting Recognition6 bench
DeepWriting,DeepWriting Dataset,A new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation.,/paper/deepwriting-making-digital-ink-editable-via,EditUnknown,"Image, Text",English,,,,,,,"Handwriting Recognition, Style Transfer, Handwriting generation",,,See all 1951 tasks,Handwriting Recognition6 bench,Handwriting Recognition6 bench
HKR,HKR Dataset,"The database is written in Cyrillic and shares the same 33 characters. Besides these characters, the Kazakh alphabet also contains 9 additional specific characters. This dataset is a collection of forms. The sources of all the forms in the datasets were generated by LATEX which subsequently was filled out by persons with their handwriting. The database consists of more than 1400 filled forms. There are approximately 63000 sentences, more than 715699 symbols produced by approximately 200 diferent writers. We utilized three different datasets described as following:

Handwritten samples (Forms) of keywords in Kazakh and Russian (Areas, Cities , Village , etc.)
Handwritten Kazakh and Russian alphabet in cyrillic
Handwritten samples (Forms) of poems in Russian",https://production-media.paperswithcode.com/datasets/Screenshot_2021-06-30_at_15.11.59.png,EditCC-BY-NC-ND-4.0,"Image, Text",English,,,,63000 sentences,,,"Handwriting Recognition, Handwritten Text Recognition, Handwriting generation, Handwritten Word Generation",handwritten-text-recognition-on-hkr,,See all 1951 tasks,Handwriting Recognition6 bench,Handwriting Recognition6 bench
KOHTD,KOHTD Dataset,Kazakh offline Handwritten Text dataset (KOHTD) has 3000 handwritten exam papers and more than 140335 segmented images and there are approximately 922010 symbols. It can serve researchers in the field of handwriting recognition tasks by using deep and machine learning.,https://production-media.paperswithcode.com/datasets/8d73a243-7cd9-42e8-a17f-66029376455a.png,EditUnknown,Image,,,,,,,,Handwriting Recognition,handwriting-recognition-on-kohtd,,See all 1951 tasks,Handwriting Recognition6 bench,Handwriting Recognition6 bench
READ_2016,READ 2016 Dataset,"This dataset arises from the READ project (Horizon 2020).

The dataset consists of a subset of documents from the Ratsprotokolle collection composed of minutes of the council meetings held from 1470 to 1805 (about 30.000 pages), which will be used in the READ project. This dataset is written in Early Modern German. The number of writers is unknown. Handwriting in this collection is complex enough to challenge the HTR software.

The training dataset is composed of 400 pages; most of the pages consist of a single block with many difficulties for line detection and extraction. The ground-truth in this set is in PAGE format and it is provided annotated at line level in the PAGE files.

The previous dataset is the same that is located at https://zenodo.org/record/218236#.WnLhaCHhBGF

The new file includes the test set corresponding to the HTR competition held at ICFHR 2016

Toselli, A.H., Romero, V., Villegas, M., Vidal, E., & Sánchez, J.A. (2018). HTR Dataset ICFHR 2016 (Version 1.2.0) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.1297399",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution 4.0 International,"Image, Text",English,2020,,,,,,"Handwriting Recognition, Handwritten Text Recognition",handwritten-text-recognition-on-read-2016,,See all 1951 tasks,Handwriting Recognition6 bench,Handwriting Recognition6 bench
Ricordi,Ricordi Dataset,"Ricordi contains handwritten texts written in Italian. Train sample consists of 295 lines, validation - 19 lines and test - 69 lines.",https://production-media.paperswithcode.com/datasets/de4c8077-6243-4e8d-999f-9c55de9c498d.png,EditUnknown,"Image, Text",English,,,,,,,"Handwriting Recognition, Data Augmentation, Handwriting generation",,,See all 1951 tasks,Handwriting Recognition6 bench,Handwriting Recognition6 bench
RIMES,RIMES Dataset,"The RIMES database (Reconnaissance et Indexation de données Manuscrites et de fac similÉS / Recognition and Indexing of handwritten documents and faxes) was created to evaluate automatic systems of recognition and indexing of handwritten letters. Of particular interest are cases such as those sent by postal mail or fax by individuals to companies or administrations.

The database was collected by asking volunteers to write handwritten letters in exchange of gift vouchers. Volunteer were given a fictional identity (same sex as the real one) and up to 5 scenarios. Each scenario has been chosen among 9 realistic following themes : change of personal information (address, bank account), information request, opening and closing (customer account), modification of contract or order, complaint (bad service quality…), payment difficulties (asking for a delay, tax exemption…), reminder letter, damage declaration with further circumstances and a destination (administrations or service providers (telephone, power, bank, insurances). The volunteers composed a letter with those pieces of information using their own words. The layout was free and it was only asked to use white paper and to write in a readable way with black ink.

The collect was a success with more than 1,300 people who have participated to the RIMES database creation by writing up to 5 mails. The RIMES database thus obtained contains 12,723 pages corresponding to 5605 mails of two to three pages.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditData non-distribution and Non-commercial use agreement,Image,,,,,,,,Handwriting Recognition,,,See all 1951 tasks,Handwriting Recognition6 bench,Handwriting Recognition6 bench
Schiller,Schiller Dataset,"Schiller contains handwritten texts written in modern German. Train sample consists of 244 lines, validation - 21 lines and test - 63 lines.",https://production-media.paperswithcode.com/datasets/ec0abc3b-f929-4357-b182-31d7e0fc3f3a.png,EditUnknown,"Image, Text",English,,,,,,,"Handwriting Recognition, Data Augmentation, Handwriting generation",,,See all 1951 tasks,Handwriting Recognition6 bench,Handwriting Recognition6 bench
DigiLeTs,DigiLeTs Dataset,"A dataset with $23\,870$ digital trajectories (i.e. time series) of handwritten lower- and uppercase Latin letters and Arabic numbers ($a$-$z$, $A$-$Z$, $0$-$9$), generated by $77$ experts using a Wacom Pen Tablet. An expert is considered a proficient user of the recorded symbols, in this case adult native German speakers.

DigiLetTs was created to extend the Omniglot dataset and contains five variants per character per subject to allow the quantification of intra-subject variability and to assess and account for individual writing styles. The determination and imitation of subject-dependent writing styles is introduced as a new task in this paper.

For more information about the dataset, please refer to the repository (Homepage button below).",https://production-media.paperswithcode.com/datasets/9612e95d-2b63-427d-9f37-4ef211db8bde.png,EditUnknown,"Image, Text",English,,,,,,,"Handwriting Recognition, Handwriting generation, Handwriting Verification, Handwritten Digit Recognition",,,See all 1951 tasks,Handwritten Digit Recognition2,Handwritten Digit Recognition2
Digits,Digits Dataset,The DIGITS dataset consists of 1797 8×8 grayscale images (1439 for training and 360 for testing) of handwritten digits.,https://arxiv.org/abs/1712.02629,EditCustom,"Graph, Image",,,,,,,,"Graph Classification, Feature Importance, Handwritten Digit Recognition","graph-classification-on-digits, feature-importance-on-digits, handwritten-digit-recognition-on-digits-1",,See all 1951 tasks,Handwritten Digit Recognition2,Handwritten Digit Recognition2
MatriVasha_,MatriVasha: Dataset,"MatriVasha the largest dataset of handwritten Bangla compound characters for research on handwritten Bangla compound character recognition. The proposed dataset contains 120 different types of compound characters that consist of 306,464‬ images written where 152,950 male and 153,514 female handwritten Bangla compound characters. This dataset can be used for other issues such as gender, age, district base handwriting research because the sample was collected that included district authenticity, age group, and an equal number of men and women.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditThe annotations in this dataset along with this website belong to the Ekush Consortium and are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.,"Image, Text",English,,,,,,,"Handwritten Digit Recognition, Document Text Classification, Handwriting Recognition, Handwritten Digit Image Synthesis, Handwritten Text Recognition, Optical Charater Recogntion, Optical Character Recognition (OCR), Handwriting Verification, Handwriting generation",,,See all 1951 tasks,Handwritten Digit Recognition2,Handwritten Digit Recognition2
MNIST-MIX,MNIST-MIX Dataset,MNIST-MIX is a multi-language handwritten digit recognition dataset. It contains digits from 10 different languages.,https://github.com/jwwthu/MNIST-MIX,EditUnknown,Image,,,,,,,,Handwritten Digit Recognition,,,See all 1951 tasks,Handwritten Digit Recognition2,Handwritten Digit Recognition2
NumtaDB,NumtaDB Dataset,"To benchmark Bengali digit recognition algorithms, a large publicly available dataset is required which is free from biases originating from geographical location, gender, and age. With this aim in mind, NumtaDB, a dataset consisting of more than 85,000 images of hand-written Bengali digits, has been assembled.",https://production-media.paperswithcode.com/datasets/577ffcff-bb8f-4cb5-8acc-1e13f17066f8.png,EditCC-BY-SA,Image,,,,,000 images,,,"Image Classification, Handwritten Digit Recognition",,,See all 1951 tasks,Handwritten Digit Recognition2,Handwritten Digit Recognition2
BIOSCAN-5M,BIOSCAN-5M Dataset,"As part of an ongoing worldwide effort to comprehend and monitor insect biodiversity, we present the BIOSCAN-5M Insect dataset to the machine learning community. BIOSCAN-5M is a comprehensive dataset containing multi-modal information for over 5 million insect specimens, and it significantly expands existing image-based biological datasets by including taxonomic labels, raw nucleotide barcode sequences, assigned barcode index numbers, geographical information, and specimen size.

Every record has both image and DNA data. Each record of the BIOSCAN-5M dataset contains six primary attributes:

RGB image
DNA barcode sequence
Barcode Index Number (BIN)
Biological taxonomic classification
Geographical information
Specimen size",https://production-media.paperswithcode.com/datasets/d441bfce-819c-4b22-bc37-741c594f626e.png,EditCreative Commons Attribution 3.0 Unported,Image,,,,,,,,"Hierarchical Multi-label Classification, Contrastive Learning, Self-Supervised Image Classification, Multimodal Deep Learning, Classification, Transfer Learning, Domain Generalization, Self-Supervised Learning, Open-World Semi-Supervised Learning, Semi-Supervised Image Classification",,,See all 1951 tasks,Hierarchical Multi-label Class,Hierarchical Multi-label Class
Cellcycle_Funcat,Cellcycle Funcat Dataset,Hierarchical multi-label classification dataset for functional genomics,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Hierarchical Multi-label Classification,hierarchical-multi-label-classification-on,,See all 1951 tasks,Hierarchical Multi-label Class,Hierarchical Multi-label Class
Derisi_Funcat,Derisi Funcat Dataset,Hierarchical-multilabel classification dataset for functional genomics,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Hierarchical Multi-label Classification,hierarchical-multi-label-classification-on-1,,See all 1951 tasks,Hierarchical Multi-label Class,Hierarchical Multi-label Class
Eisen_Funcat,Eisen Funcat Dataset,Hierarchical-multilabel classification dataset for functional genomics,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Hierarchical Multi-label Classification,hierarchical-multi-label-classification-on-2,,See all 1951 tasks,Hierarchical Multi-label Class,Hierarchical Multi-label Class
EURLEX57K,EURLEX57K Dataset,"EURLEX57K is a new publicly available legal LMTC dataset, dubbed EURLEX57K, containing 57k English EU legislative documents from the EUR-LEX portal, tagged with ∼4.3k labels (concepts) from the European Vocabulary (EUROVOC).",https://www.aclweb.org/anthology/P19-1636.pdf,EditUnknown,"Image, Text",English,,,,,,,"Hierarchical Multi-label Classification, Multilabel Text Classification, Zero-Shot Learning, Text Classification, Multi-Label Text Classification","multilabel-text-classification-on-eurlex57k, hierarchical-multi-label-classification-on-19",,See all 1951 tasks,Hierarchical Multi-label Class,Hierarchical Multi-label Class
Seq_Funcat,Seq Funcat Dataset,Hierarchical-multilabel classification dataset for functional genomics,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Hierarchical Multi-label Classification,hierarchical-multi-label-classification-on-6,,See all 1951 tasks,Hierarchical Multi-label Class,Hierarchical Multi-label Class
Spo_Funcat,Spo Funcat Dataset,Hierarchical-multilabel classification dataset for functional genomics,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Hierarchical Multi-label Classification,hierarchical-multi-label-classification-on-7,,See all 1951 tasks,Hierarchical Multi-label Class,Hierarchical Multi-label Class
WOS,WOS Dataset,"Web of Science (WOS) is a document classification dataset that contains 46,985 documents with 134 categories which include 7 parents categories.",/paper/hdltex-hierarchical-deep-learning-for-text,EditUnknown,"Image, Text",English,,,,985 documents,,134,"Image Classification, Text Classification, Hierarchical Multi-label Classification, Document Classification","hierarchical-multi-label-classification-on-16, document-classification-on-wos-11967, document-classification-on-wos-46985, document-classification-on-wos-5736",,See all 1951 tasks,Hierarchical Multi-label Class,Hierarchical Multi-label Class
QVHighlights,QVHighlights Dataset,"The Query-based Video Highlights (QVHighlights) dataset is a dataset for detecting customized moments and highlights from videos given natural language (NL). It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-07-21_at_15.45.43.png,EditAttribution-NonCommercial-ShareAlike 4.0 International,"Image, Video",,,,,,,,"Video Grounding, Zero-shot Moment Retrieval, Moment Retrieval, Highlight Detection","zero-shot-moment-retrieval-on-qvhighlights, video-grounding-on-qvhighlights, highlight-detection-on-qvhighlights, moment-retrieval-on-qvhighlights",,See all 1951 tasks,Highlight Detection4 benchmark,Highlight Detection4 benchmark
TvSum,TvSum Dataset,"Introduced by Song et al. in TVSum: Summarizing web videos using titles.

The TVSum dataset comprises 50 videos, with durations ranging from 1 to 11 minutes. These videos belong to 10 different categories associated with the TRECVid MED task, with 5 videos in each category, and were collected from YouTube. The video categories include various activities like changing a vehicle tire, making a sandwich, and flash mob gatherings. For annotation, each video was reviewed and rated by 20 users, who assigned frame-level importance scores on a scale from 1 (not important) to 5 (very important).",https://production-media.paperswithcode.com/datasets/0ffd550e-516f-4fe5-9af0-d243fa82832b.png,EditCreative Commons CC-BY (v3.0) license,"Image, Text, Video",English,,,,,,,"Supervised Video Summarization, Unsupervised Video Summarization, Highlight Detection, Video Summarization","video-summarization-on-tvsum, supervised-video-summarization-on-tvsum, highlight-detection-on-tvsum, unsupervised-video-summarization-on-tvsum",,See all 1951 tasks,Highlight Detection4 benchmark,Highlight Detection4 benchmark
FLAG3D,FLAG3D Dataset,"FLAG3D is a large-scale 3D fitness activity dataset with language instruction containing 180K sequences of 60 categories. FLAG3D features the following three aspects: 1) accurate and dense 3D human pose captured from advanced MoCap system to handle the complex activity and large movement, 2) detailed and professional language instruction to describe how to perform a specific activity, 3) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments.",https://arxiv.org/pdf/2212.04638v1.pdf,EditUnknown,"3D, Image, Text, Video",English,,,,,,60,"Human Activity Recognition, 3D Action Recognition, Human Mesh Recovery, Human action generation",,,See all 1951 tasks,Human Activity Recognition9 be,Human Activity Recognition9 be
HAR,HAR Dataset,"The Human Activity Recognition Dataset has been collected from 30 subjects performing six different activities (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying). It consists of inertial sensor data that was collected using a smartphone carried by the subjects.",http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones,EditPublic domain,"Image, Video",,,,,,,,"Human Activity Recognition, Recognizing And Localizing Human Actions, Image Clustering","recognizing-and-localizing-human-actions-on, human-activity-recognition-on-har, image-clustering-on-har",,See all 1951 tasks,Human Activity Recognition9 be,Human Activity Recognition9 be
MPHOI-72,MPHOI-72 Dataset,"MPHOI-72 is a multi-person human-object interaction dataset that can be used for a wide variety of HOI/activity recognition and pose estimation/object tracking tasks. The dataset is challenging due to many body occlusions among the humans and objects. It consists of 72 videos captured from 3 different angles at 30 fps, with totally 26,383 frames and an average length of 12 seconds. It involves 5 humans performing in pairs, 6 object types, 3 activities and 13 sub-activities. The dataset includes color video, depth video, human skeletons, human and object bounding boxes.",https://production-media.paperswithcode.com/datasets/5be8b048-b25f-4b06-b142-39cd5be8a711.png,EditUnknown,"3D, Image, Video",,,,,,,,"Pose Estimation, Human Activity Recognition, Activity Recognition, Object Tracking, Action Recognition, Human-Object-interaction motion tracking, Human-Object Interaction Detection",,,See all 1951 tasks,Human Activity Recognition9 be,Human Activity Recognition9 be
OAD_dataset,OAD dataset Dataset,"The Online Action Detection Dataset (OAD) was captured using the Kinect V2 sensor, which collects color images, depth images and human skeleton joints synchronously.  This dataset includes 59 long sequences and 10 actions.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,,,,Human Activity Recognition,human-activity-recognition-on-oad-dataset,,See all 1951 tasks,Human Activity Recognition9 be,Human Activity Recognition9 be
PAMAP2,PAMAP2 Dataset,"The PAMAP2 Physical Activity Monitoring dataset contains data of 18 different physical activities (such as walking, cycling, playing soccer, etc.), performed by 9 subjects wearing 3 inertial measurement units and a heart rate monitor. The dataset can be used for activity recognition and intensity estimation, while developing and applying algorithms of data processing, segmentation, feature extraction and classification.

 Sensors 
3 Colibri wireless inertial measurement units (IMU):
  - sampling frequency: 100Hz
  - position of the sensors:
       - 1 IMU over the wrist on the dominant arm 
       - 1 IMU on the chest 
       - 1 IMU on the dominant side's ankle 
HR-monitor:
  - sampling frequency: ~9Hz

 Data collection protocol 
Each of the subjects had to follow a protocol, containing 12 different activities. The folder Protocol contains these recordings by subject.
Furthermore, some of the subjects also performed a few optional activities. The folder Optional contains these recordings by subject.

 Data files 
Raw sensory data can be found in space-separated text-files (.dat), 1 data file per subject per session (protocol or optional). Missing values are indicated with NaN. One line in the data files correspond to one timestamped and labeled instance of sensory data. The data files contain 54 columns: each line consists of a timestamp, an activity label (the ground truth) and 52 attributes of raw sensory data.",https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring,EditCC BY 4.0,"Image, Video",,,,,,,,Human Activity Recognition,human-activity-recognition-on-pamap2,,See all 1951 tasks,Human Activity Recognition9 be,Human Activity Recognition9 be
RHM,RHM Dataset,"The Robot House Multi-View dataset (RHM) contains four views: Front, Back, Ceiling, and Robot Views. There are 14 classes with 6701 video clips for each view, making a total of 26804 video clips for the four views. The lengths of the video clips are between 1 to 5 seconds. The videos with the same number and the same classes are synchronized in different views.",https://production-media.paperswithcode.com/datasets/9a2b19be-9761-4717-9248-c7baf032c7b7.png,EditMIT,"Image, Video",,,,,,,14,Human Activity Recognition,human-activity-recognition-on-rhm,,See all 1951 tasks,Human Activity Recognition9 be,Human Activity Recognition9 be
Wallhack1.8k,Wallhack1.8k Dataset,"The Wallhack1.8k dataset comprises 1,806 CSI amplitude spectrograms (and raw WiFi packet time series) corresponding to three activity classes: ""no presence,"" ""walking,"" and ""walking + arm-waving."" WiFi packets were transmitted at a frequency of 100 Hz, and each spectrogram captures a temporal context of approximately 4 seconds (400 WiFi packets).

To assess cross-scenario and cross-system generalization, WiFi packet sequences were collected in LoS and through-wall (NLoS) scenarios, utilizing two different WiFi systems (BQ: biquad antenna and PIFA: printed inverted-F antenna). The dataset is structured accordingly:

LOS/BQ/ <- WiFi packets collected in the LoS scenario using the BQ system
LOS/PIFA/ <- WiFi packets collected in the LoS scenario using the PIFA system
NLOS/BQ/ <- WiFi packets collected in the NLoS scenario using the BQ system
NLOS/PIFA/ <- WiFi packets collected in the NLoS scenario using the PIFA system

These directories contain the raw WiFi packet time series (see Table 1). Each row represents a single WiFi packet with the complex CSI vector H being stored in the ""data"" field and the class label being stored in the ""class"" field. H is of the form [I, R, I, R, ..., I, R], where two consecutive entries represent imaginary and real parts of complex numbers (the Channel Frequency Responses of subcarriers). Taking the absolute value of H (e.g., via numpy.abs(H)) yields the subcarrier amplitudes A.",https://production-media.paperswithcode.com/datasets/bdd7b50e-5d34-47fd-8720-1b27ad7e4b0f.png,EditUnknown,"Image, Video",,,,,,,,Human Activity Recognition,,,See all 1951 tasks,Human Activity Recognition9 be,Human Activity Recognition9 be
WEAR,WEAR Dataset,"WEAR is an outdoor sports dataset for both vision- and inertial-based human activity recognition (HAR). The dataset comprises data from 22 participants performing a total of 18 different workout activities with untrimmed inertial (acceleration) and camera (egocentric video) data recorded at 11 different outside locations. Unlike previous egocentric datasets, WEAR provides a challenging prediction scenario marked by purposely introduced activity variations as well as an overall small information overlap across modalities.",https://production-media.paperswithcode.com/datasets/d7158181-cbaf-4541-bfee-26a8030b0acd.jpg,EditCC BY-NC-SA 4.0,"Image, Time Series, Video",,,,,,,,"Temporal Action Localization, Action Detection, Human Activity Recognition",,,See all 1951 tasks,Human Activity Recognition9 be,Human Activity Recognition9 be
AFHQ,AFHQ Dataset,"Animal FacesHQ (AFHQ) is a dataset of animal faces consisting of 15,000 high-quality images at 512 × 512 resolution. The dataset includes three domains of cat, dog, and wildlife, each providing 5000 images. By having multiple (three) domains and diverse images of various
breeds (≥ eight) per each domain, AFHQ sets a more challenging image-to-image translation problem. 
All images are vertically and horizontally aligned to have the eyes at the center. The low-quality images were discarded by human effort.",https://arxiv.org/abs/1912.01865,EditCC BY-NC 4.0,"Image, Text",English,,,,5000 images,,,"Multimodal Unsupervised Image-To-Image Translation, Image Generation, Image-to-Image Translation","image-to-image-translation-on-afhq, image-generation-on-afhq-cat, image-generation-on-afhq-dog, image-generation-on-afhq-wild, image-generation-on-afhqv2, multimodal-unsupervised-image-to-image-5",,See all 1951 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
COCO-Stuff,COCO-Stuff Dataset,"The Common Objects in COntext-stuff (COCO-stuff) dataset is a dataset for scene understanding tasks like semantic segmentation, object detection and image captioning. It is constructed by annotating the original COCO dataset, which originally annotated things while neglecting stuff annotations. There are 164k images in COCO-stuff dataset that span over 172 categories including 80 things, 91 stuff, and 1 unlabeled class.",https://arxiv.org/abs/2008.10774,EditVarious,"Image, Text",English,,,,164k images,,172,"Layout-to-Image Generation, Unsupervised Semantic Segmentation, Image-to-Image Translation, Open Vocabulary Semantic Segmentation, Sketch-to-Image Translation, Unsupervised Semantic Segmentation with Language-image Pre-training, Zero-Shot Semantic Segmentation, Unsupervised Image Segmentation, Real-Time Semantic Segmentation, Semantic Segmentation","semantic-segmentation-on-coco-stuff-27, open-vocabulary-semantic-segmentation-on-coco, layout-to-image-generation-on-coco-stuff-2, unsupervised-semantic-segmentation-on-coco-7, unsupervised-image-segmentation-on-coco-stuff, image-to-image-translation-on-coco-stuff, semantic-segmentation-on-coco-stuff-test, unsupervised-semantic-segmentation-on-coco-8, real-time-semantic-segmentation-on-coco-stuff-1, unsupervised-semantic-segmentation-on-coco-1, unsupervised-semantic-segmentation-on-coco, semantic-segmentation-on-coco-stuff, semantic-segmentation-on-coco-stuff-full, unsupervised-semantic-segmentation-with-1, unsupervised-semantic-segmentation-with-9, layout-to-image-generation-on-coco-stuff-3, layout-to-image-generation-on-coco-stuff-4, sketch-to-image-translation-on-coco-stuff, zero-shot-semantic-segmentation-on-coco-stuff, unsupervised-semantic-segmentation-on-coco-6",,See all 1951 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
DeepFashion,DeepFashion Dataset,"DeepFashion is a dataset containing around 800K diverse fashion images with their rich annotations (46 categories, 1,000 descriptive attributes, bounding boxes and landmark information) ranging from well-posed product images to real-world-like consumer photos.",https://arxiv.org/abs/2007.05080,"EditCustom (research-only, non-commercial, attribution)","3D, Image, Text",English,,,,,,46,"Unsupervised Human Pose Estimation, Image-to-Image Translation, Pose Transfer, Image Retrieval, Virtual Try-on, Text-to-3D-Human Generation","pose-transfer-on-deep-fashion, image-to-image-translation-on-deep-fashion-1, image-retrieval-on-deepfashion, unsupervised-human-pose-estimation-on, text-to-3d-human-generation-on-deepfashion, virtual-try-on-on-deep-fashion",,See all 1951 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
GTA5,GTA5 Dataset,The GTA5 dataset contains 24966 synthetic images with pixel level semantic annotation. The images have been rendered using the open-world video game Grand Theft Auto 5 and are all from the car perspective in the streets of American-style virtual cities. There are 19 semantic classes which are compatible with the ones of Cityscapes dataset.,https://arxiv.org/abs/1909.00781,EditResearch and educational use only,"Image, Text",English,,,,,,,"Image-to-Image Translation, One-shot Unsupervised Domain Adaptation, Source-Free Domain Adaptation, Domain Adaptation, Synthetic-to-Real Translation, Semantic Segmentation, Unsupervised Domain Adaptation","source-free-domain-adaptation-on-gta5-to, semantic-segmentation-on-gtav-to-cityscapes-1, one-shot-unsupervised-domain-adaptation-on, image-to-image-translation-on-gtav-to, synthetic-to-real-translation-on-gtav-to, unsupervised-domain-adaptation-on-gtav-to, domain-adaptation-on-gta5-synscapes-to, domain-adaptation-on-gta5-to-cityscapes",,See all 1951 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
Perceptual_Similarity,Perceptual Similarity Dataset,Perceptual Similarity is a dataset of human perceptual similarity judgments.,https://arxiv.org/pdf/1801.03924.pdf,EditUnknown,"Image, Text",English,,,,,,,"Image Super-Resolution, Image Generation, Image-to-Image Translation",,,See all 1951 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
Intel_Image_Classification,Intel Image Classification Dataset,"Context
This is image data of Natural Scenes around the world.

Content
This Data contains around 25k images of size 150x150 distributed under 6 categories.
{'buildings' -> 0,
'forest' -> 1,
'glacier' -> 2,
'mountain' -> 3,
'sea' -> 4,
'street' -> 5 }

The Train, Test and Prediction data is separated in each zip files. There are around 14k images in Train, 3k in Test and 7k in Prediction.
This data was initially published on https://datahack.analyticsvidhya.com by Intel to host a Image classification Challenge.

Acknowledgements
Thanks to https://datahack.analyticsvidhya.com for the challenge and Intel for the Data

Photo by Jan Böttinger on Unsplash

Inspiration
Want to build powerful Neural network that can classify these images with more accuracy.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,25k images,"Train, Test and Prediction data is separated in each zip files. There are around 14k images",6,"Image Classification, Image Augmentation","image-augmentation-on-intel-image, image-classification-on-intel-image",,See all 1951 tasks,Image Augmentation1 benchmark1,Image Augmentation1 benchmark1
COCO_Captions,COCO Captions Dataset,"COCO Captions contains over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions are be provided for each image.",https://arxiv.org/abs/1504.00325,EditCC BY,"Image, Text",English,,,,000 images,,,"Text Generation, Image Captioning, Concept-To-Text Generation","image-captioning-on-coco-captions-test, text-generation-on-coco-captions, concept-to-text-generation-on-coco-captions, image-captioning-on-coco-captions",,See all 1951 tasks,Image Captioning40 benchmarks7,Image Captioning40 benchmarks7
NoCaps,NoCaps Dataset,"The nocaps benchmark consists of 166,100 human-generated captions describing 15,100 images from the OpenImages validation and test sets.",/paper/nocaps-novel-object-captioning-at-scale,EditUnknown,"Image, Text",English,,,,100 images,,,"Object Detection, Image Captioning","image-captioning-on-nocaps-entire, image-captioning-on-nocaps-val-near-domain, image-captioning-on-nocaps-val-in-domain, image-captioning-on-nocaps-near-domain, image-captioning-on-nocaps-xd-entire, image-captioning-on-nocaps-val-overall, image-captioning-on-nocaps-val-out-domain, image-captioning-on-nocaps-val, image-captioning-on-nocaps-out-of-domain, image-captioning-on-nocaps-xd-out-of-domain, image-captioning-on-nocaps-xd-in-domain, image-captioning-on-nocaps-in-domain, image-captioning-on-nocaps-xd-near-domain",,See all 1951 tasks,Image Captioning40 benchmarks7,Image Captioning40 benchmarks7
Winoground,Winoground Dataset,"Winoground is a dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning. Given two images and two captions, the goal is to match them correctly -- but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Visual Reasoning, Image Captioning",visual-reasoning-on-winoground,,See all 1951 tasks,Image Captioning40 benchmarks7,Image Captioning40 benchmarks7
Fashion-MNIST,Fashion-MNIST Dataset,"Fashion-MNIST is a dataset comprising of 28×28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST.",https://arxiv.org/abs/1807.02588,EditMIT,"Image, Text",English,,,,000 images,"training set has 60,000 images",10,"Outlier Detection, Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Image Clustering, Multiview Clustering, Model Poisoning, Clustering Algorithms Evaluation, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, Unsupervised Anomaly Detection, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Image Generation, Domain Generalization, Out-of-Distribution Detection, Anomaly Detection, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, Image Classification, General Classification","unsupervised-anomaly-detection-with-specified-25, unsupervised-anomaly-detection-with-specified-14, unsupervised-anomaly-detection-with-specified-11, image-classification-on-fashion-mnist, anomaly-detection-on-fashion-mnist, unsupervised-anomaly-detection-with-specified-18, out-of-distribution-detection-on-fashion, general-classification-on-fashion-mnist, clustering-algorithms-evaluation-on-fashion-2, image-generation-on-fashion-mnist, unsupervised-anomaly-detection-with-specified-27, outlier-detection-on-fashion-mnist, unsupervised-anomaly-detection-on-fashion-1, domain-generalization-on-rotated-fashion, image-clustering-on-fashion-mnist, model-poisoning-on-fashion-mnist, multiview-clustering-on-fashion-mnist",,See all 1951 tasks,Image Classification493 benchm,Image Classification493 benchm
SVHN,SVHN Dataset,"Street View House Numbers (SVHN) is a digit classification benchmark dataset that contains 600,000 32×32 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images are centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and an extra set with 530,000 images that are less difficult and can be used for helping with the training process.",https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37648.pdf,EditCC,Image,,,,,000 images,"training, testing sets and an extra set with 530,000 images",,"Sparse Representation-based Classification, Domain Adaptation, Novel Class Discovery, Anomaly Detection, Image Classification, Unsupervised Image Classification, Semi-Supervised Image Classification","anomaly-detection-on-svhn, semi-supervised-image-classification-on-svhn-4, semi-supervised-image-classification-on-svhn-2, domain-adaptation-on-svhn-to-mnist, semi-supervised-image-classification-on-svhn-1, sparse-representation-based-classification-on, semi-supervised-image-classification-on-svhn, novel-class-discovery-on-svhn, semi-supervised-image-classification-on-svhn-5, semi-supervised-image-classification-on-svhn-3, unsupervised-image-classification-on-svhn, image-classification-on-svhn",,See all 1951 tasks,Image Classification493 benchm,Image Classification493 benchm
Tiny_ImageNet,Tiny ImageNet Dataset,"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64×64 colored images. Each class has 500 training images, 50 validation images and 50 test images.",https://arxiv.org/abs/2007.06712,EditUnknown,"Image, Text",English,,,,100000 images,"training images, 50 validation images",200,"Image Clustering, Weakly-Supervised Object Localization, Clean-label Backdoor Attack (0.05%), Personalized Federated Learning, Conditional Image Generation, Self-Supervised Learning, Image Classification","clean-label-backdoor-attack-0-05-on-tiny, image-clustering-on-tiny-imagenet, personalized-federated-learning-on-tiny, weakly-supervised-object-localization-on-tiny, image-classification-on-tiny-imagenet-2, self-supervised-learning-on-tiny-imagenet, image-classification-on-tiny-imagenet-1, conditional-image-generation-on-tiny-imagenet",,See all 1951 tasks,Image Classification493 benchm,Image Classification493 benchm
Leishmania_parasite_dataset,Leishmania parasite dataset Dataset,"This dataset includes sharp-blur pairs of Leishmania image, which is a protozoan parasite microscopy image dataset of Leishmania, obtained from the preserved slides stained with Giemsa. The paired blur-sharp images are acquired by employing a bright-field microscope (Olympus IX53) with 100× magnification oil immersion objectives.We first capture the sharp images as ground truth, then acquire its corresponding out-of-focus images. The extent and nature of defocusing are random along the optical axis, where the degree of out-of-focus is inconsistent from image-to-image. This dataset includes 764 in-focus and 764 corresponding out-of-focus images, where each image is composed of 2304 × 1728 pixels in 24-bit JPG format.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Image Deblurring, Medical Image Generation",,,See all 1951 tasks,Image Deblurring10 benchmarks1,Image Deblurring10 benchmarks1
ELD,ELD Dataset,"Extreme low-light denoising (ELD) dataset that covers 10 indoor scenes and 4 camera devices from multiple brands (SonyA7S2, NikonD850, CanonEOS70D, CanonEOS700D).
It has three levels (800, 1600, 3200) and two low light factors(100, 200) for noisy images, resulting in 240 (3×2×10×4) raw image pairs in total.",https://production-media.paperswithcode.com/datasets/4bb52842-049c-45eb-83a2-83bfb3b001f8.png,EditUnknown,Image,,,,,,,,Image Denoising,"image-denoising-on-eld-sonya7s2-x200, image-denoising-on-eld-sonya7s2-x100",,See all 1951 tasks,Image Denoising22 benchmarks48,Image Denoising22 benchmarks48
CUHK_Image_Cropping,CUHK Image Cropping Dataset,"CUHK Image Cropping  is a dataset for image cropping. The photos are of varying aesthetic quality and span a variety of image categories, including animal, architecture, human, landscape, night, plant and man-made objects. Each image is manually cropped by three expert photographers (graduate students in art whose primary medium is photography) to form three training sets. There are 1,000 photos in the dataset.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_4.16.42_PM.png,EditUnknown,Image,,,,,,,,"Decision Making, Image Enhancement, Image Cropping",,,See all 1951 tasks,Image Enhancement60 benchmarks,Image Enhancement60 benchmarks
Exposure-Errors,Exposure-Errors Dataset,"A dataset of over 24,000 images exhibiting the broadest range of exposure values to date with a corresponding properly exposed image.",https://production-media.paperswithcode.com/datasets/112195940-e7091780-8be0-11eb-869d-8a40675beb3a.jpg,EditCustom,Image,,,,,000 images,,,Image Enhancement,image-enhancement-on-exposure-errors,,See all 1951 tasks,Image Enhancement60 benchmarks,Image Enhancement60 benchmarks
LLVIP,LLVIP Dataset,"Visible-infrared Paired Dataset for Low-light Vision
30976  images (15488  pairs)
24 dark  scenes, 2 daytime scenes
Support for  image-to-image translation (visible to infrared, or infrared to visible), visible and infrared image fusion, low-light pedestrian detection, and infrared pedestrian detection
(The original image and video pairs (before registration) of LLVIP are also released!)",https://production-media.paperswithcode.com/datasets/figure1-LR.png,"EditThis LLVIP Dataset is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation. Permission is granted to use the data given that you agree to our license terms.","Image, Text",English,,,,30976  images,,,"Object Detection, Image-to-Image Translation, Image Registration, Zero-shot Classification (unified classes), Low-light Pedestrian Detection, Image Generation, Pedestrian Detection, Thermal Infrared Pedestrian Detection, Infrared And Visible Image Fusion, Image Enhancement, Multispectral Object Detection, Low-Light Image Enhancement","thermal-infrared-pedestrian-detection-on, image-to-image-translation-on-llvip, object-detection-on-llvip, multispectral-object-detection-on-llvip, image-generation-on-llvip, pedestrian-detection-on-llvip, zero-shot-classification-unified-classes-on",,See all 1951 tasks,Image Enhancement60 benchmarks,Image Enhancement60 benchmarks
MIT-Adobe_FiveK,MIT-Adobe FiveK Dataset,"The MIT-Adobe FiveK dataset consists of 5,000 photographs taken with SLR cameras by a set of different photographers. They are all in RAW format; that is, all the information recorded by the camera sensor is preserved. We made sure that these photographs cover a broad range of scenes, subjects, and lighting conditions. We then hired five photography students in an art school to adjust the tone of the photos. Each of them retouched all the 5,000 photos using a software dedicated to photo adjustment (Adobe Lightroom) on which they were extensively trained. We asked the retouchers to achieve visually pleasing renditions, akin to a postcard. The retouchers were compensated for their work.

This dataset was collected for our project on learning photographic adjustments. When using images from this dataset, please cite this dataset using the following BibTeX:

@inproceedings{fivek,
    author = ""Vladimir Bychkovsky and Sylvain Paris and Eric Chan and Fr{\'e}do Durand"",
    title = ""Learning Photographic Global Tonal Adjustment with a Database of Input / Output Image Pairs"",
    booktitle = ""The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition"",
    year = ""2011""
}",https://data.csail.mit.edu/graphics/fivek/,EditCustom,Image,,2011,,,,,,"Photo Retouching, Image Enhancement, Low-Light Image Enhancement","low-light-image-enhancement-on-mit-adobe-1, image-enhancement-on-mit-adobe-fivek, image-enhancement-on-mit-adobe-5k, photo-retouching-on-mit-adobe-5k",,See all 1951 tasks,Image Enhancement60 benchmarks,Image Enhancement60 benchmarks
SICE-Mix,SICE-Mix Dataset,A test dataset SICE_Mix image datasets to represent complex mixed over-/under-exposed scenes.,https://production-media.paperswithcode.com/datasets/3387745e-60a5-4a59-80d3-492dd7d1fe2e.png,EditUnknown,Image,,,,,,,,Image Enhancement,image-enhancement-on-sice-mix,,See all 1951 tasks,Image Enhancement60 benchmarks,Image Enhancement60 benchmarks
SQUID,SQUID Dataset,"A dataset of images taken in different locations with varying water properties, showing color charts in the scenes. Moreover, to obtain ground truth, the 3D structure of the scene was calculated based on stereo imaging. This dataset enables a quantitative evaluation of restoration algorithms on natural images.",/paper/underwater-single-image-color-restoration,EditUnknown,Image,,,,,,,,"Image Enhancement, Single Image Dehazing, Image Dehazing",,,See all 1951 tasks,Image Enhancement60 benchmarks,Image Enhancement60 benchmarks
TIP_2018,TIP 2018 Dataset,"The first large demoire dataset. The dataset contains 135,000 image pairs, each containing an image contaminated with moire patterns and its corresponding uncontaminated reference image.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Image Enhancement,image-enhancement-on-tip-2018,,See all 1951 tasks,Image Enhancement60 benchmarks,Image Enhancement60 benchmarks
UIEB,UIEB Dataset,"Includes 950 real-world underwater images, 890 of which have the corresponding reference images.",/paper/an-underwater-image-enhancement-benchmark,EditUnknown,"Image, Time Series",,,,,,,,"Object Detection, Image Enhancement, Single Image Dehazing, Saliency Prediction",single-image-dehazing-on-uieb,,See all 1951 tasks,Image Enhancement60 benchmarks,Image Enhancement60 benchmarks
ApolloScape,ApolloScape Dataset,"ApolloScape is a large dataset consisting of over 140,000 video frames (73 street scene videos) from various locations in China under varying weather conditions. Pixel-wise semantic annotation of the recorded data is provided in 2D, with point-wise semantic annotation in 3D for 28 classes. In addition, the dataset contains lane marking annotations in 2D.",https://arxiv.org/abs/2004.06320,EditCustom (research-only),"Image, Time Series, Video",,,,,,,28,"Object Detection, Image Inpainting, Motion Segmentation, Autonomous Driving, Semantic Segmentation, Trajectory Prediction","semantic-segmentation-on-apolloscape, trajectory-prediction-on-apolloscape, image-inpainting-on-apolloscape, test-results-on-apolloscape, motion-segmentation-on-apolloscape",,See all 1951 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
CASIA_V2,CASIA V2 Dataset,"CASIA V2 is a dataset for forgery classification. It contains 4795 images, 1701 authentic and 3274 forged.",https://arxiv.org/abs/1911.07932,EditUnknown,Image,,,,,4795 images,,,"Image Manipulation, Image Inpainting, Domain Adaptation",,,See all 1951 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
FVI,FVI Dataset,"The Free-Form Video Inpainting dataset is a dataset used for training and evaluation video inpainting models. It consists of 1940 videos from the YouTube-VOS dataset and 12,600 videos from the YouTube-BoundingBoxes.",https://arxiv.org/abs/1904.10247,EditUnknown,"Image, Video",,1940,,,,,,"Image Inpainting, Video Inpainting",,,See all 1951 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
Places,Places Dataset,"The Places dataset is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5,000 images per category.",https://arxiv.org/abs/1902.06162,EditCC BY,Image,,,,,000 images,,,"Uncropping, Image Inpainting, Cross-Domain Few-Shot","image-inpainting-on-places2-1, cross-domain-few-shot-on-places, uncropping-on-places2-val, image-inpainting-on-places2-val",,See all 1951 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
Places365,Places365 Dataset,"The Places365 dataset is a scene recognition dataset. It is composed of 10 million images comprising 434 scene classes. There are two versions of the dataset: Places365-Standard with 1.8 million train and 36000 validation images from K=365 scene classes, and Places365-Challenge-2016, in which the size of the training set is increased up to 6.2 million extra images, including 69 new scene classes (leading to a total of 8 million train images from 434 scene classes).",https://arxiv.org/abs/1909.02410,EditUnknown,Image,,2016,,,,train and 36000 validation images,,"Scene Recognition, Image Inpainting, Semantic Segmentation, Scene Classification, Out-of-Distribution Detection, Image Classification, Image Outpainting","scene-classification-on-places365-standard, image-outpainting-on-places365-standard, image-classification-on-places365-standard, image-inpainting-on-places365, scene-recognition-on-places365, out-of-distribution-detection-on-imagenet-1k-12, image-classification-on-places365, out-of-distribution-detection-on-imagenet-1k-9",,See all 1951 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
CASIA__OSN-transmitted_-_Facebook_,CASIA (OSN-transmitted - Facebook) Dataset,"This dataset is an OSN-transmitted (OSN = Online Social Network) version of the CASIA dataset. 
The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN  - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing 
and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against
transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Image Manipulation, Image Forensics, Image Manipulation Detection",image-manipulation-detection-on-casia-osn,,See all 1951 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
CASIA__OSN-transmitted_-_Weibo_,CASIA (OSN-transmitted - Weibo) Dataset,"This dataset is an OSN-transmitted (OSN = Online Social Network) version of the CASIA dataset. 
The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN  - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing 
and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against
transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Detecting Image Manipulation, Image Manipulation, Image Manipulation Localization, Image Manipulation Detection, Image Forgery Detection",image-manipulation-detection-on-casia-osn-3,,See all 1951 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
CASIA__OSN-transmitted_-_Whatsapp_,CASIA (OSN-transmitted - Whatsapp) Dataset,"This dataset is an OSN-transmitted (OSN = Online Social Network) version of the CASIA dataset. 
The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN  - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing 
and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against
transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Image Manipulation, Image Manipulation Localization, Image Forensics, Image Manipulation Detection",image-manipulation-detection-on-casia-osn-2,,See all 1951 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
CelebAMask-HQ,CelebAMask-HQ Dataset,"CelebAMask-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has segmentation mask of facial attributes corresponding to CelebA.",https://github.com/switchablenorms/CelebAMask-HQ,EditCustom (non-commercial),"3D, Image, Text",English,,,,,,,"Reconstruction, Image-to-Image Translation, 3D-Aware Image Synthesis, Image Manipulation, Pose Transfer, Conditional Image Generation, Image Generation, Face Parsing","conditional-image-generation-on-celebamask-hq, reconstruction-on-celebamask-hq, face-parsing-on-celebamask-hq, 3d-aware-image-synthesis-on-celebamask-hq, pose-transfer-on-celebamask-hq",,See all 1951 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
Columbia__OSN-transmitted_-_Facebook_,Columbia (OSN-transmitted - Facebook) Dataset,"This dataset is an OSN-transmitted (Online Social Network) version of the Columbia dataset. Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Image Manipulation, Detecting Image Manipulation, Image Manipulation Detection, Image Forensics",image-manipulation-detection-on-columbia-osn,,See all 1951 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
Digital_Forensics_2023_dataset_-_DF2023,Digital Forensics 2023 dataset - DF2023 Dataset,"The deliberate manipulation of public opinion, especially through altered images, poses a significant danger to society. To fight this issue on a technical level we support the research community by releasing the Digital Forensics 2023 (DF2023) training and validation dataset.

The DF2023 training dataset comprises one million images from four major forgery categories:


splicing (400K)
copy-move (300K)
enhancement (200K)
removal (100K)

This dataset enables an objective comparison of network architectures and can significantly reduce the time and effort of researchers preparing datasets.

For a detailed description of the DF2023 dataset, please refer to:

@inproceedings{Fischinger2023DFNet,
title={DF2023: The Digital Forensics 2023 Dataset for Image Forgery Detection},
author={David Fischinger and Martin Boyer},
journal={The 25th Irish Machine Vision and Image Processing conference. (IMVIP)},
year={2023}
}
available from: Zenodo

Naming convention
The naming convention of DF2023 encodes information about the applied manipulations. Each image name has the following form:

COCO_DF_0123456789_NNNNNNNN.{EXT} (e.g. COCO_DF_E000G40117_00200620.jpg)

After the identifier of the image data source (""COCO"") and the self-reference to the Digital Forensics (""DF"") dataset, there are 10 digits as placeholders for the manipulation. Position 0 defines the manipulation types copy-move, splicing, removal, enhancement ([C,S,R,E]). The following digits 1-9 represent donor patch manipulations. For positions [1,2,7,8] (resample, flip, noise and brightness), a binary value indicates if this manipulation was applied to the donor image patch. Position 3 (rotate) indicates by the values 0-3 if the rotation was executed by 0, 90, 180 or 270 degrees. Position 4 defines if BoxBlur (B) or GaussianBlur (G) was used. Position 5 specifies the blurring radius. A value of 0 indicates that no blurring was executed. Position 6 indicates which of the Python-PIL contrast filters EDGE ENHANCE, EDGE ENHANCE MORE, SHARPEN, UnsharpMask or ImageEnhance (values 1-5) was applied. If none of them was applied, this value is set to 0. Finally, position 9 is set to the JPEG compression factor modulo 10, a value of 0 indicates that no JPEG compression was applied. The 8 characters NNNNNNNN in the image name template stand for a running number of the images.",https://production-media.paperswithcode.com/datasets/197859aa-633b-4b18-8fdd-73be98ee9226.jpg,"EditThe DF2023 dataset is based on the MS COCO dataset. Therefore, rules for using the images form MS COCO apply also for DF2023:      Images      The COCO Consortium does not own the copyright of the images. Use of the images must abide by the Flickr Terms of Use. The users of the images accept full responsibility for the use of the dataset, including but not limited to the use of any copies of copyrighted images that they may create from the dataset.",Image,,2023,,,,"val, enhancement ([C,S,R,E]). The following digits 1-9 represent donor patch manipulations. For positions [1,2,7,8] (resample, flip, noise and brightness), a binary value indicates if this manipulation was applied to the donor image patch. Position 3 (rotate) indicates by the values 0-3 if the rotation was executed by 0, 90, 180 or 270 degrees. Position 4 defines if BoxBlur (B) or GaussianBlur (G) was used. Position 5 specifies the blurring radius. A value of 0 indicates that no blurring was executed. Position 6 indicates which of the Python-PIL contrast filters EDGE ENHANCE, EDGE ENHANCE MORE, SHARPEN, UnsharpMask or ImageEnhance (values 1-5) was applied. If none of them was applied, this value is set to 0. Finally, position 9 is set to the JPEG compression factor modulo 10, a value of 0 indicates that no JPEG compression was applied. The 8 characters NNNNNNNN in the image name template stand for a running number of the images",,"Image Manipulation, Image Manipulation Localization, Image Forgery Detection, Image Manipulation Detection",,,See all 1951 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
DSO__OSN-transmitted_-_Facebook_,DSO (OSN-transmitted - Facebook) Dataset,"This dataset is an OSN-transmitted (Online Social Network) version of the DSO dataset. Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Image Manipulation, Image Manipulation Localization, Image Forensics, Image Manipulation Detection",image-manipulation-detection-on-dso-osn,,See all 1951 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
LRS2,LRS2 Dataset,"The Oxford-BBC Lip Reading Sentences 2 (LRS2) dataset is one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists of mainly news and talk shows from BBC programs. Each sentence is up to 100 characters in length. The training, validation and test sets are divided according to broadcast date. It is a challenging set since it contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances and the test set contains 1,242 utterances.",https://arxiv.org/abs/2001.01656,EditCustom (non-commercial),"Audio, Image, Text",English,,,,,,,"Audio-Visual Speech Recognition, Speech Recognition, Automatic Speech Recognition (ASR), Image Manipulation, Visual Keyword Spotting, Unconstrained Lip-synchronization, Speech Separation, Visual Speech Recognition, Landmark-based Lipreading, Lipreading","lipreading-on-lrs2, landmark-based-lipreading-on-lrs2, visual-speech-recognition-on-lrs2, speech-recognition-on-lrs2, visual-keyword-spotting-on-lrs2, speech-separation-on-lrs2, lip-sync-on-lrs2, automatic-speech-recognition-on-lrs2, audio-visual-speech-recognition-on-lrs2, image-manipulation-on-lrs2",,See all 1951 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
NIST__OSN-transmitted_-_Facebook_,NIST (OSN-transmitted - Facebook) Dataset,"This dataset is an OSN-transmitted (Online Social Network) version of the NIST dataset (https://www.nist.gov/itl/iad/mig/nimble-challenge-2017-evaluation). Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2017,,,,,,"Image Manipulation, Image Manipulation Localization, Image Manipulation Detection, Image Forgery Detection, Image Forensics",image-manipulation-detection-on-nist-osn,,See all 1951 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
CED,CED Dataset,Contains 50 minutes of footage with both color frames and events. CED features a wide variety of indoor and outdoor scenes.,/paper/ced-color-event-camera-dataset,EditUnknown,"3D, Image",,,,,,,,"Image Reconstruction, Event-based vision",,,See all 1951 tasks,Image Reconstruction31 benchma,Image Reconstruction31 benchma
General-100,General-100 Dataset,The General-100 dataset is a dataset for image super-resolution. It contains 100 bmp format images with no compression) The size of the 100 images ranges from 710 x 704 (large) to 131 x 112 (small).,https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_4.28.24_PM.png,EditUnknown,"3D, Image",,,,,100 images,,,"Image Reconstruction, Image Super-Resolution",,,See all 1951 tasks,Image Reconstruction31 benchma,Image Reconstruction31 benchma
OADAT,OADAT Dataset,"An experimental and synthetic (simulated) OA raw signals and reconstructed image domain datasets rendered with different experimental parameters and tomographic acquisition geometries.

For detailed information, see github.com/berkanlafci/oadat.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-NonCommercial 4.0 International,"3D, Image, Text",English,,,,,,,"Unsupervised Pre-training, Image-to-Image Translation, Image Reconstruction, Semantic Segmentation, Self-Supervised Learning",,,See all 1951 tasks,Image Reconstruction31 benchma,Image Reconstruction31 benchma
RGB-DAVIS_Dataset,RGB-DAVIS Dataset Dataset,"Used to show systematic performance improvement in applications such as high frame-rate video synthesis, feature/corner detection and tracking, as well as high dynamic range image reconstruction.",/paper/joint-filtering-of-intensity-images-and,EditUnknown,"3D, Image, Video",,,,,,,,"Image Reconstruction, Event-based vision, Motion Compensation",,,See all 1951 tasks,Image Reconstruction31 benchma,Image Reconstruction31 benchma
SEN12MS-CR-TS,SEN12MS-CR-TS Dataset,SEN12MS-CR-TS is a multi-modal and multi-temporal data set for cloud removal. It contains time-series of paired and co-registered Sentinel-1 and cloudy as well as cloud-free Sentinel-2 data from European Space Agency's Copernicus mission. Each time series contains 30 cloudy and clear observations regularly sampled throughout the year 2018. Our multi-temporal data set is readily pre-processed and backward-compatible with SEN12MS-CR.,https://production-media.paperswithcode.com/datasets/36eecda9-bfdf-4273-a9af-8db41d9129cf.jpg,EditUnknown,"3D, Image, Text, Video",English,2018,,,,,,"Image-to-Image Translation, Image Reconstruction, Image Dehazing, Image Denoising, Video Reconstruction, Cloud Removal",cloud-removal-on-sen12ms-cr-ts,,See all 1951 tasks,Image Reconstruction31 benchma,Image Reconstruction31 benchma
SEN12MS-CR,SEN12MS-CR Dataset,"SEN12MS-CR is a multi-modal and mono-temporal data set for cloud removal. It contains observations covering 175 globally distributed Regions of Interest recorded in one of four seasons throughout the year of 2018. For each region, paired and co-registered synthetic aperture radar (SAR) Sentinel-1 measurements as well as cloudy and cloud-free optical multi-spectral Sentinel-2 observations from European Space Agency's Copernicus mission are provided. The Sentinel satellites provide public access data and are among the most prominent satellites in Earth observation.",/paper/multi-sensor-data-fusion-for-cloud-removal-in,EditUnknown,"3D, Image",,2018,,,,,,"Image Reconstruction, Image Denoising, Cloud Removal",cloud-removal-on-sen12ms-cr,,See all 1951 tasks,Image Reconstruction31 benchma,Image Reconstruction31 benchma
Spike-X4K,Spike-X4K Dataset,"Overview
The Spike-X4K Dataset is a high-resolution image reconstruction resource tailored for the latest advancements in spike camera technology. It is designed to meet the demands of modern spike cameras with a resolution of 1000×1000 pixels, surpassing the capabilities of previous datasets like spike-REDS, which was limited to a resolution of 250×400 pixels.

Dataset Characteristics

Resolution: 1000×1000 pixels, aligning with state-of-the-art spike camera imaging standards.
Temporal Depth: The dataset captures the temporal dynamics of scenes with high-speed motion, providing a temporal sequence of spike frames.
Content: It includes both synthetic and real-world datasets, offering a diverse range of high-speed motion scenarios for training and testing image reconstruction models.
Pairs: The dataset comprises 1200 spike stream-ground truth image pairs for training and 45 pairs for testing, ensuring robust model evaluation.

Motivation and Content Summary
The development of the Spike-X4K Dataset was motivated by the need for a more representative and higher resolution dataset that could effectively train and evaluate image reconstruction models for spike cameras. Spike cameras, with their unique ability to capture photons independently at each pixel and generate binary spike streams, offer high temporal resolution and low latency, which are crucial for high-speed imaging. However, converting these spike streams into high-quality images requires sophisticated algorithms, and the Spike-X4K Dataset provides the necessary data to develop and refine these algorithms.

Potential Use Cases

Algorithm Development: For researchers and engineers working on spike image reconstruction algorithms specific to spike camera data.
Benchmarking: As a standard for benchmarking the performance of various image reconstruction models against state-of-the-art techniques.
Training and Testing: Providing a large and diverse set of data for training deep learning models to handle high-speed motion imaging.
Feature Extraction: Enabling the study and improvement of feature extraction techniques from spike streams, including spatial and temporal features.
Spike Image Reconstruction Tasks: Supporting a wide range of computer vision tasks that can benefit from high-resolution, high-speed imaging, such as motion analysis, object tracking, and event detection.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"3D, Image",,,,,,"train and evaluate image reconstruction models for spike cameras. Spike cameras, with their unique ability to capture photons independently at each pixel and generate binary spike streams, offer high temporal resolution and low latency, which are crucial for high-speed imaging. However, converting these spike streams into high-quality images",,"Image Reconstruction, Event-based vision",image-reconstruction-on-spike-x4k,,See all 1951 tasks,Image Reconstruction31 benchma,Image Reconstruction31 benchma
WiFiCam,WiFiCam Dataset,"WiFiCam dataset for through-wall imaging based on WiFi channel state information. The corresponding source code repository is located at: https://github.com/StrohmayerJ/wificam

Dataset Structure:

/wificam

├── j3

└── 320  <-- 320x240 resolution subset

└── csi.csv <-- raw WiFi packet sequence recorded with the ESP32-S3

└── csiComplex.npy <-- complex CSI sequence (cache)

└── 92108.png <-- 320x240 RGB image

└── 92112.png

└── ...

└── 640 <-- 640x480 resolution subset

└── csi.csv <-- raw WiFi packet sequence recorded with the ESP32-S3

└── csiComplex.npy <-- complex CSI sequence (cache)

└── 154.png <-- 640x480 RGB image

└── 155.png

└── ...

├── statistics320.csv <-- per-channel means and standard deviations for 320x240 images

├── statistics640.csv <-- per-channel means and standard deviations for 640x480 images",https://production-media.paperswithcode.com/datasets/e81adfb8-988e-4a61-a642-2dbcdebfc140.png,EditUnknown,"3D, Image, Text",English,,,,240 images,,,"Image Reconstruction, Image Generation",,,See all 1951 tasks,Image Reconstruction31 benchma,Image Reconstruction31 benchma
Dynamic_OLAT_Dataset,Dynamic OLAT Dataset Dataset,"To provide ground truth supervision for video consistency modeling, we build up a high-quality dynamic OLAT dataset.
Our capture system consists of a light stage setup with 114 LED light sources and Phantom Flex4K-GS camera (global shutter, stationary 4K ultra-high-speed camera at 1000 fps), resulting in dynamic OLAT imageset recording at 25 fps using the overlapping method.
Our dynamic OLAT dataset provides sufficient semantic, temporal and lighting consistency supervision to train our neural video portrait relighting scheme, which can generalize to in-the-wild scenarios.",https://production-media.paperswithcode.com/datasets/226e549d-9988-468b-815b-fdccb90bde6e.png,EditUnknown,Image,,,,,,,,"Single-Image Portrait Relighting, Image Relighting",,,See all 1951 tasks,Image Relighting2 benchmarks30,Image Relighting2 benchmarks30
NRHints-RealCapture,NRHints-RealCapture Dataset,A high-quality captured dataset for object relighting. Covering a wide range of geometry and material.,https://production-media.paperswithcode.com/datasets/88cda4c6-3c62-4bc2-8639-d1c43a3f2e07.jpg,EditUnknown,Image,,,,,,,,"Image Relighting, Novel View Synthesis",,,See all 1951 tasks,Image Relighting2 benchmarks30,Image Relighting2 benchmarks30
NRHints-Synthetic,NRHints-Synthetic Dataset,A high-quality synthetic dataset for object relighting. Covering a wide range of geometry and material.,https://production-media.paperswithcode.com/datasets/05e97291-33f2-4a8b-9263-a04303bffada.jpg,EditUnknown,Image,,,,,,,,"Image Relighting, Novel View Synthesis",,,See all 1951 tasks,Image Relighting2 benchmarks30,Image Relighting2 benchmarks30
Stanford-ORB,Stanford-ORB Dataset,"We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering Benchmark. Recent advances in inverse rendering have enabled a wide range of real-world applications in 3D content generation, moving rapidly from research and commercial use cases to consumer devices. While the results continue to improve, there is no real-world benchmark that can quantitatively assess and compare the performance of various inverse rendering methods. Existing real-world datasets typically only consist of the shape and multi-view images of objects, which are not sufficient for evaluating the quality of material recovery and object relighting. Methods capable of recovering material and lighting often resort to synthetic data for quantitative evaluation, which on the other hand does not guarantee generalization to complex real-world environments. We introduce a new dataset of real-world objects captured under a variety of natural scenes with ground-truth 3D scans, multi-view images, and environment lighting. Using this dataset, we establish the first comprehensive real-world evaluation benchmark for object inverse rendering tasks from in-the-wild scenes, and compare the performance of various existing methods.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"3D, Image, Time Series",,,,,,"valuating the quality of material recovery and object relighting. Methods capable of recovering material and lighting often resort to synthetic data for quantitative evaluation, which on the other hand does not guarantee generalization to complex real-world environments. We introduce a new dataset of real-world objects captured under a variety of natural scenes with ground-truth 3D scans, multi-view images",,"Depth Prediction, Inverse Rendering, Surface Reconstruction, Image Relighting, Novel View Synthesis, Surface Normals Estimation","surface-normals-estimation-on-stanford-orb, surface-reconstruction-on-stanford-orb, depth-prediction-on-stanford-orb, inverse-rendering-on-stanford-orb, image-relighting-on-stanford-orb",,See all 1951 tasks,Image Relighting2 benchmarks30,Image Relighting2 benchmarks30
VIDIT,VIDIT Dataset,"VIDIT  is a reference evaluation benchmark and to push forward the development of illumination manipulation methods. VIDIT includes 390 different Unreal Engine scenes, each captured with 40 illumination settings, resulting in 15,600 images. The illumination settings are all the combinations of 5 color temperatures (2500K, 3500K, 4500K, 5500K and 6500K) and 8 light directions (N, NE, E, SE, S, SW, W, NW). Original image resolution is 1024x1024.",/paper/vidit-virtual-image-dataset-for-illumination,EditUnknown,"Image, Text",English,,,,600 images,"valuation benchmark and to push forward the development of illumination manipulation methods. VIDIT includes 390 different Unreal Engine scenes, each captured with 40 illumination settings, resulting in 15,600 images",,"Image Relighting, SSIM, Image-to-Image Translation, Domain Adaptation",image-relighting-on-vidit20-validation-set,,See all 1951 tasks,Image Relighting2 benchmarks30,Image Relighting2 benchmarks30
CBSD68,CBSD68 Dataset,Color BSD68 dataset for image denoising benchmarks is part of The Berkeley Segmentation Dataset and Benchmark. It is used for measuring image denoising algorithms performance. It contains 68 images.,https://github.com/clausmichele/CBSD68-dataset,"EditCustom (research-only, non-commercial)",Image,,,,,68 images,,,"Image Restoration, Image Compressed Sensing, Denoising, Color Image Denoising, Image Denoising","image-compressed-sensing-on-cbsd68, color-image-denoising-on-cbsd68-sigma75, color-image-denoising-on-cbsd68-sigma15, color-image-denoising-on-cbsd68-sigma20, color-image-denoising-on-cbsd68-sigma30, color-image-denoising-on-cbsd68-sigma40, color-image-denoising-on-cbsd68-sigma35, color-image-denoising-on-cbsd68-sigma65, color-image-denoising-on-cbsd68-sigma25, color-image-denoising-on-cbsd68-sigma5, color-image-denoising-on-cbsd68-sigma45, color-image-denoising-on-cbsd68-sigma10, color-image-denoising-on-cbsd68-sigma55, color-image-denoising-on-cbsd68-sigma60, color-image-denoising-on-cbsd68-sigma50, color-image-denoising-on-cbsd68-sigma70",,See all 1951 tasks,Image Restoration73 benchmarks,Image Restoration73 benchmarks
DocUNet,DocUNet Dataset,"Various documents dataset.
Each of the 65 documents includes scanned ground truth images, both hard and easy distorted photos, and document-centered cropped images.",https://production-media.paperswithcode.com/datasets/de671871-e3dc-45b4-a938-0d95975677d8.png,EditUnknown,"Image, Text",English,,,,65 documents,,,"Image Restoration, Local Distortion, SSIM, MS-SSIM, Document Enhancement","ssim-on-docunet, ms-ssim-on-docunet, local-distortion-on-docunet",,See all 1951 tasks,Image Restoration73 benchmarks,Image Restoration73 benchmarks
Fingerprint_inpainting_and_denoising,Fingerprint inpainting and denoising Dataset,"Synthetic training set: This set is constructed in the following two steps and will be used for estimation/training purposes. i) 84,000 275 pixel x 400 pixel ground-truth fingerprint images without any noise or scratches, but with random transformations (at most five pixels translation and +/-10 degrees rotation) were generated by using the software Anguli: Synthetic Fingerprint Generator. ii) 84,000 275 pixel x 400 pixel degraded fingerprint images were generated by applying random artifacts (blur, brightness, contrast, elastic transformation, occlusion, scratch, resolution, rotation) and backgrounds to the ground-truth fingerprint images. In total, it contains 168,000 fingerprint images (84,000 fingerprints, and two impressions - one ground-truth and one degraded - per fingerprint).

Synthetic test set: This set is constructed similarly to the synthetic training set and will be used to evaluate the reconstruction performance. In total, it contains 16,800 fingerprint images (8,400 fingerprints and two impressions - one ground-truth and one degraded - per fingerprint). Since this set will be used for the purpose of evaluating the reconstruction performance, only the degraded and not the ground-truth fingerprint images will be provided to participants.

Real test set: This set is constructed by systematically drawing fingerprint images with varying sizes from publicly available datasets. In total, it contains 1680 fingerprint images (140 fingerprints and 12 impressions - high-quality scans under operational conditions - per fingerprint).

Description from: Fingerprint inpainting and denoising (WCCI'18, ECCV'18)",https://production-media.paperswithcode.com/datasets/531f1d9f-9376-418d-9f76-4400ee8ca5a4.png,EditUnknown,Image,,,,,,"training set: This set is constructed in the following two steps and will be used for estimation/training purposes. i) 84,000 275 pixel x 400 pixel ground-truth fingerprint images",,"Image Restoration, Image Inpainting, Denoising, Color Image Denoising, Image Denoising",,,See all 1951 tasks,Image Restoration73 benchmarks,Image Restoration73 benchmarks
PIRM,PIRM Dataset,"The PIRM dataset consists of 200 images, which are divided into two equal sets for validation and testing. These images cover diverse contents, including people, objects, environments, flora, natural scenery, etc. Images vary in size, and are typically ~300K pixels in resolution.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_2.35.07_PM.png,EditCC BY-NC-SA 4.0,Image,,,,,200 images,"validation and testing. These images cover diverse contents, including people, objects, environments, flora, natural scenery, etc. Images",,"Image Restoration, Image Super-Resolution",image-super-resolution-on-pirm-test,,See all 1951 tasks,Image Restoration73 benchmarks,Image Restoration73 benchmarks
Raindrop,Raindrop Dataset,"Raindrop is a set of image pairs, where
each pair contains exactly the same background scene, yet
one is degraded by raindrops and the other one is free from
raindrops. To obtain this, the images are captured through two pieces of exactly the
same glass: one sprayed with water, and the other is left
clean. The dataset consists of 1,119 pairs of images, with various
background scenes and raindrops. They were captured with a Sony A6000
and a Canon EOS 60.",/paper/attentive-generative-adversarial-network-for,EditUnknown,Image,,,,,,,,"Image Restoration, Rain Removal, Single Image Deraining",single-image-deraining-on-raindrop,,See all 1951 tasks,Image Restoration73 benchmarks,Image Restoration73 benchmarks
Real_Rain_Dataset,Real Rain Dataset Dataset,A large-scale dataset of ~29.5K rain/rain-free image pairs that covers a wide range of natural rain scenes.,https://arxiv.org/pdf/1904.01538v2.pdf,EditCC BY-NC-SA,Image,,,,,,,,"Image Restoration, Rain Removal, Single Image Deraining",,,See all 1951 tasks,Image Restoration73 benchmarks,Image Restoration73 benchmarks
SIDD,SIDD Dataset,"SIDD is an image denoising dataset containing 30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras. Ground truth images are provided along with the noisy images.",/paper/a-high-quality-denoising-dataset-for,EditMIT,"Audio, Image",,,,,,,,"Image Restoration, Image Denoising, Noise Estimation, Denoising","image-denoising-on-sidd, noise-estimation-on-sidd",,See all 1951 tasks,Image Restoration73 benchmarks,Image Restoration73 benchmarks
TinyPerson,TinyPerson Dataset,"TinyPerson is a benchmark for tiny object detection in a long distance and with massive backgrounds. The images in TinyPerson are collected from the Internet. First, videos with a high resolution are collected from different websites. Second, images from the video are sampled every 50 frames. Then images with a certain repetition (homogeneity) are deleted, and the resulting images are annotated with 72,651 objects with bounding boxes by hand.",https://arxiv.org/abs/1912.10664,EditUnknown,Image,,,,,,,,"Image Restoration, Object Detection, Human Detection",,,See all 1951 tasks,Image Restoration73 benchmarks,Image Restoration73 benchmarks
UHDM,UHDM Dataset,"The first ultra-high-definition image demoireing dataset,  consisting of 4,500 4K resolution training pairs and 500 standard 4K resolution validation pairs.",https://production-media.paperswithcode.com/datasets/96fcc2ce-9d0d-47ce-adda-a1d815c4d25b.png,EditUnknown,Image,,,,,,,,"Image Restoration, Image Enhancement",image-restoration-on-uhdm,,See all 1951 tasks,Image Restoration73 benchmarks,Image Restoration73 benchmarks
Flickr30k,Flickr30k Dataset,"The Flickr30k dataset contains 31,000 images collected from Flickr, together with 5 reference sentences provided by human annotators.",https://arxiv.org/abs/1509.04942,"EditCustom (research-only, non-commercial)","Image, Text, Video",English,,,,000 images,,,"mage-to-Text Retrieval, Cross-Modal Retrieval, Image Retrieval, Image-to-Text Retrieval, Image Captioning, Zero-Shot Cross-Modal Retrieval, Phrase Grounding, Zero-shot Text-to-Image Retrieval, Video Description, Semi Supervised Learning for Image Captioning, Node Classification","cross-modal-retrieval-on-flickr30k, semi-supervised-learning-for-image-captioning-2, zero-shot-cross-modal-retrieval-on-flickr30k, node-classification-on-flickr, image-retrieval-on-flickr30k, phrase-grounding-on-flickr30k, mage-to-text-retrieval-on-flickr30k, zero-shot-text-to-image-retrieval-on-1, image-captioning-on-flickr30k-captions-test, image-retrieval-on-flickr30k-1k-test, image-to-text-retrieval-on-flickr30k",,See all 1951 tasks,Image Retrieval80 benchmarks80,Image Retrieval80 benchmarks80
In-Shop,In-Shop Dataset,"In-shop Clothes Retrieval Benchmark evaluates the performance of in-shop Clothes Retrieval. This is a large subset of DeepFashion, containing large pose and scale variations. It also has large diversities, large quantities, and rich annotations, including:


7,982 number of clothing items;
52,712 number of in-shop clothes images, and ~200,000 cross-pose/scale pairs;

Each image is annotated by bounding box, clothing type and pose type.",https://production-media.paperswithcode.com/datasets/a60e38d9-01df-40ec-b806-3b0ac98e82c9.png,EditUnknown,Image,,,,,,,,"Metric Learning, Image Retrieval","metric-learning-on-in-shop-1, image-retrieval-on-in-shop",,See all 1951 tasks,Image Retrieval80 benchmarks80,Image Retrieval80 benchmarks80
Stanford_Online_Products,Stanford Online Products Dataset,"Stanford Online Products (SOP) dataset has 22,634 classes with 120,053 product images. The first 11,318 classes (59,551 images) are split for training and the other 11,316 (60,502 images) classes are used for testing",https://arxiv.org/abs/1907.07585,EditUnknown,Image,,,,,551 images,"split for training and the other 11,316 (60,502 images",634,"Fine-Grained Image Classification, Metric Learning, Image Retrieval, Learning with coarse labels, Image Classification","fine-grained-image-classification-on-sop, metric-learning-on-stanford-online-products-1, image-retrieval-on-sop, learning-with-coarse-labels-on-stanford, image-classification-on-stanford-online",,See all 1951 tasks,Image Retrieval80 benchmarks80,Image Retrieval80 benchmarks80
YFCC100M,YFCC100M Dataset,"YFCC100M is a that dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014.",https://arxiv.org/pdf/1503.01817v2.pdf,EditUnknown,Image,,2004,,,,,,"Image Classification, Image Retrieval",,,See all 1951 tasks,Image Retrieval80 benchmarks80,Image Retrieval80 benchmarks80
2DeteCT,2DeteCT Dataset,"Maximilian B. Kiss, Sophia B. Coban, K. Joost Batenburg, Tristan van Leeuwen, and Felix Lucka ""2DeteCT - A large 2D expandable, trainable, experimental Computed Tomography dataset for machine learning"",  Sci Data 10, 576 (2023) or arXiv:2306.05907 (2023)

Abstract:
""Recent research in computational imaging largely focuses on developing machine learning (ML) techniques for image reconstruction, which requires large-scale training datasets consisting of measurement data and ground-truth images. However, suitable experimental datasets for X-ray Computed Tomography (CT) are scarce, and methods are often developed and evaluated only on simulated data. We fill this gap by providing the community with a versatile, open 2D fan-beam CT dataset suitable for developing ML techniques for a range of image reconstruction tasks. To acquire it, we designed a sophisticated, semi-automatic scan procedure that utilizes a highly-flexible laboratory X-ray CT setup. A diverse mix of samples with high natural variability in shape and density was scanned slice-by-slice (5000 slices in total) with high angular and spatial resolution and three different beam characteristics: A high-fidelity, a low-dose and a beam-hardening-inflicted mode. In addition, 750 out-of-distribution slices were scanned with sample and beam variations to accommodate robustness and segmentation tasks. We provide raw projection data, reference reconstructions and segmentations based on an open-source data processing pipeline.""

The data collection has been acquired using a highly flexible, programmable and custom-built X-ray CT scanner, the FleX-ray scanner, developed by TESCAN-XRE NV, located in the FleX-ray Lab at the Centrum Wiskunde & Informatica (CWI) in Amsterdam, Netherlands. It consists of a cone-beam microfocus X-ray point source (limited to 90 kV and 90 W) that projects polychromatic X-rays onto a 14-bit CMOS (complementary metal-oxide semiconductor) flat panel detector with CsI(Tl) scintillator (Dexella 1512NDT) and 1536-by-1944 pixels,  each. To create a 2D dataset, a fan-beam geometry was mimicked by only reading out the central row of the detector. Between source and detector there is a rotation stage, upon which samples can be mounted. The machine components (i.e., the source, the detector panel, and the rotation stage) are mounted on translation belts that allow the moving of the components independently from one another.

Please refer to the paper for all further technical details.

The complete data collection can be found via the following links: 1-1,000, 1,001-2,000, 2,001-3,000, 3,001-4,000, 4,001-5,000, 5,521-6,370.

Each slice folder ‘slice00001 - slice05000’ and ‘slice05521 - slice06370’ contains three folders for each mode: ‘mode1’, ‘mode2’, ‘mode3’. In each of these folders there are the sinogram, the dark-field, and the two flat-fields for the raw data archives, or just the reconstructions and for mode2 the additional reference segmentation.

The corresponding reference reconstructions and segmentations can be found via the following links: 1-1,000, 1,001-2,000, 2,001-3,000, 3,001-4,000, 4,001-5,000, 5,521-6,370.

The corresponding Python scripts for loading, pre-processing, reconstructing and segmenting the projection data in the way described in the paper can be found on github. A machine-readable file with the used scanning parameters and instrument data for each acquisition mode as well as a script loading it can be found on the GitHub repository as well.

Note: It is advisable to use the graphical user interface when decompressing the .zip archives. If you experience a zipbomb error when unzipping the file on a Linux system rerun the command with the UNZIP_DISABLE_ZIPBOMB_DETECTION=TRUE environment variable by setting in your .bashrc “export UNZIP_DISABLE_ZIPBOMB_DETECTION=TRUE”.

For more information or guidance in using the data collection, please get in touch with

Maximilian.Kiss [at] cwi.nl

Felix.Lucka [at] cwi.nl",https://production-media.paperswithcode.com/datasets/bbe47a7b-9ba1-4b7b-aaaf-c0316e74ad75.png,EditCC Attribution 4.0 International,"3D, Image",,2023,,,,"training datasets consisting of measurement data and ground-truth images. However, suitable experimental datasets for X-ray Computed Tomography (CT) are scarce, and methods are often developed and evaluated only on simulated data. We fill this gap by providing the community with a versatile, open 2D fan-beam CT dataset suitable for developing ML techniques for a range of image reconstruction tasks. To acquire it, we designed a sophisticated, semi-automatic scan procedure that utilizes a highly-flexible laboratory X-ray CT setup. A diverse mix of samples",,"Low-Dose X-Ray Ct Reconstruction, Image Reconstruction, Denoising, CT Reconstruction, Image Denoising, Image Segmentation, Tomographic Reconstructions, Computed Tomography (CT)",,,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
CheXmask,CheXmask Dataset,"The CheXmask Database presents a comprehensive, uniformly annotated collection of chest radiographs, constructed from five public databases: ChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest and VinDr-CXR. The database aggregates 657,566 anatomical segmentation masks derived from images which have been processed using the HybridGNet model to ensure consistent, high-quality segmentation. To confirm the quality of the segmentations, we include in this database individual Reverse Classification Accuracy (RCA) scores for each of the segmentation masks. This dataset is intended to catalyze further innovation and refinement in the field of semantic chest X-ray analysis, offering a significant resource for researchers in the medical imaging domain.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution 4.0 International Public License,Image,,,,,,,,"Medical Image Segmentation, Heart Segmentation, Image Segmentation, Medical X-Ray Image Segmentation",,,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
EntitySeg,EntitySeg Dataset,"The EntitySeg dataset contains 33,227 images with high-quality mask annotations. Compared with existing dataets, there are three distinct properties in EntitySeg. First, 71.25% and 86.23% of the images are of high resolution with at least 2000px×2000px and 1000px×1000px which is more consistent with current digital imaging trends. Second, the dataset is open-world and is not limited to predefined classes. Third, the mask annotation along the boundaries are more accurate than existing datasets.",https://arxiv.org/pdf/2211.05776v1.pdf,EditCreative Commons Attribution-NonCommercial 4.0 International License,Image,,,,,227 images,,,"Image Segmentation, Semantic Segmentation",,,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
MARIDA,MARIDA Dataset,"MARIDA (Marine Debris Archive) is the first dataset based on the multispectral Sentinel-2 (S2) satellite data, which distinguishes Marine Debris from various marine features that co-exist, including Sargassum macroalgae, Ships, Natural Organic Material, Waves, Wakes, Foam, dissimilar water types (i.e., Clear, Turbid Water, Sediment-Laden Water, Shallow Water), and Clouds. MARIDA is an open-access dataset which enables the research community to explore the spectral behaviour of certain floating materials, sea state features and water types, to develop and evaluate Marine Debris detection solutions based on artificial intelligence and deep learning architectures, as well as satellite pre-processing pipelines.  Although it is designed to be beneficial for several machine learning tasks, it primarily aims to benchmark weakly supervised pixel-level semantic segmentation learning methods. 

MARIDA can be downloaded from the repository Zenodo (https://doi.org/10.5281/zenodo.5151941). A quick start guide for all ML benchmarks and the detailed overview of the dataset are available at https://marine-debris.github.io/.",https://production-media.paperswithcode.com/datasets/1b137f41-d688-438b-9daa-9d3b5d5c3d55.jpg,EditCreative Commons Attribution 4.0 International,Image,,,,,,,,"Weakly supervised segmentation, Image Segmentation",image-segmentation-on-marida,,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
MCubeS,MCubeS Dataset,"Multimodal material segmentation (MCubeS) dataset contains 500 sets of images from 42 street scenes. Each scene has images for four modalities: RGB, angle of linear polarization (AoLP), degree of linear polarization (DoLP), and near-infrared (NIR). The dataset provides annotated ground truth labels for both material and semantic segmentation for every pixel. The dataset is divided training set with 302 image sets, validation set with 96 image sets, and test set with 102 image sets. Each image has  1224 x 1024 pixels and a total of 20 class labels per pixel.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMiT,Image,,,,,,,,"Image Segmentation, Material Recognition, Semantic Segmentation",semantic-segmentation-on-mcubes,,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
MSD__Mirror_Segmentation_Dataset_,MSD (Mirror Segmentation Dataset) Dataset,"We construct the first large-scale mirror dataset, named MSD. It includes 4, 018 pairs of images containing mirrors and their corresponding manually annotated masks.",https://production-media.paperswithcode.com/datasets/8c3386db-8473-4a03-aa98-314e72e96e7e.jpg,EditUnknown,Image,,,,,,,,"Mirror Detection, Image Segmentation",image-segmentation-on-msd-mirror-segmentation,,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
Pascal_Panoptic_Parts,Pascal Panoptic Parts Dataset,The Pascal Panoptic Parts dataset consists of annotations for the part-aware panoptic segmentation task on the PASCAL VOC 2010 dataset. It is created by merging scene-level labels from PASCAL-Context with part-level labels from PASCAL-Part,https://arxiv.org/abs/2106.06351s,EditUnknown,Image,,2010,,,,,,"Human Part Segmentation, Part-aware Panoptic Segmentation, Panoptic Segmentation, Image Segmentation, Scene Understanding","part-aware-panoptic-segmentation-on-pascal, image-segmentation-on-pascal-panoptic-parts",,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
PASCAL_VOC,PASCAL VOC Dataset,"The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set.",https://arxiv.org/abs/1902.06162,EditCustom,"3D, Graph, Image, Text",English,2012,,,464 images,"train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images",,"Object Detection, Single-object colocalization, 3D Face Animation, Talking Face Generation, Open Vocabulary Semantic Segmentation, Single-object discovery, Interactive Segmentation, Unsupervised Semantic Segmentation with Language-image Pre-training, Multi-object colocalization, Zero-Shot Semantic Segmentation, Graph Matching, Semantic Segmentation, Knowledge Distillation, Image Segmentation, Node Classification, Object Counting, Multi-object discovery","single-object-discovery-on-voc-all, single-object-discovery-on-voc-6x2, 3d-face-animation-on-vocaset, open-vocabulary-semantic-segmentation-on-9, multi-object-colocalization-on-voc-all, node-classification-on-pascalvoc-sp-1, image-segmentation-on-pascal-voc, knowledge-distillation-on-pascal-voc, semantic-segmentation-on-pascal-voc, interactive-segmentation-on-pascal-voc, single-object-colocalization-on-voc-all, multi-object-discovery-on-voc-all, unsupervised-semantic-segmentation-with-7, object-detection-on-pascal-voc-10, object-counting-on-pascal-voc, object-detection-on-pascal-voc, object-counting-on-pascal-voc-2007-count-test, unsupervised-semantic-segmentation-with-11, graph-matching-on-pascal-voc, zero-shot-semantic-segmentation-on-pascal-voc, single-object-colocalization-on-voc-6x2, open-vocabulary-semantic-segmentation-on-5",,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
PMD,PMD Dataset,"We propose a large-scale benchmark here, which contains a total of 6,461 mirror images with ground truth annotations.",https://production-media.paperswithcode.com/datasets/fe60cb9c-afc3-4d35-af4f-b7d2e57bb7f9.jpg,EditUnknown,Image,,,,,,,,"Mirror Detection, Image Segmentation",image-segmentation-on-pmd,,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
SA-1B,SA-1B Dataset,"SA-1B consists of 11M diverse, high resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks.",https://arxiv.org/pdf/2304.02643v1.pdf,EditUnknown,Image,,,,,,,,"Segmentation, Image Segmentation",segmentation-on-sa-1b,,See all 1951 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
CodeSCAN,CodeSCAN Dataset,"CodeSCAN is the first large-scale and diverse dataset of coding screenshots with pixel-perfect annotations. It features:


24 popular programming languages (according to Github)
100 random repositories per language (with MIT, BSD-3 or WTFPL License), i.e. 2.400 repositories in total
Per repository we use 5 files, i.e. 12.000 files in total
~100 different themes and 25 different fonts
Diverse layouts changes, such as menu bar visibility, sidebar position, output window content, etc.
Numerous realistic interactions such as searching, typing and selecting within a file, etc.

Check our project page (https://a-nau.github.io/codescan/) for details.",https://production-media.paperswithcode.com/datasets/5417245a-7fe1-44f3-af3e-4d06e7a5414e.jpg,EditOther (Non-Commercial),Image,,,,,,,,"Object Detection, Image Stylization, Code Classification, Optical Character Recognition (OCR), Code Search",,,See all 1951 tasks,Image Stylization28 papers wit,Image Stylization28 papers wit
BSD,BSD Dataset,"BSD is a dataset used frequently for image denoising and super-resolution. Of the subdatasets, BSD100 is aclassical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. BSD100 is the testing set of the Berkeley segmentation dataset BSD300.",https://arxiv.org/abs/1904.07523,"EditCustom (research-only, non-commercial, attribution)","Audio, Image",,,,,,,,"Image Super-Resolution, Density Estimation, Compressive Sensing, Image Denoising, Color Image Denoising, Unified Image Restoration, Salt-And-Pepper Noise Removal, Grayscale Image Denoising, Blind Super-Resolution","grayscale-image-denoising-on-bsd200-sigma50, unified-image-restoration-on-bsd68-sigma25, grayscale-image-denoising-on-bsd200-sigma10, color-image-denoising-on-bsd68-sigma75, color-image-denoising-on-bsd68-sigma25, grayscale-image-denoising-on-bsd68-sigma35, blind-super-resolution-on-bsd100-3x-upscaling, grayscale-image-denoising-on-bsd68-sigma55, image-super-resolution-on-bsd200-2x-upscaling, blind-super-resolution-on-bsd100-4x-upscaling, grayscale-image-denoising-on-bsd68-sigma10, grayscale-image-denoising-on-bsd68-sigma70, grayscale-image-denoising-on-bsd68-sigma45, density-estimation-on-bsds300, image-super-resolution-on-bsd100-2x-upscaling, grayscale-image-denoising-on-bsd68-sigma75, color-image-denoising-on-bsd68-sigma30, color-image-denoising-on-bsd68-sigma15, color-image-denoising-on-bsd68-sigma10, image-super-resolution-on-bsds100-2x, grayscale-image-denoising-on-bsd68-sigma60, image-super-resolution-on-bsds100-8x, grayscale-image-denoising-on-bsd68-sigma30, color-image-denoising-on-bsd68-sigma5, salt-and-pepper-noise-removal-on-bsd300-noise, compressive-sensing-on-bsds100-2x-upscaling, grayscale-image-denoising-on-bsd68-sigma5, salt-and-pepper-noise-removal-on-bsd300-noise-1, color-image-denoising-on-bsd68-sigma70, image-super-resolution-on-bsd100-3x-upscaling, grayscale-image-denoising-on-bsd200-sigma30, image-super-resolution-on-bsd100-16x, grayscale-image-denoising-on-bsd68-sigma25, image-super-resolution-on-bsd100-4x-upscaling, compressive-sensing-on-bsd68-cs-50, blind-super-resolution-on-bsd100-2x-upscaling, image-denoising-on-bsd68-sigma50, grayscale-image-denoising-on-bsd68-sigma40, grayscale-image-denoising-on-bsd68-sigma65, grayscale-image-denoising-on-bsd68-sigma50, image-super-resolution-on-bsd100-8x-upscaling, salt-and-pepper-noise-removal-on-bsd300-noise-2, image-denoising-on-bsd68-sigma30, grayscale-image-denoising-on-bsd68-sigma20, color-image-denoising-on-bsd68-sigma35, grayscale-image-denoising-on-bsd200-sigma70, grayscale-image-denoising-on-bsd68-sigma15, image-super-resolution-on-bsds100-4x",,See all 1951 tasks,Image Super-Resolution84 bench,Image Super-Resolution84 bench
CelebA-HQ,CelebA-HQ Dataset,"The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.",https://arxiv.org/abs/1807.06358,EditCC BY-NC 4,"Image, Text",English,,,,000 images,,,"Image Super-Resolution, Density Estimation, Image-to-Image Translation, Unconditional Image Generation, Image Inpainting, Blind Face Restoration, Multimodal Unsupervised Image-To-Image Translation, Image Generation","image-generation-on-celeba-hq, image-generation-on-celeba-hq-256x256, blind-face-restoration-on-celeba-hq, image-to-image-translation-on-celeba-hq, image-super-resolution-on-celeba-hq-128x128, image-inpainting-on-celeba-hq, unconditional-image-generation-on-celeba-hq, density-estimation-on-celeba-hq-256x256, image-generation-on-celeba-hq-128x128, image-super-resolution-on-celeb-hq-4x, image-generation-on-celeba-hq-64x64, image-generation-on-celeba-hq-512x512, multimodal-unsupervised-image-to-image-4, image-generation-on-celeba-hq-1024x1024",,See all 1951 tasks,Image Super-Resolution84 bench,Image Super-Resolution84 bench
CelebA,CelebA Dataset,"CelebFaces Attributes dataset contains 202,599 face images of the size 178×218 from 10,177 celebrities, each annotated with 40 binary labels indicating facial attributes like hair color, gender and age.",https://arxiv.org/abs/1811.07483,EditUnknown,"Image, Text, Time Series",English,,,,,,,"HeavyMakeup/Bias-conflicting, Face Alignment, Facial Expression Translation, Physical Attribute Prediction, Image Super-Resolution, Image Compressed Sensing, Multi-Task Learning, HairColor/Bias-conflicting, Concept-based Classification, Image Attribution, Image Classification, Blind Face Restoration, Image Deblurring, HairColor/Unbiased, Image Colorization, Image Inpainting, Interpretability Techniques for Deep Learning, HeavyMakeup/Unbiased, Image Generation, Long-tail Learning","image-deblurring-on-celeba, image-generation-on-celeba-3, concept-based-classification-on-celeba, face-alignment-on-celeba-aligned, multi-task-learning-on-celeba, heavymakeup-unbiased-on-celeba, haircolor-bias-conflicting-on-celeba, image-classification-on-celeba-64x64, facial-expression-translation-on-celeba, image-generation-on-celeba-64x64, haircolor-unbiased-on-celeba, interpretability-techniques-for-deep-learning-1, image-super-resolution-on-celeba, image-generation-on-celeba-128x128, image-generation-on-celeba-256x256, image-inpainting-on-celeba, heavymakeup-bias-conflicting-on-celeba, face-alignment-on-celeba-aflw-unaligned, image-compressed-sensing-on-celeba, blind-face-restoration-on-celeba-test, image-attribution-on-celeba, long-tail-learning-on-celeba-5, image-colorization-on-celeba",,See all 1951 tasks,Image Super-Resolution84 bench,Image Super-Resolution84 bench
DIV2K,DIV2K Dataset,"DIV2K is a popular single-image super-resolution dataset which contains 1,000 images with different scenes and is splitted to 800 for training, 100 for validation and 100 for testing. It was collected for NTIRE2017 and NTIRE2018 Super-Resolution Challenges in order to encourage research on image super-resolution with more realistic degradation. This dataset contains low resolution images with different types of degradations. Apart from the standard bicubic downsampling, several types of degradations are considered in synthesizing low resolution images for different tracks of the challenges. Track 2 of NTIRE 2017 contains low resolution images with unknown x4 downscaling. Track 2 and track 4 of NTIRE 2018 correspond to realistic mild ×4 and realistic wild ×4 adverse conditions, respectively. Low-resolution images under realistic mild x4 setting suffer from motion blur, Poisson noise and pixel shifting. Degradations under realistic wild x4 setting are further extended to be of different levels from image to image.",https://arxiv.org/abs/1910.02593,EditCustom (research-only),Image,,2017,,,000 images,"splitted to 800 for training, 100 for validation and 100 for testing. It was collected for NTIRE2017 and NTIRE2018 Super-Resolution Challenges in order to encourage research on image super-resolution with more realistic degradation. This dataset contains low resolution images",,"Image Super-Resolution, JPEG Artifact Correction, Denoising, Image Rescaling, Jpeg Compression Artifact Reduction","image-super-resolution-on-div2k-val-16x, denoising-on-div2k, image-super-resolution-on-div2k-val-4x",,See all 1951 tasks,Image Super-Resolution84 bench,Image Super-Resolution84 bench
FFHQ,FFHQ Dataset,"Flickr-Faces-HQ (FFHQ) consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped using dlib. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Amazon Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.",https://github.com/NVlabs/ffhq-dataset,EditCC BY-NC-SA 4.0,"3D, Image, Text",English,,,,,,,"Image Super-Resolution, 3D-Aware Image Synthesis, Image Inpainting, Image Denoising, Image Generation, Face Hallucination, Facial Inpainting","image-generation-on-ffhq-64x64-4x-upscaling, 3d-aware-image-synthesis-on-ffhq-512-x-512-4x, image-denoising-on-ffhq, image-denoising-on-ffhq-64x64-4x-upscaling, facial-inpainting-on-ffhq, image-super-resolution-on-ffhq-512-x-512-4x, image-generation-on-ffhq-256-x-256, 3d-aware-image-synthesis-on-ffhq-256-x-256, image-super-resolution-on-ffhq-1024-x-1024-4x, image-inpainting-on-ffhq-1024-x-1024, image-generation-on-ffhq-1024-x-1024, face-hallucination-on-ffhq-512-x-512-16x, image-generation-on-ffhq-u, image-inpainting-on-ffhq-512-x-512, image-generation-on-ffhq-512-x-512, image-generation-on-ffhq, image-super-resolution-on-ffhq-256-x-256-4x",,See all 1951 tasks,Image Super-Resolution84 bench,Image Super-Resolution84 bench
Set14,Set14 Dataset,The Set14 dataset is a dataset consisting of 14 images commonly used for testing performance of Image Super-Resolution models.,https://production-media.paperswithcode.com/datasets/Set14-0000003442-673ab28f.jpg,EditUnknown,Image,,,,,14 images,,,"Image Super-Resolution, Blind Super-Resolution","blind-super-resolution-on-set14-4x-upscaling, image-super-resolution-on-set14-8x-upscaling, blind-super-resolution-on-set14-2x-upscaling, image-super-resolution-on-set14, image-super-resolution-on-set14-3x-upscaling, image-super-resolution-on-set14-4x-upscaling, image-super-resolution-on-set14-2x-upscaling, blind-super-resolution-on-set14-3x-upscaling",,See all 1951 tasks,Image Super-Resolution84 bench,Image Super-Resolution84 bench
Urban100,Urban100 Dataset,The Urban100 dataset contains 100 images of urban scenes. It commonly used as a test set to evaluate the performance of super-resolution models.,https://production-media.paperswithcode.com/datasets/Urban100-0000003447-44a59270.jpg,EditUnknown,Image,,,,,100 images,,,"Image Super-Resolution, Compressive Sensing, Joint Demosaicing and Denoising, Image Denoising, Color Image Denoising, Grayscale Image Denoising, Blind Super-Resolution","color-image-denoising-on-urban100-sigma50, color-image-denoising-on-urban100-sigma10, compressive-sensing-on-urban100-2x-upscaling, grayscale-image-denoising-on-urban100-sigma25, blind-super-resolution-on-urban100-2x, color-image-denoising-on-urban100-sigma25, joint-demosaicing-and-denoising-on-urban100, grayscale-image-denoising-on-urban100-sigma50, image-denoising-on-urban100-sigma15, image-super-resolution-on-urban100-8x, grayscale-image-denoising-on-urban100-sigma15-1, grayscale-image-denoising-on-urban100-sigma30, color-image-denoising-on-urban100-sigma70, blind-super-resolution-on-urban100-4x, image-super-resolution-on-urban100-4x, grayscale-image-denoising-on-urban100-sigma70, image-denoising-on-urban100-sigma50, grayscale-image-denoising-on-urban100-sigma10, image-super-resolution-on-urban100-3x, color-image-denoising-on-urban100-sigma30, image-super-resolution-on-urban100-2x, color-image-denoising-on-urban100-sigma15-1, blind-super-resolution-on-urban100-3x, image-super-resolution-on-urban100-16x",,See all 1951 tasks,Image Super-Resolution84 bench,Image Super-Resolution84 bench
VGGFace2,VGGFace2 Dataset,"VGGFace2 is a large-scale face recognition dataset. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession. VGGFace2 contains images from identities spanning a wide range of different ethnicities, accents, professions and ages. All face images are captured ""in the wild"", with pose and emotion variations and different lighting and occlusion conditions. Face distribution for different identities is varied, from 87 to 843, with an average of 362 images for each subject.",https://www.tensorflow.org/datasets/catalog/vgg_face2,EditUnknown,Image,,,,,362 images,,,"Image Super-Resolution, Image Attribution, Facial Inpainting","image-attribution-on-vggface2, facial-inpainting-on-vggface2, image-super-resolution-on-vggface2-8x",,See all 1951 tasks,Image Super-Resolution84 bench,Image Super-Resolution84 bench
YouTube-VIS_2019,YouTube-VIS 2019 Dataset,"YouTubeVIS is a new dataset tailored for tasks like simultaneous detection, segmentation and tracking of object instances in videos and is collected based on the current largest video object segmentation dataset YouTubeVOS.",https://github.com/youtubevos/MaskTrackRCNN,EditCC BY 4.0,"Image, Video",,,,,,,,"Instance Segmentation, Video Instance Segmentation, Semantic Segmentation","video-instance-segmentation-on-youtube-vis-1, video-instance-segmentation-on-youtube-vis",,See all 1951 tasks,Instance Segmentation106 bench,Instance Segmentation106 bench
Alpaca_Data_Galician,Alpaca Data Galician Dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache License 2.0,Text,English,,,,,,,"Text Generation, Instruction Following, Conversational Question Answering, Conversational Response Generation",,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
Bactrian-X,Bactrian-X Dataset,"Bactrian-X is a comprehensive multilingual parallel dataset of 3.4 million instruction-response pairs across 52 languages. The instructions were obtained from alpaca-52k, and dolly-15k, and tranlated into 52 languages (52 languages x 67k instances = 3.4M instances).",https://arxiv.org/pdf/2305.15011v1.pdf,EditCC BY NC 4.0,,,,,,67k instances,,,Instruction Following,,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
CIDAR,CIDAR Dataset,"CIDAR contains 10,000 instructions and their output. The dataset was created by selecting around 9,109 samples from Alpagasus dataset then translating it to Arabic using ChatGPT. In addition, we append that with around 891 Arabic grammar instructions from the webiste Ask the teacher. All the 10,000 samples were reviewed by around 12 reviewers.",https://production-media.paperswithcode.com/datasets/c42f975b-412a-458b-9857-e0957122ea89.png,EditCC BY-NC 4.0,,,,,,109 samples,,,Instruction Following,,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
GSCAN,GSCAN Dataset,"Grounded SCAN poses a simple task, where an agent must execute action sequences based on a synthetic language instruction.

The agent is presented with a simple grid world containing a collection of objects, each of which is associated with a vector of features. The agent is evaluated on its ability to follow one or more instructions in this environment. Some instructions require interaction with particular kinds of objects.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT Licence,,,,,,,,,"Instruction Following, Systematic Generalization",,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
IFEval,IFEval Dataset,"This dataset evaluates instruction following ability of large language models. There are 500+ prompts with instructions such as ""write an article with more than 800 words"", ""wrap your response with double quotation marks"", etc.",https://production-media.paperswithcode.com/datasets/afa3db9b-c9d9-432e-a741-534d5a3022ee.png,EditUnknown,Text,English,,,,,,,"Instruction Following, Large Language Model",instruction-following-on-ifeval,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
MIMIC-IT,MIMIC-IT Dataset,"MultI-Modal In-Context Instruction Tuning (MIMIC-IT) is a dataset for instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. The data sample consists of a queried image-instruction-answer triplet, with the instruction-answer tailored to the image, and context. The context contains a series of image-instruction-answer triplets that contextually correlate with the queried triplet, emulating the relationship between the context and the queried image-text pair found in the MMC4 dataset.",https://arxiv.org/pdf/2305.03726v1.pdf,EditMIT license,,,,,,,,,Instruction Following,,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
SurgeGlobal_LaMini,SurgeGlobal/LaMini Dataset,"Overview
The LaMini Dataset is an instruction dataset generated using h2ogpt-gm-oasst1-en-2048-falcon-40b-v2. It is designed for instruction-tuning pre-trained models to specialize them in a variety of downstream tasks.

Dataset Generation

Base Model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2.
Seed Instructions: Sourced from databricks/databricks-dolly-15k dataset.
Generation Approach: Example-guided and topic-guided strategies.
Total Instructions: 1,504 unique instruction examples.

Dataset Sources

Repository: Bitbucket Project
Paper : Pre-Print

Structure
Each entry in the dataset contains:
- Instruction
- Response

Usage
The LaMini Dataset can be used to fine-tune language models to improve their ability to follow instructions and generate relevant responses.

Access
The dataset is available on HuggingFace at the following link: https://huggingface.co/datasets/SurgeGlobal/LaMini

Citation
If you find our work useful, please cite our paper as follows:
@misc{surge2024openbezoar,
      title={OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data}, 
      author={Chandeepa Dissanayake and Lahiru Lowe and Sachith Gunasekara and Yasiru Ratnayake},
      year={2024},
      eprint={2404.12195},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

Dataset Authors
Chandeepa Dissanayake, Lahiru Lowe, Sachith Gunasekara, and Yasiru Ratnayake",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache 2.0,,,2048,,,,,,Instruction Following,,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
Tamil_Alpaca,Tamil Alpaca Dataset,"Dataset Card for ""tamil-alpaca""

This repository includes a Tamil-translated version of the Alpaca dataset. 

This dataset is part of the release of Tamil LLaMA family of models – an important step in advancing LLMs for the Tamil language. To dive deep into the development and capabilities of this model, please read the research paper and the introductory blog post (WIP)  that outlines our journey and the model's potential impact.

GitHub Repository: https://github.com/abhinand5/tamil-llama

Models trained using this dataset
| Model                    | Type                        | Data              | Base Model           | # Params | Download Links                                                         |
|--------------------------|-----------------------------|-------------------|----------------------|------|------------------------------------------------------------------------|
| Tamil LLaMA 7B Instruct  | Instruction following model | 145k instructions | Tamil LLaMA 7B Base  | 7B   | HF Hub |
| Tamil LLaMA 13B Instruct | Instruction following model | 145k instructions | Tamil LLaMA 13B Base | 13B  | HF Hub                       |

Meet the Developers
Get to know the creators behind this innovative model and follow their contributions to the field:


Abhinand Balachandran

Citation
If you use this model or any of the the Tamil-Llama datasets in your research, please cite:

bibtex
@misc{balachandran2023tamilllama,
      title={Tamil-Llama: A New Tamil Language Model Based on Llama 2}, 
      author={Abhinand Balachandran},
      year={2023},
      eprint={2311.05845},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Editgpl-3.0,Text,English,2023,,,,,,"Text Generation, Instruction Following",,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
Tamil_Alpaca_Orca,Tamil Alpaca Orca Dataset,"Dataset Card for ""tamil-alpaca""
This repository includes a Tamil-translated versions of the Alpaca dataset and a subset of OpenOrca dataset. 

This dataset is part of the release of Tamil LLaMA family of models – an important step in advancing LLMs for the Tamil language. To dive deep into the development and capabilities of this model, please read the research paper and the introductory blog post (WIP)  that outlines our journey and the model's potential impact.

GitHub Repository: https://github.com/abhinand5/tamil-llama

Models trained using this dataset
| Model                    | Type                        | Data              | Base Model           | # Params | Download Links                                                         |
|--------------------------|-----------------------------|-------------------|----------------------|------|------------------------------------------------------------------------|
| Tamil LLaMA 7B Instruct  | Instruction following model | 145k instructions | Tamil LLaMA 7B Base  | 7B   | HF Hub |
| Tamil LLaMA 13B Instruct | Instruction following model | 145k instructions | Tamil LLaMA 13B Base | 13B  | HF Hub                       |

Meet the Developers
Get to know the creators behind this innovative model and follow their contributions to the field:


Abhinand Balachandran

Citation
If you use this model or any of the the Tamil-Llama datasets in your research, please cite:

bibtex
@misc{balachandran2023tamilllama,
      title={Tamil-Llama: A New Tamil Language Model Based on Llama 2}, 
      author={Abhinand Balachandran},
      year={2023},
      eprint={2311.05845},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Editgpl-3.0,Text,English,2023,,,,,,"Text Generation, Instruction Following",,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
UGIF,UGIF Dataset,"UGIF is a multi-lingual, multi-modal UI grounded dataset for step-by-step task completion on the smartphone. It contains 523 natural language instructions with paired sequences of multilingual UI screens and actions that show how to execute the task in eight languages.",https://arxiv.org/pdf/2211.07615v1.pdf,EditUnknown,,,,,,,,,Instruction Following,,,See all 1951 tasks,Instruction Following2 benchma,Instruction Following2 benchma
UTRSet-Real,UTRSet-Real Dataset,"The UTRSet-Real dataset is a comprehensive, manually annotated dataset specifically curated for Printed Urdu OCR research. It contains over 11,000 printed text line images, each of which has been meticulously annotated. One of the standout features of this dataset is its remarkable diversity, which includes variations in fonts, text sizes, colours, orientations, lighting conditions, noises, styles, and backgrounds. This diversity closely mirrors real-world scenarios, making the dataset highly suitable for training and evaluating models that aim to excel in real-world Urdu text recognition tasks.

The availability of the UTRSet-Real dataset addresses the scarcity of comprehensive real-world printed Urdu OCR datasets. By providing researchers with a valuable resource for developing and benchmarking Urdu OCR models, this dataset promotes standardized evaluation and reproducibility and fosters advancements in the field of Urdu OCR. Further, to complement the UTRSet-Real for training purposes, we also present UTRSet-Synth, a high-quality synthetic dataset closely resembling real-world representations of Urdu text. For more information and details about the UTRSet-Real & UTRSet-Synth datasets, please refer to the paper ""UTRNet: High-Resolution Urdu Text Recognition In Printed Documents""",https://production-media.paperswithcode.com/datasets/832b65bb-0f5e-4cc8-999b-5231e66c6404.png,EditCC BY-NC-ND,"Image, Text",English,,,,,,,"Irregular Text Recognition, Optical Character Recognition (OCR), Scene Text Recognition, Printed Text Recognition",printed-text-recognition-on-utrset-real,,See all 1951 tasks,Irregular Text Recognition5 pa,Irregular Text Recognition5 pa
UTRSet-Synth,UTRSet-Synth Dataset,"The UTRSet-Synth dataset is introduced as a complementary training resource to the UTRSet-Real Dataset, specifically designed to enhance the effectiveness of Urdu OCR models. It is a high-quality synthetic dataset comprising 20,000 lines that closely resemble real-world representations of Urdu text.

To generate the dataset, a custom-designed synthetic data generation module which offers precise control over variations in crucial factors such as font, text size, colour, resolution, orientation, noise, style, and background, was employed. Moreover, the UTRSet-Synth dataset tackles the limitations observed in existing datasets. It addresses the challenge of standardizing fonts by incorporating over 130 diverse Urdu fonts, which were thoroughly refined to ensure consistent rendering schemes. It overcomes the scarcity of Arabic words, numerals, and Urdu digits by incorporating a significant number of samples representing these elements. Additionally, the dataset is enriched by randomly selecting words from a vocabulary of 100,000 words during the text generation process. As a result, UTRSet-Synth contains a total of 28,187 unique words, with an average word length of 7 characters.

The availability of the UTRSet-Synth dataset, a synthetic dataset that closely emulates real-world variations, addresses the scarcity of comprehensive real-world printed Urdu OCR datasets. By providing researchers with a valuable resource for developing and benchmarking Urdu OCR models, this dataset promotes standardized evaluation, and reproducibility, and fosters advancements in the field of Urdu OCR. For more information and details about the UTRSet-Real & UTRSet-Synth datasets, please refer to the paper ""UTRNet: High-Resolution Urdu Text Recognition In Printed Documents""",https://production-media.paperswithcode.com/datasets/51e62a1b-e2a9-426c-8853-72fd534468e9.png,EditCC BY-NC-ND,"Image, Text",English,,,,,,,"Irregular Text Recognition, Optical Character Recognition (OCR), Scene Text Recognition",,,See all 1951 tasks,Irregular Text Recognition5 pa,Irregular Text Recognition5 pa
AwA_Pose,AwA Pose Dataset,AwA Pose is a large scale animal keypoint dataset with ground truth annotations for keypoint detection of quadruped animals from images.,https://production-media.paperswithcode.com/datasets/Screenshot_2021-09-08_at_16.11.17.jpg,EditUnknown,"3D, Image",,,,,,,,"Animal Pose Estimation, Keypoint Detection",,,See all 1951 tasks,Keypoint Detection9 benchmarks,Keypoint Detection9 benchmarks
Fish_Keypoints_Detection,Fish Keypoints Detection Dataset,"The researchers collected 3,500 images of Tilapia fish, with each image containing three fish in a small bowl. These images were manually annotated using Roboflow, a tool for creating and managing annotated datasets. Four keypoints were labeled on each fish: mouth, peduncle, belly, and back. While the primary goal was to measure fish length using the mouth and peduncle points, the additional keypoints (belly and back) were included to support potential future research, such as using girth to determine fish weight. This dataset was used to train a YOLOv8 model for keypoint detection, achieving high accuracy in identifying these crucial points on the Tilapia fish.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,Image,,,,,500 images,,,Keypoint Detection,,,See all 1951 tasks,Keypoint Detection9 benchmarks,Keypoint Detection9 benchmarks
GRIT,GRIT Dataset,"The General Robust Image Task (GRIT) Benchmark is an evaluation-only benchmark for evaluating the performance and robustness of vision systems across multiple image prediction tasks, concepts, and data sources. GRIT hopes to encourage our research community to pursue the following research directions:


General purpose vision models - GRIT facilitates the evaluation of unified and general-purpose vision models that demonstrate a wide range of skills across a diverse set of concepts.
Robust specialized models - GRIT simplifies and unifies quantification of misinformation, calibration, and generalization under distribution shifts due to novel concepts, novel data sources or image distortions for 7 standard vision and vision-language tasks.
Efficient learning - GRIT includes a restricted and an unrestricted track. The restrictedtrack constrains the allowed training data to a selected but rich set of data sources that allows more scientific and meaningful comparison between models. This is meant to encourage resource constrained researchers to participate in the GRIT challenge and to spur interest in efficient learning methods as opposed to the dominant paradigm of training larger models on ever increasing amounts of training data. The unrestricted track allows much more flexibility in training data selection to test the capability of vision models trained with massive data and compute.",https://production-media.paperswithcode.com/datasets/6c493d17-c035-4a46-b62f-e2d9111d2e73.png,EditApache License 2.0,"Image, Text",English,,,,,,,"Object Categorization, Surface Normal Estimation, Object Segmentation, Keypoint Detection, Object Localization, Visual Question Answering, Instance Segmentation, Referring Expression Comprehension, Surface Normals Estimation, Visual Question Answering (VQA), Keypoint Estimation, Referring Expression","keypoint-estimation-on-grit, object-localization-on-grit, referring-expression-comprehension-on-grit, surface-normal-estimation-on-grit, visual-question-answering-on-grit-1, visual-question-answering-on-grit, object-categorization-on-grit, object-segmentation-on-grit",,See all 1951 tasks,Keypoint Detection9 benchmarks,Keypoint Detection9 benchmarks
KeypointNet,KeypointNet Dataset,"KeypointNet is a large-scale and diverse 3D keypoint dataset that contains 83,231 keypoints and 8,329 3D models from 16 object categories, by leveraging numerous human annotations, based on ShapeNet models.",https://github.com/qq456cvb/KeypointNet,EditUnknown,"3D, Image",,,,,,,,"3D Shape Representation, Pose Estimation, Keypoint Detection",,,See all 1951 tasks,Keypoint Detection9 benchmarks,Keypoint Detection9 benchmarks
OCHuman,OCHuman Dataset,"This dataset focuses on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13,360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human.",https://github.com/liruilong940607/OCHumanApi,EditUnknown,"3D, Image",,,,,5081 images,,,"Pose Estimation, Human Instance Segmentation, Pose-Based Human Instance Segmentation, Keypoint Detection, Multi-Person Pose Estimation, 2D Human Pose Estimation","multi-person-pose-estimation-on-ochuman, 2d-human-pose-estimation-on-ochuman, pose-estimation-on-ochuman, human-instance-segmentation-on-ochuman, pose-based-human-instance-segmentation-on, keypoint-detection-on-ochuman",,See all 1951 tasks,Keypoint Detection9 benchmarks,Keypoint Detection9 benchmarks
TAMPAR,TAMPAR Dataset,"TAMPAR is a real-world dataset of parcel photos for tampering detection with annotations in COCO format. For details see the paper and for visual samples the project page. Features are: 




900 annotated real-world images with >2,700 visible parcel side surfaces



6 different tampering types
6 different distortion strengths",https://production-media.paperswithcode.com/datasets/c04302c9-139f-482b-9532-ee41a1ddb44c.jpg,EditCreative Commons Attribution 4.0 International,Image,,,,,,,,"Change Detection, Object Detection, Instance Segmentation, Keypoint Detection",,,See all 1951 tasks,Keypoint Detection9 benchmarks,Keypoint Detection9 benchmarks
ANIMAL,ANIMAL Dataset,"10 classes with 50, 000 training and 5, 000 testing images. Please note that, in ANIMAL10N, noisy labels were injected naturally by human mistakes, where its noise rate was estimated at 8%.",https://production-media.paperswithcode.com/datasets/animal.JPG,EditUnknown,,,,,,,"training and 5, 000 testing images",10,Learning with noisy labels,learning-with-noisy-labels-on-animal,,See all 1951 tasks,Learning with noisy labels20 b,Learning with noisy labels20 b
Chaoyang,Chaoyang Dataset,"Chaoyang dataset contains 1111 normal, 842 serrated, 1404 adenocarcinoma, 664 adenoma, and 705 normal, 321 serrated, 840 adenocarcinoma, 273 adenoma samples for training and testing, respectively. This noisy dataset is constructed in the real scenario.  



Details: Colon slides from Chaoyang hospital, the patch size is 512 × 512. We invited 3 professional pathologists to label the patches, respectively. We took the parts of labeled patches with consensus results from 3 pathologists as the testing set. Others we used as the training set. For the samples with inconsistent labeling opinions of the three doctors in the training set (this part accounts for about 40%), we randomly selected the opinions from one of the three doctors.



The original WSIs are scanned at X20 objective magnification.",https://production-media.paperswithcode.com/datasets/238b8867-fa08-4281-8a53-ca1851132301.png,"EditThis dataset is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation. Permission is granted to use the data given that you agree to our license terms in ""https://github.com/bupt-ai-cz/HSA-NRL""",Image,,,,,,,,"Learning with noisy labels, Image Classification, Histopathological Image Classification, Colon Cancer Detection In Confocal Laser Microscopy Images","image-classification-on-chaoyang, learning-with-noisy-labels-on-chaoyang",,See all 1951 tasks,Learning with noisy labels20 b,Learning with noisy labels20 b
CIFAR-100N,CIFAR-100N Dataset,"This work presents two new benchmark datasets (CIFAR-10N, CIFAR-100N), equipping the training dataset of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels that we collect from Amazon Mechanical Turk.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Learning with noisy labels, Image Classification",learning-with-noisy-labels-on-cifar-100n,,See all 1951 tasks,Learning with noisy labels20 b,Learning with noisy labels20 b
CIFAR-10N,CIFAR-10N Dataset,"This work presents two new benchmark datasets (CIFAR-10N, CIFAR-100N), equipping the training dataset of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels that we collect from Amazon Mechanical Turk.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Learning with noisy labels, Image Classification","learning-with-noisy-labels-on-cifar-10n-3, learning-with-noisy-labels-on-cifar-10n-2, learning-with-noisy-labels-on-cifar-10n-1, learning-with-noisy-labels-on-cifar-10n, learning-with-noisy-labels-on-cifar-10n-worst, learning-with-noisy-labels-on-cifar-10n-4",,See all 1951 tasks,Learning with noisy labels20 b,Learning with noisy labels20 b
Clothing1M,Clothing1M Dataset,"Clothing1M contains 1M clothing images in 14 classes. It is a dataset with noisy labels, since the data is collected from several online shopping websites and include many mislabelled samples. This dataset also contains 50k, 14k, and 10k images with clean labels for training, validation, and testing, respectively.",https://arxiv.org/abs/1811.11165,EditUnknown,Image,,,,,10k images,,14,"Learning with noisy labels, Image Classification","image-classification-on-clothing1m, learning-with-noisy-labels-on-clothing1m, image-classification-on-clothing1m-using, learning-with-noisy-labels-on-clothing1m-2",,See all 1951 tasks,Learning with noisy labels20 b,Learning with noisy labels20 b
VoxCeleb1,VoxCeleb1 Dataset,"VoxCeleb1 is an audio dataset containing over 100,000 utterances for 1,251 celebrities, extracted from videos uploaded to YouTube.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-08_at_4.15.45_PM.png,EditUnknown,"3D, Audio, Image, Text, Video",English,,,,,,,", Speaker Identification, Learning with noisy labels, Few-Shot Audio Classification, Video Reconstruction, Speaker Recognition, Speaker Verification, Talking Head Generation","video-reconstruction-on-voxceleb, few-shot-audio-classification-on-voxceleb1, speaker-verification-on-voxceleb1, speaker-recognition-on-voxceleb1, speaker-verification-on-voxceleb, talking-head-generation-on-voxceleb1-1-shot, talking-head-generation-on-voxceleb1-32-shot, on-voxceleb1, talking-head-generation-on-voxceleb1-8-shot, speaker-identification-on-voxceleb1",,See all 1951 tasks,Learning with noisy labels20 b,Learning with noisy labels20 b
WebVision,WebVision Dataset,"The WebVision dataset is designed to facilitate the research on learning visual representation from noisy web data. It is a large scale web images dataset that contains more than 2.4 million of images crawled from the Flickr website and Google Images search. 

The same 1,000 concepts as the ILSVRC 2012 dataset are used for querying images, such that a bunch of existing approaches can be directly investigated and compared to the models trained from the ILSVRC 2012 dataset, and also makes it possible to study the dataset bias issue in the large scale scenario. The textual information accompanied with those images (e.g., caption, user tags, or description) are also provided as additional meta information. A validation set contains 50,000 images (50 images per category) is provided to facilitate the algorithmic development.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_15.54.02.png,EditCustom,Image,,2012,,,000 images,"trained from the ILSVRC 2012 dataset, and also makes it possible to study the dataset bias issue in the large scale scenario. The textual information accompanied with those images",,"Learning with noisy labels, Image Classification","image-classification-on-mini-webvision-1-0, image-classification-on-webvision, image-classification-on-webvision-1000, learning-with-noisy-labels-on-mini-webvision",,See all 1951 tasks,Learning with noisy labels20 b,Learning with noisy labels20 b
FGADR,FGADR Dataset,"This dataset has 1,842 images with pixel-level DR-related lesion annotations, and 1,000 images with image-level labels graded by six board-certified ophthalmologists with intra-rater consistency. The proposed dataset will enable extensive studies on DR diagnosis.",/paper/a-benchmark-for-studying-diabetic-retinopathy,EditUnknown,Image,,,,,842 images,,,Lesion Segmentation,,,See all 1951 tasks,Lesion Segmentation10 benchmar,Lesion Segmentation10 benchmar
HAM10000,HAM10000 Dataset,"HAM10000 is a dataset of 10000 training images for detecting pigmented skin lesions. The authors collected dermatoscopic images from different populations, acquired and stored by different modalities.",https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000,EditCC BY-NC,Image,,,,,,,,"Skin Lesion Classification, Lesion Segmentation, Semantic Segmentation, Lesion Classification","semantic-segmentation-on-ham10000, lesion-classification-on-ham10000, lesion-segmentation-on-ham10000",,See all 1951 tasks,Lesion Segmentation10 benchmar,Lesion Segmentation10 benchmar
ISIC2016,ISIC2016 Dataset,"Lesion segmentation data includes the original image, paired with the expert manual tracing of the lesion boundaries in the form of a binary mask. The Training Data file is a ZIP file, containing 900 dermoscopic lesion images in JPEG format. All images are named using the scheme ISIC_<image_id>.jpg, where <image_id> is a 7-digit unique identifier. EXIF tags in the images have been removed; any remaining EXIF tags should not be relied upon to provide accurate metadata. The Training Ground Truth file is a ZIP file, containing 900 binary mask images in PNG format. All masks are named using the scheme ISIC_<image_id>_Segmentation.png, where <image_id> matches the corresponding Training Data image for the mask. All mask images will have the exact same dimensions as their corresponding lesion image. Mask images are encoded as single-channel (grayscale) 8-bit PNGs (to provide lossless compression), where each pixel is either:

0: representing the background of the image or areas outside the lesion
255: representing the foreground of the image or areas inside the lesion",https://production-media.paperswithcode.com/datasets/f2f3eb5f-e5b7-4d83-b0f0-78a15ba8795d.png,Edithttps://creativecommons.org/share-your-work/public-domain/cc0/,Image,,,,,,"Training Data file is a ZIP file, containing 900 dermoscopic lesion images",,"Lesion Segmentation, Skin Lesion Segmentation",skin-lesion-segmentation-on-isic2016,,See all 1951 tasks,Lesion Segmentation10 benchmar,Lesion Segmentation10 benchmar
ISIC_2017_Task_1,ISIC 2017 Task 1 Dataset,"The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 1 challenge dataset for lesion segmentation contains 2,000 images for training with ground truth segmentations (2000 binary mask images).",https://challenge.isic-archive.com/landing/2017/42,EditUnknown,Image,,2017,,,000 images,training with ground truth segmentations (2000 binary mask images,,"Lesion Segmentation, Lesion Classification",,,See all 1951 tasks,Lesion Segmentation10 benchmar,Lesion Segmentation10 benchmar
ISIC_2018_Task_1,ISIC 2018 Task 1 Dataset,The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. This Task 1 dataset is the challenge on lesion segmentation. It includes 2594 images.,https://arxiv.org/abs/1909.00166,EditUnknown,Image,,2018,,,2594 images,,,"Lesion Classification, Lesion Segmentation, Semantic Segmentation",lesion-segmentation-on-isic-2018-task-1,,See all 1951 tasks,Lesion Segmentation10 benchmar,Lesion Segmentation10 benchmar
ISIC_2018_Task_3,ISIC 2018 Task 3 Dataset,"The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 3 dataset is the challenge on lesion classification. It includes 2594 images. The task is to classify the dermoscopic images into one of the following categories: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowen’s disease, benign keratosis, dermatofibroma, and vascular lesion.",https://arxiv.org/abs/1909.00166,EditUnknown,Image,,2018,,,2594 images,,,Lesion Segmentation,,,See all 1951 tasks,Lesion Segmentation10 benchmar,Lesion Segmentation10 benchmar
PH2,PH2 Dataset,"The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. The PH² dataset has been developed for research and benchmarking purposes, in order to facilitate comparative studies on both segmentation and classification algorithms of dermoscopic images. PH² is a dermoscopic image database acquired at the Dermatology Service of Hospital Pedro Hispano, Matosinhos, Portugal.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Lesion Segmentation, Skin Cancer Segmentation, Semantic Segmentation","semantic-segmentation-on-ph2, lesion-segmentation-on-ph2, skin-cancer-segmentation-on-ph2",,See all 1951 tasks,Lesion Segmentation10 benchmar,Lesion Segmentation10 benchmar
SD-198,SD-198 Dataset,"The SD-198 dataset contains 198 different diseases from different types of eczema, acne and various cancerous conditions. There are 6,584 images in total. A subset include the classes with more than 20 image samples, namely SD-128.""",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,584 images,,,"Lesion Segmentation, Partial Label Learning, Skin Cancer Segmentation, Classification, Semantic Segmentation, Skin Lesion Classification","partial-label-learning-on-isic-2019, semantic-segmentation-on-ph2, skin-cancer-segmentation-on-ph2, skin-lesion-classification-on-isic-2019, lesion-segmentation-on-ph2, classification-on-isic-2019",,See all 1951 tasks,Lesion Segmentation10 benchmar,Lesion Segmentation10 benchmar
Lighting_Estimation18_papers_with_code_Dataset,Lighting Estimation18 papers with code Dataset,,https://paperswithcode.com/dataset/lighting-estimation,,,,,,,,,,,,,See all 1951 tasks,Lighting Estimation18 papers w,Lighting Estimation18 papers w
3DMatch,3DMatch Dataset,"The 3DMATCH benchmark evaluates how well descriptors (both 2D and 3D) can establish correspondences between RGB-D frames of different views. The dataset contains 2D RGB-D patches and 3D patches (local TDF voxel grid volumes) of wide-baselined correspondences. 

The pixel size of each 2D patch is determined by the projection of the 0.3m3 local 3D patch around the interest point onto the image plane.",/paper/3dmatch-learning-local-geometric-descriptors,EditVarious,"3D, Image",,,,,,,,"3D Feature Matching, Low-Light Image Enhancement, Point Cloud Registration","point-cloud-registration-on-3dmatch-benchmark, 3d-feature-matching-on-3dmatch-benchmark, low-light-image-enhancement-on-3dmatch",,See all 1951 tasks,Low-Light Image Enhancement22 ,Low-Light Image Enhancement22 
AFLW,AFLW Dataset,"The Annotated Facial Landmarks in the Wild (AFLW) is a large-scale collection of annotated face images gathered from Flickr, exhibiting a large variety in appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total about 25K faces are annotated with up to 21 landmarks per image.",https://arxiv.org/abs/1812.00739,EditCustom (non-commercial),"3D, Image",,,,,,,,"Unsupervised Facial Landmark Detection, Face Alignment, Head Pose Estimation, Low-Light Image Enhancement, Facial Landmark Detection","facial-landmark-detection-on-aflw-full, face-alignment-on-aflw-pifa-21-points-1, unsupervised-facial-landmark-detection-on-2, face-alignment-on-aflw-lfpa, low-light-image-enhancement-on-aflw-zhang, face-alignment-on-aflw, facial-landmark-detection-on-aflw-front, face-alignment-on-aflw-full-1, head-pose-estimation-on-aflw, face-alignment-on-aflw-pifa-34-points-1, unsupervised-facial-landmark-detection-on-3",,See all 1951 tasks,Low-Light Image Enhancement22 ,Low-Light Image Enhancement22 
DICM,DICM Dataset,DICM is a dataset for low-light enhancement which consists of 69 images collected with commercial digital cameras.,https://arxiv.org/abs/1808.04560,EditUnknown,Image,,,,,69 images,,,Low-Light Image Enhancement,low-light-image-enhancement-on-dicm,,See all 1951 tasks,Low-Light Image Enhancement22 ,Low-Light Image Enhancement22 
LOL-v2,LOL-v2 Dataset,LOL-v2-real contains 689 low-/normal-light image pairs for training and 100 pairs for testing.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Low-Light Image Enhancement,low-light-image-enhancement-on-lol-v2,,See all 1951 tasks,Low-Light Image Enhancement22 ,Low-Light Image Enhancement22 
LOL,LOL Dataset,The LOL dataset is composed of 500 low-light and normal-light image pairs and divided into 485 training pairs and 15 testing pairs. The low-light images contain noise produced during the photo capture process. Most of the images are indoor scenes. All the images have a resolution of 400×600.,https://arxiv.org/abs/2005.02818,EditUnknown,Image,,,,,,training pairs and 15 testing pairs. The low-light images,,"Unified Image Restoration, Low-Light Image Enhancement","low-light-image-enhancement-on-lol, unified-image-restoration-on-lol",,See all 1951 tasks,Low-Light Image Enhancement22 ,Low-Light Image Enhancement22 
SMID,SMID Dataset,"This is the low-light image enhancement dataset collected by the CVPR 2018 paper ""Seeing Motion in the Dark"".",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditNone,Image,,2018,,,,,,Low-Light Image Enhancement,low-light-image-enhancement-on-smid,,See all 1951 tasks,Low-Light Image Enhancement22 ,Low-Light Image Enhancement22 
C3,C3 Dataset,C3 is a free-form multiple-Choice Chinese machine reading Comprehension dataset.,https://huggingface.co/datasets/dataset-org/c3,EditUnknown,Text,English,,,,,,,"Machine Reading Comprehension, Common Sense Reasoning (Few-Shot), Common Sense Reasoning (One-Shot), Language Modelling, Common Sense Reasoning (Zero-Shot), Reading Comprehension","common-sense-reasoning-one-shot-on-c3, common-sense-reasoning-zero-shot-on-c3, common-sense-reasoning-few-shot-on-c3",,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
CBT,CBT Dataset,Children’s Book Test (CBT) is designed to measure directly how well language models can exploit wider linguistic context. The CBT is built from books that are freely available thanks to Project Gutenberg.,https://research.fb.com/downloads/babi/,EditGNU Free Documentation License,"Text, Time Series",English,,,,,,,"Reading Comprehension, Machine Reading Comprehension, Click-Through Rate Prediction, Question Answering","click-through-rate-prediction-on-childrens, question-answering-on-childrens-book-test",,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
CMRC,CMRC Dataset,"CMRC is a dataset is annotated by human experts with near 20,000 questions as well as a challenging set which is composed of the questions that need reasoning over multiple clues.",https://www.aclweb.org/anthology/D19-1600.pdf,EditCC-BY-SA-4.0,Text,English,,,,,,,"Reading Comprehension (Zero-Shot), Reading Comprehension (Few-Shot), Machine Reading Comprehension, Cloze (multi-choices) (One-Shot), Cloze (multi-choices) (Few-Shot), Language Modelling, Reading Comprehension (One-Shot), Cloze (multi-choices) (Zero-Shot), Reading Comprehension, Chinese Reading Comprehension","reading-comprehension-few-shot-on-cmrc-2018, cloze-multi-choices-zero-shot-on-cmrc-2019, cloze-multi-choices-one-shot-on-cmrc-2019, cloze-multi-choices-few-shot-on-cmrc-2019, chinese-reading-comprehension-on-cmrc-2019, reading-comprehension-one-shot-on-cmrc-2018, cloze-multi-choices-one-shot-on-cmrc-2017, reading-comprehension-zero-shot-on-cmrc-2018, cloze-multi-choices-few-shot-on-cmrc-2017, cloze-multi-choices-zero-shot-on-cmrc-2017, chinese-reading-comprehension-on-cmrc-2018-3",,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
DRCD,DRCD Dataset,"Delta Reading Comprehension Dataset (DRCD) is an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators.",https://github.com/DRCKnowledgeTeam/DRCD,EditCC-BY-SA 3.0,Text,English,,,,014 paragraphs,,,"Reading Comprehension (Zero-Shot), Reading Comprehension (Few-Shot), Machine Reading Comprehension, Reading Comprehension (One-Shot), Reading Comprehension, Chinese Reading Comprehension, Question Answering","reading-comprehension-one-shot-on-drcd, chinese-reading-comprehension-on-drcd-1, chinese-reading-comprehension-on-drcd, reading-comprehension-zero-shot-on-drcd, reading-comprehension-few-shot-on-drcd",,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
DREAM,DREAM Dataset,"DREAM is a multiple-choice Dialogue-based REAding comprehension exaMination dataset. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding.

DREAM contains 10,197 multiple choice questions for 6,444 dialogues, collected from English-as-a-foreign-language examinations designed by human experts. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge.",https://dataset.org/dream/,"EditCustom (research-only, non-commercial)","Image, Text",English,,,,,,,"Reading Comprehension, Sleep spindles detection, Machine Reading Comprehension, Question Answering","sleep-spindles-detection-on-dreams-sleep, machine-reading-comprehension-on-dream",,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
LogiQA,LogiQA Dataset,"LogiQA consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. The dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting.",/paper/logiqa-a-challenge-dataset-for-machine,EditUnknown,Text,English,,,,,,,"Decision Making, Machine Reading Comprehension, Natural Language Understanding",,,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
MCTest,MCTest Dataset,"MCTest is a freely available set of stories and associated questions intended for research on the machine comprehension of text. 

MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.",https://www.aclweb.org/anthology/D13-1020.pdf,EditCustom (see LICENSE.pdf),Text,English,,,,,,,"Reading Comprehension, Machine Reading Comprehension, Question Answering","question-answering-on-mctest-160, question-answering-on-mctest-500",,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
MRQA,MRQA Dataset,The MRQA (Machine Reading for Question Answering) dataset is a dataset for evaluating the generalization capabilities of reading comprehension systems.,/paper/mrqa-2019-shared-task-evaluating,EditUnknown,Text,English,,,,,,,"Reading Comprehension, Machine Reading Comprehension, Question Answering",question-answering-on-mrqa-2019,,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
MuTual,MuTual Dataset,"MuTual is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified from Chinese high school English listening comprehension test data. It tests dialogue reasoning via next utterance prediction.",https://github.com/Nealcly/MuTual,EditUnknown,Text,English,,,,,,,"Text Generation, Task-Oriented Dialogue Systems, Machine Reading Comprehension",,,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
ReClor,ReClor Dataset,"Logical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary language as the definition from Law School Admission Council. ReClor is a dataset extracted from logical reasoning questions of standardized graduate admission examinations.",https://whyu.me/reclor/,EditUnknown,Text,English,,,,,,,"Logical Reasoning Question Answering, Reading Comprehension, Machine Reading Comprehension, Question Answering","reading-comprehension-on-reclor, question-answering-on-reclor, logical-reasoning-question-ansering-on-reclor, machine-reading-comprehension-on-reclor",,See all 1951 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
MIMIC-III,MIMIC-III Dataset,"The Medical Information Mart for Intensive Care III (MIMIC-III) dataset is a large, de-identified and publicly-available collection of medical records. Each record in the dataset includes ICD-9 codes, which identify diagnoses and procedures performed. Each code is partitioned into sub-codes, which often include specific circumstantial details. The dataset consists of 112,000 clinical reports records (average length 709.3 tokens) and 1,159 top-level ICD-9 codes. Each report is assigned to 7.6 codes, on average. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. 

The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.",https://imes.mit.edu/supporting-clinical-research-with-the-mimic-iii-critical-care-database/,EditMIT,"Image, Text, Time Series",English,,,,,,,"Length-of-Stay prediction, Blood pressure estimation, Medical Code Prediction, Multivariate Time Series Forecasting, Multi-Label Text Classification, Multi-Label Classification Of Biomedical Texts, Mortality Prediction","multivariate-time-series-forecasting-on-mimic, mortality-prediction-on-mimic-iii, blood-pressure-estimation-on-mimic-iii, length-of-stay-prediction-on-mimic-iii, multi-label-classification-of-biomedical, medical-code-prediction-on-mimic-iii, multi-label-text-classification-on-mimic-iii",,See all 1951 tasks,Medical Code Prediction7 bench,Medical Code Prediction7 bench
MIMIC-IV-ICD-10-full,MIMIC-IV-ICD-10-full Dataset,"The MIMIC-IV-ICD10-full dataset, including occurring labels.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Time Series,,,,,,,,Medical Code Prediction,medical-code-prediction-on-mimic-iv-icd-10-1,,See all 1951 tasks,Medical Code Prediction7 bench,Medical Code Prediction7 bench
MIMIC-IV-ICD10-top50,MIMIC-IV-ICD10-top50 Dataset,"The MIMIC-IV-ICD10 dataset, featuring the top 50 most frequently occurring labels.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Time Series,,,,,,,,Medical Code Prediction,medical-code-prediction-on-mimic-iv-icd10,,See all 1951 tasks,Medical Code Prediction7 bench,Medical Code Prediction7 bench
MIMIC-IV-ICD9-full,MIMIC-IV-ICD9-full Dataset,"The MIMIC-IV-ICD9 dataset, including all occurring labels.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Time Series,,,,,,,,Medical Code Prediction,medical-code-prediction-on-mimic-iv-icd9-full,,See all 1951 tasks,Medical Code Prediction7 bench,Medical Code Prediction7 bench
MIMIC-IV-ICD9-top50,MIMIC-IV-ICD9-top50 Dataset,"The MIMIC-IV-ICD9 dataset, featuring the top 50 most frequently occurring labels.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Time Series,,,,,,,,Medical Code Prediction,medical-code-prediction-on-mimic-iv-icd9,,See all 1951 tasks,Medical Code Prediction7 bench,Medical Code Prediction7 bench
MIMIC-IV_ICD-10,MIMIC-IV ICD-10 Dataset,"MIMIC-IV ICD-10 contains 122,279 discharge summaries—free-text medical documents—annotated with ICD-10 diagnosis and procedure codes. It contains data for patients admitted to the Beth Israel Deaconess Medical Center emergency department or ICU between 2008-2019. All codes with fewer than ten examples have been removed, and the train-val-test split was created using multi-label stratified sampling. The dataset is described further in Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study, and the code to use the dataset is found here.

The dataset is intended for medical code prediction and was created using MIMIC-IV v2.2 and MIMIC-IV-NOTE v2.2. Using the two datasets requires a license obtained in Physionet; this can take a couple of days.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditPhysioNet Credentialed Health Data License 1.5.0,Time Series,,2008,,,,,,Medical Code Prediction,medical-code-prediction-on-mimic-iv-icd-10,,See all 1951 tasks,Medical Code Prediction7 bench,Medical Code Prediction7 bench
MIMIC-IV_ICD-9,MIMIC-IV ICD-9 Dataset,"MIMIC-IV ICD-9 contains 209,326 discharge summaries—free-text medical documents—annotated with ICD-9 diagnosis and procedure codes. It contains data for patients admitted to the Beth Israel Deaconess Medical Center emergency department or ICU between 2008-2019. All codes with fewer than ten examples have been removed, and the train-val-test split was created using multi-label stratified sampling. The dataset is described further in Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study, and the code to use the dataset is found here.

The dataset is intended for medical code prediction and was created using MIMIC-IV v2.2 and MIMIC-IV-NOTE v2.2. Using the two datasets requires a license obtained in Physionet; this can take a couple of days.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditPhysioNet Credentialed Health Data License 1.5.0,Time Series,,2008,,,,,,Medical Code Prediction,medical-code-prediction-on-mimic-iv-icd-9,,See all 1951 tasks,Medical Code Prediction7 bench,Medical Code Prediction7 bench
ACNE04,ACNE04 Dataset,"The ACNE04 dataset includes 3756 Chinese face images with Acne.  The ACNE04 dataset includes the annotations of local lesion numbers and global acne
severity based on Hayashi Criterion.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Acne Severity Grading, Medical Image Classification",acne-severity-grading-on-acne04,,See all 1951 tasks,Medical Image Classification11,Medical Image Classification11
COVIDGR,COVIDGR Dataset,"Under a close collaboration with an expert radiologist team of the Hospital Universitario San Cecilio, the COVIDGR-1.0 dataset of patients' anonymized X-ray images has been built. 852 images have been collected following a strict labeling protocol. They are categorized into 426 positive cases and 426 negative cases. Positive images correspond to patients who have been tested positive for COVID-19 using RT-PCR within a time span of at most 24h between the X-ray image and the test. Every image has been taken using the same type of equipment and with the same format: only the posterior-anterior view is considered.",https://github.com/ari-dasci/covidgr,EditUnknown,Image,,,,,852 images,,,"COVID-19 Diagnosis, Medical Image Classification, Domain Adaptation","covid-19-diagnosis-on-covidgr, medical-image-classification-on-covidgr",,See all 1951 tasks,Medical Image Classification11,Medical Image Classification11
IDRiD,IDRiD Dataset,Indian Diabetic Retinopathy Image Dataset (IDRiD) dataset consists of typical diabetic retinopathy lesions and normal retinal structures annotated at a pixel level. This dataset also provides information on the disease severity of diabetic retinopathy and diabetic macular edema for each image. This dataset is perfect for the development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.,https://production-media.paperswithcode.com/datasets/idrid_logo.png,EditUnknown,Image,,,,,,,,"Optic Disc Detection, Fovea Detection, Medical Image Classification","optic-disc-detection-on-idrid, medical-image-classification-on-idrid, fovea-detection-on-idrid",,See all 1951 tasks,Medical Image Classification11,Medical Image Classification11
ISIC_2020_Challenge_Dataset,ISIC 2020 Challenge Dataset Dataset,"The dataset contains 33,126 dermoscopic training images of unique benign and malignant skin lesions from over 2,000 patients. Each image is associated with one of these individuals using a unique patient identifier. All malignant diagnoses have been confirmed via histopathology, and benign diagnoses have been confirmed using either expert agreement, longitudinal follow-up, or histopathology. A thorough publication describing all features of this dataset is available in the form of a pre-print that has not yet undergone peer review.

The dataset was generated by the International Skin Imaging Collaboration (ISIC) and images are from the following sources: Hospital Clínic de Barcelona, Medical University of Vienna, Memorial Sloan Kettering Cancer Center, Melanoma Institute Australia, University of Queensland, and the University of Athens Medical School.

The dataset was curated for the SIIM-ISIC Melanoma Classification Challenge hosted on Kaggle during the Summer of 2020.

DOI: https://doi.org/10.34970/2020-ds01",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-Non Commercial 4.0 International License.,Image,,2020,,,,,,"Skin Cancer Classification, Skin Lesion Classification, Skin Cancer Segmentation, Medical Image Classification",medical-image-classification-on-isic-2020,,See all 1951 tasks,Medical Image Classification11,Medical Image Classification11
LIMUC,LIMUC Dataset,"The LIMUC dataset is the largest publicly available labeled ulcerative colitis dataset that compromises 11276 images from 564 patients and 1043 colonoscopy procedures. Three experienced gastroenterologists were involved in the annotation process, and all images are labeled according to the Mayo endoscopic score (MES).",https://production-media.paperswithcode.com/datasets/d4fa3957-7be3-4a6c-93ae-6cd2284384f9.png,EditCreative Commons Attribution 4.0 International,Image,,,,,11276 images,,,"Medical Image Classification, Self-Supervised Learning, Image Classification, Semi-Supervised Image Classification, Medical Diagnosis",image-classification-on-limuc,,See all 1951 tasks,Medical Image Classification11,Medical Image Classification11
Malaria_Dataset,Malaria Dataset Dataset,"The dataset contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells.",https://lhncbc.nlm.nih.gov/publication/pub9932,EditUnknown,Image,,,,,,,,"Image Classification, Medical Image Classification","medical-image-classification-on-malaria, image-classification-on-malaria-dataset",,See all 1951 tasks,Medical Image Classification11,Medical Image Classification11
NCT-CRC-HE-100K,NCT-CRC-HE-100K Dataset,"The NCT-CRC-HE-100K dataset is a set of 100,000 non-overlapping image patches extracted from 86 H$\&$E stained human cancer tissue slides and normal tissue from the NCT biobank (National Center for Tumor Diseases) and the UMM pathology archive (University Medical Center Mannheim). While the dataset Colorectal Cacner-Validation-Histology-7K (CRC-VAL-HE-7K) consist of 7180 images extracted from 50 patients with colorectal adenocarcinoma and were used to create a dataset that does not overlap with patients in the NCT-CRC-HE-100K dataset. It was created by pathologists by manually delineating tissue regions in whole slide images into the following nine tissue classes: Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR), colorectal adenocarcinoma epithelium (TUM).",https://production-media.paperswithcode.com/datasets/91495aef-1582-41c6-abc2-1268c00bbd66.png,EditUnknown,Image,,,,,7180 images,Validation-Histology-7K (CRC-VAL-HE-7K) consist of 7180 images,,"Image Classification, Medical Image Classification","image-classification-on-nct-crc-he-100k, medical-image-classification-on-nct-crc-he",,See all 1951 tasks,Medical Image Classification11,Medical Image Classification11
NIH-CXR-LT,NIH-CXR-LT Dataset,"NIH-CXR-LT. NIH ChestXRay14 contains over 100,000 chest X-rays labeled with 14 pathologies, plus a “No Findings” class. We construct a single-label, long-tailed version of the NIH ChestXRay14 dataset by introducing five new disease findings described above. The resulting NIH-CXR-LT dataset has 20 classes, including 7 head classes, 10 medium classes, and 3 tail classes. NIH-CXR-LT contains 88,637 images labeled with one of 19 thorax diseases, with 68,058 training and 20,279 test images. The validation and balanced test sets contain 15 and 30 images per class, respectively.",https://production-media.paperswithcode.com/datasets/e5b719ab-afb1-417a-b520-ab19c665c218.png,EditUnknown,Image,,,,,637 images,"training and 20,279 test images",20,"Image Classification, Long-tail Learning, Medical Image Classification",long-tail-learning-on-nih-cxr-lt,,See all 1951 tasks,Medical Image Classification11,Medical Image Classification11
OASIS,OASIS Dataset,"A dataset for single-image 3D in the wild consisting of annotations of detailed 3D geometry for 140,000 images.",/paper/oasis-a-large-scale-dataset-for-single-image-1,EditUnknown,"3D, Graph, Image",,,,,000 images,,,"Depth Estimation, Graph Classification, Medical Image Classification, Medical Image Registration","medical-image-classification-on-oasis-3, medical-image-registration-on-oasis, graph-classification-on-oasis",,See all 1951 tasks,Medical Image Classification11,Medical Image Classification11
BreakHis,BreakHis Dataset,"The Breast Cancer Histopathological Image Classification (BreakHis) is  composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients using different magnifying factors (40X, 100X, 200X, and 400X).  It contains 2,480  benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format). This database has been built in collaboration with the P&D Laboratory - Pathological Anatomy and Cytopathology, Parana, Brazil.

Paper: F. A. Spanhol, L. S. Oliveira, C. Petitjean and L. Heutte, ""A Dataset for Breast Cancer Histopathological Image Classification,"" in IEEE Transactions on Biomedical Engineering, vol. 63, no. 7, pp. 1455-1462, July 2016, doi: 10.1109/TBME.2015.2496264",https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/,EditUnknown,Image,,2016,,,,,,"Medical Image Retrieval, Breast Cancer Detection, Breast Cancer Histology Image Classification, Breast Cancer Histology Image Classification (20% labels), Image Classification","breast-cancer-histology-image-classification, image-classification-on-breakhis, breast-cancer-detection-on-breakhis, medical-image-retrieval-on-breakhis, breast-cancer-histology-image-classification-1",,See all 1951 tasks,Medical Image Retrieval1 bench,Medical Image Retrieval1 bench
BreastDICOM4,BreastDICOM4 Dataset,"Several datasets are fostering innovation in higher-level functions for everyone, everywhere. By providing this repository, we hope to encourage the research community to focus on hard problems. In this repository, we present our medical imaging DICOM files of patients from our User Tests and Analysis 4 (UTA4) study. Here, we provide a dataset of the used medical images during the UTA4 tasks. This repository and respective dataset should be paired with the dataset-uta4-rates repository dataset. Work and results are published on a top Human-Computer Interaction (HCI) conference named AVI 2020 (page). Results were analyzed and interpreted on our Statistical Analysis charts. The user tests were made in clinical institutions, where clinicians diagnose several patients for a Single-Modality vs Multi-Modality comparison. For example, in these tests, we used both prototype-single-modality and prototype-multi-modality repositories for the comparison. On the same hand, the hereby dataset represents the pieces of information of both BreastScreening and MIDA projects. These projects are research projects that deal with the use of a recently proposed technique in literature: Deep Convolutional Neural Networks (CNNs). From a developed User Interface (UI) and framework, these deep networks will incorporate several datasets in different modes. For more information about the available datasets please follow the Datasets page on the Wiki of the meta information repository. Last but not least, you can find further information on the Wiki in this repository. We also have several demos to see in our YouTube Channel, please follow us.",https://production-media.paperswithcode.com/datasets/c387aa6f-e5c7-49c9-844e-a9e2cb193ad3.png,EditAGPL-3.0,Image,,2020,,,,"Tests and Analysis 4 (UTA4) study. Here, we provide a dataset of the used medical images",,"Medical Image Segmentation, Medical Diagnosis, Medical Image Retrieval, Medical Image Registration",medical-diagnosis-on-mimbcd-ui-uta7-medical,,See all 1951 tasks,Medical Image Retrieval1 bench,Medical Image Retrieval1 bench
BreastRates4,BreastRates4 Dataset,"Several datasets are fostering innovation in higher-level functions for everyone, everywhere. By providing this repository, we hope to encourage the research community to focus on hard problems. In this repository, we present our severity rates (BIRADS) of clinicians while diagnosing several patients from our User Tests and Analysis 4 (UTA4) study. Here, we provide a dataset for the measurements of severity rates (BIRADS) concerning the patient diagnostic. Work and results are published on a top Human-Computer Interaction (HCI) conference named AVI 2020 (page). Results were analyzed and interpreted from our Statistical Analysis charts. The user tests were made in clinical institutions, where clinicians diagnose several patients for a Single-Modality vs Multi-Modality comparison. For example, in these tests, we used both prototype-single-modality and prototype-multi-modality repositories for the comparison. On the same hand, the hereby dataset represents the pieces of information of both BreastScreening and MIDA projects. These projects are research projects that deal with the use of a recently proposed technique in literature: Deep Convolutional Neural Networks (CNNs). From a developed User Interface (UI) and framework, these deep networks will incorporate several datasets in different modes. For more information about the available datasets please follow the Datasets page on the Wiki of the meta information repository. Last but not least, you can find further information on the Wiki in this repository. We also have several demos to see in our YouTube Channel, please follow us.",https://production-media.paperswithcode.com/datasets/dabeda64-dbfc-4eb0-b87b-67ec6f6e65fa.png,EditAGPL-3.0,Image,,2020,,,,,,"Medical Image Retrieval, Medical Diagnosis",,,See all 1951 tasks,Medical Image Retrieval1 bench,Medical Image Retrieval1 bench
ACDC,ACDC Dataset,"The goal of the Automated Cardiac Diagnosis Challenge (ACDC) challenge is to:


compare the performance of automatic methods on the segmentation of the left ventricular endocardium and epicardium as the right ventricular endocardium for both end diastolic and end systolic phase instances;
compare the performance of automatic methods for the classification of the examinations in five classes (normal case, heart failure with infarction, dilated cardiomyopathy, hypertrophic cardiomyopathy, abnormal right ventricle).

The overall ACDC dataset was created from real clinical exams acquired at the University Hospital of Dijon. Acquired data were fully anonymized and handled within the regulations set by the local ethical committee of the Hospital of Dijon (France). Our dataset covers several well-defined pathologies with enough cases to (1) properly train machine learning methods and (2) clearly assess the variations of the main physiological parameters obtained from cine-MRI (in particular diastolic volume and ejection fraction). The dataset is composed of 150 exams (all from different patients) divided into 5 evenly distributed subgroups (4 pathological plus 1 healthy subject groups) as described below. Furthermore, each patient comes with the following additional information : weight, height, as well as the diastolic and systolic phase instants.

The database is made available to participants through two datasets from the dedicated online evaluation website after a personal registration: i) a training dataset of 100 patients along with the corresponding manual references based on the analysis of one clinical expert; ii) a testing dataset composed of 50 new patients, without manual annotations but with the patient information given above. The raw input images are provided through the Nifti format.",https://acdc.creatis.insa-lyon.fr/description/databases.html,EditUnknown,"Image, Text",English,,,,,"valuation website after a personal registration: i) a training dataset of 100 patients along with the corresponding manual references based on the analysis of one clinical expert; ii) a testing dataset composed of 50 new patients, without manual annotations but with the patient information given above. The raw input images",,"Medical Image Segmentation, Semi-supervised Medical Image Segmentation, Diffeomorphic Medical Image Registration, Medical Image Generation","diffeomorphic-medical-image-registration-on-1, semi-supervised-medical-image-segmentation-on-2, medical-image-generation-on-acdc, medical-image-segmentation-on-acdc, medical-image-segmentation-on-automatic",,See all 1951 tasks,Medical Image Segmentation138 ,Medical Image Segmentation138 
CHASE_DB1,CHASE_DB1 Dataset,CHASE_DB1 is a dataset for retinal vessel segmentation which contains 28 color retina images with the size of 999×960 pixels which are collected from both left and right eyes of 14 school children. Each image is annotated by two independent human experts.,https://arxiv.org/abs/1910.08728,EditUnknown,Image,,,,,,,,"Medical Image Segmentation, Retinal Vessel Segmentation","retinal-vessel-segmentation-on-chase_db1, medical-image-segmentation-on-chase-db1",,See all 1951 tasks,Medical Image Segmentation138 ,Medical Image Segmentation138 
CVC-ClinicDB,CVC-ClinicDB Dataset,"CVC-ClinicDB is an open-access dataset of 612 images with a resolution of 384×288 from 31 colonoscopy sequences.It is used for medical image segmentation, in particular polyp detection in colonoscopy videos.",https://arxiv.org/abs/1911.07067,EditUnknown,Image,,,,,612 images,,,Medical Image Segmentation,medical-image-segmentation-on-cvc-clinicdb,,See all 1951 tasks,Medical Image Segmentation138 ,Medical Image Segmentation138 
DRIVE,DRIVE Dataset,"The Digital Retinal Images for Vessel Extraction (DRIVE) dataset is a dataset for retinal vessel segmentation. It consists of a total of JPEG 40 color fundus images; including 7 abnormal pathology cases. The images were obtained from a diabetic retinopathy screening program in the Netherlands. The images were acquired using Canon CR5 non-mydriatic 3CCD camera with FOV equals to 45 degrees. Each image resolution is 584*565 pixels with eight bits per color channel (3 channels). 

The set of 40 images was equally divided into 20 images for the training set and 20 images for the testing set. Inside both sets, for each image, there is circular field of view (FOV) mask of diameter that is approximately 540 pixels. Inside training set, for each image, one manual segmentation by an ophthalmological expert has been applied. Inside testing set, for each image, two manual segmentations have been applied by two different observers, where the first observer segmentation is accepted as the ground-truth for performance evaluation.",https://arxiv.org/abs/1403.1735,EditCC-BY-4.0,Image,,,,,40 images,training set and 20 images,,"Medical Image Segmentation, Retinal Vessel Segmentation","medical-image-segmentation-on-drive-1, retinal-vessel-segmentation-on-drive",,See all 1951 tasks,Medical Image Segmentation138 ,Medical Image Segmentation138 
GlaS,GlaS Dataset,"The dataset used in this challenge consists of 165 images derived from 16 H&E stained histological sections of stage T3 or T42 colorectal adenocarcinoma. Each section belongs to a different patient, and sections were processed in the laboratory on different occasions. Thus, the dataset exhibits high inter-subject variability in both stain distribution and tissue architecture. The digitization of these histological sections into whole-slide images (WSIs) was accomplished using a Zeiss MIRAX MIDI Slide Scanner with a pixel resolution of 0.465µm.",https://arxiv.org/pdf/1603.00275.pdf,"EditCustom (research-only, non-commercial, attribution)",Image,,,,,165 images,,,Medical Image Segmentation,medical-image-segmentation-on-glas,,See all 1951 tasks,Medical Image Segmentation138 ,Medical Image Segmentation138 
Medical_Segmentation_Decathlon,Medical Segmentation Decathlon Dataset,"The Medical Segmentation Decathlon is a collection of medical image segmentation datasets. It contains a total of 2,633 three-dimensional images collected across multiple anatomies of interest, multiple modalities and multiple sources. Specifically, it contains data for the following body organs or parts: Brain, Heart, Liver, Hippocampus, Prostate, Lung, Pancreas, Hepatic Vessel, Spleen and Colon.",https://arxiv.org/pdf/1902.09063.pdf,EditCC-BY-SA 4.0,Image,,,,,,,,"Medical Image Segmentation, Image Registration, Semantic Segmentation",medical-image-segmentation-on-medical,,See all 1951 tasks,Medical Image Segmentation138 ,Medical Image Segmentation138 
PROMISE12,PROMISE12 Dataset,The PROMISE12 dataset was made available for the MICCAI 2012 prostate segmentation challenge. Magnetic Resonance (MR) images (T2-weighted) of 50 patients with various diseases were acquired at different locations with several MRI vendors and scanning protocols.,https://arxiv.org/abs/1904.04205,EditCustom,Image,,2012,,,,,,"Data Augmentation, Medical Image Segmentation, Volumetric Medical Image Segmentation, Semantic Segmentation","volumetric-medical-image-segmentation-on, medical-image-segmentation-on-promise12",,See all 1951 tasks,Medical Image Segmentation138 ,Medical Image Segmentation138 
CODEBRIM,CODEBRIM Dataset,Dataset for multi-target classification of five commonly appearing concrete defects.,/paper/190408486,EditOther (Non-Commercial),,,,,,,,,"Meta-Learning, Neural Architecture Search",,,See all 1951 tasks,Meta-Learning70 benchmarks1355,Meta-Learning70 benchmarks1355
ExtremeWeather,ExtremeWeather Dataset,Encourages machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change.,https://arxiv.org/pdf/1612.02095v2.pdf,EditCustom,"Image, Time Series",,,,,,,,"Anomaly Detection, Weather Forecasting, Meta-Learning",,,See all 1951 tasks,Meta-Learning70 benchmarks1355,Meta-Learning70 benchmarks1355
Kuzushiji-49,Kuzushiji-49 Dataset,"Kuzushiji-49 is an MNIST-like dataset that has 49 classes (28x28 grayscale, 270,912 images) from 48 Hiragana characters and one Hiragana iteration mark.",/paper/deep-learning-for-classical-japanese,EditUnknown,Image,,,,,912 images,,49,"Image Classification, Meta-Learning",,,See all 1951 tasks,Meta-Learning70 benchmarks1355,Meta-Learning70 benchmarks1355
M4,M4 Dataset,"The M4 dataset is a collection of 100,000 time series used for the fourth edition of the Makridakis forecasting Competition. The M4 dataset consists of time series of yearly, quarterly, monthly and other (weekly, daily and hourly) data, which are divided into training and test sets. The minimum numbers of observations in the training test are 13 for yearly, 16 for quarterly, 42 for monthly, 80 for weekly, 93 for daily and 700 for hourly series. The participants were asked to produce the following numbers of forecasts beyond the available data that they had been given: six for yearly, eight for quarterly, 18 for monthly series, 13 for weekly series and 14 and 48 forecasts respectively for the daily and hourly ones.

The M4 dataset was created by selecting a random sample of 100,000 time series from the ForeDeCk database. The selected series were then scaled to prevent negative observations and values lower than 10, thus avoiding possible problems when calculating various error measures. The scaling was performed by simply adding a constant to the series so that their minimum value was equal to 10 (29 occurrences across the whole dataset). In addition, any information that could possibly lead to the identification of the original series was removed so as to ensure the objectivity of the results. This included the starting dates of the series, which did not become available to the participants until the M4 had ended.",https://doi.org/10.1016/j.ijforecast.2018.06.001,EditUnknown,Time Series,,,,,,,,"Time Series Forecasting, Meta-Learning",,,See all 1951 tasks,Meta-Learning70 benchmarks1355,Meta-Learning70 benchmarks1355
Meta-Dataset,Meta-Dataset Dataset,"The Meta-Dataset benchmark is a large few-shot learning benchmark and consists of multiple datasets of different data distributions. It does not restrict few-shot tasks to have fixed ways and shots, thus representing a more realistic scenario. It consists of 10 datasets from diverse domains: 


ILSVRC-2012 (the ImageNet dataset, consisting of natural images with 1000 categories)
Omniglot (hand-written characters, 1623 classes)
Aircraft (dataset of aircraft images, 100 classes)
CUB-200-2011 (dataset of Birds, 200 classes)
Describable Textures (different kinds of texture images with 43 categories)
Quick Draw (black and white sketches of 345 different categories)
Fungi (a large dataset of mushrooms with 1500 categories)
VGG Flower (dataset of flower images with 102 categories), 
Traffic Signs (German traffic sign images with 43 classes)
MSCOCO (images collected from Flickr, 80 classes). 

All datasets except Traffic signs and MSCOCO have a training, validation and test split (proportioned roughly into 70%, 15%, 15%). The datasets Traffic Signs and MSCOCO are reserved for testing only.",https://arxiv.org/abs/2001.07926,EditMultiple licenses,Image,,2012,,,,,1000,"Few-Shot Learning, Image Classification, Few-Shot Image Classification, Meta-Learning","few-shot-image-classification-on-meta-dataset, few-shot-image-classification-on-meta-dataset-1",,See all 1951 tasks,Meta-Learning70 benchmarks1355,Meta-Learning70 benchmarks1355
Meta-World_Benchmark,Meta-World Benchmark Dataset,An open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks.,/paper/meta-world-a-benchmark-and-evaluation-for,EditUnknown,,,,,,,,,"Meta Reinforcement Learning, Multi-Task Learning, Meta-Learning",,,See all 1951 tasks,Meta-Learning70 benchmarks1355,Meta-Learning70 benchmarks1355
MQ2007,MQ2007 Dataset,"The MQ2007 dataset consists of queries, corresponding retrieved documents and labels provided by human experts. The possible relevance labels for each document are “relevant”, “partially relevant”, and “not relevant”.",https://arxiv.org/abs/1911.00465,EditCustom,,,,,,,,,"Learning-To-Rank, Meta-Learning, Information Retrieval",,,See all 1951 tasks,Meta-Learning70 benchmarks1355,Meta-Learning70 benchmarks1355
Omniglot,Omniglot Dataset,"The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds.",https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.Omniglot,EditMIT,Image,,,,,,,,"Density Estimation, Multi-Task Learning, One-Shot Segmentation, Few-Shot Image Classification, Meta-Learning, Personalized Federated Learning","few-shot-image-classification-on-omniglot-1-2, density-estimation-on-omniglot, one-shot-segmentation-on-cluttered-omniglot, multi-task-learning-on-omniglot, few-shot-image-classification-on-omniglot-2, few-shot-image-classification-on-omniglot-5-1, few-shot-image-classification-on-omniglot-5-4, few-shot-image-classification-on-omniglot-1-1, few-shot-image-classification-on-omniglot-5-2, few-shot-image-classification-on-omniglot, meta-learning-on-omniglot-1-shot-20-way, personalized-federated-learning-on-omniglot, few-shot-image-classification-on-omniglot-1-4, few-shot-image-classification-on-omniglot-1-5, few-shot-image-classification-on-omniglot-5-5",,See all 1951 tasks,Meta-Learning70 benchmarks1355,Meta-Learning70 benchmarks1355
TyDiQA-GoldP,TyDiQA-GoldP Dataset,"TyDiQA is the gold passage version of the Typologically Diverse Question Answering (TyDiWA) dataset, a benchmark for information-seeking question answering, which covers nine languages. The gold passage version is a simplified version of the primary task, which uses only the gold passage as context and excludes unanswerable questions. It is thus similar to XQuAD and MLQA, while being more challenging as questions have been written without seeing the answers, leading to 3× and 2× less lexical overlap compared to XQuAD and MLQA respectively.",https://arxiv.org/pdf/2003.11080.pdf,EditUnknown,Text,English,,,,,,,"Cross-Lingual Question Answering, Cross-Lingual Transfer, Meta-Learning, Question Answering",cross-lingual-question-answering-on-tydiqa,,See all 1951 tasks,Meta-Learning70 benchmarks1355,Meta-Learning70 benchmarks1355
ABO,ABO Dataset,"ABO is a large-scale dataset designed for material prediction and multi-view retrieval experiments. The dataset contains Blender renderings of 30 viewpoints for each of the 7,953 3D objects, as well as camera intrinsics and extrinsic for each rendering.",https://arxiv.org/pdf/2110.06199.pdf,EditUnknown,3D,,,,,,,,"Metric Learning, 3D Reconstruction, Single-View 3D Reconstruction",,,See all 1951 tasks,Metric Learning8 benchmarks600,Metric Learning8 benchmarks600
CARS196,CARS196 Dataset,"CARS196  is composed of 16,185 car images of 196 classes.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,196,"Metric Learning, Image Classification, Image Clustering, Image Retrieval","image-classification-on-cars196, metric-learning-on-cars196, image-clustering-on-cars196, image-retrieval-on-cars196",,See all 1951 tasks,Metric Learning8 benchmarks600,Metric Learning8 benchmarks600
Structured3D,Structured3D Dataset,"Structured3D is a large-scale photo-realistic dataset containing 3.5K house designs (a) created by professional designers with a variety of ground truth 3D structure annotations (b) and generate photo-realistic 2D images (c).
The dataset consists of rendering images and corresponding ground truth annotations (e.g., semantic, albedo, depth, surface normal, layout) under different lighting and furniture configurations.",https://github.com/bertjiazheng/Structured3D,EditCustom,"Image, Text",English,,,,,,,"Indoor Localization, Room Layout Estimation, Metric Learning, Image Generation, Visual Localization, Semantic Segmentation",semantic-segmentation-on-structured3d,,See all 1951 tasks,Metric Learning8 benchmarks600,Metric Learning8 benchmarks600
VeRi-Wild,VeRi-Wild Dataset,"Veri-Wild is the largest vehicle re-identification dataset (as of CVPR 2019). The dataset is captured from a large CCTV surveillance system consisting of 174 cameras across one month (30× 24h) under unconstrained scenarios. This dataset comprises 416,314 vehicle images of 40,671 identities. Evaluation on this dataset is split across three subsets: small, medium and large; comprising 3000, 5000 and 10,000 identities respectively (in probe and gallery sets).",https://arxiv.org/abs/1901.01015,EditUnknown,Image,,2019,,,,"trained scenarios. This dataset comprises 416,314 vehicle images",,"Person Re-Identification, Metric Learning, Vehicle Re-Identification","vehicle-re-identification-on-veri-wild-large, vehicle-re-identification-on-veri-wild-small, vehicle-re-identification-on-veri-wild-medium",,See all 1951 tasks,Metric Learning8 benchmarks600,Metric Learning8 benchmarks600
Missing_Labels48_papers_with_code_Dataset,Missing Labels48 papers with code Dataset,,https://paperswithcode.com/dataset/missing-labels,,,,,,,,,,,,,See all 1951 tasks,Missing Labels48 papers with c,Missing Labels48 papers with c
3DOH50K,3DOH50K Dataset,"3DOH50K is the first real 3D human dataset for the problem of human reconstruction and pose estimation in occlusion scenarios. It contains 51600 images with accurate 2D pose and 3D pose, SMPL parameters, and binary mask.",https://production-media.paperswithcode.com/datasets/196f6607-ecab-4086-97ed-bbc57bf3e82b.png,EditUnknown,"3D, Image",,,,,51600 images,,,"Monocular 3D Human Pose Estimation, 3D Human Pose Estimation, 3D human pose and shape estimation",3d-human-pose-estimation-on-3doh50k,,See all 1951 tasks,Monocular 3D Human Pose Estima,Monocular 3D Human Pose Estima
H3WB,H3WB Dataset,"Human3.6M 3D WholeBody (H3WB) is a large scale dataset with 133 whole-body keypoint annotations on 100K images, made possible by a new multi-view pipeline. It is designed for the three new tasks : i) 3D whole-body pose lifting from 2D complete whole-body pose, ii) 3D whole-body pose lifting from 2D incomplete whole-body pose, iii) 3D whole-body pose estimation from a single RGB image.",https://arxiv.org/pdf/2211.15692v1.pdf,EditMIT License,"3D, Image",,,,,100K images,,,"3D Facial Landmark Localization, Pose Estimation, 3D Human Pose Estimation, Monocular 3D Human Pose Estimation, 3D Hand Pose Estimation","3d-human-pose-estimation-on-h3wb, 3d-facial-landmark-localization-on-h3wb, 3d-hand-pose-estimation-on-h3wb",,See all 1951 tasks,Monocular 3D Human Pose Estima,Monocular 3D Human Pose Estima
HBW,HBW Dataset,"Human Bodies in the Wild (HBW) is a validation and test set for body shape estimation. It consists of images taken in the wild and ground truth 3D body scans in SMPL-X topology. To create HBW, we collect body scans of 35 participants and register the SMPL-X model to the scans. Further each participant is photographed in various outfits and poses in front of a white background and uploads full-body photos of themselves taken in the wild. The validation and test set images are released. The ground truth shape is only released for the validation set.",https://production-media.paperswithcode.com/datasets/0ad34edc-672c-491d-b286-c2d9a7e714d9.jpg,EditUnknown,"3D, Image",,,,,,"validation and test set for body shape estimation. It consists of images taken in the wild and ground truth 3D body scans in SMPL-X topology. To create HBW, we collect body scans of 35 participants and register the SMPL-X model to the scans. Further each participant is photographed in various outfits and poses in front of a white background and uploads full-body photos of themselves taken in the wild. The validation and test set images",,"3D Human Shape Estimation, Monocular 3D Human Pose Estimation, 3D Human Reconstruction",,,See all 1951 tasks,Monocular 3D Human Pose Estima,Monocular 3D Human Pose Estima
Mono3DRefer,Mono3DRefer Dataset,"We sample 2025 frames of images from the original KITTI for Mono3DRefer, containing 41,140 expressions in total and a vocabulary of 5,271 words.",https://production-media.paperswithcode.com/datasets/01db8664-9aab-4b54-a1a7-72d829bebc19.png,EditUnknown,"3D, Image",,2025,,,,,,"Mono3DVG, Monocular 3D Object Detection, Visual Grounding, 3D visual grounding, 3D Object Detection",mono3dvg-on-mono3drefer,,See all 1951 tasks,Monocular 3D Object Detection1,Monocular 3D Object Detection1
OPV2V,OPV2V Dataset,"OPV2V is a large-scale open simulated dataset for Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns in CARLA and a digital town of Culver City, Los Angeles.",https://production-media.paperswithcode.com/datasets/7a5cd65a-c46c-4e25-bee4-f20a4ea7527d.jpg,EditUnknown,"3D, Image",,,,,,,,"Monocular 3D Object Detection, 3D Object Detection","monocular-3d-object-detection-on-opv2v, 3d-object-detection-on-opv2v",,See all 1951 tasks,Monocular 3D Object Detection1,Monocular 3D Object Detection1
DIODE,DIODE Dataset,"Diode Dense Indoor/Outdoor DEpth (DIODE) is the first standard dataset for monocular depth estimation comprising diverse indoor and outdoor scenes acquired with the same hardware setup. The training set consists of 8574 indoor and 16884 outdoor samples from 20 scans each. The validation set contains 325 indoor and 446 outdoor samples with each set from 10 different scans. The ground truth density for the indoor training and validation splits are approximately 99.54% and 99%, respectively. The density of the outdoor sets are naturally lower with 67.19% for training and 78.33% for validation subsets. The indoor and outdoor ranges for the dataset are 50m and 300m, respectively.",https://arxiv.org/abs/2009.00743,EditMIT License,3D,,,,,,training set consists of 8574 indoor and 16884 outdoor samples,,"Depth Estimation, Monocular Depth Estimation, Indoor Monocular Depth Estimation","depth-estimation-on-diode, indoor-monocular-depth-estimation-on-diode",,See all 1951 tasks,Monocular Depth Estimation24 b,Monocular Depth Estimation24 b
Make3D,Make3D Dataset,"The Make3D dataset is a monocular Depth Estimation dataset that contains 400 single training RGB and depth map pairs, and 134 test samples. The RGB images have high resolution, while the depth maps are provided at low resolution.",https://arxiv.org/abs/1908.05794,EditCC BY-NC 3.0,"3D, Image",,,,,,"training RGB and depth map pairs, and 134 test samples",,"Monocular Depth Estimation, Depth Estimation, Semantic Segmentation",monocular-depth-estimation-on-make3d,,See all 1951 tasks,Monocular Depth Estimation24 b,Monocular Depth Estimation24 b
fastMRI,fastMRI Dataset,"The fastMRI dataset includes two types of MRI scans: knee MRIs and the brain (neuro) MRIs, and containing training, validation, and masked test sets.
The deidentified imaging dataset provided by NYU Langone comprises raw k-space data in several sub-dataset groups. Curation of these data are part of an IRB approved study. Raw and DICOM data have been deidentified via conversion to the vendor-neutral ISMRMD format and the RSNA clinical trial processor, respectively. Also, each DICOM image is manually inspected for the presence of any unexpected protected health information (PHI), with spot checking of both metadata and image content.
Knee MRI: Data from more than 1,500 fully sampled knee MRIs obtained on 3 and 1.5 Tesla magnets and DICOM images from 10,000 clinical knee MRIs also obtained at 3 or 1.5 Tesla. The raw dataset includes coronal proton density-weighted images with and without fat suppression. The DICOM dataset contains coronal proton density-weighted with and without fat suppression, axial proton density-weighted with fat suppression, sagittal proton density, and sagittal T2-weighted with fat suppression.
Brain MRI: Data from 6,970 fully sampled brain MRIs obtained on 3 and 1.5 Tesla magnets. The raw dataset includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent.",https://fastmri.med.nyu.edu/,EditCustom (internal research-only),3D,,,,,,,,MRI Reconstruction,"mri-reconstruction-on-fastmri-knee-4x, mri-reconstruction-on-fastmri-brain-4x, mri-reconstruction-on-fastmri-knee-8x, mri-reconstruction-on-fastmri-brain-8x, mri-reconstruction-on-fastmri-knee-val-8x",,See all 1951 tasks,MRI Reconstruction6 benchmarks,MRI Reconstruction6 benchmarks
IXI,IXI Dataset,"IXI Dataset is a collection of 600 MR brain images from normal, healthy subjects. The MR image acquisition protocol for each subject includes:


T1, T2 and PD-weighted images
MRA images
Diffusion-weighted images (15 directions)

The data has been collected at three different hospitals in London:


Hammersmith Hospital using a Philips 3T system (details of scanner parameters)
Guy’s Hospital using a Philips 1.5T system (details of scanner parameters)
Institute of Psychiatry using a GE 1.5T system (details of the scan parameters not available at the moment)

The data has been collected as part of the project:


IXI – Information eXtraction from Images (EPSRC GR/S21533/02)

The images in NIFTI format can be downloaded from here:

This data is made available under the Creative Commons CC BY-SA 3.0 license. If you use the IXI data please acknowledge the source of the IXI data.",https://production-media.paperswithcode.com/datasets/IXI263-HH-1684-T1_70.png,EditUnknown,"3D, Image, Text",English,,,,,,,"Image Super-Resolution, Image-to-Image Translation, MRI Reconstruction, Medical Image Registration","image-to-image-translation-on-ixi-dataset, mri-reconstruction-on-ixi-dataset, image-super-resolution-on-ixi, medical-image-registration-on-ixi",,See all 1951 tasks,MRI Reconstruction6 benchmarks,MRI Reconstruction6 benchmarks
M4Raw,M4Raw Dataset,"Recently, low-field magnetic resonance imaging (MRI) has gained renewed interest to promote MRI accessibility and affordability worldwide. The presented M4Raw dataset aims to facilitate methodology development and reproducible research in this field. The dataset comprises multi-channel brain k-space data collected from 183 healthy volunteers using a 0.3 Tesla whole-body MRI system, and includes T1-weighted, T2-weighted, and fluid attenuated inversion recovery (FLAIR) images with in-plane resolution of ~1.2 mm and through-plane resolution of 5 mm. Importantly, each contrast contains multiple repetitions, which can be used individually or to form multi-repetition averaged images. After excluding motion-corrupted data, the partitioned training and validation subsets contain 1024 and 240 volumes, respectively. To demonstrate the potential utility of this dataset, we trained deep learning models for image denoising and parallel imaging tasks and compared their performance with traditional reconstruction methods. This M4Raw dataset will be valuable for the development of advanced data-driven methods specifically for low-field MRI. It can also serve as a benchmark dataset for general MRI reconstruction algorithms.

Reference:
Lyu, M., Mei, L., Huang, S. et al. M4Raw: A multi-contrast, multi-repetition, multi-channel MRI k-space dataset for low-field MRI research. Sci Data 10, 264 (2023). https://doi.org/10.1038/s41597-023-02181-4",https://production-media.paperswithcode.com/datasets/fdccd808-7b6c-4340-9055-40225a3c9454.webp,EditCC-BY-4.0 license,"3D, Image",,2023,,,,,,"Image Denoising, MRI Reconstruction",,,See all 1951 tasks,MRI Reconstruction6 benchmarks,MRI Reconstruction6 benchmarks
SKM-TEA,SKM-TEA Dataset,"The SKM-TEA dataset pairs raw quantitative knee MRI (qMRI) data, image data, and dense labels of tissues and pathology for end-to-end exploration and evaluation of the MR imaging pipeline.  This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient knee MRI scans, the corresponding scanner-generated DICOM images, manual segmentations of four tissues, and bounding box annotations for sixteen clinically relevant pathologies.

Challenge Tracks
DICOM Track: The DICOM benchmarking track uses scanner-generated DICOM images as the input for image segmentation and detection tasks.

Raw Data Track: The Raw Data benchmarking track uses raw MRI data (i.e. k-space) as the input for image reconstruction, segmentation and detection tasks.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditStanford University Dataset Research Use Agreement,"3D, Image",,,,,,"valuation of the MR imaging pipeline.  This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient knee MRI scans, the corresponding scanner-generated DICOM images",,"Medical Object Detection, 3D Medical Imaging Segmentation, MRI segmentation, MRI Reconstruction",,,See all 1951 tasks,MRI Reconstruction6 benchmarks,MRI Reconstruction6 benchmarks
BurstSR,BurstSR Dataset,BurstSR is a dataset consisting of smartphone bursts and high-resolution DSLR ground-truth,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Burst Image Super-Resolution, Multi-Frame Super-Resolution",burst-image-super-resolution-on-burstsr,,See all 1951 tasks,Multi-Frame Super-Resolution1 ,Multi-Frame Super-Resolution1 
ChestX-ray14,ChestX-ray14 Dataset,"ChestX-ray14 is a medical imaging dataset which comprises 112,120 frontal-view X-ray images of 30,805 (collected from the year of 1992 to 2015) unique patients with the text-mined fourteen common disease labels, mined from the text radiological reports via NLP techniques. It expands on ChestX-ray8 by adding six additional thorax diseases: Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia.",https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610,EditUnknown,"Image, Text",English,1992,,,,,,"Multi-Task Learning, Pneumonia Detection, Multi-Label Classification, Thoracic Disease Classification, Medical Image Generation","medical-image-generation-on-chestxray14, thoracic-disease-classification-on-chestx, medical-image-generation-on-chestx-ray14, multi-label-classification-on-chestx-ray14, pneumonia-detection-on-chestx-ray14, multi-task-learning-on-chestx-ray14",,See all 1951 tasks,Multi-Label Classification37 b,Multi-Label Classification37 b
CheXpert,CheXpert Dataset,"The CheXpert dataset contains 224,316 chest radiographs of 65,240 patients with both frontal and lateral views available. The task is to do automated chest x-ray interpretation, featuring uncertainty labels and radiologist-labeled reference standard evaluation sets.",https://arxiv.org/abs/2006.03796,EditCustom,Image,,,,,,,,"Multi-Label Classification, Concept-based Classification","multi-label-classification-on-chexpert, concept-based-classification-on-chexpert",,See all 1951 tasks,Multi-Label Classification37 b,Multi-Label Classification37 b
ECHR,ECHR Dataset,"ECHR is an English legal judgment prediction dataset of cases from the European Court of Human Rights (ECHR). The dataset contains ~11.5k cases, including the raw text.

For each case, the dataset provides a list of facts extracted using regular expressions from the case description. Each case is also mapped to articles of the Convention that were violated (if any). An importance score is also assigned by ECHR.",https://arxiv.org/abs/1906.02059,EditUnknown,"Image, Text",English,,,,,,,"Multi-Label Classification, Binary text classification",binary-text-classification-on-echr-non,,See all 1951 tasks,Multi-Label Classification37 b,Multi-Label Classification37 b
Friedman1,Friedman1 Dataset,The friedman1 data set is commonly used to test semi-supervised regression methods.,http://search.r-project.org/library/ssr/html/friedman1.html,EditCustom,"Image, Time Series",,,,,,,,"Prediction Intervals, Multi-Label Classification",,,See all 1951 tasks,Multi-Label Classification37 b,Multi-Label Classification37 b
MIMIC-CXR,MIMIC-CXR Dataset,"MIMIC-CXR from Massachusetts Institute of Technology presents 371,920 chest X-rays associated with 227,943 imaging studies from 65,079 patients. The studies were performed at Beth Israel Deaconess Medical Center in Boston, MA.",https://arxiv.org/abs/1909.01940,EditUnknown,"Image, Text",English,,,,,,,"Multi-Label Classification, Medical Report Generation","medical-report-generation-on-mimic-cxr, multi-label-classification-on-mimic-cxr",,See all 1951 tasks,Multi-Label Classification37 b,Multi-Label Classification37 b
MRNet,MRNet Dataset,"The MRNet dataset consists of 1,370 knee MRI exams performed at Stanford University Medical Center. The dataset contains 1,104 (80.6%) abnormal exams, with 319 (23.3%) ACL tears and 508 (37.1%) meniscal tears; labels were obtained through manual extraction from clinical reports.",https://stanfordmlgroup.github.io/competitions/mrnet/,EditUnknown,Image,,,,,,,,"Multi-Label Classification, Data Augmentation",multi-label-classification-on-mrnet,,See all 1951 tasks,Multi-Label Classification37 b,Multi-Label Classification37 b
VizWiz-Classification,VizWiz-Classification Dataset,"Our goal is to improve upon the status quo for designing image classification models trained in one domain that perform well on images from another domain. Complementing existing work in robustness testing, we introduce the first test dataset for this purpose which comes from an authentic use case where photographers wanted to learn about the content in their images. We built a new test set using 8,900 images taken by people who are blind for which we collected metadata to indicate the presence versus absence of 200 ImageNet object categories. We call this dataset VizWiz-Classification.",https://production-media.paperswithcode.com/datasets/0bba040e-0767-4b70-8d35-a3c5108c7735.png,EditUnknown,Image,,,,,900 images,"trained in one domain that perform well on images from another domain. Complementing existing work in robustness testing, we introduce the first test dataset for this purpose which comes from an authentic use case where photographers wanted to learn about the content in their images",,"Multi-Label Classification, Image Classification, Domain Generalization, Multi-Label Image Classification","domain-generalization-on-vizwiz, image-classification-on-vizwiz-classification, multi-label-image-classification-on-vizwiz",,See all 1951 tasks,Multi-Label Classification37 b,Multi-Label Classification37 b
NUS-WIDE,NUS-WIDE Dataset,"The NUS-WIDE dataset contains 269,648 images with a total of 5,018 tags collected from Flickr. These images are manually annotated with 81 concepts, including objects and scenes.",https://arxiv.org/abs/1803.11370,EditUnknown,Image,,,,,648 images,,,"Cross-Modal Retrieval, Multiview Clustering, Image Retrieval, Multi-label zero-shot learning, Multi-Label Classification","multi-label-classification-on-nus-wide, multi-label-zero-shot-learning-on-nus-wide, image-retrieval-on-nus-wide, multiview-clustering-on-nus-wide",,See all 1951 tasks,Multi-label zero-shot learning,Multi-label zero-shot learning
Open_Images_V4,Open Images V4 Dataset,"Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images) are provided. The images often show complex scenes with several objects (8 annotated objects per image on average). Visual relationships between them are annotated, which support visual relationship detection, an emerging task that requires structured reasoning.",https://arxiv.org/pdf/1811.00982,EditCustom,Image,,,,,9M images,,57,"Object Detection, Image Classification, Semantic Segmentation, Multi-label zero-shot learning",multi-label-zero-shot-learning-on-open-images,,See all 1951 tasks,Multi-label zero-shot learning,Multi-label zero-shot learning
Multi-Objective_Reinforcement_Learning49_papers_wi,Multi-Objective Reinforcement Learning49 papers with code Dataset,,https://paperswithcode.com/dataset/multi-objective-reinforcement-learning,,,,,,,,,,,,,See all 1951 tasks,Multi-Objective Reinforcement ,Multi-Objective Reinforcement 
DanceTrack,DanceTrack Dataset,"A large-scale multi-object tracking dataset for human tracking in occlusion, frequent crossover, uniform appearance and diverse body gestures. It is proposed to emphasize the importance of motion analysis in multi-object tracking instead of mainly appearance-matching-based diagram.",https://production-media.paperswithcode.com/datasets/90700c9b-da1d-4aa2-a645-d008544ab8d4.jpg,EditMIT,"Image, Video",,,,,,,,Multi-Object Tracking,multi-object-tracking-on-dancetrack,,See all 1951 tasks,Multi-Object Tracking28 benchm,Multi-Object Tracking28 benchm
MOT15,MOT15 Dataset,"MOT2015 is a dataset for multiple object tracking. It contains 11 different indoor and outdoor scenes of public places with pedestrians as the objects of interest, where camera motion, camera angle and imaging condition vary greatly. The dataset provides detections generated by the ACF-based detector.",https://arxiv.org/abs/1904.04989,EditCC BY-NC-SA 3.0,"Image, Video",,,,,,,,"Multi-Object Tracking, Online Multi-Object Tracking","multi-object-tracking-on-2dmot15-1, online-multi-object-tracking-on-2d-mot-2015, multi-object-tracking-on-mot15, online-multi-object-tracking-on-mot15, multi-object-tracking-on-2d-mot-2015",,See all 1951 tasks,Multi-Object Tracking28 benchm,Multi-Object Tracking28 benchm
MOT16,MOT16 Dataset,"The MOT16 dataset is a dataset for multiple object tracking. It a collection of existing and new data (part of the sources are from and ), containing 14 challenging real-world videos of both static scenes and moving scenes, 7 for training and 7 for testing. It is a large-scale dataset, composed of totally 110407 bounding boxes in training set and 182326 bounding boxes in test set. All video sequences are annotated under strict standards, their ground-truths are highly accurate, making the evaluation meaningful.",https://arxiv.org/abs/1712.01059,EditCC BY-NC-SA 3.0,"Image, Video",,,,,,,,"Multi-Object Tracking, Online Multi-Object Tracking","online-multi-object-tracking-on-mot16, multi-object-tracking-on-mot16",,See all 1951 tasks,Multi-Object Tracking28 benchm,Multi-Object Tracking28 benchm
MOT17,MOT17 Dataset,"The Multiple Object Tracking 17 (MOT17) dataset is a dataset for multiple object tracking. Similar to its previous version MOT16, this challenge contains seven different indoor and outdoor scenes of public places with pedestrians as the objects of interest. A video for each scene is divided into two clips, one for training and the other for testing. The dataset provides detections of objects in the video frames with three detectors, namely SDP, Faster-RCNN and DPM. The challenge accepts both on-line and off-line tracking approaches, where the latter are allowed to use the future video frames to predict tracks.",https://arxiv.org/abs/1810.11780,EditCC BY-NC-SA 3.0,"Image, Video",,,,,,,,"Multi-Object Tracking, Online Multi-Object Tracking","online-multi-object-tracking-on-mot17, multi-object-tracking-on-mot17",,See all 1951 tasks,Multi-Object Tracking28 benchm,Multi-Object Tracking28 benchm
Wildtrack,Wildtrack Dataset,"Wildtrack is a large-scale and high-resolution dataset. It has been captured with seven static cameras in a public open area, and unscripted dense groups of pedestrians standing and walking. Together with the camera frames, we provide an accurate joint (extrinsic and intrinsic) calibration, as well as 7 series of 400 annotated
frames for detection at a rate of 2 frames per second. This results in over 40 000 bounding boxes delimiting every person present in the area of interest, for a total of more than
300 individuals.",https://production-media.paperswithcode.com/datasets/wildtrack.png,EditUnknown,"Image, Video",,,,,,,,"Multi-Object Tracking, Multiview Detection","multiview-detection-on-wildtrack, multi-object-tracking-on-wildtrack",,See all 1951 tasks,Multi-Object Tracking28 benchm,Multi-Object Tracking28 benchm
AESI,AESI Dataset,"The development of ecologically valid procedures for collecting reliable and unbiased emotional data towards computer interfaces with social and affective intelligence targeting patients with mental disorders. Following its development, presented with, the Athens Emotional States Inventory (AESI) proposes the design, recording and validation of an audiovisual database for five emotional states: anger, fear, joy, sadness and neutral. The items of the AESI consist of sentences each having content indicative of the corresponding emotion. Emotional content was assessed through a survey of 40 young participants with a questionnaire following the Latin square design. The emotional sentences that were correctly identified by 85% of the participants were recorded in a soundproof room with microphones and cameras. A preliminary validation of AESI is performed through automatic emotion recognition experiments from speech. The resulting database contains 696 recorded utterances in Greek language by 20 native speakers and has a total duration of approximately 28 min. Speech classification results yield accuracy up to 75.15% for automatically recognizing the emotions in AESI. These results indicate the usefulness of our approach for collecting emotional data with reliable content, balanced across classes and with reduced environmental variability.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Image",,,,,,,,"Speech Emotion Recognition, Multimodal Emotion Recognition",,,See all 1951 tasks,Multimodal Emotion Recognition,Multimodal Emotion Recognition
DEAP,DEAP Dataset,"The DEAP dataset consists of two parts:


The ratings from an online self-assessment where 120 one-minute extracts of music videos were each rated by 14-16 volunteers based on arousal, valence and dominance.
The participant ratings, physiological recordings and face video of an experiment where 32 volunteers watched a subset of 40 of the above music videos. EEG and physiological signals were recorded and each participant also rated the videos as above. For 22 participants frontal face video was also recorded.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Emotion Recognition, Emotion Classification, Multimodal Emotion Recognition, EEG Emotion Recognition",eeg-emotion-recognition-on-deap,,See all 1951 tasks,Multimodal Emotion Recognition,Multimodal Emotion Recognition
EMOTIC,EMOTIC Dataset,"The EMOTIC dataset, named after EMOTions In Context, is a database of images with people in real environments, annotated with their apparent emotions. The images are annotated with an extended list of 26 emotion categories combined with the three common continuous dimensions Valence, Arousal and Dominance.",https://arxiv.org/abs/2003.13401,EditUnknown,"Image, Video",,,,,,,,"Valence Estimation, Emotion Recognition, Dominance Estimation, Emotion Recognition in Context, Arousal Estimation, Action Recognition, Multimodal Emotion Recognition, Age Classification","emotion-recognition-in-context-on-emotic, arousal-estimation-on-emotic, emotion-recognition-on-emotic, valence-estimation-on-emotic, dominance-estimation-on-emotic, age-classification-on-emotic",,See all 1951 tasks,Multimodal Emotion Recognition,Multimodal Emotion Recognition
RESD,RESD Dataset,"Russian dataset of emotional speech dialogues. This dataset was assembled from ~3.5 hours of live speech by actors who voiced pre-distributed emotions in the dialogue for ~3 minutes each. <br>
Each sample of dataset contains name of part from the original dataset studio source, speech file (16000 or 44100Hz) of human voice, 1 of 7 labeled emotions and the speech-to-texted part of voice speech. <br>

Emotions are represented in 7 states: anger, disgust, fear, enthusiasm, happiness, neutral and sadness.

This dataset was created by Artem Amentes, Nikita Davidchuk and Ilya Lubenets

@misc{Aniemore,
  author = {Артем Аментес, Илья Лубенец, Никита Давидчук},
  title = {Открытая библиотека искусственного интеллекта для анализа и выявления эмоциональных оттенков речи человека},
  year = {2022},
  publisher = {Hugging Face},
  journal = {Hugging Face Hub},
  howpublished = {\url{https://huggingface.com/aniemore/Aniemore}},
  email = {hello@socialcode.ru}
}",https://production-media.paperswithcode.com/datasets/bf78eec4-81d8-48eb-8645-e250824e404a.png,EditMIT,"Audio, Image, Text",English,2022,,,,,,"Multimodal Emotion Recognition, Text Classification, Speech Emotion Recognition, Speech Recognition",speech-emotion-recognition-on-resd,,See all 1951 tasks,Multimodal Emotion Recognition,Multimodal Emotion Recognition
SES,SES Dataset,"Currently, an essential point in speech synthesis is the addressing of the variability of human speech. One of the main sources of this diversity is the emotional state of the speaker. Most of the recent work in this area has been focused on the prosodic aspects of speech and on rule-based formant synthesis experiments. Even when adopting an improved voice source, we cannot achieve a smiling happy voice or the menacing quality of cold anger. For this reason, we have performed two experiments aimed at developing a concatenative emotional synthesiser, a synthesiser that can copy the quality of an emotional voice without an explicit mathematical model.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Image",,,,,,,,"Speech Emotion Recognition, Multimodal Emotion Recognition",,,See all 1951 tasks,Multimodal Emotion Recognition,Multimodal Emotion Recognition
B-T4SA,B-T4SA Dataset,,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Multimodal Sentiment Analysis,multimodal-sentiment-analysis-on-b-t4sa,,See all 1951 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
CH-SIMS,CH-SIMS Dataset,"CH-SIMS is a Chinese single- and multimodal sentiment analysis dataset which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.",https://www.aclweb.org/anthology/2020.acl-main.343.pdf,EditUnknown,Text,English,,,,,,,"Multi-Task Learning, Multimodal Sentiment Analysis, Sentiment Analysis",multimodal-sentiment-analysis-on-ch-sims,,See all 1951 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
CMU-MOSEI,CMU-MOSEI Dataset,CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) is the largest dataset of sentence-level sentiment analysis and emotion recognition in online videos. CMU-MOSEI contains over 12 hours of annotated video from over 1000 speakers and 250 topics.,https://production-media.paperswithcode.com/datasets/CMU-MOSEI-0000001483-1f1c1e18_qDoVx1a.jpg,EditCustom,"Image, Text, Video",English,,,,,,,"Video Emotion Detection, Emotion Classification, Multimodal Emotion Recognition, Facial Expression Recognition, Multimodal Sentiment Analysis","facial-expression-recognition-on-cmu-mosei, multimodal-sentiment-analysis-on-cmu-mosei-1, emotion-classification-on-cmu-mosei",,See all 1951 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
CMU-MOSI,CMU-MOSI Dataset,"The Multimodal Corpus of Sentiment Intensity (CMU-MOSI) dataset is a collection of 2199 opinion video clips. Each opinion video is annotated with sentiment in the range [-3,3]. The dataset is rigorously annotated with labels for subjectivity, sentiment intensity, per-frame and per-opinion annotated visual features, and per-milliseconds annotated audio features.",https://production-media.paperswithcode.com/datasets/bccc5f4c-f9a9-4efa-addf-07e96f4357d8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Emotion Recognition in Conversation, Multimodal Sentiment Analysis","emotion-recognition-in-conversation-on-cmu, multimodal-sentiment-analysis-on-cmu-mosi",,See all 1951 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
Multimodal_Opinionlevel_Sentiment_Intensity,Multimodal Opinionlevel Sentiment Intensity Dataset,"Multimodal Opinionlevel Sentiment Intensity (MOSI) contains: (1) multimodal observations including transcribed speech and visual gestures as well as automatic audio and visual features, (2) opinion-level subjectivity segmentation, (3) sentiment intensity annotations with high coder agreement, and (4) alignment between words, visual and acoustic features.",https://arxiv.org/pdf/1606.06259.pdf,EditUnknown,Text,English,,,,,,,Multimodal Sentiment Analysis,multimodal-sentiment-analysis-on-mosi,,See all 1951 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
MuSe-CaR,MuSe-CaR Dataset,"The MuSe-CAR database is a large, multimodal (video, audio, and text) dataset which has been gathered in-the-wild with the intention of further understanding Multimodal Sentiment Analysis in-the-wild, e.g., the emotional engagement that takes place during product reviews (i.e., automobile reviews) where a sentiment is linked to a topic or entity. 

The estimated age range of the professional, semi-professional (influncers), and casual reviewers is between the middle of 20s until the late 50s. Most are native English speakers from the UK or the US, while a small minority are non-native, yet fluent English speakers.",https://production-media.paperswithcode.com/datasets/musecar.png,EditUnknown,Text,English,,,,,,,Multimodal Sentiment Analysis,,,See all 1951 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
SemEval-2020_Task-8,SemEval-2020 Task-8 Dataset,A multimodal dataset for sentiment analysis on internet memes.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Multimodal Sentiment Analysis, Sentiment Analysis",,,See all 1951 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
GMOT-40,GMOT-40 Dataset,"GMOT-40 is the first public dense dataset for Generic Multiple Object Tracking (GMOT). It contains 40 carefully annotated sequences evenly distributed among 10 object categories. Beyond the data, a challenging protocal, one-shot GMOT, is adopted and a series of baseline algorithms is introduced. GMOT-40 is featured in

Dense objects. Over 80 objects of the same class could appear in one frame.
High-quality label. Manual annotation with careful inspection in each frame.
Diversity in target. Large variability between classes and within the sequences of the same class.
Real world challenges. Occlusion, target enter/exiting, motion blur, deformation and so on.
Challenging protocol. One-shot GMOT protocol is adopted for evaluation.
New baselines. A series of baseline methods for one-shot GMOT task is introduced.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,,,,"Object Detection, Multi-Object Tracking, Multiple Object Tracking","object-detection-on-gmot-40, multiple-object-tracking-on-gmot-40",,See all 1951 tasks,Multiple Object Tracking12 ben,Multiple Object Tracking12 ben
HiEve,HiEve Dataset,"A new large-scale dataset for understanding human motions, poses, and actions in a variety of realistic events, especially crowd & complex events. It contains a record number of poses (>1M), the largest number of action labels (>56k) for complex events, and one of the largest number of trajectories lasting for long terms (with average trajectory length >480). Besides, an online evaluation server is built for researchers to evaluate their approaches.",/paper/human-in-events-a-large-scale-benchmark-for,EditUnknown,"Image, Time Series, Video",,,,,,,,"Temporal Action Localization, Object Tracking, Multi-Object Tracking, Multiple Object Tracking",multi-object-tracking-on-hieve,,See all 1951 tasks,Multiple Object Tracking12 ben,Multiple Object Tracking12 ben
SportsMOT,SportsMOT Dataset,"Motivation
Multi-object tracking (MOT) is a fundamental task in computer vision, aiming to estimate objects (e.g., pedestrians and vehicles) bounding boxes and identities in video sequences.

Prevailing human-tracking MOT datasets mainly focus on pedestrians in crowded street scenes (e.g., MOT17/20) or dancers in static scenes (DanceTrack). 

In spite of the increasing demands for sports analysis, there is a lack of multi-object tracking datasets for a variety of sports scenes, where the background is complicated, players possess rapid motion and the camera lens moves fast.

To this purpose, we propose a large-scale multi-object tracking dataset named SportsMOT, consisting of 240 video clips from 3 categories (i.e., basketball, football and volleyball). 

The objective is to only track players on the playground (i.e., except for a number of spectators, referees and coaches) in various sports scenes. We expect SportsMOT to encourage the community to concentrate more on the complicated sports scenes.

Characteristics

Large scale
Fine Annotations
Player id consistency
No shot change
High and fixed resolution(1080P)
...

Focus

Diverse sports scenes
Complex motion patterns
Challenging re-id

Download
Examples
You can download the example for SportsMOT.


OneDrive
Baidu Netdisk, password: 4dnw

Official Dataset
Please Sign up in codalab, and participate in our competition. Download links are available in  Participate/Get Data.

News


SportsMOT is used for  DeeperAction@ECCV-2022. 



Refer to github repo: MCG-NJU/SportsMOT for the latest info.",https://production-media.paperswithcode.com/datasets/66dd9f8e-6af9-42d3-8413-447fd0d65e27.gif,EditCC BY-NC 4.0,"Image, Video",,2022,,,,,3,"Multi-Object Tracking, Multiple Object Tracking","multiple-object-tracking-on-sportsmot, multi-object-tracking-on-sportsmot",,See all 1951 tasks,Multiple Object Tracking12 ben,Multiple Object Tracking12 ben
CityStreet,CityStreet Dataset,"Datasets for multi-view crowd counting in wide-area scenes. Includes our CityStreet dataset, as well as the counting and metadata for multi-view counting on PETS2009 and DukeMTMC.
CityStreet is a real-world city scene dataset collected around the intersection of a crowded street. The scene size of the dataset is around 58m×72m. The ground plane map resolution is 320×384.",https://production-media.paperswithcode.com/datasets/eb9727d3-8c14-4054-8008-74dc62139167.jpg,EditUnknown,Image,,,,,,,,"Crowd Counting, Multiview Detection",multiview-detection-on-citystreet,,See all 1951 tasks,Multiview Detection5 benchmark,Multiview Detection5 benchmark
CVCS,CVCS Dataset,"CVCS is a synthetic multi-view people dataset, containing 31 scenes, where 23 are for training and the rest 8 for testing. The scene size varies from about 10m∗20m to 90m∗80m. Each scene contains 100 multi-view frames. The ground plane map resolution is 900×800, where each grid stands for 0.1 meters in the real world. In training, 5 views are randomly selected 5 times in each iteration per scene frame, and the same view number is randomly selected 21 times in evaluation.",https://production-media.paperswithcode.com/datasets/1dec0a6c-4ba2-4138-9ad2-6c00b02e56eb.jpg,EditUnknown,Image,,,,,,,,"Crowd Counting, Multiview Detection",multiview-detection-on-cvcs,,See all 1951 tasks,Multiview Detection5 benchmark,Multiview Detection5 benchmark
DOLPHINS,DOLPHINS Dataset,"Vehicle-to-Everything (V2X) network has enabled collaborative perception in autonomous driving, which is a promising solution to the fundamental defect of stand-alone intelligence including blind zones and long-range perception. However, the lack of datasets has severely blocked the development of collaborative perception algorithms. In this work, we release DOLPHINS: Dataset for cOllaborative Perception enabled Harmonious and INterconnected Self-driving, as a new simulated large-scale various-scenario multi-view multi-modality autonomous driving dataset, which provides a ground-breaking benchmark platform for interconnected autonomous driving. DOLPHINS outperforms current datasets in six dimensions: temporally-aligned images and point clouds from both vehicles and Road Side Units (RSUs) enabling both Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) based collaborative perception; 6 typical scenarios with dynamic weather conditions make the most various interconnected autonomous driving dataset; meticulously selected viewpoints providing full coverage of the key areas and every object; 42376 frames and 292549 objects, as well as the corresponding 3D annotations, geo-positions, and calibrations, compose the largest dataset for collaborative perception; Full-HD images and 64-line LiDARs construct high-resolution data with sufficient details; well-organized APIs and open-source codes ensure the extensibility of DOLPHINS. We also construct a benchmark of 2D detection, 3D detection, and multi-view collaborative perception tasks on DOLPHINS. The experiment results show that the raw-level fusion scheme through V2X communication can help to improve the precision as well as to reduce the necessity of expensive LiDAR equipment on vehicles when RSUs exist, which may accelerate the popularity of interconnected self-driving vehicles.",https://production-media.paperswithcode.com/datasets/5ea6da6a-0ef9-42f2-9fcd-e1cb3a5cb8df.png,EditCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,"3D, Image, Video",,,,,,,,"2D Object Detection, Object Tracking, Multiview Detection, 3D Object Detection",,,See all 1951 tasks,Multiview Detection5 benchmark,Multiview Detection5 benchmark
GMVD,GMVD Dataset,"The GMVD dataset consists of synthetic scenes captured using the GTA-V and Unity graphics engines. The dataset covers a variety of scenes, along with different conditions including day time variations (morning, afternoon, evening, night) and weather variations (sunny, cloudy, rainy, snowy). The purpose of the dataset is twofold. The first is to benchmark the generalization capabilities of Multi-View Detection algorithms. The second purpose is to serve as a synthetic training source from which the trained models can be directly applied on real-world data.",https://production-media.paperswithcode.com/datasets/174051a6-103c-4402-b5ab-2c29f2199b0d.jpg,EditCC BY-NC 4.0,Image,,,,,,,,Multiview Detection,multiview-detection-on-gmvd,,See all 1951 tasks,Multiview Detection5 benchmark,Multiview Detection5 benchmark
MultiviewX,MultiviewX Dataset,"MultiviewX is a synthetic Multiview pedestrian detection dataset. It is build using pedestrian models from PersonX, in Unity.
The MultiviewX dataset covers a square of 16 meters by 25 meters. The ground plane is quantized into a 640x1000 grid. There are 6 cameras with overlapping field-of-view in the MultiviewX dataset, each of which outputs a 1080x1920 resolution image. On average, 4.41 cameras are covering the same location.",https://github.com/hou-yz/MVDet,EditUnknown,"Image, Video",,,,,,,,"Multi-Object Tracking, Multiview Detection","multi-object-tracking-on-multiviewx, multiview-detection-on-multiviewx",,See all 1951 tasks,Multiview Detection5 benchmark,Multiview Detection5 benchmark
RailEye3D_Dataset,RailEye3D Dataset Dataset,"The RailEye3D dataset, a collection of train-platform scenarios for applications targeting passenger safety and automation of train dispatching, consists of 10 image sequences captured at 6 railway stations in Austria. Annotations for multi-object tracking are provided in both an unified format as well as the ground-truth format used in the MOTChallenge.",https://production-media.paperswithcode.com/datasets/train.jpg,EditCustom (non-commercial),"Image, Video",,,,,,,,"Object Detection, Multi-Object Tracking, Pedestrian Detection, Multiview Detection, Scene Understanding",,,See all 1951 tasks,Multiview Detection5 benchmark,Multiview Detection5 benchmark
Three-view_Synthetic_data,Three-view Synthetic data Dataset,"10000 instances of three-view numerical data set with 4 clusters and 2 feature components are considered. The data points in each view are generated from a 2-component 2-variate Gaussian mixture model (GMM) where their mixing proportions $\alpha_1^{(1)}=\alpha_1^{(2)}=\alpha_1^{(3)}=\alpha_1^{(4)}=0.3$;  $\alpha_2^{(1)}=\alpha_2^{(2)}=\alpha_2^{(3)}=\alpha_2^{(4)}=0.15$; $\alpha_3^{(1)}=\alpha_3^{(2)}=\alpha_3^{(3)}=\alpha_3^{(4)}=0.15$  and $\alpha_4^{(1)}=\alpha_4^{(2)}=\alpha_4^{(3)}=\alpha_4^{(4)}=0.4$. The means  $\mu_{ik}^{(1)}$ for the first view are $[-10 ~-5)]$,$[-9 ~ 11]$, $[0~ 6]$   and $[4~0]$;  The means  $\mu_{ik}^{(2)}$ for the view 2 are $[-8 ~-12]$,$[-6 ~ -3]$, $[-2~ 7]$   and $[2~1]$; And the means  $\mu_{ik}^{(3)}$ for the third view are $[-5 ~-10]$,$[-8 ~ -1]$, $[0~ 5]$   and $[5~-4]$. The covariance matrices for the three views are $\Sigma_1^{(1)}=\Sigma_1^{(2)}=\Sigma_1^{(3)}=\Sigma_1^{(4)}=\left[ \begin{array}{cc} 1 & 0\0&1\end{array}\right]$; $\Sigma_2^{(1)}=\Sigma_2^{(2)}=\Sigma_2^{(3)}=\Sigma_2^{(4)}=3 \left[ \begin{array}{cc} 1 & 0\0&1\end{array}\right]$;  $\Sigma_3^{(1)}=\Sigma_3^{(2)}=\Sigma_3^{(3)}=\Sigma_3^{(4)}=2 \left[ \begin{array}{cc} 1 & 0\0&1\end{array}\right]$; and $\Sigma_4^{(1)}=\Sigma_4^{(2)}=\Sigma_4^{(3)}=\Sigma_4^{(4)}=0.5 \left[ \begin{array}{cc} 1 & 0\0&1\end{array}\right]$. These $x_1^{(1)}$  and $x_2^{(1)}$  are the coordinates for the view 1,  $x_1^{(2)}$  and  $x_2^{(2)}$ are the coordinates for the view 2,  $x_1^{(3)}$ and $x_2^{(3)}$ are the coordinates for the view 3. While the original distribution of data points for cluster 1, cluster 2, cluster 3, and cluster 4 are 1514, 3046, 3903, and 1537, respectively.",https://production-media.paperswithcode.com/datasets/afbf2bb8-844c-43cd-ac62-37d99eaab19c.png,EditMIT,Image,,,,,10000 instances,,,Multiview Detection,,,See all 1951 tasks,Multiview Detection5 benchmark,Multiview Detection5 benchmark
EMOPIA,EMOPIA Dataset,"EMOPIA (pronounced ‘yee-mò-pi-uh’) dataset is a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators.",https://production-media.paperswithcode.com/datasets/emopia.png,EditCC BY,"Audio, Image, Text",English,,,,,,,"Music Tagging, Music Classification, Music Generation, Emotion Recognition, Music Information Retrieval, Emotion Classification, Music Style Transfer",,,See all 1951 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
JSB_Chorales,JSB Chorales Dataset,"The JSB chorales are a set of short, four-voice pieces of music well-noted for their stylistic homogeneity. The chorales were originally composed by Johann Sebastian Bach in the
18th century. He wrote them by first taking pre-existing melodies from contemporary Lutheran hymns and then harmonising them to create the parts for the remaining
three voices. The version of the dataset used canonically in representation learning contexts consists of 382 such chorales, with a train/validation/test split of 229, 76 and 77 samples respectively.",https://production-media.paperswithcode.com/datasets/Johann_Sebastian_Bach.jpeg,EditPublic Domain,"Audio, Text",English,,,,77 samples,"train/validation/test split of 229, 76 and 77 samples",,"Music Generation, Music Modeling",music-modeling-on-jsb-chorales,,See all 1951 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
Lakh_MIDI_Dataset,Lakh MIDI Dataset Dataset,"The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files). Around 10% of all MIDI files include timestamped lyrics events with lyrics are often transcribed at the word, syllable or character level.

LMD-full denotes the whole dataset. LMD-matched is the subset of LMD-full that consists of MIDI files matched with the Million Song Dataset entries. LMD-aligned contains all the files of LMD-matched, aligned to preview MP3s from the Million Song Dataset.

A lakh is a unit of measure used in the Indian number system which signifies 100,000.",https://colinraffel.com/projects/lmd/,EditCC-BY 4.0,"Audio, Text",English,,,,,,,"Music Generation, Music Modeling",,,See all 1951 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
MAESTRO,MAESTRO Dataset,"The MAESTRO dataset contains over 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. The MIDI data includes key strike velocities and sustain/sostenuto/una corda pedal positions. Audio and MIDI files are aligned with ∼3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. Uncompressed audio is of CD quality or higher (44.1–48 kHz 16-bit PCM stereo).",https://magenta.tensorflow.org/datasets/maestro,EditUnknown,"Audio, Text",English,,,,,,,"Audio Generation, Music Generation, Music Transcription",music-transcription-on-maestro,,See all 1951 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
MuseData,MuseData Dataset,MuseData is an electronic library of orchestral and piano classical music from CCARH. It consists of around 3MB of 783 files.,https://arxiv.org/pdf/1206.6392v1.pdf,EditUnknown,"Audio, Text",English,,,,,,,"Music Generation, Language Modelling",,,See all 1951 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
Music21,Music21 Dataset,Music21 is an untrimmed video dataset crawled by keyword query from Youtube. It contains music performances belonging to 21 categories. This dataset is relatively clean and collected for the purpose of training and evaluating visual sound source separation models.,https://arxiv.org/abs/2004.09476,EditBSD-3,"Audio, Text",English,,,,,,21,"Music Generation, Music Transcription, Music Modeling",,,See all 1951 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
MusicCaps,MusicCaps Dataset,"MusicCaps is a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts. For each 10-second music clip, MusicCaps provides: 

1) A free-text caption consisting of four sentences on average, describing the music and 

2) A list of music aspects, describing genre, mood, tempo, singer voices, instrumentation, dissonances, rhythm, etc.",https://arxiv.org/pdf/2301.11325v1.pdf,EditCC BY-SA 4.0,"Audio, Image, Text",English,,,,,,,"Text-to-Music Generation, Music Generation, Music Captioning",text-to-music-generation-on-musiccaps,,See all 1951 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
NSynth,NSynth Dataset,"NSynth is a dataset of one shot instrumental notes, containing 305,979 musical notes with unique pitch, timbre and envelope. The sounds were collected from 1006 instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. Four second monophonic 16kHz audio snippets were generated (notes) for the instruments.",https://arxiv.org/abs/1907.08520,EditCC BY 4.0,"Audio, Image, Text",English,,,,,,,"Audio Generation, Music Generation, Pitch Classification, Instrument Recognition, Few-Shot Audio Classification, Self-Supervised Learning","instrument-recognition-on-nsynth, pitch-classification-on-nsynth, few-shot-audio-classification-on-nsynth",,See all 1951 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
URMP,URMP Dataset,"URMP (University of Rochester Multi-Modal Musical Performance) is a dataset for facilitating audio-visual analysis of musical performances. The dataset comprises 44 simple multi-instrument musical pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece the dataset provided the musical score in MIDI format, the high-quality individual instrument audio recordings and the videos of the assembled pieces.",http://www2.ece.rochester.edu/projects/air/projects/URMP.html,EditUnknown,"Audio, Text",English,,,,,,,"Multi-instrument Music Transcription, Music Transcription, Music Generation, Music Information Retrieval","music-transcription-on-urmp, multi-instrument-music-transcription-on-urmp",,See all 1951 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
arXiv_Astro-Ph,arXiv Astro-Ph Dataset,"Arxiv ASTRO-PH (Astro Physics) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to Astro Physics category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.",https://snap.stanford.edu/data/ca-AstroPh.html,EditUnknown,"Graph, Image, Time Series",,,,,,,,"Multi-Label Classification, Link Prediction, Clique Prediction, Network Embedding","clique-prediction-on-arxiv-astroph-4-clique, clique-prediction-on-arxiv-astroph-2-clique",,See all 1951 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
BIRDeep,BIRDeep Dataset,"The BIRDeep Audio Annotations dataset is a collection of bird vocalizations from Doñana National Park, Spain. It was created as part of the BIRDeep project, which aims to optimize the detection and classification of bird species in audio recordings using deep learning techniques. The dataset is intended for use in training and evaluating models for bird vocalization detection and identification.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Graph, Image",,,,,,,,"Bird Audio Detection, Event Detection, Network Embedding, Bird Species Classification With Audio-Visual Data",,,See all 1951 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
IS-A,IS-A Dataset,"The IS-A dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are related by the “is a” relation. For example, ‘acute leukemia’ is a ‘leukemia’. The dataset has 294,693 nodes with 356,541 edges between them.",https://arxiv.org/pdf/1906.05939.pdf,EditUnknown,"Graph, Time Series",,,,,,,,"Link Prediction, Network Embedding, Graph Embedding",,,See all 1951 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
Netzschleuder,Netzschleuder Dataset,"This is a catalogue and repository of network datasets with the aim of aiding scientific research.

This website is meant to be browsed both by humans and machines alike, and can also be accessed via a convenient JSON API, or via the graph-tool library. The network datasets themselves are available in several machine-readable formats, in particular gt, GraphML, GML and CSV.

The upstream origin of each dataset is meant to be as transparent as possible. Each dataset contains its own publicly available extraction and parsing script, accessible via a git repository, which also includes the entire code for this website, released as Free Software under the AGPLv3.

Users are encouraged to inspect the entire pipeline from original upstream data publication, downloading, parsing and format conversion.

Users are also welcome to report problems or omissions with the datasets, as well as suggest new ones, either by opening an issue, or simply by forking the git repository and proposing a merge request.",https://production-media.paperswithcode.com/datasets/netzschleuder.png,EditAGPLv3,"3D, Graph",,,,,,,,"Graph Reconstruction, Graph Mining, Graphon Estimation, Network Embedding, Graph Clustering, Network Pruning, Network Community Partition",,,See all 1951 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
PART-OF,PART-OF Dataset,"The PART-OF dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are parts of the human body. The dataset has 16,894 nodes with 19,436 edges between them.",https://arxiv.org/pdf/1906.05939.pdf,EditUnknown,"Graph, Time Series",,,,,,,,"Link Prediction, Network Embedding",,,See all 1951 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
BUP20,BUP20 Dataset,"Video sequences from a glasshouse environment in Campus Kleinaltendorf(CKA), University of Bonn, captured by PATHoBot, a glasshouse monitoring robot. 


10 Video sequences, 120s long.
2 cultivar Mavera (yellow) and All- rounder (red).
RGB-D images (Intel RealSense D435i cameras).
Robot odometry and IMU.
High quality sparese instance segmentation labels.",https://production-media.paperswithcode.com/datasets/66abe318-3b55-48c4-8e04-313d7c9f921e.png,EditUnknown,Image,,,,,,,,"Neural Rendering, Instance Segmentation, Panoptic Segmentation, Semantic Segmentation",,,See all 1951 tasks,Neural Rendering191 papers wit,Neural Rendering191 papers wit
DONeRF__Evaluation_Dataset,DONeRF: Evaluation Dataset Dataset,"This is the dataset for the CGF 2021 paper ""DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks"".

Please note the original creators of the individual 3D scenes themselves (individual license files can be found in the individual .zip archives in the dataset):

Bulldozer by ""Heinzelnisse"" (CC-BY-NC): https://www.blendswap.com/blend/11490

Forest by Robin Tran (CC-BY-SA 3.0): https://cloud.blender.org/p/gallery/5fbd186ec57d586577c57417

Classroom by Christophe Seux (CC-0): https://download.blender.org/demo/test/classroom.zip

San Miguel by Guillermo M. Leal Llaguno (CC-BY 3.0): https://casual-effects.com/g3d/data10/index.html#

Pavillon by Hamza Cheggour / ""eMirage"" (CC-BY): https://download.blender.org/demo/test/pabellon_barcelona_v1.scene_.zip

Barbershop by Blender Animation Studio (CC-BY): https://svn.blender.org/svnroot/bf-blender/trunk/lib/benchmarks/cycles/barbershop_interior/",https://production-media.paperswithcode.com/datasets/donerf_teaser2.png,EditCC-BY 4.0 International,,,2021,,,,,,"Neural Rendering, Novel View Synthesis",novel-view-synthesis-on-donerf-evaluation,,See all 1951 tasks,Neural Rendering191 papers wit,Neural Rendering191 papers wit
FutureHouse,FutureHouse Dataset,"We present a new large-scale photorealistic panoramic dataset named FutureHouse, which has the following characteristics. 
     1) It contains over 70,000 high-quality models with high-resolution meshes and physical material. All models are measured in real world standards. 
     2) Selected scene layouts are carefully designed by over 100 excellent artists. All of selected layouts are used in realworld display. 
     3) It contains 28,579 good panoramic views from 1,752 house-scale scenes. Therefore, it can be used for perspective image tasks as well as omnidirectional image tasks. 
     4) More physical material representation. Most materials are represent by microfacet BRDF modeling metalness, and the rest are represent by special shading models, e.g., cloth material and transmission material. 
     5) High rendering quality. Benefiting from commercial rendering engine, Unreal engine 4, and powerful deep learning super sampling (DLSS), our renderings have less noise. 
Our SVBRDF representation including base color and metalness is capable of producing nonmonochrome specular reflectance.",https://production-media.paperswithcode.com/datasets/217e1298-5796-4df1-b584-ac3a275d4536.jpg,EditGNU 2.0,"3D, Image",,,,,,,,"Neural Rendering, Depth Estimation, Intrinsic Image Decomposition, Surface Normal Estimation",,,See all 1951 tasks,Neural Rendering191 papers wit,Neural Rendering191 papers wit
MatrixCity,MatrixCity Dataset,"We build a large-scale, comprehensive, and high-quality synthetic dataset for city-scale neural rendering researches. Leveraging the Unreal Engine 5 City Sample project, we developed a pipeline to easily collect aerial and street city views with ground-truth camera poses, as well as a series of additional data modalities. Flexible control on environmental factors like light, weather, human and car crowd is also available in our pipeline, supporting the need of various tasks covering city-scale neural rendering and beyond. The resulting pilot dataset, MatrixCity, contains 67k aerial images and 452k street images from two city maps of total size 28km^2.",https://production-media.paperswithcode.com/datasets/3bfafe74-f4c2-43f6-abbc-4847c2b64ce1.png,EditCC BY-NC 4.0,,,,,,,,,Neural Rendering,,,See all 1951 tasks,Neural Rendering191 papers wit,Neural Rendering191 papers wit
SB20,SB20 Dataset,"Video sequences captured at a field on Campus Kleinaltendorf (CKA), University of Bonn, captured by BonBot-I, an autonomous weeding robot. The data was captured by mounting an Intel  RealSense D435i sensor with a nadir view of the ground.


RGB-D video sequences (Intel RealSense D435i cameras).
Robot odometry and IMU.
Crops and 8 different categories of weeds at different growth stages.
Different illumination conditions.
Three herbicide treatment regimes (30%, 70%, 100%), impacting weed density directly.
High quality sparese instance segmentation labels.",https://production-media.paperswithcode.com/datasets/3a58ade3-2ac2-4762-8c0c-6008e5dc4946.png,EditUnknown,Image,,,,,,,,"Neural Rendering, Instance Segmentation, Panoptic Segmentation, Semantic Segmentation",,,See all 1951 tasks,Neural Rendering191 papers wit,Neural Rendering191 papers wit
SyntheticFur,SyntheticFur Dataset,"SyntheticFur is a dataset for neural rendering. Collecting and generating high quality fur images is an expensive and difficult process that requires content specialists to generate. By releasing this unique dataset with high quality lighting simulation via ray tracing, this can save time for researchers seeking to advance studies of fur rendering and simulation, without having to recreate this laborious process.

The dataset was used for neural rendering research at Google that takes advantage of rasterized image buffers and converts them into high quality raytraced fur renders. We believe that this dataset can contribute to the computer graphics and machine learning community to develop more advanced techniques with fur rendering.

It contains approximately 140,000 procedurally generated images and 15 simulations with Houdini. The images consist of fur groomed with different skin primitives and move with various motions in a predefined set of lighting environments.",https://production-media.paperswithcode.com/datasets/bunny.gif,EditApache License 2.0,,,,,,,,,Neural Rendering,,,See all 1951 tasks,Neural Rendering191 papers wit,Neural Rendering191 papers wit
ThermoScenes,ThermoScenes Dataset,Dataset of paired thermal and RGB images comprising ten diverse scenes—six indoor and four outdoor scenes— for 3D scene reconstruction and novel view synthesis (e.g. with NeRF).,https://production-media.paperswithcode.com/datasets/93ed9ce7-1995-47cb-8ff5-995c73df0739.png,EditUnknown,"3D, Image",,,,,,,,"Infrared And Visible Image Fusion, 3D Reconstruction, Novel View Synthesis, Neural Rendering",,,See all 1951 tasks,Neural Rendering191 papers wit,Neural Rendering191 papers wit
NeRF,NeRF Dataset,Neural Radiance Fields (NeRF) is a method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. The dataset contains three parts with the first 2 being synthetic renderings of objects called Diffuse Synthetic 360◦ and Realistic Synthetic 360◦ while the third is real images of complex scenes. Diffuse Synthetic 360◦ consists of four Lambertian objects with simple geometry. Each object is rendered at 512x512 pixels from viewpoints sampled on the upper hemisphere. Realistic Synthetic 360◦ consists of eight objects of complicated geometry and realistic non-Lambertian materials. Six of them are rendered from viewpoints sampled on the upper hemisphere and the two left are from viewpoints sampled on a full sphere with all of them at 800x800 pixels. The real images of complex scenes consist of 8 forward-facing scenes captured with a cellphone at a size of 1008x756 pixels.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,,,,,,,Novel View Synthesis,novel-view-synthesis-on-nerf,,See all 1951 tasks,Novel View Synthesis20 benchma,Novel View Synthesis20 benchma
Object3869_papers_with_code_Dataset,Object3869 papers with code Dataset,,https://paperswithcode.com/dataset/object,,,,,,,,,,,,,See all 1951 tasks,Object3869 papers with code,Object3869 papers with code
BDD100K,BDD100K Dataset,"Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue. More detail is at the dataset home page.",https://production-media.paperswithcode.com/datasets/bdd.gif,EditMixed License,"Image, Video",,,,,,,,"Steering Control, Object Detection, Multiple Object Track and Segmentation, Instance Segmentation, Multi-Object Tracking, Domain Adaptation, 2D Object Detection, Lane Detection, Semantic Segmentation, Multiple Object Tracking, Image Classification, Semi-Supervised Instance Segmentation, Amodal Panoptic Segmentation, Video Instance Segmentation, Drivable Area Detection, Panoptic Segmentation, Multi-Object Tracking and Segmentation, Traffic Object Detection, Video Segmentation","drivable-area-detection-on-bdd100k-val, multiple-object-track-and-segmentation-on-2, object-detection-on-bdd100k-val, multi-object-tracking-on-bdd100k, object-detection-on-bdd100k, multiple-object-tracking-on-bdd100k-val, multi-object-tracking-and-segmentation-on-3, traffic-object-detection-on-bdd100k-val, video-instance-segmentation-on-bdd100k-val, instance-segmentation-on-bdd100k-val, steering-control-on-bdd100k-val, multiple-object-tracking-on-bdd100k-test-1, semantic-segmentation-on-bdd100k-val, lane-detection-on-bdd100k-val, 2d-object-detection-on-bdd100k-val, amodal-panoptic-segmentation-on-bdd100k-val",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
GQA,GQA Dataset,"The GQA dataset is a large-scale visual question answering dataset with real images from the Visual Genome dataset and balanced question-answer pairs. Each training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images and question-answer pairs, the GQA dataset provides two types of pre-extracted visual features for each image – convolutional grid features of size 7×7×2048 extracted from a ResNet-101 network trained on ImageNet, and object detection features of size Ndet×2048 (where Ndet is the number of detected objects in each image with a maximum of 100 per image) from a Faster R-CNN detector.",https://arxiv.org/abs/1905.04405,EditCC BY 4.0,"Graph, Image, Text",English,2048,,,,"training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images",,"Object Detection, Scene Graph Generation, Visual Question Answering, Visual Question Answering (VQA), Graph Question Answering","visual-question-answering-on-gqa, visual-question-answering-on-gqa-1, visual-question-answering-on-gqa-test-dev, visual-question-answering-on-gqa-test-std, visual-question-answering-on-gqa-test2019, object-detection-on-gqa, graph-question-answering-on-gqa, scene-graph-generation-on-gqa",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
KITTI,KITTI Dataset,"KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. Álvarez et al. generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. Zhang et al. annotated 252 (140 for training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. Ros et al. labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.",https://arxiv.org/abs/1704.06857,EditCC BY-NC-SA 3.0,"3D, Image, Text, Time Series, Video",English,,,,323 images,"training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. Ros et al. labeled 170 training images",11,"Stereo Depth Estimation, Object Detection, Visual Place Recognition, Monocular Cross-View Road Scene Parsing(Vehicle), Visual Odometry, Scene Generation, Depth Prediction, Image Dehazing, Dense Pixel Correspondence Estimation, Knowledge Distillation, Transfer Learning, Egocentric Pose Estimation, Unsupervised Object Detection, Image Super-Resolution, Optical Flow Estimation, Image Clustering, Object Localization, Novel View Synthesis, 3D Object Tracking, Birds Eye View Object Detection, Semantic Segmentation, Multiple Object Tracking, Monocular Cross-View Road Scene Parsing(Road), Text-To-SQL, Stereo Image Super-Resolution, 3D Object Detection, Real-time Instance Segmentation, Image-to-Image Translation, Pose Estimation, Monocular Depth Estimation, 3D Single Object Tracking, Depth Estimation, Point Cloud Registration, Object Tracking, Horizon Line Estimation, Stereo Disparity Estimation, Vehicle Pose Estimation, Depth Completion, Prediction Of Occupancy Grid Maps, Scene Flow Estimation, Video Prediction, Monocular 3D Object Detection, 3D Object Detection From Stereo Images, Panoptic Segmentation, Unsupervised Monocular Depth Estimation, Image to Point Cloud Registration","birds-eye-view-object-detection-on-kitti, object-detection-on-kitti-cars-hard, optical-flow-estimation-on-kitti-2015, monocular-cross-view-road-scene-parsing-road, object-localization-on-kitti-cars-hard, monocular-3d-object-detection-on-kitti-cars, 3d-object-detection-on-kitti-cyclists-hard, image-to-point-cloud-registration-on-kitti, point-cloud-registration-on-kitti-trained-on, knowledge-distillation-on-kitti, object-localization-on-kitti-pedestrian-easy, point-cloud-registration-on-kitti, image-to-image-translation-on-kitti-object, stereo-image-super-resolution-on-kitti2012-2x-1, object-localization-on-kitti-cars-moderate, stereo-image-super-resolution-on-kitti2015-2x, monocular-3d-object-detection-on-kitti-cars-2, unsupervised-monocular-depth-estimation-on-1, stereo-image-super-resolution-on-kitti2012-2x-2, 3d-object-detection-on-kitti-pedestrian-1, birds-eye-view-object-detection-on-kitti-3, birds-eye-view-object-detection-on-kitti-10, scene-flow-estimation-on-kitti-2015-scene-1, monocular-cross-view-road-scene-parsing-road-1, object-detection-on-kitti-cars-moderate, birds-eye-view-object-detection-on-kitti-13, egocentric-pose-estimation-on-kitti-odometry, birds-eye-view-object-detection-on-kitti-4, object-localization-on-kitti-pedestrians-hard, multiple-object-tracking-on-kitti-test-online, semantic-segmentation-on-kitti-semantic, test-results-on-kitti, optical-flow-estimation-on-kitti-2015-train, birds-eye-view-object-detection-on-kitti-cars-2, optical-flow-estimation-on-kitti-2015-2, object-detection-on-kitti-cars-easy, monocular-cross-view-road-scene-parsing, novel-view-synthesis-on-kitti-novel-view, 3d-object-detection-on-kitti-cars-hard, image-super-resolution-on-kitti-2012-2x, 3d-object-detection-on-kitti-pedestrian-hard, 3d-object-detection-on-kitti-pedestrian-easy-1, object-detection-on-kitti-pedestrians-easy, unsupervised-object-detection-on-kitti, birds-eye-view-object-detection-on-kitti-cars-5, monocular-3d-object-detection-on-kitti-1, monocular-depth-estimation-on-kitti-eigen-1, 3d-object-detection-on-kitti-cars-easy-val, 3d-object-detection-from-stereo-images-on-1, monocular-3d-object-detection-on-kitti-7, depth-completion-on-kitti, object-detection-on-kitti-cyclists-hard, point-cloud-registration-on-kitti-fcgf, 3d-object-detection-on-kitti-pedestrians-easy, vehicle-pose-estimation-on-kitti, stereo-depth-estimation-on-kitti2012, object-localization-on-kitti-pedestrians, video-prediction-on-kitti, unsupervised-monocular-depth-estimation-on-4, 3d-object-detection-on-kitti-pedestrian-2, stereo-depth-estimation-on-kitti2015, 3d-object-detection-on-kitti-pedestrians-1, image-super-resolution-on-kitti-2015-2x, scene-flow-estimation-on-kitti-2015-scene, stereo-image-super-resolution-on-kitti2012-4x, depth-prediction-on-kitti-2015, monocular-3d-object-detection-on-kitti-3, monocular-depth-estimation-on-kitti-2, monocular-3d-object-detection-on-kitti-4, stereo-disparity-estimation-on-kitti-2015, optical-flow-estimation-on-kitti-2012-2, 3d-object-detection-on-kitti-cars-hard-val, depth-estimation-on-kitti-eigen-split-3, 3d-object-detection-on-kitti-cyclist-moderate, image-super-resolution-on-kitti-2012-4x, 3d-object-detection-on-kitti-pedestrian, birds-eye-view-object-detection-on-kitti-9, stereo-depth-estimation-on-kitti-2015, optical-flow-estimation-on-kitti-2012, birds-eye-view-object-detection-on-kitti-1, 3d-object-detection-on-kitti-cars-easy, panoptic-segmentation-on-kitti-panoptic-1, 3d-object-detection-on-kitti-cyclist-hard-val, object-tracking-on-kitti, monocular-3d-object-detection-on-kitti, 3d-object-detection-on-kitti-pedestrians, scene-generation-on-kitti, birds-eye-view-object-detection-on-kitti-cars-4, object-localization-on-kitti-cyclists, vehicle-pose-estimation-on-kitti-cars-hard, birds-eye-view-object-detection-on-kitti-2, object-detection-on-kitti-cyclists-easy, monocular-depth-estimation-on-kitti-eigen, birds-eye-view-object-detection-on-kitti-cars-1, object-localization-on-kitti-cyclists-hard, object-localization-on-kitti-cars-easy, object-detection-on-kitti-cyclists-moderate, 3d-object-detection-from-stereo-images-on-2, real-time-instance-segmentation-on-kitti, 3d-object-detection-on-kitti-cyclists-1, 3d-object-detection-on-kitti-cyclists, image-super-resolution-on-kitti-2015-4x, 3d-object-detection-on-kitti-pedestrian-easy, multiple-object-tracking-on-kitti-test, 3d-object-detection-on-kitti-pedestrian-hard-1, monocular-3d-object-detection-on-kitti-6, birds-eye-view-object-detection-on-kitti-7, object-localization-on-kitti-pedestrians-easy, birds-eye-view-object-detection-on-kitti-12, birds-eye-view-object-detection-on-kitti-6, dense-pixel-correspondence-estimation-on-1, 3d-object-detection-on-kitti-pedestrians-hard, image-clustering-on-kitti, object-detection-on-kitti-pedestrians-hard, novel-view-synthesis-on-kitti, birds-eye-view-object-detection-on-kitti-11, unsupervised-monocular-depth-estimation-on-5, dense-pixel-correspondence-estimation-on-2, monocular-3d-object-detection-on-kitti-5, image-dehazing-on-kitti, monocular-depth-estimation-on-kitti-object, birds-eye-view-object-detection-on-kitti-8, depth-estimation-on-kitti-2015, birds-eye-view-object-detection-on-kitti-cars-3, birds-eye-view-object-detection-on-kitti-17, horizon-line-estimation-on-kitti-horizon, transfer-learning-on-kitti-object-tracking, text-to-sql-on-2d-kitti-cars-easy, stereo-image-super-resolution-on-kitti2012-2x-3, visual-place-recognition-on-kitti, 3d-object-detection-on-kitti-cyclists-easy, object-localization-on-kitti-cyclists-easy, 3d-object-detection-on-kitti-cyclist-easy-val, birds-eye-view-object-detection-on-kitti-16, monocular-3d-object-detection-on-kitti-cars-1, 3d-object-detection-on-kitti-cars-moderate-1, object-detection-on-kitti-pedestrians, pose-estimation-on-kitti-2015, birds-eye-view-object-detection-on-kitti-cars, stereo-image-super-resolution-on-kitti2015-4x, 3d-object-detection-from-stereo-images-on-3, birds-eye-view-object-detection-on-kitti-5",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
LVIS,LVIS Dataset,LVIS is a dataset for long tail instance segmentation. It has annotations for over 1000 object categories in 164k images.,https://arxiv.org/pdf/1908.03195.pdf,EditCustom (CC BY 4.0 + COCO license),Image,English,,,,164k images,,,"Object Detection, Zero-Shot Object Detection, Open Vocabulary Object Detection, Novel Object Detection, Instance Segmentation, Zero-Shot Instance Segmentation, Few-Shot Object Detection, Long-tailed Object Detection, Unsupervised Object Detection","few-shot-object-detection-on-lvis-v1-0-test, unsupervised-object-detection-on-lvis-v1-0, zero-shot-instance-segmentation-on-lvis-v1-0, novel-object-detection-on-lvis-v1-0-val, instance-segmentation-on-lvis-v1-0-test-dev, long-tailed-object-detection-on-lvis-v1-0-val, object-detection-on-lvis-v1-0-val, zero-shot-object-detection-on-lvis-v1-0, zero-shot-object-detection-on-lvis-v1-0-val, instance-segmentation-on-lvis-v1-0-val, few-shot-object-detection-on-lvis-v1-0-val, object-detection-on-lvis-v1-0-1, object-detection-on-lvis-v1-0-minival, open-vocabulary-object-detection-on-lvis-v1-0",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
Manga109,Manga109 Dataset,"Manga109 has been compiled by the Aizawa Yamasaki Matsui Laboratory, Department of Information and Communication Engineering, the Graduate School of Information Science and Technology, the University of Tokyo. The compilation is intended for use in academic research on the media processing of Japanese manga. Manga109 is composed of 109 manga volumes drawn by professional manga artists in Japan. These manga were commercially made available to the public between the 1970s and 2010s, and encompass a wide range of target readerships and genres (see the table in Explore for further details.) Most of the manga in the compilation are available at the manga library “Manga Library Z” (formerly the “Zeppan Manga Toshokan” library of out-of-print manga).",http://www.manga109.org/en/,"EditCustom (Manga109: research-only, non-commercial. Manga109-s: available for commercial use under appropriate conditions.)",Image,Japanese,,,,,,,"Object Detection, Face Detection, Image Super-Resolution, Body Detection, Blind Super-Resolution","object-detection-on-manga109-s-15test, object-detection-on-manga109, image-super-resolution-on-manga109-4x, blind-super-resolution-on-manga109-2x, face-detection-on-manga109, image-super-resolution-on-manga109-3x, image-super-resolution-on-manga109-8x, blind-super-resolution-on-manga109-4x, body-detection-on-manga109, image-super-resolution-on-manga109-2x, blind-super-resolution-on-manga109-3x, image-super-resolution-on-manga109-16x",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
MS_COCO,MS COCO Dataset,"The MS COCO (Microsoft Common Objects in Context) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images.

Splits:
The first version of MS COCO dataset was released in 2014. It contains 164K images split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images was released, including all the previous test images and 40K new images.

Based on community feedback, in 2017 the training/validation split was changed from 83K/41K to 118K/5K. The new split uses the same images and annotations. The 2017 test set is a subset of 41K images of the 2015 test set. Additionally, the 2017 release contains a new unannotated dataset of 123K images.

Annotations:
The dataset has annotations for


object detection: bounding boxes and per-instance segmentation masks with 80 object categories,
captioning: natural language descriptions of the images (see MS COCO Captions),
keypoints detection: containing more than 200,000 images and 250,000 person instances labeled with keypoints (17 possible keypoints, such as left eye, nose, right hip, right ankle),
stuff image segmentation – per-pixel segmentation masks with 91 stuff categories, such as grass, wall, sky (see MS COCO Stuff),
panoptic: full scene segmentation, with 80 thing categories (such as person, bicycle, elephant) and a subset of 91 stuff categories (grass, sky, road),
dense pose: more than 39,000 images and 56,000 person instances labeled with DensePose annotations – each labeled person is annotated with an instance id and a mapping between image pixels that belong to that person body and a template 3D model.
The annotations are publicly available only for training and validation images.",https://cocodataset.org/,EditCustom,"3D, Graph, Image, Text",English,2014,,,328K images,"split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images",,"Object Detection, Scene Graph Generation, Active Object Detection, Homography Estimation, Multi-Person Pose Estimation, Instance Segmentation, Weakly Supervised Object Detection, Conditional Image Generation, Image-level Supervised Instance Segmentation, Knowledge Distillation, Real-Time Object Detection, Layout-to-Image Generation, Open Vocabulary Object Detection, Interactive Segmentation, Object Localization, Keypoint Detection, Zero-Shot Composed Image Retrieval (ZS-CIR), Text-to-Image Generation, Unsupervised Object Localization, Box-supervised Instance Segmentation, Image Captioning, Question Generation, Open World Object Detection, Zero-shot Text-to-Image Retrieval, Visual Question Answering (VQA), Semantic Segmentation, Semi Supervised Learning for Image Captioning, Robust Object Detection, Real-time Instance Segmentation, Cross-Modal Retrieval, mage-to-Text Retrieval, Pose Estimation, Region Proposal, Unsupervised Semantic Segmentation with Language-image Pre-training, One-Shot Object Detection, Point-Supervised Instance Segmentation, Object Proposal Generation, Image Retrieval, Image-to-Text Retrieval, Zero-Shot Cross-Modal Retrieval, Multi-Label Classification, Paraphrase Generation, Few-Shot Object Detection, Multi-Label Learning, One-Shot Instance Segmentation, Image Outpainting, Object Counting, Question Answering, Single-object discovery, Unsupervised Semantic Segmentation, Zero-Shot Object Detection, Generalized Zero-Shot Object Detection, Visual Question Answering, Quantization, Panoptic Segmentation, Multi-Label Image Classification, Activeness Detection, Weakly-supervised instance segmentation, Few Shot Open Set Object Detection, Multi-object discovery","homography-estimation-on-coco-2014, pose-estimation-on-coco, zero-shot-object-detection-on-ms-coco, active-object-detection-on-coco, object-detection-on-coco-minival, image-captioning-on-coco, multi-label-image-classification-on-mscoco, zero-shot-text-to-image-retrieval-on-ms-coco, weakly-supervised-object-detection-on-coco, open-vocabulary-object-detection-on-mscoco, box-supervised-instance-segmentation-on-coco, semantic-segmentation-on-coco-1, cross-modal-retrieval-on-mscoco-1k, unsupervised-semantic-segmentation-on-coco-1, one-shot-object-detection-on-coco, image-to-text-retrieval-on-coco, robust-object-detection-on-coco, activeness-detection-on-coco-test-dev, single-object-discovery-on-coco-20k, open-world-object-detection-on-coco-2017-1, unsupervised-object-localization-on-coco-20k, zero-shot-cross-modal-retrieval-on-coco-2014, visual-question-answering-on-coco-visual-4, object-counting-on-coco-count-test, weakly-supervised-instance-segmentation-on-2, question-generation-on-coco-visual-question, multi-label-learning-on-coco-2014, keypoint-detection-on-coco-test-dev, weakly-supervised-object-detection-on-mscoco, multi-person-pose-estimation-on-coco-test-dev, conditional-image-generation-on-coco-animals, point-supervised-instance-segmentation-on-1, question-answering-on-coco-visual-question, visual-question-answering-on-coco-visual-2, real-time-instance-segmentation-on-mscoco-1k, visual-question-answering-on-coco-visual, instance-segmentation-on-coco-minival, multi-label-classification-on-ms-coco, few-shot-object-detection-on-coco-2017, multi-person-pose-estimation-on-coco, image-outpainting-on-mscoco, image-captioning-on-mscoco-1, interactive-segmentation-on-coco-minival, object-proposal-generation-on-coco, few-shot-open-set-object-detection-on-mscoco, pose-estimation-on-coco-test-dev, image-retrieval-on-coco, panoptic-segmentation-on-coco-panoptic, visual-question-answering-on-coco, real-time-object-detection-on-coco, few-shot-object-detection-on-ms-coco-10-shot, text-to-image-generation-on-coco, semi-supervised-learning-for-image-captioning, zero-shot-object-detection-on-mscoco, visual-question-answering-on-coco-visual-1, weakly-supervised-object-detection-on-coco-2, layout-to-image-generation-on-coco-stuff-4, multi-object-discovery-on-coco-20k, object-detection-on-coco-1, object-detection-on-mscoco-6, keypoint-detection-on-coco, scene-graph-generation-on-ms-coco, generalized-zero-shot-object-detection-on-ms, panoptic-segmentation-on-coco-minival, visual-question-answering-on-coco-visual-3, real-time-instance-segmentation-on-mscoco, open-world-object-detection-on-coco-2017-2, knowledge-distillation-on-coco, one-shot-instance-segmentation-on-coco, keypoint-detection-on-coco-test-challenge, unsupervised-semantic-segmentation-with-5, region-proposal-on-coco-test-dev, cross-modal-retrieval-on-mscoco, image-captioning-on-ms-coco, visual-question-answering-on-coco-visual-5, object-detection-on-coco-2017, instance-segmentation-on-coco-minval, object-detection-on-mscoco-7, multi-person-pose-estimation-on-coco-minival, pose-estimation-on-densepose-coco, paraphrase-generation-on-mscoco, zero-shot-composed-image-retrieval-zs-cir-on-4, mage-to-text-retrieval-on-mscoco, object-detection-on-coco-5, pose-estimation-on-ms-coco, pose-estimation-on-coco-minival, object-detection-on-coco, cross-modal-retrieval-on-coco-2014, image-level-supervised-instance-segmentation-1, open-world-object-detection-on-coco-2017, panoptic-segmentation-on-coco-test-dev, text-to-image-generation-on-ms-coco, quantization-on-coco, instance-segmentation-on-coco, interactive-segmentation-on-coco, image-retrieval-on-mscoco",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
nuScenes,nuScenes Dataset,"The nuScenes dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360° coverage. The 3D object detection challenge evaluates the performance on 10 classes: cars, trucks, buses, trailers, construction vehicles, pedestrians, motorcycles, bicycles, traffic cones and barriers.",https://arxiv.org/abs/1911.10150,EditCustom (CC BY-NC-SA 4.0 with exceptions for startups and research; separate commercial licenses),"3D, Image, Time Series, Video",English,,,,28130 samples,"training, 6019 samples",10,"Object Detection, Instance Segmentation, 3D Semantic Segmentation, HD semantic map learning, Lane Detection, Motion Planning, Trajectory Prediction, 3D Object Detection, Semi-Supervised Semantic Segmentation, Weather Forecasting, LIDAR Semantic Segmentation, Bird's-Eye View Semantic Segmentation, Motion Detection, 3D Multi-Object Tracking, Weakly supervised Semantic Segmentation, Prediction Of Occupancy Grid Maps, Online Vectorized HD Map Construction, Monocular 3D Object Detection, Trajectory Planning","3d-object-detection-on-nuscenes, 3d-semantic-segmentation-on-nuscenes, motion-detection-on-nuscenes, object-detection-on-nuscenes, online-vectorized-hd-map-construction-on, semi-supervised-semantic-segmentation-on-25, lidar-semantic-segmentation-on-nuscenes, 3d-multi-object-tracking-on-nuscenes, prediction-of-occupancy-grid-maps-on-nuscenes, 3d-multi-object-tracking-on-nuscenes-camera-1, 3d-object-detection-on-nuscenes-camera-radar, trajectory-prediction-on-nuscenes, monocular-3d-object-detection-on-nuscenes, 3d-object-detection-on-nuscenes-camera-only, bird-s-eye-view-semantic-segmentation-on, 3d-multi-object-tracking-on-nuscenes-camera-2, motion-planning-on-nuscenes, hd-semantic-map-learning-on-nuscenes, trajectory-planning-on-nuscenes, weakly-supervised-semantic-segmentation-on-9, instance-segmentation-on-nuscenes, lane-detection-on-nuscenes",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
SUN_RGB-D,SUN RGB-D Dataset,"The SUN RGBD dataset contains 10335 real RGB-D images of room scenes. Each RGB image has a corresponding depth and segmentation map. As many as 700 object categories are labeled. The training and testing sets contain 5285 and 5050 images, respectively.",https://arxiv.org/abs/1903.04294,EditUnknown,"3D, Image",,,,,5050 images,training and testing sets contain 5285 and 5050 images,,"Object Detection, Scene Recognition, Monocular Depth Estimation, Object Detection In Indoor Scenes, Scene Segmentation, Room Layout Estimation, Scene Classification (unified classes), Monocular 3D Object Detection, Panoptic Segmentation, Semantic Segmentation, Robust Semi-Supervised RGBD Semantic Segmentation, Panoptic Segmentation (PanopticNDT instances), 3D Object Detection","semantic-segmentation-on-sun-rgbd, monocular-3d-object-detection-on-sun-rgb-d, scene-recognition-on-sun-rgbd, 3d-object-detection-on-sun-rgbd-val, robust-semi-supervised-rgbd-semantic-1, object-detection-in-indoor-scenes-on-sun-rgb, scene-classification-unified-classes-on-sun, object-detection-on-sun-rgbd-val, panoptic-segmentation-panopticndt-instances, scene-segmentation-on-sun-rgbd, monocular-depth-estimation-on-sun-rgbd, 3d-object-detection-on-sun-rgbd, room-layout-estimation-on-sun-rgb-d, panoptic-segmentation-on-sun-rgbd",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
Visual_Genome,Visual Genome Dataset,"Visual Genome contains Visual Question Answering data in a multi-choice setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs, 17 questions per image on average. Compared to the Visual Question Answering dataset, Visual Genome represents a more balanced distribution over 6 question types: What, Where, When, Who, Why and How. The Visual Genome dataset also presents 108K images with densely annotated objects, attributes and relationships.",https://arxiv.org/abs/1903.12314,EditCC BY 4.0,"Graph, Image, Text",English,,,,174 images,,,"Layout-to-Image Generation, Object Detection, Predicate Classification, Dense Captioning, Scene Graph Generation, Visual Relationship Detection, Unbiased Scene Graph Generation, Multi-label Image Recognition with Partial Labels, Bidirectional Relationship Classification, Phrase Grounding, Scene Graph Detection, Visual Question Answering (VQA), Unsupervised semantic parsing, Unsupervised KG-to-Text Generation, Image Generation from Scene Graphs, Scene Graph Classification","scene-graph-classification-on-visual-genome, predicate-classification-on-visual-genome, layout-to-image-generation-on-visual-genome-3, visual-question-answering-on-visual-genome-1, bidirectional-relationship-classification-on, scene-graph-generation-on-visual-genome, visual-question-answering-on-visual-genome, layout-to-image-generation-on-visual-genome-4, dense-captioning-on-visual-genome, unsupervised-semantic-parsing-on-vg-graph, image-generation-from-scene-graphs-on-visual, multi-label-image-recognition-with-partial-2, unbiased-scene-graph-generation-on-visual, object-detection-on-visual-genome, unsupervised-kg-to-text-generation-on-vg, phrase-grounding-on-visual-genome, layout-to-image-generation-on-visual-genome-2, scene-graph-detection-on-visual-genome, visual-relationship-detection-on-visual",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
Waymo_Open_Dataset,Waymo Open Dataset Dataset,"The Waymo Open Dataset is comprised of high resolution sensor data collected by autonomous vehicles operated by the Waymo Driver in a wide variety of conditions. 

The Waymo Open Dataset currently contains 1,950 segments. The authors plan to grow this dataset in the future. Currently the datasets includes:


1,950 segments of 20s each, collected at 10Hz (390,000 frames) in diverse geographies and conditions
Sensor data
1 mid-range lidar
4 short-range lidars
5 cameras (front and sides)
Synchronized lidar and camera data
Lidar to camera projections
Sensor calibrations and vehicle poses


Labeled data
Labels for 4 object classes - Vehicles, Pedestrians, Cyclists, Signs
High-quality labels for lidar data in 1,200 segments
12.6M 3D bounding box labels with tracking IDs on lidar data
High-quality labels for camera data in 1,000 segments
11.8M 2D bounding box labels with tracking IDs on camera data",https://production-media.paperswithcode.com/datasets/waymo.jpg,EditCustom (non-commercial),"3D, Image, Video",,,,,,,,"Object Detection, 3D Multi-Object Tracking, 3D Semantic Segmentation, Video Object Detection, Autonomous Driving, 3D Human Pose Estimation, Multiple Object Tracking, 3D Object Detection From Monocular Images, 3D Object Detection","3d-object-detection-on-waymo-all-ns, 3d-object-detection-on-waymo-vehicle, video-object-detection-on-waymo-open-dataset, 3d-semantic-segmentation-on-waymo-open, 3d-multi-object-tracking-on-waymo-open-1, 3d-object-detection-on-waymo-open-dataset, object-detection-on-waymo-open-dataset, 3d-multi-object-tracking-on-waymo-open, 3d-object-detection-on-waymo-cyclist, object-detection-on-waymo-2d-detection-all-ns, 3d-human-pose-estimation-on-waymo-open, 3d-object-detection-on-waymo-pedestrian, object-detection-on-waymo-2d-detection-all-ns-1, 3d-object-detection-from-monocular-images-on-6, multiple-object-tracking-on-waymo-open",,See all 1951 tasks,Object Detection401 benchmarks,Object Detection401 benchmarks
CIFAR10-DVS,CIFAR10-DVS Dataset,"CIFAR10-DVS is an event-stream dataset for object classification. 10,000 frame-based images that come from CIFAR-10 dataset are converted into 10,000 event streams with an event-based sensor, whose resolution is 128×128 pixels. The dataset has an intermediate difficulty with 10 different classes. The repeated closed-loop smooth (RCLS) movement of frame-based images is adopted to implement the conversion. Due to the transformation, they produce rich local intensity changes in continuous time which are quantized by each pixel of the event-based camera.",https://arxiv.org/abs/2008.06204,EditUnknown,Image,,,,,,,,"Object Recognition, Event data classification","event-data-classification-on-cifar10-dvs-1, object-recognition-on-cifar10-dvs",,See all 1951 tasks,Object Recognition14 benchmark,Object Recognition14 benchmark
DVS128_Gesture,DVS128 Gesture Dataset,Comprises 11 hand gesture categories from 29 subjects under 3 illumination conditions.,/paper/a-low-power-fully-event-based-gesture,EditUnknown,"Image, Text, Video",English,,,,,,,"Gesture Recognition, Object Recognition, Gesture Generation, Event data classification, Action Recognition, Image Classification, Question Answering","image-classification-on-dvs128-gesture, action-recognition-on-dvs128-gesture, gesture-generation-on-dvs128-gesture, object-recognition-on-dvs128-gesture, gesture-recognition-on-dvs128-gesture, event-data-classification-on-dvs128-gesture",,See all 1951 tasks,Object Recognition14 benchmark,Object Recognition14 benchmark
N-Caltech_101,N-Caltech 101 Dataset,"The Neuromorphic-Caltech101 (N-Caltech101) dataset is a spiking version of the original frame-based Caltech101 dataset. The original dataset contained both a ""Faces"" and ""Faces Easy"" class, with each consisting of different versions of the same images. The ""Faces"" class has been removed from N-Caltech101 to avoid confusion, leaving 100 object classes plus a background class. The N-Caltech101 dataset was captured by mounting the ATIS sensor on a motorized pan-tilt unit and having the sensor move while it views Caltech101 examples on an LCD monitor as shown in the video below. A full description of the dataset and how it was created can be found in the paper below. Please cite this paper if you make use of the dataset.",https://production-media.paperswithcode.com/datasets/728431c5-e73a-4786-abd2-9e420fe80319.jpg,EditCreative Commons 4.0,Image,,,,,101 examples,,,"Object Recognition, Image Classification, Classification, Event data classification","event-data-classification-on-n-caltech-101, object-recognition-on-n-caltech-101, image-classification-on-n-caltech-101",,See all 1951 tasks,Object Recognition14 benchmark,Object Recognition14 benchmark
OCID,OCID Dataset,"Developing robot perception systems for handling objects in the real-world requires computer vision algorithms to be carefully scrutinized with respect to the expected operating domain. This demands large quantities of ground truth data to rigorously evaluate the performance of algorithms.

The Object Cluttered Indoor Dataset is an RGBD-dataset containing point-wise labeled point-clouds for each object. The data was captured using two ASUS-PRO Xtion cameras that are positioned at different heights. It captures diverse settings of objects, background, context, sensor to scene distance, viewpoint angle and lighting conditions. The main purpose of OCID is to allow systematic comparison of existing object segmentation methods in scenes with increasing amount of clutter. In addition OCID does also provide ground-truth data for other vision tasks like object-classification and recognition.",https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/,EditUnknown,Image,,,,,,,,"Object Recognition, Instance Segmentation, Semantic Segmentation, Unseen Object Instance Segmentation",unseen-object-instance-segmentation-on-ocid,,See all 1951 tasks,Object Recognition14 benchmark,Object Recognition14 benchmark
shape_bias,shape bias Dataset,"The 'shape bias' dataset was introduced in Geirhos et al. (ICLR 2019) and consists of 224x224 images with conflicting texture and shape information (e.g., cat shape with elephant texture). This is used to measure the shape vs. texture bias of image classifiers.",https://production-media.paperswithcode.com/datasets/9fb36d92-7ca3-49c6-a376-bb3114432da8.png,EditCC-BY 4.0,Image,,2019,,,224 images,,,"Object Recognition, Out-of-Distribution Generalization, Domain Generalization",object-recognition-on-shape-bias,,See all 1951 tasks,Object Recognition14 benchmark,Object Recognition14 benchmark
Event-Camera_Dataset,Event-Camera Dataset Dataset,"The Event-Camera Dataset is a collection of datasets with an event-based camera for high-speed robotics. The data also include intensity images, inertial measurements, and ground truth from a motion-capture system. An event-based camera is a revolutionary vision sensor with three key advantages: a measurement rate that is almost 1 million times faster than standard cameras, a latency of 1 microsecond, and a high dynamic range of 130 decibels (standard cameras only have 60 dB). These properties enable the design of a new class of algorithms for high-speed robotics, where standard cameras suffer from motion blur and high latency. All the data are released both as text files and binary (i.e., rosbag) files.",/paper/the-event-camera-dataset-and-simulator-event,EditCC BY-NC-SA 3.0,"3D, Image, Video",,,,,,,,"Optical Flow Estimation, Visual Odometry, Event-Based Video Reconstruction, Object Tracking, Video Reconstruction","video-reconstruction-on-event-camera-dataset, event-based-video-reconstruction-on-event",,See all 1951 tasks,Object Tracking89 benchmarks72,Object Tracking89 benchmarks72
GOT-10k,GOT-10k Dataset,"The GOT-10k dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labelled bounding boxes. The dataset contains more than 560 classes of real-world moving objects and 80+ classes of motion patterns.",http://got-10k.aitestunion.com/,EditCC BY-NC-SA 4.0,"Image, Video",,,,,,,560,"Visual Object Tracking, Video Object Tracking, Object Tracking","video-object-tracking-on-got-10k-1, visual-object-tracking-on-got-10k",,See all 1951 tasks,Object Tracking89 benchmarks72,Object Tracking89 benchmarks72
LaSOT,LaSOT Dataset,"LaSOT is a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT one of the largest densely annotated
tracking benchmark. The average video length of LaSOT
is more than 2,500 frames, and each sequence comprises
various challenges deriving from the wild where target objects may disappear and re-appear again in the view.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_16.27.06.png,EditApache 2.0,"Image, Video",,,,,,,,"Visual Object Tracking, Zero-Shot Single Object Tracking, Object Tracking, Visual Tracking","visual-object-tracking-on-lasot, zero-shot-single-object-tracking-on-lasot, visual-tracking-on-lasot",,See all 1951 tasks,Object Tracking89 benchmarks72,Object Tracking89 benchmarks72
MOTChallenge,MOTChallenge Dataset,"The MOTChallenge datasets are designed for the task of multiple object tracking. There are several variants of the dataset released each year, such as MOT15, MOT17, MOT20.",https://arxiv.org/pdf/1504.01942v1.pdf,EditCC BY-NC-SA 3.0,"Image, Video",,,,,,,,"Multiple Object Tracking with Transformer, Multi-Object Tracking, Online Multi-Object Tracking, Object Tracking, Multiple Object Tracking","multi-object-tracking-on-mot15, online-multi-object-tracking-on-mot16, multiple-object-tracking-with-transformer-on, multi-object-tracking-on-mot17, online-multi-object-tracking-on-mot17, multi-object-tracking-on-mot20-1, online-multi-object-tracking-on-mot15, multi-object-tracking-on-mot16",,See all 1951 tasks,Object Tracking89 benchmarks72,Object Tracking89 benchmarks72
UA-DETRAC,UA-DETRAC Dataset,"Consists of 100 challenging video sequences captured from real-world traffic scenes (over 140,000 frames with rich annotations, including occlusion, weather, vehicle category, truncation, and vehicle bounding boxes) for object detection, object tracking and MOT system.",/paper/ua-detrac-a-new-benchmark-and-protocol-for,EditUnknown,"Image, Video",,,,,,,,"Object Detection, Object Tracking, Multiple Object Tracking",object-detection-on-ua-detrac,,See all 1951 tasks,Object Tracking89 benchmarks72,Object Tracking89 benchmarks72
UAVDT,UAVDT Dataset,"UAVDT is a large scale challenging UAV Detection and Tracking benchmark (i.e., about 80, 000 representative frames from 10 hours raw videos) for 3 important fundamental tasks, i.e., object DETection
(DET), Single Object Tracking (SOT) and Multiple Object Tracking (MOT).

The dataset is captured by UAVs in various complex scenarios. The objects of
interest in this benchmark are vehicles. The frames are manually annotated with bounding boxes and some useful attributes, e.g., vehicle category and occlusion. 

The UAVDT benchmark consists of 100 video sequences, which are selected
from over 10 hours of videos taken with an UAV platform at a number of locations in urban areas, representing various common scenes including squares, arterial streets, toll stations, highways, crossings and T-junctions. The videos
are recorded at 30 frames per seconds (fps), with the JPEG image resolution of 1080 × 540 pixels.",https://production-media.paperswithcode.com/datasets/uavdt.jpg,EditCustom (research-only),"Image, Video",,,,,,,,"Object Detection, Object Tracking, Multi-Object Tracking","object-detection-on-uavdt, multi-object-tracking-on-uavdt",,See all 1951 tasks,Object Tracking89 benchmarks72,Object Tracking89 benchmarks72
VOT2017,VOT2017 Dataset,VOT2017 is a Visual Object Tracking dataset for different tasks that contains 60 short sequences annotated with 6 different attributes.,https://arxiv.org/abs/1909.06800,EditUnknown,"Image, Video",,,,,,,,"Visual Object Tracking, Object Tracking","visual-object-tracking-on-vot201718, visual-object-tracking-on-vot2017",,See all 1951 tasks,Object Tracking89 benchmarks72,Object Tracking89 benchmarks72
VOT2018,VOT2018 Dataset,VOT2018 is a dataset for visual object tracking. It consists of 60 challenging videos collected from real-life datasets.,https://arxiv.org/abs/1905.06648,EditUnknown,"Image, Video",,,,,,,,"Visual Object Tracking, Object Tracking",visual-object-tracking-on-vot2018,,See all 1951 tasks,Object Tracking89 benchmarks72,Object Tracking89 benchmarks72
Occluded_Person_Re-Identification23_papers_with_co,Occluded Person Re-Identification23 papers with code Dataset,,https://paperswithcode.com/dataset/occluded-person-re-identification,,,,,,,,,,,,,See all 1951 tasks,Occluded Person Re-Identificat,Occluded Person Re-Identificat
Off-policy_evaluation98_papers_with_code_Dataset,Off-policy evaluation98 papers with code Dataset,,https://paperswithcode.com/dataset/off-policy-evaluation,,,,,,,,,,,,,See all 1951 tasks,Off-policy evaluation98 papers,Off-policy evaluation98 papers
One-Class_Classification69_papers_with_code_Datase,One-Class Classification69 papers with code Dataset,,https://paperswithcode.com/dataset/one-class-classification,,,,,,,,,,,,,See all 1951 tasks,One-Class Classification69 pap,One-Class Classification69 pap
MatSim,MatSim Dataset,"MatSim is a synthetic dataset, and natural image benchmark for computer vision-based recognition of similarities and transitions between materials and textures, focusing on identifying any material under any conditions using one or a few examples (one-shot learning), including materials states and subclasses.",https://arxiv.org/pdf/2212.00648v1.pdf,EditMIT,Image,,,,,,,,"One-Shot Learning, Contrastive Learning, Material Recognition",,,See all 1951 tasks,One-Shot Learning1 benchmark10,One-Shot Learning1 benchmark10
TACO,TACO Dataset,"TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches. These images are manually labelled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms. The annotations are provided in COCO format.",https://github.com/pedropro/TACO,EditUnknown,Image,,,,,,,,"Instance Segmentation, One-Shot Learning, Semantic Segmentation",,,See all 1951 tasks,One-Shot Learning1 benchmark10,One-Shot Learning1 benchmark10
TopLogo-10,TopLogo-10 Dataset,Collected from top 10 most popular clothing/wearable brandname logos captured in rich visual context.,/paper/deep-learning-logo-detection-with-data,EditUnknown,Image,,,,,,,,"Metric Learning, Incremental Learning, Traffic Sign Recognition, One-Shot Learning",traffic-sign-recognition-on-toplogo-10,,See all 1951 tasks,One-Shot Learning1 benchmark10,One-Shot Learning1 benchmark10
FineAction,FineAction Dataset,"FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. FineAction introduces new opportunities and challenges for temporal action localization, thanks to its distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes.",https://production-media.paperswithcode.com/datasets/refinedaction.gif,EditUnknown,"Image, Text, Time Series, Video",English,,,,,,,"Temporal Action Localization, Temporal Action Proposal Generation, Weakly Supervised Temporal Action Localization, Online Action Detection, Weakly Supervised Action Localization","temporal-action-localization-on-fineaction, online-action-detection-on-fineaction, weakly-supervised-action-localization-on-7",,See all 1951 tasks,Online Action Detection3 bench,Online Action Detection3 bench
Online_nonnegative_CP_decomposition1_papers_with_c,Online nonnegative CP decomposition1 papers with code Dataset,,https://paperswithcode.com/dataset/online-nonnegative-cp-decomposition,,,,,,,,,,,,,See all 1951 tasks,Online nonnegative CP decompos,Online nonnegative CP decompos
Description_Detection_Dataset,Description Detection Dataset Dataset,"Description Detection Dataset ($D^3$, /dikju:b/) is an attempt at creating a next-generation object detection dataset. Unlike traditional detection datasets, the class names of the objects are no longer simple nouns or noun phrases, but rather complex and descriptive, such as a dog not being held by a leash. For each image in the dataset, any object that matches the description is annotated. The dataset provides annotations such as bounding boxes and finely crafted instance masks.It comprises of 422 well-designed descriptions and 24,282 positive object-description pairs.

The dataset is meant for the Described Object Detection (DOD) task. OVD detects object based on category name, and each category can have zero to multiple instances; REC grounds one region based on a language description, whether the object truly exits or not; DOD detects all instances on each image in the dataset, based on a flexible reference.",https://production-media.paperswithcode.com/datasets/6f78b6d6-6b11-4e49-b659-c217b3086776.jpg,EditCreative Commons Attribution-NonCommercial 4.0 International License,Image,,,,,,,,"Object Detection, Open Vocabulary Object Detection, Referring Expression Comprehension, Described Object Detection",described-object-detection-on-description,,See all 1951 tasks,Open Vocabulary Object Detecti,Open Vocabulary Object Detecti
MSCOCO,MSCOCO Dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Object Detection, Cross-Modal Retrieval, Real-time Instance Segmentation, Zero-Shot Object Detection, Open Vocabulary Object Detection, mage-to-Text Retrieval, Image Retrieval, Image Captioning, Multi-Label Image Classification, Weakly Supervised Object Detection, Paraphrase Generation, Image Outpainting, Few Shot Open Set Object Detection","open-vocabulary-object-detection-on-mscoco, zero-shot-object-detection-on-mscoco, paraphrase-generation-on-mscoco, object-detection-on-mscoco-7, real-time-instance-segmentation-on-mscoco, few-shot-open-set-object-detection-on-mscoco, mage-to-text-retrieval-on-mscoco, cross-modal-retrieval-on-mscoco, image-outpainting-on-mscoco, multi-label-image-classification-on-mscoco, weakly-supervised-object-detection-on-mscoco, image-captioning-on-mscoco-1, object-detection-on-mscoco-6, image-retrieval-on-mscoco",,See all 1951 tasks,Open Vocabulary Object Detecti,Open Vocabulary Object Detecti
Objects365,Objects365 Dataset,"Objects365 is a large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community.",https://paperswithcode.com/paper/objects365-a-large-scale-high-quality-dataset,"EditCustom (research-only, attribution)",Image,,,,,,,,"Object Detection, Open Vocabulary Object Detection, Instance Segmentation, Semantic Segmentation, Unsupervised Object Detection","open-vocabulary-object-detection-on-1, unsupervised-object-detection-on-objects365, object-detection-on-objects365",,See all 1951 tasks,Open Vocabulary Object Detecti,Open Vocabulary Object Detecti
OVAD_benchmark,OVAD benchmark Dataset,"Vision-language modeling has enabled open-vocabulary tasks where predictions can be queried using any text prompt in a zero-shot manner. Existing open-vocabulary tasks focus on object classes, whereas research on object attributes is limited due to the lack of a reliable attribute-focused evaluation benchmark. This paper introduces the Open-Vocabulary Attribute Detection (OVAD) task and the corresponding OVAD benchmark. The objective of the novel task and benchmark is to probe object-level attribute information learned by vision-language models. To this end, we created a clean and densely annotated test set covering 117 attribute classes on the 80 object classes of MS COCO. It includes positive and negative annotations, which enables open-vocabulary evaluation. Overall, the benchmark consists of 1.4 million annotations. For reference, we provide a first baseline method for open-vocabulary attribute detection. Moreover, we demonstrate the benchmark's value by studying the attribute detection performance of several foundation models.",https://arxiv.org/pdf/2211.12914v1.pdf,EditCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,"Image, Text",English,,,,,,,"Language Modelling, Visual Question Answering (VQA), Open Vocabulary Object Detection, Open Vocabulary Attribute Detection","open-vocabulary-attribute-detection-on-ovad, visual-question-answering-vqa-on-ovad, open-vocabulary-attribute-detection-on-ovad-1",,See all 1951 tasks,Open Vocabulary Object Detecti,Open Vocabulary Object Detecti
OVIC_Datasets,OVIC Datasets Dataset,"Due to the free-form nature of the open vocabulary image classification task, special annotations are required for image sets used for evaluation purposes. Three such image datasets are presented here:


World: 272 images of which the grand majority are originally sourced (have never been on the internet) from 10 countries by 12 people, with an active focus on covering as wide and varied concepts as possible, including unusual, deceptive and/or indirect representations of objects,
Wiki: 1000 Wikipedia lead images sampled from a scraped pool of 18K,
Val3K: 3000 images from the ImageNet-1K validation set, sampled uniformly across the classes.

It is not in general possible to exhaustively annotate ground truth classification labels for open vocabulary image sets, as this would require annotations for every possible correct object noun in the English language for every visible entity in every part of every image. It is possible however, to annotate the thousands of predictions that have been made across the image sets by open vocabulary models trained thus far. All three image datasets presented here have been individually annotated by both human and multimodal LLM annotators for the object nouns that were predicted by trained models. The annotations specify whether each classification is correct, close, or incorrect, and for the human annotations, whether it relates to a primary or secondary element of the image. It is customary to use the suffixes -H and -L to clearly specify which annotations are being referred to at any time, e.g. Wiki-H is the Wiki dataset with corresponding human annotations. All three datasets together contain a total of 17.4K human and 112K LLM class annotations.

The data is directly available at the following links:


World dataset
Wiki dataset
Val3K dataset

Refer to the NOVIC code for an example of how the datasets can be used, as well as tools for updating the class annotations for newer model predictions.",https://production-media.paperswithcode.com/datasets/297655b7-b8df-4849-a062-b93f6d619089.jpg,EditCC BY-NC-SA 4.0,Image,,,,,272 images,Val3K: 3000 images,,"Open Vocabulary Image Classification, Open Vocabulary Object Detection, Zero-Shot Image Classification","open-vocabulary-image-classification-on-ovic-3, open-vocabulary-image-classification-on-ovic-1, open-vocabulary-image-classification-on-ovic, open-vocabulary-image-classification-on-ovic-2",,See all 1951 tasks,Open Vocabulary Object Detecti,Open Vocabulary Object Detecti
105_941_Images_Natural_Scenes_OCR_Data_of_12_Langu,"105,941 Images Natural Scenes OCR Data of 12 Languages Dataset","Description:
105,941 Images Natural Scenes OCR Data of 12 Languages. The data covers 12 languages (6 Asian languages, 6 European languages), multiple natural scenes, multiple photographic angles. For annotation, line-level quadrilateral bounding box annotation and transcription for the texts were annotated in the data. The data can be used for tasks such as OCR of multi-language.

Data size:
105,941 images, including Asian language family: Japanese 9,997 images, Korean 10,231 images, Indonesian 7,591 images, Malay 5,650 images, Vietnamese 8,822 images, Thai 9,645 images; European language family: French 10,015 images, German 7,213 images, Italian 8,824 images, Portuguese 7,754 images, Russian 10,376 images and Spanish 9,823 images

Collecting environment:
including shop plaque, stop board, poster, ticket, road sign, comic, cover picture, prompt/reminder, warning, packing instruction, menu, building sign, etc.",https://production-media.paperswithcode.com/datasets/dbcf8312-2b10-4cae-942c-aa5372676c42.png,EditCommercial license,Image,,,,,941 images,,,Optical Character Recognition (OCR),,,See all 1951 tasks,Optical Character Recognition ,Optical Character Recognition 
DocBank,DocBank Dataset,A benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the \LaTeX{} documents available on the arXiv.com.,/paper/docbank-a-benchmark-dataset-for-document,EditUnknown,"Image, Text",English,,,,,,,"Optical Character Recognition (OCR), Document Layout Analysis",,,See all 1951 tasks,Optical Character Recognition ,Optical Character Recognition 
FUNSD,FUNSD Dataset,"Form Understanding in Noisy Scanned Documents (FUNSD) comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking.",/paper/190513538,EditCustom,"Graph, Image, Tabular, Text",English,,,,,,,"Table Detection, Named Entity Recognition (NER), Semantic entity labeling, Entity Linking, Relation Extraction, Optical Character Recognition (OCR)","relation-extraction-on-funsd, semantic-entity-labeling-on-funsd, entity-linking-on-funsd",,See all 1951 tasks,Optical Character Recognition ,Optical Character Recognition 
ICDAR_2003,ICDAR 2003 Dataset,The ICDAR2003 dataset is a dataset for scene text recognition. It contains 507 natural scene images (including 258 training images and 249 test images) in total. The images are annotated at character level. Characters and words can be cropped from the images.,https://arxiv.org/abs/1512.08669,EditCustom (research-only),"Image, Text",English,,,,,training images and 249 test images,,"Optical Character Recognition (OCR), Scene Text Recognition",scene-text-recognition-on-icdar-2003,,See all 1951 tasks,Optical Character Recognition ,Optical Character Recognition 
SciTSR,SciTSR Dataset,"SciTSR is a large-scale table structure recognition dataset, which contains 15,000 tables in PDF format and their corresponding structure labels obtained from LaTeX source files.",https://github.com/Academic-Hammer/SciTSR,EditUnknown,"Image, Tabular",,,,,,,,"Optical Character Recognition (OCR), Table Detection, Information Retrieval",,,See all 1951 tasks,Optical Character Recognition ,Optical Character Recognition 
ST-VQA,ST-VQA Dataset,ST-VQA aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the VQA process.,/paper/scene-text-visual-question-answering,EditUnknown,"Image, Text",English,,,,,,,"Visual Question Answering (VQA), Optical Character Recognition (OCR), Question Answering",,,See all 1951 tasks,Optical Character Recognition ,Optical Character Recognition 
TextCaps,TextCaps Dataset,"Contains 145k captions for 28k images. The dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects.",/paper/textcaps-a-dataset-for-image-captioning-with,EditUnknown,"Image, Text",English,,,,28k images,,,"Visual Reasoning, Optical Character Recognition (OCR), Image Captioning",image-captioning-on-textcaps-2020,,See all 1951 tasks,Optical Character Recognition ,Optical Character Recognition 
TextOCR,TextOCR Dataset,"TextOCR is a dataset to benchmark text recognition on arbitrary shaped scene-text. TextOCR requires models to perform text-recognition on arbitrary shaped scene-text present on natural images. TextOCR provides ~1M high quality word annotations on TextVQA images allowing application of end-to-end reasoning on downstream tasks such as visual question answering or image captioning.

Dataset statistics:


28,134 natural images from TextVQA
903,069 annotated scene-text words
32 words per image on average",https://production-media.paperswithcode.com/datasets/Screenshot_2021-05-13_at_10.17.05.jpg,EditCC BY 4.0,"Image, Text",English,,,,,,,"Scene Text Detection, Optical Character Recognition (OCR), Scene Text Recognition",,,See all 1951 tasks,Optical Character Recognition ,Optical Character Recognition 
UFPR-ALPR,UFPR-ALPR Dataset,"This dataset includes 4,500 fully annotated images (over 30,000 license plate characters) from 150 vehicles in real-world scenarios where both the vehicle and the camera (inside another vehicle) are moving.

The images were acquired with three different cameras and are available in the Portable Network Graphics (PNG) format with a size of 1,920 × 1,080 pixels. The cameras used were: GoPro Hero4 Silver, Huawei P9 Lite, and iPhone 7 Plus.

We collected 1,500 images with each camera, divided as follows:

- 900 of cars with gray license plates;
- 300 of cars with red license plates;
- 300 of motorcycles with gray license plates.

The dataset is split as follows: 40% for training, 40% for testing and 20% for validation. Every image has the following annotations available in a text file: the camera in which the image was taken, the vehicle’s position and information such as type (car or motorcycle), manufacturer, model and year; the identification and position of the license plate, as well as the position of its characters.",/paper/a-robust-real-time-automatic-license-plate,EditResearch Only,"Image, Text, Video",English,,,,500 images,,,"License Plate Recognition, Object Tracking, License Plate Detection, Optical Character Recognition (OCR), Scene Text Recognition",license-plate-recognition-on-ufpr-alpr,,See all 1951 tasks,Optical Character Recognition ,Optical Character Recognition 
FlyingThings3D,FlyingThings3D Dataset,"FlyingThings3D is a synthetic dataset for optical flow, disparity and  scene flow estimation. It consists of everyday objects flying along randomized 3D trajectories. We generated about 25,000 stereo frames with ground truth data. Instead of focusing on a particular task (like KITTI) or enforcing strict naturalism (like Sintel), we rely on randomness and a large pool of rendering assets to generate orders of magnitude more data than any existing option, without running a risk of repetition or saturation.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-05-18_at_22.56.03.jpg,EditCustom (research-only),Video,,,,,,,,"Scene Flow Estimation, Optical Flow Estimation, Disparity Estimation",,,See all 1951 tasks,Optical Flow Estimation10 benc,Optical Flow Estimation10 benc
MVSEC,MVSEC Dataset,"The Multi Vehicle Stereo Event Camera (MVSEC) dataset is a collection of data designed for the development of novel 3D perception algorithms for event based cameras. Stereo event data is collected from car, motorbike, hexacopter and handheld data, and fused with lidar, IMU, motion capture and GPS to provide ground truth pose and depth images.",/paper/ev-flownet-self-supervised-optical-flow,EditUnknown,"3D, Video",,,,,,,,"Event-based Optical Flow, Optical Flow Estimation, Video Reconstruction, Self-Supervised Learning","video-reconstruction-on-mvsec, event-based-optical-flow-on-mvsec",,See all 1951 tasks,Optical Flow Estimation10 benc,Optical Flow Estimation10 benc
N-CARS,N-CARS Dataset,A large real-world event-based dataset for object classification.,/paper/hats-histograms-of-averaged-time-surfaces-for,EditCustom,"Image, Video",,,,,,,,"Object Recognition, Optical Flow Estimation, Classification","object-recognition-on-n-cars, classification-on-n-cars",,See all 1951 tasks,Optical Flow Estimation10 benc,Optical Flow Estimation10 benc
VisDrone,VisDrone Dataset,"VisDrone is a large-scale benchmark with carefully annotated ground-truth for various important computer vision tasks, to make vision meet drones. The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining, Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). Note that, the dataset was collected using various drone platforms (i.e., drones with different models), in different scenarios, and under various weather and lighting conditions. These frames are manually annotated with more than 2.6 million bounding boxes of targets of frequent interests, such as pedestrians, cars, bicycles, and tricycles. Some important attributes including scene visibility, object class and occlusion, are also provided for better data utilization.",https://github.com/VisDrone/VisDrone-Dataset,EditUnknown,"Image, Video",,,,,,,,"Object Detection, Optical Flow Estimation",object-detection-on-visdrone-det2019-1,,See all 1951 tasks,Optical Flow Estimation10 benc,Optical Flow Estimation10 benc
ATIS,ATIS Dataset,"The ATIS (Airline Travel Information Systems) is a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories. The original split contains 4478, 500 and 893 intent-labeled reference utterances in train, development and test set respectively.",https://arxiv.org/abs/1904.03576,EditUnknown,"Image, Text",English,,,,,,,"Slot Filling, Out of Distribution (OOD) Detection, Open Intent Discovery, SQL Parsing, Intent Detection, Semantic Parsing, Intent Discovery","sql-parsing-on-atis, semantic-parsing-on-atis, out-of-distribution-ood-detection-on-atis, intent-detection-on-atis, intent-discovery-on-atis, open-intent-discovery-on-atis, slot-filling-on-atis",,See all 1951 tasks,Out of Distribution  OOD  Dete,Out of Distribution  OOD  Dete
FathomNet2023,FathomNet2023 Dataset,"The FathomNet2023 competition dataset is a subset of the broader FathomNet marine image repository. The training and test images for the competition were all collected in the Monterey Bay Area between the surface and 1300 meters depth by the Monterey Bay Aquarium Research Institute. The images contain bounding box annotations of 290 categories of bottom dwelling animals. The training and validation data are split across an 800 meter depth threshold: all training data is collected from 0-800 meters, evaluation data comes from the whole 0-1300 meter range. Since an organisms' habitat range is partially a function of depth, the species distributions in the two regions are overlapping but not identical. Test images are drawn from the same region but may come from above or below the depth horizon. The competition goal is to label the animals present in a given image (i.e. multi-label classification) and determine whether the image is out-of-sample.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-Non Commercial-No Derivatives 4.0 International License,Image,,,,,,training and test images for the competition were all collected in the Monterey Bay Area between the surface and 1300 meters depth by the Monterey Bay Aquarium Research Institute. The images,290,"Image Classification, 2D Object Detection, Out of Distribution (OOD) Detection",,,See all 1951 tasks,Out of Distribution  OOD  Dete,Out of Distribution  OOD  Dete
ImageNet-1k_vs_NINCO,ImageNet-1k vs NINCO Dataset,"The NINCO (No ImageNet Class Objects) dataset is introduced in the ICML 2023 paper In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation. The images in this dataset are free from objects that belong to any of the 1000 classes of ImageNet-1K (ILSVRC2012), which makes NINCO suitable for evaluating out-of-distribution detection on ImageNet-1K .

The NINCO main dataset consists of 64 OOD classes with a total of 5879 samples. These OOD classes were selected to have no categorical overlap with any classes of ImageNet-1K. Each sample was inspected individually by the authors to not contain ID objects.

Besides NINCO, included are (in the same .tar.gz file) truly OOD versions of 11 popular OOD datasets with in total 2715 OOD samples.

Further included are 17 OOD unit-tests, with 400 samples each.

Code for loading and evaluating on each of the three datasets is provided at https://github.com/j-cb/NINCO.

When using NINCO, please consider citing (besides the bibtex given below) the following data sources that were used to create NINCO:

Hendrycks et al.: ”Scaling out-of-distribution detection for real-world settings”, ICML, 2022.  
Bossard et al.: ”Food-101 – mining discriminative components with random forests”, ECCV 2014.  
Zhou et al.: ”Places: A 10 million image database for scene recognition”, IEEE PAMI 2017.  
Huang et al.: ”Mos: Towards scaling out-of-distribution detection for large semantic space”, CVPR 2021.  
Li et al.: ”Caltech 101 (1.0)”, 2022.
Ismail et al.: ”MYNursingHome: A fully-labelled image dataset for indoor object classification.”, Data in Brief (V. 32) 2020.
The iNaturalist project: https://www.inaturalist.org/

When using NINCO_popular_datasets_subsamples, additionally to the above, please consider citing:

Cimpoi et al.: ”Describing textures in the wild”, CVPR 2014.  
Hendrycks et al.: ”Natural adversarial examples”, CVPR 2021.  
Wang et al.: ”Vim: Out-of-distribution with virtual-logit matching”, CVPR 2022.  
Bendale et al.: ”Towards Open Set Deep Networks”, CVPR 2016.  
Vaze et al.: ”Open-set Recognition: a Good Closed-set Classifier is All You Need?”, ICLR 2022.  
Wang et al.: ”Partial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition.” ICML, 2022.  
Galil et al.: “A framework for benchmarking Class-out-of-distribution detection and its application to ImageNet”, ICLR 2023.",https://production-media.paperswithcode.com/datasets/cf559e20-79f3-42e8-9c92-d3e088381871.png,EditCreative Commons Attribution 4.0 International,Image,English,2023,,,5879 samples,"tests, with 400 samples",1000,"Anomaly Detection, Out of Distribution (OOD) Detection, Open Set Learning, Out-of-Distribution Detection",out-of-distribution-detection-on-imagenet-1k-13,,See all 1951 tasks,Out of Distribution  OOD  Dete,Out of Distribution  OOD  Dete
ImageNet-O,ImageNet-O Dataset,ImageNet-O consists of images from classes that are not found in the ImageNet-1k dataset. It is used to test the robustness of vision models to out-of-distribution samples. It's reported using the AUPR metric.,https://production-media.paperswithcode.com/datasets/Screenshot_2021-05-18_at_11.05.34_AM.png,EditMIT,Image,English,,,,,,,"Outlier Detection, Image Classification, Out of Distribution (OOD) Detection",,,See all 1951 tasks,Out of Distribution  OOD  Dete,Out of Distribution  OOD  Dete
MUAD,MUAD Dataset,"The MUAD dataset (Multiple Uncertainties for Autonomous Driving), consisting of 10,413 realistic synthetic images with diverse adverse weather conditions (night, fog, rain, snow), out-of-distribution objects, and annotations for semantic segmentation, depth estimation, object, and instance detection. Predictive uncertainty estimation is essential for the safe deployment of Deep Neural Networks in real-world autonomous systems and MUAD allows to a better assess the impact of different sources of uncertainty on model performance.",https://production-media.paperswithcode.com/datasets/cb9ad779-002b-48ef-a08b-ab2cc32482fb.png,EditUnknown,"3D, Image",,,,,,,,"Object Detection, Monocular Depth Estimation, Out of Distribution (OOD) Detection, Depth Aleatoric Uncertainty Estimation, Uncertainty Quantification, Autonomous Driving, Decision Making Under Uncertainty, Uncertainty Visualization, Semantic Segmentation, Anomaly Detection",,,See all 1951 tasks,Out of Distribution  OOD  Dete,Out of Distribution  OOD  Dete
OpenImage-O,OpenImage-O Dataset,"It is manually annotated, comes with a naturally diverse distribution, and has a large scale. It is built to overcome several shortcomings of existing OOD benchmarks. OpenImage-O is image-by-image filtered from the test set of OpenImage-V3, which has been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias.",https://production-media.paperswithcode.com/datasets/df3c1ef7-df73-4ad9-8517-62e2dc479c07.jpeg,EditUnknown,Image,,,,,,,,"Out of Distribution (OOD) Detection, Out-of-Distribution Detection",,,See all 1951 tasks,Out of Distribution  OOD  Dete,Out of Distribution  OOD  Dete
Persian-ATIS,Persian-ATIS Dataset,The PATIS is a Persian language dataset for intent detection and slot filling.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Intent Discovery, Intent Detection, slot-filling, Out of Distribution (OOD) Detection","intent-discovery-on-persian-atis, out-of-distribution-ood-detection-on-persian",,See all 1951 tasks,Out of Distribution  OOD  Dete,Out of Distribution  OOD  Dete
RMOT-223,RMOT-223 Dataset,"In this dataset, various objects are arranged on a white table. A UR5e robot picks and place a target object specified on the title of the video/image sequence. Videos under auto- folder are collected with automatic operation of the robot. Videos under human- folders are collected with the tele-operation of the robot. Ground-truth tracking bounding boxes are generated with STARK, and when the target exits the camera frame, the bounding box estimation is switched to [-1, -1, -1, -1], indicating target not shown.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,,,,"Object Tracking, Out of Distribution (OOD) Detection",,,See all 1951 tasks,Out of Distribution  OOD  Dete,Out of Distribution  OOD  Dete
SNIPS,SNIPS Dataset,"The SNIPS Natural Language Understanding benchmark is a dataset of over 16,000 crowdsourced queries distributed among 7 user intents of various complexity:


SearchCreativeWork (e.g. Find me the I, Robot television show),
GetWeather (e.g. Is it windy in Boston, MA right now?),
BookRestaurant (e.g. I want to book a highly rated restaurant in Paris tomorrow night),
PlayMusic (e.g. Play the last track from Beyoncé off Spotify),
AddToPlaylist (e.g. Add Diamonds to my roadtrip playlist),
RateBook (e.g. Give 6 stars to Of Mice and Men),
SearchScreeningEvent (e.g. Check the showtimes for Wonder Woman in Paris).
The training set contains of 13,084 utterances, the validation set and the test set contain 700 utterances each, with 100 queries per intent.",https://paperswithcode.com/paper/snips-voice-platform-an-embedded-spoken/,EditUnknown,Image,,,,,,,,"Slot Filling, Out of Distribution (OOD) Detection, Open Intent Discovery, Zero-Shot Learning, Intent Detection, Intent Discovery","slot-filling-on-snips, open-intent-discovery-on-snips, zero-shot-learning-on-snips, intent-discovery-on-snips, intent-detection-on-snips, out-of-distribution-ood-detection-on-snips",,See all 1951 tasks,Out of Distribution  OOD  Dete,Out of Distribution  OOD  Dete
S3DIS,S3DIS Dataset,The Stanford 3D Indoor Scene Dataset (S3DIS) dataset contains 6 large-scale indoor areas with 271 rooms. Each point in the scene point cloud is annotated with one of the 13 semantic categories.,https://arxiv.org/abs/1912.02984,EditUnknown,"3D, Image",,,,,,,,"3D Open-Vocabulary Instance Segmentation, Panoptic Segmentation, 3D Instance Segmentation, Generalized Zero-Shot Learning, 3D Semantic Segmentation, Semantic Segmentation, Few-shot 3D Point Cloud Semantic Segmentation, 3D Object Detection","panoptic-segmentation-on-s3dis-area5, 3d-instance-segmentation-on-s3dis, semantic-segmentation-on-s3dis-area5, 3d-open-vocabulary-instance-segmentation-on-2, generalized-zero-shot-learning-on-s3dis, panoptic-segmentation-on-s3dis, 3d-object-detection-on-s3dis, few-shot-3d-point-cloud-semantic-segmentation, 3d-semantic-segmentation-on-s3dis, semantic-segmentation-on-s3dis",,See all 1951 tasks,Panoptic Segmentation33 benchm,Panoptic Segmentation33 benchm
SemanticKITTI,SemanticKITTI Dataset,"SemanticKITTI is a large-scale outdoor-scene dataset for point cloud semantic segmentation. It is derived from the KITTI Vision Odometry Benchmark which it extends with dense point-wise annotations for the complete 360 field-of-view of the employed automotive LiDAR. The dataset consists of 22 sequences. Overall, the dataset provides 23201 point clouds for training and 20351 for testing.",https://arxiv.org/abs/2008.01550,EditCC BY-NC-SA 4.0,"3D, Image",English,,,,,,,"Semi-Supervised Semantic Segmentation, 3D Semantic Scene Completion, LIDAR Semantic Segmentation, Weakly supervised Semantic Segmentation, Panoptic Segmentation, Generalized Zero-Shot Learning, 3D Semantic Segmentation, 3D Semantic Scene Completion from a single RGB image, Real-Time 3D Semantic Segmentation, 4D Panoptic Segmentation","weakly-supervised-semantic-segmentation-on-7, real-time-3d-semantic-segmentation-on-1, semi-supervised-semantic-segmentation-on-24, 3d-semantic-scene-completion-on-semantickitti, generalized-zero-shot-learning-on, lidar-semantic-segmentation-on-semantickitti, 3d-semantic-segmentation-on-semantickitti, 4d-panoptic-segmentation-on-semantickitti, 3d-semantic-scene-completion-from-a-single-1, panoptic-segmentation-on-semantickitti",,See all 1951 tasks,Panoptic Segmentation33 benchm,Panoptic Segmentation33 benchm
CUHK-SYSU,CUHK-SYSU Dataset,"The CUKL-SYSY dataset is a large scale benchmark for person search, containing 18,184 images and 8,432 identities. Different from previous re-id benchmarks, matching query persons with manually cropped pedestrians, this dataset is much closer to real application scenarios by searching person from whole images in the gallery.",http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html,EditUnknown,Image,,,,,184 images,,,"Person Search, Person Re-Identification","person-search-on-cuhk-sysu, person-re-identification-on-cuhk-sysu",,See all 1951 tasks,Person Re-Identification68 ben,Person Re-Identification68 ben
CUHK03,CUHK03 Dataset,"The CUHK03 consists of 14,097 images of 1,467 different identities, where 6 campus cameras were deployed for image collection and each identity is captured by 2 campus cameras. This dataset provides two types of annotations, one by manually labelled bounding boxes and the other by bounding boxes produced by an automatic detector. The dataset also provides 20 random train/test splits in which 100 identities are selected for testing and the rest for training",https://arxiv.org/abs/1810.05866,EditUnknown,Image,,,,,097 images,,,"Defocus Estimation, Person Re-Identification, Generalizable Person Re-identification, Face Sketch Synthesis, Defocus Blur Detection","person-re-identification-on-cuhk03-detected, face-sketch-synthesis-on-cuhk, person-re-identification-on-cuhk03, person-re-identification-on-cuhk03-detected-1, person-re-identification-on-cuhk03-labeled, defocus-estimation-on-cuhk-blur-detection, defocus-blur-detection-on-cuhk, generalizable-person-re-identification-on-22",,See all 1951 tasks,Person Re-Identification68 ben,Person Re-Identification68 ben
MARS,MARS Dataset,"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).",https://arxiv.org/abs/1706.06196,EditUnknown,"Image, Video",,,,,,,,"Unsupervised Person Re-Identification, Person Re-Identification, Video-Based Person Re-Identification","person-re-identification-on-mars, unsupervised-person-re-identification-on-mars",,See all 1951 tasks,Person Re-Identification68 ben,Person Re-Identification68 ben
MSMT17,MSMT17 Dataset,"MSMT17 is a multi-scene multi-time person re-identification dataset. The dataset consists of 180 hours of videos, captured by 12 outdoor cameras, 3 indoor cameras, and during 12 time slots. The videos cover a long period of time and present complex lighting variations, and it contains a large number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes.

The dataset can be requested at http://www.pkuvmc.com/dataset.html",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Unsupervised Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification","unsupervised-person-re-identification-on-7, unsupervised-person-re-identification-on-12, generalizable-person-re-identification-on-20, person-re-identification-on-msmt17, unsupervised-person-re-identification-on-2, unsupervised-person-re-identification-on-3",,See all 1951 tasks,Person Re-Identification68 ben,Person Re-Identification68 ben
Occluded_REID,Occluded REID Dataset,"Occluded REID is an occluded person dataset captured by mobile cameras, consisting of 2,000 images of 200 occluded persons (see Fig. (c)). Each identity has 5 full-body person images and 5 occluded person images with different types of occlusion.",https://arxiv.org/abs/1904.04975,EditUnknown,"Graph, Image, Text",English,,,,000 images,,,"Person Re-Identification, Semantic Parsing, Graph Matching",person-re-identification-on-occluded-reid-1,,See all 1951 tasks,Person Re-Identification68 ben,Person Re-Identification68 ben
PRW,PRW Dataset,"PRW is a large-scale dataset for end-to-end pedestrian detection and person recognition in raw video frames. PRW is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities.",https://production-media.paperswithcode.com/datasets/pipeline_prw.png,EditUnknown,Image,,,,,,,,"Person Recognition, Person Search, Person Re-Identification, Pedestrian Detection",person-search-on-prw,,See all 1951 tasks,Person Re-Identification68 ben,Person Re-Identification68 ben
SYSU-MM01,SYSU-MM01 Dataset,"The SYSU-MM01 is a dataset collected for the Visible-Infrared Re-identification problem. The images in the dataset were obtained from 491 different persons by recording them using 4 RGB and 2 infrared cameras. Within the dataset, the persons are divided into 3 fixed splits to create training, validation and test sets. In the training set, there are 20284 RGB and 9929 infrared images of 296 persons. The validation set contains 1974 RGB and 1980 infrared images of 99 persons. The testing set consists of the images of 96 persons where 3803 infrared images are used as query and 301 randomly selected RGB images are used as gallery.",https://arxiv.org/abs/1907.06498,EditCustom (non-commercial),Image,,1974,,,,"splits to create training, validation and test sets. In the training set, there are 20284 RGB and 9929 infrared images",,"Person Re-Identification, Cross-Modal  Person Re-Identification","person-re-identification-on-sysu-mm01, cross-modal-person-re-identification-on-sysu",,See all 1951 tasks,Person Re-Identification68 ben,Person Re-Identification68 ben
VIPeR,VIPeR Dataset,"The Viewpoint Invariant Pedestrian Recognition (VIPeR) dataset includes 632 people and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128×48 pixels. It provides the pose angle of each person as 0° (front), 45°, 90° (right), 135°, and 180° (back).",https://arxiv.org/abs/1705.06011,EditUnknown,Image,,,,,,,,"Patch Matching, Person Re-Identification, Metric Learning",,,See all 1951 tasks,Person Re-Identification68 ben,Person Re-Identification68 ben
Point_Cloud_Pre-training15_papers_with_code_Datase,Point Cloud Pre-training15 papers with code Dataset,,https://paperswithcode.com/dataset/point-cloud-pre-training,,,,,,,,,,,,,See all 1951 tasks,Point Cloud Pre-training15 pap,Point Cloud Pre-training15 pap
Perception_Test,Perception Test Dataset,"Perception Test is a benchmark designed to evaluate the perception and reasoning skills of multimodal models. It introduces real-world videos designed to show perceptually interesting situations and defines multiple tasks that require understanding of memory, abstract patterns, physics, and semantics – across visual, audio, and text modalities. The benchmark consists of 11.6k videos, 23s average length, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels: object and point tracks, temporal action and sound segments, multiple-choice video question-answers and grounded video question-answers. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or fine tuning regime.",https://production-media.paperswithcode.com/datasets/6544dfe1-1dd7-47a3-8507-dce068b7644a.png,EditCreative Common CC-BY 4.0,"Image, Text, Time Series, Video",English,,,,,,,"Point Tracking, Video Question Answering, Temporal Action Localization, Object Tracking, Question Answering","video-question-answering-on-perception-test, point-tracking-on-perception-test, object-tracking-on-perception-test",,See all 1951 tasks,Point Tracking8 benchmarks49 p,Point Tracking8 benchmarks49 p
PointOdyssey,PointOdyssey Dataset,"PointOdyssey is a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms. The dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work.",https://production-media.paperswithcode.com/datasets/c280a975-07b7-411f-bcf7-bf77abd690cd.gif,EditUnknown,"Image, Video",,,,,,,,Point Tracking,point-tracking-on-pointodyssey,,See all 1951 tasks,Point Tracking8 benchmarks49 p,Point Tracking8 benchmarks49 p
TAPVid-3D__A_Benchmark_for_Tracking_Any_Point_in_3,TAPVid-3D: A Benchmark for Tracking Any Point in 3D Dataset,"TAPVid-3D is a dataset and benchmark for evaluating the task of long-range Tracking Any Point in 3D (TAP-3D). The dataset consists of 4,000+ real-world videos and 2.1 million metric 3D point trajectories, spanning a variety of object types, motion patterns, and indoor and outdoor environments.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,"EditApache 2.0, with additional restrictions","Image, Video",,,,,,,,Point Tracking,,,See all 1951 tasks,Point Tracking8 benchmarks49 p,Point Tracking8 benchmarks49 p
10_000_People_-_Human_Pose_Recognition_Data,"10,000 People - Human Pose Recognition Data Dataset","Description:
10,000 People - Human Pose Recognition Data. This dataset includes indoor and outdoor scenes.This dataset covers males and females. Age distribution ranges from teenager to the elderly, the middle-aged and young people are the majorities. The data diversity includes different shooting heights, different ages, different light conditions, different collecting environment, clothes in different seasons, multiple human poses. For each subject, the labels of gender, race, age, collecting environment and clothes were annotated. The data can be used for human pose recognition and other tasks.

Data size:
10,000 people

Race distribution:
Asian (Chinese)",https://production-media.paperswithcode.com/datasets/da76c793-ff57-4d89-90bf-ec04e02fe149.png,EditCommercial license,"3D, Image, Video",,,,,,,,"Object Detection, Contrastive Learning, Pose Estimation, Pose Tracking","object-detection-on-10000-people-human-pose, contrastive-learning-on-10000-people-human",,See all 1951 tasks,Pose Estimation198 benchmarks1,Pose Estimation198 benchmarks1
300W,300W Dataset,"The 300-W is a face dataset that consists of 300 Indoor and 300 Outdoor in-the-wild images. It covers a large variation of identity, expression, illumination conditions, pose, occlusion and face size. The images were downloaded from google.com by making queries such as “party”, “conference”, “protests”, “football” and “celebrities”. Compared to the rest of in-the-wild datasets, the 300-W database contains a larger percentage of partially-occluded images and covers more expressions than the common “neutral” or “smile”, such as “surprise” or “scream”.
Images were annotated with the 68-point mark-up using a semi-automatic methodology. The images of the database were carefully selected so that they represent a characteristic sample of challenging but natural face instances under totally unconstrained conditions. Thus, methods that achieve accurate performance on the 300-W database can demonstrate the same accuracy in most realistic cases.
Many images of the database contain more than one annotated faces (293 images with 1 face, 53 images with 2 faces and 53 images with [3, 7] faces). Consequently, the database consists of 600 annotated face instances, but 399 unique images. Finally, there is a large variety of face sizes. Specifically, 49.3% of the faces have size in the range [48.6k, 2.0M] and the overall mean size is 85k (about 292 × 292) pixels.",https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf,"EditCustom (research-only, non-commercial)","3D, Image",,,,,293 images,"tests”, “football” and “celebrities”. Compared to the rest of in-the-wild datasets, the 300-W database contains a larger percentage of partially-occluded images",,"2D Pose Estimation, Pose Estimation, Unsupervised Facial Landmark Detection, Face Alignment, 3D Reconstruction, Facial Landmark Detection","face-alignment-on-300w-split-2, unsupervised-facial-landmark-detection-on, face-alignment-on-300w-split-2-300w-lp, pose-estimation-on-300w-full, 3d-reconstruction-on-300w, facial-landmark-detection-on-300w-full, 2d-pose-estimation-on-300w, facial-landmark-detection-on-300w, face-alignment-on-300w",,See all 1951 tasks,Pose Estimation198 benchmarks1,Pose Estimation198 benchmarks1
3DPW,3DPW Dataset,"The 3D Poses in the Wild dataset is the first dataset in the wild with accurate 3D poses for evaluation. While other datasets outdoors exist, they are all restricted to a small recording volume. 3DPW is the first one that includes video footage taken from a moving phone camera.

The dataset includes:


60 video sequences.
2D pose annotations.
3D poses obtained with the method introduced in the paper.
Camera poses for every frame in the sequences.
3D body scans and 3D people models (re-poseable and re-shapeable). Each sequence contains its corresponding models.
18 3D models in different clothing variations.",http://virtualhumans.mpi-inf.mpg.de/3DPW,"EditCustom (research-only, non-commercial)","3D, Image, Time Series",,,,,,,,"Pose Estimation, Cross-domain 3D Human Pose Estimation, 3D Human Pose Estimation, Human Pose Forecasting, Hand Pose Estimation","cross-domain-3d-human-pose-estimation-on-3dpw, pose-estimation-on-3dpw, hand-pose-estimation-on-3dpw, human-pose-forecasting-on-3dpw, 3d-human-pose-estimation-on-3dpw",,See all 1951 tasks,Pose Estimation198 benchmarks1,Pose Estimation198 benchmarks1
AMASS,AMASS Dataset,"AMASS is a large database of human motion unifying different optical marker-based motion capture datasets by representing them within a common framework and parameterization. AMASS is readily useful for animation, visualization, and generating training data for deep learning.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_14.25.22.png,"EditCustom (research-only, non-commercial)","3D, Image, Time Series",,,,,,,,"Human Pose Forecasting, 3D Human Pose Estimation, Pose Estimation",human-pose-forecasting-on-amass,,See all 1951 tasks,Pose Estimation198 benchmarks1,Pose Estimation198 benchmarks1
DensePose,DensePose Dataset,"DensePose-COCO is a large-scale ground-truth dataset with image-to-surface correspondences manually annotated
on 50K COCO images and train DensePose-RCNN, to densely regress part-specific UV coordinates within every human
region at multiple frames per second.",https://arxiv.org/pdf/1802.00434v1.pdf,EditCC BY-NC 2.0,"3D, Image, Text",English,,,,,,,"Image Generation, Pose Estimation, 3D Human Pose Estimation",,,See all 1951 tasks,Pose Estimation198 benchmarks1,Pose Estimation198 benchmarks1
JHMDB,JHMDB Dataset,"JHMDB is an action recognition dataset that consists of 960 video sequences belonging to 21 actions. It is a subset of the larger HMDB51 dataset collected from digitized movies and YouTube videos. The dataset contains video and annotation for puppet flow per frame (approximated optimal flow on the person), puppet mask per frame, joint positions per frame, action label per clip and meta label per clip (camera motion, visible body parts, camera viewpoint, number of people, video quality).",https://arxiv.org/abs/2008.09880,EditUnknown,"3D, Image, Video",,,,,,,,"Skeleton Based Action Recognition, Pose Estimation, 2D Human Pose Estimation, Action Detection, Referring Expression Segmentation","pose-estimation-on-j-hmdb, skeleton-based-action-recognition-on-j-hmbd, referring-expression-segmentation-on-j-hmdb, skeleton-based-action-recognition-on-j-hmdb, 2d-human-pose-estimation-on-jhmdb-2d-poses, skeleton-based-action-recognition-on-jhmdb, action-detection-on-j-hmdb, skeleton-based-action-recognition-on-jhmdb-2d",,See all 1951 tasks,Pose Estimation198 benchmarks1,Pose Estimation198 benchmarks1
MPII,MPII Dataset,"The MPII Human Pose Dataset for single person pose estimation is composed of about 25K images of which 15K are training samples, 3K are validation samples and 7K are testing samples (which labels are withheld by the authors). The images are taken from YouTube videos covering 410 different human activities and the poses are manually annotated with up to 16 body joints.",https://arxiv.org/abs/1802.09232,EditSimplified BSD,"3D, Image, Time Series, Video",English,,,,25K images,"training samples, 3K are validation samples",,"Temporal Action Localization, Multi-Person Pose Estimation, Pose Estimation, Keypoint Detection","keypoint-detection-on-mpii-multi-person, pose-estimation-on-mpii, multi-person-pose-estimation-on-mpii-multi, pose-estimation-on-mpii-single-person",,See all 1951 tasks,Pose Estimation198 benchmarks1,Pose Estimation198 benchmarks1
PASCAL3D_,PASCAL3D+ Dataset,"The Pascal3D+ multi-view dataset consists of images in the wild, i.e., images of object categories exhibiting high variability, captured under uncontrolled settings, in cluttered scenes and under many different poses. Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset. These objects are annotated with pose information (azimuth, elevation and distance to camera). Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset.",https://arxiv.org/abs/1511.05175,EditUnknown,"3D, Image",,2012,,,,,12,"Object Detection, Pose Estimation, Viewpoint Estimation, Keypoint Detection",keypoint-detection-on-pascal3d,,See all 1951 tasks,Pose Estimation198 benchmarks1,Pose Estimation198 benchmarks1
Drunkard_s_Dataset,Drunkard's Dataset Dataset,"Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. To tackle this issue with a common benchmark, we introduce the Drunkard’s Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality.",https://production-media.paperswithcode.com/datasets/aad693c6-ff93-4f75-a9db-1cb51ceffb7c.png,EditMIT License,"3D, Image, Time Series, Video",,,,,,,,"Pose Estimation, Monocular Visual Odometry, Visual Odometry, 3D Pose Estimation, Drone Pose Estimation, Real-Time Visual Tracking, Simultaneous Localization and Mapping, Pose Tracking, 6D Pose Estimation using RGBD, 6D Pose Estimation, Visual Tracking, Single-View 3D Reconstruction, 6D Pose Estimation using RGB, 3D Reconstruction, Pose Prediction",6d-pose-estimation-using-rgbd-on-drunkard-s,,See all 1951 tasks,Pose Prediction3 benchmarks68 ,Pose Prediction3 benchmarks68 
Expi,Expi Dataset,"Extreme Pose Interaction (ExPI) Dataset is a new person interaction dataset of Lindy Hop dancing actions. In Lindy Hop, the two dancers are called leader and
follower. The authors recorded two couples of dancers in a multi-camera setup equipped also with a motion-capture system.
16 different actions are performed in ExPI dataset, some by the two couples of dancers, some by only one of the couples. Each action was repeated five times
to account for variability. More precisely, for each recorded sequence, ExPI provides: 
(i) Multi-view videos at 25FPS from all the cameras in the recording setup; 
(ii) Mocap data (3D position of 18 joints for each person) at 25FPS synchronized with the videos.; 
(iii) camera calibration information; and (iv) 3D shapes as textured meshes for each frame.

Overall, the dataset contains 115 sequences with 30k visual frames for each viewpoint and 60k 3D instances annotated",https://production-media.paperswithcode.com/datasets/208120ab-b7ca-496c-96a4-28658e1ad35a.png,EditUnknown,"3D, Image, Time Series, Video",,,,,,,,"motion prediction, Human Pose Forecasting, Multi-Person Pose forecasting, Pose Prediction","human-pose-forecasting-on-expi-common-actions, multi-person-pose-forecasting-on-expi-unseen, multi-person-pose-forecasting-on-expi-common",,See all 1951 tasks,Pose Prediction3 benchmarks68 ,Pose Prediction3 benchmarks68 
G3D,G3D Dataset,"The Gaming 3D Dataset (G3D) focuses on real-time action recognition in a gaming scenario. It contains 10 subjects performing 20 gaming actions: “punch right”, “punch left”, “kick right”, “kick left”, “defend”, “golf swing”, “tennis swing forehand”, “tennis swing backhand”, “tennis serve”, “throw bowling ball”, “aim and fire gun”, “walk”, “run”, “jump”, “climb”, “crouch”, “steer a car”, “wave”, “flap” and “clap”.",https://arxiv.org/abs/1704.05645,EditUnknown,"3D, Image, Time Series, Video",,,,,,,,"Skeleton Based Action Recognition, Pose Prediction","skeleton-based-action-recognition-on-gaming, pose-prediction-on-gaming-3d-g3d",,See all 1951 tasks,Pose Prediction3 benchmarks68 ,Pose Prediction3 benchmarks68 
InfiniteRep,InfiniteRep Dataset,"InfiniteRep is a synthetic, open-source dataset for fitness and physical therapy (PT) applications. It includes 1k videos of diverse avatars performing multiple repetitions of common exercises. It includes significant variation in the environment, lighting conditions, avatar demographics, and movement trajectories. From cadence to kinematic trajectory, each rep is done slightly differently -- just like real humans. InfiniteRep videos are accompanied by a rich set of pixel-perfect labels and annotations, including frame-specific repetition counts.

The dataset features:  


100 videos per exercise, spanning 5 to 10 repetitions each (1,000 videos total) 
7 unique indoor scenes
Realistic environmental occlusion (+ corresponding labels)
Diverse lighting conditions 
Varied body shape, skin tones, and clothing 
Rich annotations for 2D and 3D supervision  

Exercises
The dataset currently includes the following exercises:  


Pushups  
Alternating Bicep Curls (with dumbbells)  
Delt Flys (with dumbbells)
Squats
Bird Dogs
Supermans
Bicycle Crunches
Leg Raises
Front Raises (with dumbbells)
Overhead Press (with dumbbells)

Annotations
The dataset includes the following annotations:  


Bounding boxes  
Segmentation masks  
Keypoints 
Joint angles (quaternions) 
Percent occlusion 
Avatar characteristics
Camera position 
and more 

Want depth labels? They are not included in the dataset but we can send them to you. Email us at info@toinfinity.ai. 

Download
Download the dataset: toinfinity.ai/infiniterep  
Github repo with additional documentation: https://github.com/toinfinityai/InfiniteRep

Need more data?
Infinity AI specializes in generating custom synthetic data. If you need more (or different data), drop us a line at info@toinfinity.ai (we read every email).",https://production-media.paperswithcode.com/datasets/79dc28fc-1373-4daa-82fd-04bbab7e70c0.gif,EditCreative Commons Attribution 4.0 International License,"3D, Image, Time Series, Video",,,,,,,,"Activity Prediction, Pose Tracking, Action Classification, Activity Detection, 3D Pose Estimation, Activity Recognition In Videos, Action Detection, 3D Instance Segmentation, 3D Absolute Human Pose Estimation, Pose Prediction, 2D Semantic Segmentation, Pose Estimation, 3D Classification, 3D Action Recognition, Activity Recognition, 2D Human Pose Estimation, 3D Human Dynamics, 3D Human Pose Estimation, 3D Human Pose Tracking",,,See all 1951 tasks,Pose Prediction3 benchmarks68 ,Pose Prediction3 benchmarks68 
SportsPose,SportsPose Dataset,"Accurate 3D human pose estimation is essential for sports analytics, coaching, and injury prevention. However, existing datasets for monocular pose estimation do not adequately capture the challenging and dynamic nature of sports movements. In response, we introduce SportsPose, a large-scale 3D human pose dataset consisting of highly dynamic sports movements. With more than 176,000 3D poses from 24 different subjects performing 5 different sports activities, SportsPose provides a diverse and comprehensive set of 3D poses that reflect the complex and dynamic nature of sports movements. Contrary to other markerless datasets we have quantitatively evaluated the precision of SportsPose by comparing our poses with a commercial marker-based system and achieve a mean error of 34.5 mm across all evaluation sequences. This is comparable to the error reported on the commonly used 3DPW dataset. We further introduce a new metric, local movement, which describes the movement of the wrist and ankle joints in relation to the body. With this, we show that SportsPose contains more movement than the Human3.6M and 3DPW datasets in these extremum joints, indicating that our movements are more dynamic. The dataset with accompanying code can be downloaded from our website. We hope that SportsPose will allow researchers and practitioners to develop and evaluate more effective models for the analysis of sports performance and injury prevention. With its realistic and diverse dataset, SportsPose provides a valuable resource for advancing the state-of-the-art in pose estimation in sports",https://production-media.paperswithcode.com/datasets/14fb9964-cec0-4b06-ac53-4a660b82e4ff.png,Edithttps://christianingwersen.github.io/SportsPose/download.html,"3D, Image, Time Series",,,,,,,,"Pose Estimation, 3D Pose Estimation, 2D Human Pose Estimation, 3D Absolute Human Pose Estimation, 3D Human Pose Estimation, Pose Prediction",,,See all 1951 tasks,Pose Prediction3 benchmarks68 ,Pose Prediction3 benchmarks68 
VRMocap__VR_Mocap_Dataset_for_Pose_Reconstruction,VRMocap: VR Mocap Dataset for Pose Reconstruction Dataset,"Data used for the paper SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data

It contains over 1GB of high-quality motion capture data recorded with an Xsens Awinda system while using a variety of VR applications in Meta Quest devices.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution 4.0 International,"3D, Image, Time Series, Video",,,,,,,,"Pose Estimation, Motion Synthesis, Pose Tracking, 6D Pose Estimation, Pose Prediction",,,See all 1951 tasks,Pose Prediction3 benchmarks68 ,Pose Prediction3 benchmarks68 
VR_Mocap_Dataset_for_Pose_Orientation_Prediction,VR Mocap Dataset for Pose/Orientation Prediction Dataset,"Data used for the paper Combining Motion Matching and Orientation Prediction to Animate Avatars for Consumer-Grade VR Devices.

MMData.zip contains the necessary files to execute the project in Unity. Use only following the instructions on the GitHub project.

MMVR_Dataset.zip contains all .bvh files used for training the orientation prediction network. All files are captured with an Xsens Awinda motion capture system while using Virtual Reality. Visit GitHub for more information.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY-NC-SA 4.0,"3D, Image, Time Series, Video",,,,,,,,"Pose Prediction, Pose Estimation, Pose Tracking",,,See all 1951 tasks,Pose Prediction3 benchmarks68 ,Pose Prediction3 benchmarks68 
quantile_regression98_papers_with_code_Dataset,quantile regression98 papers with code Dataset,,https://paperswithcode.com/dataset/quantile-regression,,,,,,,,,,,,,See all 1951 tasks,quantile regression98 papers w,quantile regression98 papers w
IJB-B,IJB-B Dataset,"The IJB-B dataset is a template-based face dataset that contains 1845 subjects with 11,754 images, 55,025 frames and 7,011 videos where a template consists of a varying number of still images and video frames from different sources. These images and videos are collected from the Internet and are totally unconstrained, with large variations in pose, illumination, image quality etc. In addition, the dataset comes with protocols for 1-to-1 template-based face verification, 1-to-N template-based open-set face identification, and 1-to-N open-set video face identification.",https://arxiv.org/abs/1812.04058,EditCustom,Image,,,,,754 images,,,"Face Identification, Lightweight Face Recognition, Quantization, Face Verification, Face Recognition","face-verification-on-ijb-b, lightweight-face-recognition-on-ijb-b, quantization-on-ijb-b, face-identification-on-ijb-b, face-recognition-on-ijb-b",,See all 1951 tasks,Quantization12 benchmarks1450 ,Quantization12 benchmarks1450 
IJB-C,IJB-C Dataset,"The IJB-C dataset is a video-based face recognition dataset. It is an extension of the IJB-A dataset with about 138,000 face images, 11,000 face videos, and 10,000 non-face images.",https://arxiv.org/abs/1804.10275,"EditCustom (research-only, attribution)",Image,,,,,,,,"Quantization, Face Verification, Lightweight Face Recognition","lightweight-face-recognition-on-ijb-c, face-verification-on-ijb-c, quantization-on-ijb-c",,See all 1951 tasks,Quantization12 benchmarks1450 ,Quantization12 benchmarks1450 
LFW,LFW Dataset,"The LFW dataset contains 13,233 images of faces collected from the web. This dataset consists of the 5749 identities with 1680 people with two or more images. In the standard LFW evaluation protocol the verification accuracies are reported on 6000 face pairs.",https://arxiv.org/abs/1901.05903,EditUnknown,"3D, Image",,,,,233 images,,,"3D Face Modelling, Unsupervised face recognition, Lightweight Face Recognition, Face Quality Assessement, Quantization, Face Verification, Face Anonymization, Face Recognition, Blind Face Restoration, Synthetic Face Recognition","quantization-on-lfw, unsupervised-face-recognition-on-lfw, lightweight-face-recognition-on-lfw, face-recognition-on-lfw, synthetic-face-recognition-on-lfw, 3d-face-modeling-on-lfw, face-recognition-on-lfw-online-open-set, face-quality-assessement-on-lfw, blind-face-restoration-on-lfw, face-verification-on-lfw, face-verification-on-labeled-faces-in-the, face-anonymization-on-lfw",,See all 1951 tasks,Quantization12 benchmarks1450 ,Quantization12 benchmarks1450 
Tiny_Images,Tiny Images Dataset,"The image dataset TinyImages contains 80 million images of size 32×32 collected from the Internet, crawling the words in WordNet. 

The authors have decided to withdraw it because it contains offensive content, and have asked the community to stop using it.",https://arxiv.org/abs/1502.03032,EditUnknown,Image,,,,,,,,"Image Classification, Quantization, Image Retrieval",,,See all 1951 tasks,Quantization12 benchmarks1450 ,Quantization12 benchmarks1450 
Visual_Wake_Words,Visual Wake Words Dataset,"Visual Wake Words represents a common microcontroller vision use-case of identifying whether a person is present in the image or not, and provides a realistic benchmark for tiny vision models.",https://arxiv.org/pdf/1906.05721.pdf,EditCustom,Image,,,,,,,,"Image Classification, Quantization, Hardware Aware Neural Architecture Search, Neural Architecture Search","image-classification-on-visual-wake-words, hardware-aware-neural-architecture-search-on",,See all 1951 tasks,Quantization12 benchmarks1450 ,Quantization12 benchmarks1450 
Word_Sense_Disambiguation__a_Unified_Evaluation_Fr,Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison Dataset,"The Evaluation framework of Raganato et al. 2017 includes two training sets (SemCor-Miller et al., 1993- and OMSTI-Taghipour and Ng, 2015-) and five test sets from the Senseval/SemEval series (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2013; Moro and Navigli, 2015), standardized to the same format and sense inventory (i.e. WordNet 3.0).

Typically, there are two kinds of approach for WSD: supervised (which make use of sense-annotated training data) and knowledge-based (which make use of the properties of lexical resources).

Supervised: The most widely used training corpus used is SemCor, with 226,036 sense annotations from 352 documents manually annotated. All supervised systems in the evaluation table are trained on SemCor. Some supervised methods, particularly neural architectures, usually employ the SemEval 2007 dataset as development set (marked by *). The most usual baseline is the Most Frequent Sense (MFS) heuristic, which selects for each target word the most frequent sense in the training data.

Knowledge-based: Knowledge-based systems usually exploit WordNet or BabelNet as semantic network. The first sense given by the underlying sense inventory (i.e. WordNet 3.0) is included as a baseline.

Description from NLP Progress",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,2017,,,352 documents,,,"Quantization, Word Sense Disambiguation","quantization-on-knowledge-based, word-sense-disambiguation-on-supervised, word-sense-disambiguation-on-knowledge-based",,See all 1951 tasks,Quantization12 benchmarks1450 ,Quantization12 benchmarks1450 
MEIS,MEIS Dataset,"MEIS comprises a total of 2,639 images in the size of 1024 × 768 toward two recording views (Aortic Valve (AV) and
Left Ventricle (LV)) with 1,521 (747 in AV + 774 in LV) images for training and 1,118 (559 in AV + 559 in LV) for
testing, respectively. Each view must be detected with two objects to calculate the measurement indicators. That is in
total with four object classes (two objects in each view): aortic root (AoR) and left atrium (LA) in AV; interventricular
septum (IVS) and left ventricular posterior wall (LVPW) in LV. The medical meaning and purpose of each indicator are
listed in the following:
• AV: LA-Dimension and AoR-Dimension can be measured for calculating different indicators, such as AoR/LA
ratio, to examine the state of the aortic valve.
• LV: 6 measurements include IVSs, IVSd, LVIDs, LVIDd, LVPWs, and LVPWd. These concerned thicknesses
and dimensions in LV recording are used to estimate other cardiac functions through specific medical formulas,
including LV mass, LV ejection fraction, end-diastolic volume, end-systolic volume, and more.",https://production-media.paperswithcode.com/datasets/1765cda7-ae29-4ad0-9ec9-fd3f6f4b1b76.jpg,EditUnknown,Image,,,,,639 images,,,"Real-time Instance Segmentation, Real-time instance measurement","real-time-instance-segmentation-on-meis, real-time-instance-measurement-on-meis",,See all 1951 tasks,Real-time Instance Segmentatio,Real-time Instance Segmentatio
Multi30K,Multi30K Dataset,"Multi30K is a large-scale multilingual multimodal dataset for interdisciplinary machine learning research. It extends the Flickr30K dataset with German translations created by professional translators over a subset of the English descriptions, and descriptions crowdsourced independently of the original English descriptions. The dataset was introduced to stimulate multilingual multimodal research.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Translation eng-deu, Multimodal Machine Translation, Real-time Instance Segmentation, Translation deu-eng","translation-deu-eng-on-multi30k-test-2017-1, translation-deu-eng-on-multi30k-test-2017, multimodal-machine-translation-on-multi30k, translation-deu-eng-on-multi30k-test-2018, real-time-instance-segmentation-on-multi30k, translation-eng-deu-on-multi30k-test-2018, translation-deu-eng-on-multi30k-test-2016, translation-eng-deu-on-multi30k-test-2016, translation-eng-deu-on-multi30k-test-2017, translation-eng-deu-on-multi30k-test-2017-1",,See all 1951 tasks,Real-time Instance Segmentatio,Real-time Instance Segmentatio
Argoverse-HD,Argoverse-HD Dataset,"Argoverse-HD is a dataset built for streaming object detection, which encompasses real-time object detection, video object detection, tracking, and short-term forecasting. It contains the video data from Argoverse 1.1 with our own MS COCO-style bounding box annotations with track IDs. The annotations are backward-compatible with COCO as one can directly evaluate COCO pre-trained models on this dataset to estimate the efficiency or the cross-dataset generalization capability of the models. The dataset contains high-quality and temporally-dense annotations for high-resolution videos (1920 x 1200 @ 30 FPS). Overall, there are 70,000 image frames and 1.3 million bounding boxes.

Argoverse-HD is the dataset used in the Streaming Perception Challenge, which includes two tracks:


Detection-only (real-time object detection). In this track, the participants will develop single-frame object detectors as they would for COCO and LVIS challenges. The crucial distinction is that the evaluation will score latency through streaming accuracy.
Full-stack. In this track, the method is unrestricted. However, most likely tracking and forecasting will be used to compensate for the latency of the detectors.

By default, all submissions measure their latency on a V100 GPU with the official toolkit.",https://production-media.paperswithcode.com/datasets/6548de95-f8cd-404f-8e9e-a5716a4fccd7.jpg,EditMIT,"Image, Time Series, Video",,1920,,,,,,"Object Detection, Motion Forecasting, Real-Time Multi-Object Tracking, Real-Time Object Detection","real-time-object-detection-on-argoverse-hd-2, real-time-object-detection-on-argoverse-hd-4, real-time-object-detection-on-argoverse-hd-5, real-time-object-detection-on-argoverse-hd-3",,See all 1951 tasks,Real-Time Object Detection8 be,Real-Time Object Detection8 be
Endotect_Polyp_Segmentation_Challenge_Dataset,Endotect Polyp Segmentation Challenge Dataset Dataset,"A challenge that consists of three tasks, each targeting a different requirement for in-clinic use. The first task involves classifying images from the GI tract into 23 distinct classes. The second task focuses on efficiant classification measured by the amount of time spent processing each image. The last task relates to automatcially segmenting polyps.

Please cite ""The EndoTect 2020 Challenge: Evaluation andComparison of Classification, Segmentation and Inference Time for Endoscopy"" if you use the dataset.",https://endotect.com/,EditOpen-access,Image,,2020,,,,,,"Object Detection, Medical Image Segmentation, Real-Time Semantic Segmentation, Semantic Segmentation, Image Classification, Real-Time Object Detection",medical-image-segmentation-on-endotect-polyp,,See all 1951 tasks,Real-Time Object Detection8 be,Real-Time Object Detection8 be
Hyper-Kvasir_Dataset,Hyper-Kvasir Dataset Dataset,"HyperKvasir dataset contains 110,079 images and 374 videos where it captures anatomical landmarks and pathological and normal findings. A total of around 1 million images and video frames altogether.",https://production-media.paperswithcode.com/datasets/Screenshot_from_2021-05-05_23-39-13.png,EditCC BY 4.0,"Image, Text, Video",English,,,,079 images,,,"Object Detection, Medical Image Segmentation, Video Summarization, Organ Detection, Anomaly Detection, Image Classification, General Classification, Real-Time Object Detection","medical-image-segmentation-on-hyper-kvasir, anomaly-detection-on-hyper-kvasir-dataset",,See all 1951 tasks,Real-Time Object Detection8 be,Real-Time Object Detection8 be
Kvasir-Instrument,Kvasir-Instrument Dataset,"Consists of  annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists.",/paper/kvasir-instrument-diagnostic-and-therapeutic,Editopen access for academic purpose. For commercial purpose please contact the owner.,Image,,,,,,,,"Medical Image Segmentation, Semi-Supervised Semantic Segmentation, Real-Time Semantic Segmentation, Instrument Recognition, Semantic Segmentation, Real-Time Object Detection","semi-supervised-semantic-segmentation-on-33, medical-image-segmentation-on-kvasir, semantic-segmentation-on-kvasir-instrument",,See all 1951 tasks,Real-Time Object Detection8 be,Real-Time Object Detection8 be
Kvasir-SEG,Kvasir-SEG Dataset,"Kvasir-SEG is an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist.",/paper/kvasir-seg-a-segmented-polyp-dataset,"EditThe use of the Kvasir-SEG dataset is restricted for research and educational purposes. The use of the Kvasir-SEG dataset for commercial purposes is forbidden without prior written permission. For other purposes, contact us (see below). In all documents and publications that use the Kvasir-SEG dataset or report experimental results based on the Kvasir-SEG dataset, a reference to the dataset paper has to be included",Image,,,,,,,,"Object Detection, Medical Image Segmentation, Colorectal Polyps Characterization, Real-Time Semantic Segmentation, Colorectal Gland Segmentation:, Polyp Segmentation, Semantic Segmentation, Real-Time Object Detection","polyp-segmentation-on-kvasir-seg, colorectal-polyps-characterization-on-kvasir, medical-image-segmentation-on-kvasir-seg",,See all 1951 tasks,Real-Time Object Detection8 be,Real-Time Object Detection8 be
Kvasir,Kvasir Dataset,"The KVASIR Dataset was released as part of the medical multimedia challenge presented by MediaEval. It is based on images obtained from the GI tract via an endoscopy procedure. The dataset is composed of images that are annotated and verified by medical doctors, and captures 8 different classes. The classes are based on three anatomical landmarks (z-line, pylorus, cecum), three pathological findings (esophagitis, polyps, ulcerative colitis) and two other classes (dyed and lifted polyps, dyed resection margins) related to the polyp removal process. Overall, the dataset contains 8,000 endoscopic images, with 1,000 image examples per class.",https://arxiv.org/abs/2007.05914,"EditCustom (research-only, non-commercial)",Image,,,,,,"val. It is based on images obtained from the GI tract via an endoscopy procedure. The dataset is composed of images that are annotated and verified by medical doctors, and captures 8 different classes. The classes are based on three anatomical landmarks (z-line, pylorus, cecum), three pathological findings (esophagitis, polyps, ulcerative colitis) and two other classes (dyed and lifted polyps, dyed resection margins) related to the polyp removal process. Overall, the dataset contains 8,000 endoscopic images",,"Object Detection, Medical Image Segmentation, Colorectal Polyps Characterization, Real-Time Semantic Segmentation, Colorectal Gland Segmentation:, Polyp Segmentation, Semantic Segmentation, Image Classification, Real-Time Object Detection","image-classification-on-kvasir, polyp-segmentation-on-kvasir-seg, colorectal-polyps-characterization-on-kvasir, medical-image-segmentation-on-kvasir-seg",,See all 1951 tasks,Real-Time Object Detection8 be,Real-Time Object Detection8 be
PASCAL_VOC_2007,PASCAL VOC 2007 Dataset,"PASCAL VOC 2007 is a dataset for image recognition. The twenty object classes that have been selected are:

Person: person
Animal: bird, cat, cow, dog, horse, sheep
Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train
Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor

The dataset can be used for image classification and object detection tasks.",https://production-media.paperswithcode.com/datasets/pascalvoc.png,EditUnknown,"Image, Text",English,2007,,,,,,"Robust Object Detection, Object Detection, Cross-Modal Retrieval, Zero-Shot Object Detection, Real-Time Object Detection, Object Localization, Unsupervised Semantic Segmentation with Language-image Pre-training, Unsupervised Object Localization, Multi-label Image Recognition with Partial Labels, Weakly Supervised Object Detection, Multi-Label Classification, Open World Object Detection, Semantic Segmentation, Image Classification, Unsupervised Object Detection, Object Counting","weakly-supervised-object-detection-on-pascal-1, real-time-object-detection-on-pascal-voc-2007, open-world-object-detection-on-pascal-voc, object-counting-on-pascal-voc-2007-count-test, multi-label-classification-on-pascal-voc-2007, robust-object-detection-on-pascal-voc-2007, unsupervised-object-detection-on-pascal-voc, unsupervised-semantic-segmentation-with-6, image-classification-on-pascal-voc-2007, unsupervised-object-localization-on-pascal, semantic-segmentation-on-pascal-voc-2007, object-detection-on-pascal-voc-2007, multi-label-image-recognition-with-partial-1, object-localization-on-pascal-voc-2007",,See all 1951 tasks,Real-Time Object Detection8 be,Real-Time Object Detection8 be
SFCHD,SFCHD Dataset,"This work contributes a large, complex, and realistic high-quality safety clothing and helmet detection (SFCHD) dataset. The dataset comprises 12,373 images, covering 7 categories, with a total of 50,558 labeled instances. All images are captured from factory surveillance cameras, encompassing 40 different scenes across two chemical plants. It is worth noting that our SFCHD dataset not only provides a rich set of training samples but also serves as a benchmark for the evaluation of various detection tasks, such as small object detection, and high-low light object detection.",https://production-media.paperswithcode.com/datasets/af0f18e1-2653-4916-8512-7138f053ce4f.png,EditUnknown,Image,,,,,373 images,,7,"Object Detection, Open World Object Detection, Small Object Detection, Real-Time Object Detection",object-detection-on-sfchd,,See all 1951 tasks,Real-Time Object Detection8 be,Real-Time Object Detection8 be
Real-World_Adversarial_Attack12_papers_with_code_D,Real-World Adversarial Attack12 papers with code Dataset,,https://paperswithcode.com/dataset/real-world-adversarial-attack,,,,,,,,,,,,,See all 1951 tasks,Real-World Adversarial Attack1,Real-World Adversarial Attack1
Reference-based_Super-Resolution1_benchmark16_pape,Reference-based Super-Resolution1 benchmark16 papers with code Dataset,,https://paperswithcode.com/dataset/reference-based-super-resolution,,,,,,,,,,,,,See all 1951 tasks,Reference-based Super-Resoluti,Reference-based Super-Resoluti
A2Dre,A2Dre Dataset,"We obtain A2Dre by selecting only instances that were labeled as non-trivial, which are 433 REs from 190 videos. We do not use the trivial cases as the analysis of such examples is not relevant, as referents can be described by using the category alone. Each annotator was presented with a RE, a video in which the target object was marked by a bounding box, and a set of questions paraphrasing our categories. A2Dre was annotated by 3 authors of the paper. Our final set of category annotations used for analysis was derived by means of majority voting: for each nontrivial RE, we kept all category labels which were assigned to the RE by at least two annotators.",https://production-media.paperswithcode.com/datasets/a2dre.jpg,EditMIT License,Image,,,,,,,,Referring Expression Segmentation,referring-expression-segmentation-on-a2dre,,See all 1951 tasks,Referring Expression Segmentat,Referring Expression Segmentat
A2D_Sentences,A2D Sentences Dataset,"The Actor-Action Dataset (A2D) by Xu et al. [29] serves as the largest video dataset for the general actor and action segmentation task. It contains 3,782 videos from YouTube with pixel-level labeled actors and their actions. The dataset includes eight different actions, while a total of seven actor classes are considered to perform those actions. We follow [29], who split the dataset into 3,036 training videos and 746 testing videos. 

As we are interested in pixel-level actor and action segmentation from sentences, we augment the videos in A2D with natural language descriptions about what each actor is doing in the videos.  Following the guidelines set forth
in [12], we ask our annotators for a discriminative referring expression of each actor instance if multiple objects are considered in a video. The annotation process resulted in a total of 6,656 sentences, including 811 different nouns, 225 verbs and 189 adjectives. Our sentences enrich the actor and action pairs from the A2D dataset with finer granularities. For example, the actor adult in A2D may be annotated with man, woman, person and player in our sentences, while action rolling may also refer to flipping, sliding, moving and running when describing different actors in different scenarios. Our sentences contain on average more words than the ReferIt dataset [12] (7.3 vs 4.7), even when we leave out prepositions, articles and linking verbs (4.5 vs 3.6). This makes sense as our sentences contain a variety of verbs while existing referring expression datasets mostly ignore verbs.",https://production-media.paperswithcode.com/datasets/A2Dsentences.jpg,EditUnknown,Image,,,,,656 sentences,,,Referring Expression Segmentation,referring-expression-segmentation-on-a2d,,See all 1951 tasks,Referring Expression Segmentat,Referring Expression Segmentat
CLEVR-Ref_,CLEVR-Ref+ Dataset,"CLEVR-Ref+ is a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators.",https://arxiv.org/pdf/1901.00850v2.pdf,EditUnknown,"Image, Text",English,,,,,,,"Visual Reasoning, Referring Expression Segmentation, Referring Expression Comprehension, Visual Question Answering (VQA), Question Answering","referring-expression-segmentation-on-clevr, referring-expression-comprehension-on-clevr",,See all 1951 tasks,Referring Expression Segmentat,Referring Expression Segmentat
Google_Refexp,Google Refexp Dataset,"A new large-scale dataset for referring expressions, based on MS-COCO.",/paper/generation-and-comprehension-of-unambiguous,EditCC BY 4.0,"Image, Text",English,,,,,,,"Referring Expression Segmentation, Referring Expression Comprehension, Image Captioning, Deep Attention, Natural Language Visual Grounding, Zero-Shot Region Description","referring-expression-segmentation-on-refcocog, zero-shot-region-description-on-refcocog-val, referring-expression-segmentation-on-refcocog-1, referring-expression-comprehension-on, referring-expression-comprehension-on-1, zero-shot-region-description-on-refcocog-test",,See all 1951 tasks,Referring Expression Segmentat,Referring Expression Segmentat
PhraseCut,PhraseCut Dataset,"PhraseCut is a dataset consisting of 77,262 images and 345,486 phrase-region pairs. The dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated.",/paper/phrasecut-language-based-image-segmentation-1,EditUnknown,Image,,,,,262 images,,,"Referring Expression Segmentation, Semantic Segmentation",referring-expression-segmentation-on,,See all 1951 tasks,Referring Expression Segmentat,Referring Expression Segmentat
RefCOCO,RefCOCO Dataset,"The RefCOCO dataset is a referring expression generation (REG) dataset used for tasks related to understanding natural language expressions that refer to specific objects in images. Here are the key details about RefCOCO:



Collection Method:
The dataset was collected using the ReferitGame, a two-player game. In this game, the first player views an image with a segmented target object and writes a natural language expression referring to that object. The second player sees only the image and the referring expression and must click on the corresponding object. If both players perform correctly, they earn points and switch roles; otherwise, they receive a new object and image for description.



Dataset Variants:
RefCOCO: Contains 142,209 refer expressions for 50,000 objects across 19,994 images. RefCOCO+: Includes 141,564 expressions for 49,856 objects in 19,992 images. RefCOCOg: This variant has 25,799 images, 95,010 referring expressions, and 49,822 object instances.



Language and Restrictions:
RefCOCO allows any type of language in the referring expressions. RefCOCO+ disallows location words in expressions to focus purely on appearance-based descriptions (e.g., ""the man in the yellow polka-dotted shirt"") rather than viewer-dependent descriptions (e.g., ""the second man from the left"").



These datasets serve as valuable resources for tasks like referring expression segmentation, comprehension, and visual grounding in computer vision research.",https://production-media.paperswithcode.com/datasets/RefCoco-0000005165-c3b501a3.jpeg,EditUnknown,Image,English,,,,994 images,,,"Visual Reasoning, Region Proposal, Referring Expression Segmentation, Visual Grounding, Referring Expression Comprehension, Semantic Segmentation, Zero-Shot Region Description","referring-expression-segmentation-on-refcocog-1, referring-expression-segmentation-on-refcoco-3, referring-expression-comprehension-on-1, zero-shot-region-description-on-refcocog-test, referring-expression-segmentation-on-refcoco-5, visual-grounding-on-refcoco-testa, zero-shot-region-description-on-refcoco-testb, referring-expression-segmentation-on-refcoco-4, zero-shot-region-description-on-refcoco-test, zero-shot-region-description-on-refcocog-val, visual-grounding-on-refcoco-val, referring-expression-segmentation-on-refcoco-8, visual-grounding-on-refcoco-test-b, referring-expression-segmentation-on-refcoco-9, referring-expression-segmentation-on-refcoco, visual-grounding-on-refcoco-testa-1, referring-expression-segmentation-on-refcocog, referring-expression-comprehension-on-refcoco-1, referring-expression-comprehension-on-refcoco, referring-expression-comprehension-on, referring-expression-segmentation-on-refcoco-6",,See all 1951 tasks,Referring Expression Segmentat,Referring Expression Segmentat
Refer-YouTube-VOS,Refer-YouTube-VOS Dataset,"There exist previous works [6, 10] that constructed referring segmentation datasets for videos. Gavrilyuk et al. [6] extended the A2D [33] and J-HMDB [9] datasets with natural sentences; the datasets focus on describing the ‘actors’ and ‘actions’ appearing in videos, therefore the instance annotations are limited to only a few object categories corresponding to the dominant ‘actors’ performing a salient ‘action’. Khoreva et al. [10] built a dataset based on DAVIS [25], but the scales are barely sufficient to learn an end-to-end model from scratch

Youtube-VOS has 4,519 high-resolution videos with 94 common object categories. Each video has pixel-level instance segmentation annotation at every 5 frames in 30-fps videos, and their durations are around 3 to 6 seconds.

We employed Amazon Mechanical Turk to annotate referring expressions. To ensure the quality of the annotations, we selected around 50 turkers after a validation test. Each turker was given a pair of videos, the original video and the mask-overlaid one with the target object highlighted, and was asked to provide a discriminative sentence within 20 words that describes the target object accurately. We collected two kinds of annotations, which describe the highlighted object (1) based on a whole video (Full-video expression) and (2) using only the
first frame of the video (First-frame expression). After the initial annotation, we conducted verification and cleaning jobs for all annotations, and dropped objects if an object cannot be localized using language expressions only. 

The followings are the statistics and analysis of the two annotation types of the dataset after the verification.

Full-video expression: Youtube-VOS has 6,459 and 1,063 unique objects in train and validation split, respectively. Among them, we cover 6,388 unique objects in 3,471 videos (6, 388/6, 459 = 98.9%) with 12,913 expressions in train split and 1,063 unique objects in 507 videos (1, 063/1, 063 = 100%) with 2,096 expressions in validation split. On average, each video has 3.8 language expressions and each expression has 10.0 words. 

First-frame expression: There are 6,006 unique objects in 3,412 videos (6, 006 /6, 459 = 93.0%) with 10,897 expressions in train split and 1,030 unique objects in 507 videos (1, 030/1, 063 = 96.9%) with 1,993 expressions in validation split. The number of annotated objects is lower than that of the full-video expressions because using only the first frame makes annotation more ambiguous and inconsistent and we dropped more annotations during the verification. On average,
each video has 3.2 language expressions and each expression has 7.5 words.",https://production-media.paperswithcode.com/datasets/806690b8-897c-4f72-8c00-9c425ef70ba1.png,EditCreative Commons Attribution 4.0 License,"Image, Video",,,,,,,,"Referring Video Object Segmentation, Referring Expression Segmentation","referring-video-object-segmentation-on-refer, referring-expression-segmentation-on-refer-1, referring-expression-segmentation-on-refer",,See all 1951 tasks,Referring Expression Segmentat,Referring Expression Segmentat
CDR,CDR Dataset,"The BioCreative V CDR task corpus is manually annotated for chemicals, diseases and chemical-induced disease (CID) relations. It contains the titles and abstracts of 1500 PubMed articles and is split into equally sized train, validation and test sets. It is common to first tune a model on the validation set and then train on the combination of the train and validation sets before evaluating on the test set. It is also common to filter negative relations with disease entities that are hypernyms of a corresponding true relations disease entity within the same abstract (see Appendix C of this paper for details).",https://production-media.paperswithcode.com/datasets/cdr.jpg,EditUnknown,Graph,,,,,,,,"Relation Extraction, Joint Entity and Relation Extraction, Reflection Removal","joint-entity-and-relation-extraction-on-cdr, relation-extraction-on-cdr",,See all 1951 tasks,Reflection Removal5 benchmarks,Reflection Removal5 benchmarks
PolarRR,PolarRR Dataset,PolarRR is a new dataset with more than 100 types of glass in which obtained transmission images are perfectly aligned with input mixed images.,https://openaccess.thecvf.com/content_CVPR_2020/papers/Lei_Polarized_Reflection_Removal_With_Perfect_Alignment_in_the_Wild_CVPR_2020_paper.pdf,EditUnknown,Image,,,,,,,,"Image Enhancement, Reflection Removal",,,See all 1951 tasks,Reflection Removal5 benchmarks,Reflection Removal5 benchmarks
SlowFlow,SlowFlow Dataset,SlowFlow is an optical flow dataset collected by applying Slow Flow technique on data from a high-speed camera and analyzing the performance of the state-of-the-art in optical flow under various levels of motion blur.,/paper/slow-flow-exploiting-high-speed-cameras-for,EditCustom,Video,,,,,,,,"Video Frame Interpolation, Optical Flow Estimation, Reflection Removal",,,See all 1951 tasks,Reflection Removal5 benchmarks,Reflection Removal5 benchmarks
bcTCGA,bcTCGA Dataset,"This data set comes from breast cancer tissue samples deposited to The Cancer Genome Atlas (TCGA) project. TCGA contains data on tumour samples were assayed on several platforms; this data set compiles results obtained using Agilent mRNA expression microarrays.

BRCA1 is the first gene identified that increases the risk of early onset breast cancer. Because BRCA1 is likely to interact with many other genes, including tumor suppressors and regulators of the cell division cycle, it is of interest to find genes with expression levels related to that of BRCA1, which we treat as the outcome of this analysis. These genes may be functionally related to BRCA1 and are useful candidates for further studies.

Expression measurements of 17,814 genes from 536 patients; all expression measurements are recorded on the log scale. There are are 491 genes with missing data, which we have excluded.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,,,,,,,regression,,,See all 1951 tasks,regression2 benchmarks2372 pap,regression2 benchmarks2372 pap
e2006,e2006 Dataset,"From the official description:


The corpus contains 10-K reports from many US companies during years
1996-2006, as well as measured volatility of stock returns for the
twelve-month periods preceding and following each report.  The data
are organized by the year of the report.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,1996,,,,,,regression,,,See all 1951 tasks,regression2 benchmarks2372 pap,regression2 benchmarks2372 pap
FLIP,FLIP Dataset,"FLIP includes several benchmark datasets that contain a variety of protein sequences, each with a real-valued label indicating its ""fitness"" (how well the protein performs some particular function). The goal is to predict the fitness of a given protein sequence using the sequence. Different representations of protein sequences (e.g. learned embeddings from large language models) may prove helpful here.

Some of the benchmark datasets (thermostability) contain a highly diverse set of sequences from many different protein families. Others (AAV, GB1) contain all sequences that are mutants of a single parent sequence. Each benchmark dataset features multiple ""splits"" -- different ways of train-test splitting the data to assess how well a model might generalize given limited information. The AAV benchmark, for example, features the ""mutant vs designed"" split in which a model is trained on randomly generated mutants and asked to predict the fitness of designed sequences, and the ""seven vs many"" split in which a model is trained on sequences with seven mutations and asked to make predictions for sequences with a different number of mutations.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditAcademic Free License v3.0,Time Series,,,,,,,,"regression, Protein Function Prediction, Protein Design",,,See all 1951 tasks,regression2 benchmarks2372 pap,regression2 benchmarks2372 pap
FLIP_--_AAV__Designed_vs_mutant,"FLIP -- AAV, Designed vs mutant Dataset","FLIP includes several benchmark datasets that contain a variety of protein sequences, each with a real-valued label indicating its ""fitness"" (how well the protein performs some particular function). The goal is to predict the fitness of a given protein sequence using the sequence. Different representations of protein sequences (e.g. learned embeddings from large language models) may prove helpful here.

This sub-dataset (AAV) is a set of 201,426 training sequences and 82,583 test sequences in which the goal is to predict the fitness of mutants of the capsid protein from the adeno-associated virus (AAV). The training set proteins were designed, while the test set proteins are random mutants. The absolute value of the fitness is not important, but its ranking / relative value is -- protein designers would like to be able to pick a sequence with high fitness relative to those in the training set. Performance is therefore usually assessed using Spearman's r correlation coefficient.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditAcademic Free License v3.0,Time Series,,,,,,,,"regression, Protein Function Prediction, Protein Design",,,See all 1951 tasks,regression2 benchmarks2372 pap,regression2 benchmarks2372 pap
Regional_AQ_Datasets,Regional AQ Datasets Dataset,"The primary environmental health threat in the WHO European Region is air pollution, impacting the daily health and well-being of its citizens significantly. To effectively understand the impact, and dynamics of air quality a detailed investigation of different environmental, weather, and land cover indices is appropriate. To this end, this paper introduces three European cities’ spatiotemporal datasets, customized for air pollution monitoring at a regional level. The datasets are composed of major air quality, weather measurements and land use information. The duration is approximately from 2020 to 2023 with an hourly temporal resolution and a spatial resolution of 0.005°. The temporal and spatiotemporal datasets are publicly released aiming to provide a solid foundation for researchers, analysts, and practitioners to conduct in-depth analyses of air pollution dynamics.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,Time Series,,2020,,,,,,"regression, Time Series Forecasting",,,See all 1951 tasks,regression2 benchmarks2372 pap,regression2 benchmarks2372 pap
Satlas,Satlas Dataset,"Satlas is a remote sensing dataset and benchmark that is large in both breadth, featuring all of the aforementioned applications and more, as well as scale, comprising 290M labels under 137 categories and 7 label modalities.",https://arxiv.org/pdf/2211.15660v1.pdf,EditUnknown,Image,,,,,,,137,"regression, Object Detection, Instance Segmentation, Semantic Segmentation",,,See all 1951 tasks,regression2 benchmarks2372 pap,regression2 benchmarks2372 pap
SciRepEval,SciRepEval Dataset,"SciRepEval is a comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search.",https://arxiv.org/pdf/2211.13308v1.pdf,EditApache-2.0 license,Image,,,,,,,,"regression, Classification, Ad-Hoc Information Retrieval, Information Retrieval",,,See all 1951 tasks,regression2 benchmarks2372 pap,regression2 benchmarks2372 pap
Seoul_Bike_Sharing_Demand,Seoul Bike Sharing Demand Dataset,"Data variables and description.
Parameters/Features                                 Abbreviation                             Type                                Measurement
Date                                                                      Date                                    year-month-day –
Rented Bike count                                          Count                                 Continuous                           0, 1, 2, .. ., 3556
Hour                                                                       Hour                                   Continuous                           0, 1, 2, .. ., 23
Temperature                                                      Temp                                   Continuous                            ◦C
Humidity                                                               Hum                                   Continuous                              %
Windspeed                                                           Wind                                  Continuous                              m/s
Visibility                                                                  Visb                                   Continuous                              10 m
Dew point temperature                                  Dew                                   Continuous                                ◦C
Solar radiation                                                     Solar                                  Continuous                              MJ/m2
Rainfall                                                                     Rain                                   Continuous                               Mm
Snowfall                                                                   Snow                                  Continuous                               cm
Seasons                                                                     Seasons                           Categorical                              Autumn, Spring, Summer, Winter
Holiday                                                                       Holiday                           Categorical                               Holiday, Workday
Functional Day                                                        Fday                                 Categorical                              NoFunc, Func
Week status                                                              Wstatus                          Categorical                             Weekday (Wday), Weekend (Wend)
Day of the week                                                       Dweek                            Categorical                              Sunday, Monday, .. ., Saturday",https://production-media.paperswithcode.com/datasets/82bda2a9-e9ee-4cd7-8a50-521b2660a3cf.png,EditCreative Commons Attribution 4.0 International,,,,,,,,,regression,,,See all 1951 tasks,regression2 benchmarks2372 pap,regression2 benchmarks2372 pap
TTE-A_O,TTE-A&O Dataset,"The dataset includes two parts corresponding to the cities of Abakan (65524 nodes, 340012 edges) and Omsk (231688 nodes, 1149492 edges). Along with the road network graph, it includes trip records represented as sequences of visited nodes (making the dataset suitable both for path-blind and path-aware settings). There are two types of target values for a regression task: real travel time and real length of a trip.",https://production-media.paperswithcode.com/datasets/043e4f38-0e63-4c8c-8723-93f7aa5d11df.png,EditMIT,,,,,,,,,"regression, Travel Time Estimation",travel-time-estimation-on-tte-a-o,,See all 1951 tasks,regression2 benchmarks2372 pap,regression2 benchmarks2372 pap
Avalon,Avalon Dataset,"Avalon is a benchmark for generalization in Reinforcement Learning (RL). The benchmark consists of a set of tasks in which embodied agents in highly diverse procedural 3D worlds must survive by navigating terrain, hunting or gathering food, and avoiding hazards. Avalon is unique among existing RL benchmarks in that the reward function, world dynamics, and action space are the same for every task, with tasks differentiated solely by altering the environment; its 20 tasks, ranging in complexity from eat and throw to hunt and navigate, each create worlds in which the agent must perform specific skills in order to survive. This benchmark setup enables investigations of generalization within tasks, between tasks, and to compositional tasks that require combining skills learned from previous tasks.",https://arxiv.org/pdf/2210.13417v1.pdf,EditUnknown,,,,,,,,,"General Reinforcement Learning, Reinforcement Learning (RL)",,,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
CivRealm,CivRealm Dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,,,,,,,"Reinforcement Learning (RL), Multi-agent Reinforcement Learning",,,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
Gun_Detection_Dataset,Gun Detection Dataset Dataset,This is a gun detection dataset with 51K annotated gun images for gun detection and other 51K cropped gun chip images for gun classification collected from a few different sources.,https://production-media.paperswithcode.com/datasets/Screenshot_2021-05-06_at_09.30.05.jpg,EditCustom,Image,,,,,,,,"Object Detection, Novel View Synthesis, Reinforcement Learning (RL), N-Queens Problem - All Possible Solutions, EEG Signal Classification","eeg-signal-classification-on, n-queens-problem-all-possible-solutions-on, reinforcement-learning-rl-on-1, novel-view-synthesis-on",,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
ManiSkill2,ManiSkill2 Dataset,"ManiSkill2 is the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. It includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D input data simulated by fully dynamic engines.",https://arxiv.org/pdf/2302.04659v1.pdf,EditApache-2.0 license,,,2000,,,,,,"Robot Manipulation, Reinforcement Learning (RL), Imitation Learning",,,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
POPGym,POPGym Dataset,"POPGym is designed to benchmark memory in deep reinforcement learning. It contains a set of environments and a collection of memory model baselines. The environments are all Partially Observable Markov Decision Process (POMDP) environments following the Openai Gym interface. Our environments follow a few basic tenets:


Painless Setup - popgym environments require only gym, numpy, and mazelib as dependencies
Laptop-Sized Tasks - Most tasks can be solved in less than a day on the CPU 
True Generalization - All environments are heavily randomized.

The paper uses 15M environment steps for each trial.",https://production-media.paperswithcode.com/datasets/4f8276f6-0410-4020-90cf-cafa37a61e07.png,EditMIT,"Time Series, Video",,,,,,,,"Temporal Sequences, Model-based Reinforcement Learning, General Reinforcement Learning, Reinforcement Learning (RL), Partially Observable Reinforcement Learning",partially-observable-reinforcement-learning,,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
PRM800K,PRM800K Dataset,"PRM800K is a process supervision dataset containing 800,000 step-level correctness labels for model-generated solutions to problems from the MATH dataset.",https://github.com/openai/prm800k,EditMIT License,,,,,,,,,Reinforcement Learning (RL),,,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
ProcGen,ProcGen Dataset,Procgen Benchmark includes 16 simple-to-use procedurally-generated environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills.,https://production-media.paperswithcode.com/datasets/Screenshot_2021-09-13_at_10.05.41.png,EditUnknown,,,,,,,,,"General Reinforcement Learning, Reinforcement Learning (RL)",reinforcement-learning-on-procgen,,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
QDax,QDax Dataset,"QDax is a benchmark suite designed for for Deep Neuroevolution in Reinforcement Learning domains for robot control. The suite includes the definition of tasks, environments, behavioral descriptors, and fitness. It specify different benchmarks based on the complexity of both the task and the agent controlled by a deep neural network. The benchmark uses standard Quality-Diversity metrics, including coverage, QD-score, maximum fitness, and an archive profile metric to quantify the relation between coverage and fitness.",https://arxiv.org/pdf/2211.02193v1.pdf,EditMIT license,,,,,,,,,"Reinforcement Learning (RL), Robot Navigation",,,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
SMACv2,SMACv2 Dataset,SMACv2 (StarCraft Multi-Agent Challenge v2) is a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation.,https://arxiv.org/pdf/2212.07489v1.pdf,EditMIT License,,,,,,,,,Reinforcement Learning (RL),,,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
V-D4RL,V-D4RL Dataset,"V-D4RL provides pixel-based analogues of the popular D4RL benchmarking tasks, derived from the dm_control suite, along with natural extensions of two state-of-the-art online pixel-based continuous control algorithms, DrQ-v2 and DreamerV2, to the offline setting.",https://production-media.paperswithcode.com/datasets/fc631983-b048-4ed9-b29b-4ca1a0753418.png,EditMIT,,,,,,,,,Reinforcement Learning (RL),,,See all 1951 tasks,Reinforcement Learning  RL 6 b,Reinforcement Learning  RL 6 b
Relational_Captioning1_benchmark2_papers_with_code,Relational Captioning1 benchmark2 papers with code Dataset,,https://paperswithcode.com/dataset/relational-captioning,,,,,,,,,,,,,See all 1951 tasks,Relational Captioning1 benchma,Relational Captioning1 benchma
Animals-10,Animals-10 Dataset,"It contains about 28K medium quality animal images belonging to 10 categories: dog, cat, horse, spyder, butterfly, chicken, sheep, cow, squirrel, and elephant.

All the images have been collected from ""google images"" and have been checked by humans. There is some erroneous data to simulate real conditions (eg. images taken by users of your app).
The main directory is divided into folders, one for each category. The image count for each category varies from 2K to 5 K units.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://www.kaggle.com/datasets/alessiocorrado99/animals10,"Image, Video",,,,,,,10,"Image Classification, Multi-Animal Tracking with identification, Representation Learning",representation-learning-on-animals-10,,See all 1951 tasks,Representation Learning16 benc,Representation Learning16 benc
Causal_Triplet,Causal Triplet Dataset,"Causal Triplet is a causal representation learning benchmark featuring not only visually more complex scenes, but also two crucial desiderata commonly overlooked in previous works: 

1) An actionable counterfactual setting, where only certain object-level variables allow forcounterfactual observations whereas others do not. 

2) An interventional downstream task with an emphasis on out-of-distribution robustness from the independent causal mechanisms principle.",https://arxiv.org/pdf/2301.05169v1.pdf,EditApache-2.0 license,,,,,,,,,Representation Learning,,,See all 1951 tasks,Representation Learning16 benc,Representation Learning16 benc
EXTREME_CLASSIFICATION,EXTREME CLASSIFICATION Dataset,"The objective in extreme multi-label classification is to learn feature architectures and classifiers that can automatically tag a data point with the most relevant subset of labels from an extremely large label set. This repository provides resources that can be used for evaluating the performance of extreme multi-label algorithms including datasets, code, and metrics.

For more details please visit the link http://manikvarma.org/downloads/XC/XMLRepository.html",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY-NC-ND,"Image, Text",English,,,,,,,"Product Recommendation, Multi-Label Classification, Multi-Label Learning, Extreme Multi-Label Classification, Multi-Label Text Classification, Representation Learning",,,See all 1951 tasks,Representation Learning16 benc,Representation Learning16 benc
OmniBenchmark,OmniBenchmark Dataset,"Omni-Realm Benchmark (OmniBenchmark) is a diverse (21 semantic realm-wise datasets) and concise (realm-wise datasets have no concepts overlapping) benchmark for evaluating pre-trained model generalization across semantic super-concepts/realms, e.g. across mammals to aircraft. 

[ECCV2022]",https://production-media.paperswithcode.com/datasets/59a61c4e-b055-422e-8723-c63284bd8c86.png,EditCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,Image,,,,,,,,"Prompt Engineering, Image Classification, Representation Learning, Fine-Grained Image Recognition",image-classification-on-omnibenchmark,,See all 1951 tasks,Representation Learning16 benc,Representation Learning16 benc
SciDocs,SciDocs Dataset,SciDocs evaluation framework consists of a suite of evaluation tasks designed for document-level tasks.,https://github.com/allenai/scidocs,EditUnknown,"Image, Text",English,,,,,,,"Zero-shot Text Search, Re-Ranking, Language Modelling, Text Retrieval, Representation Learning, Document Classification","zero-shot-text-search-on-scidocs, re-ranking-on-scidocs, text-retrieval-on-scidocs, representation-learning-on-scidocs",,See all 1951 tasks,Representation Learning16 benc,Representation Learning16 benc
Sports10,Sports10 Dataset,"Games dataset containing 100,000 Gameplay Images of 175 Video Games across 10 Sports Genres - AMERICAN FOOTBALL, BASKETBALL, BIKE RACING, CAR RACING, FIGHTING, HOCKEY, SOCCER, TABLE TENNIS, TENNIS. 



Hand-curated images to remove menu/transition frames and only include gameplay sequences.



Games are divided into three visual styling categories: 
        RETRO (arcade-style, 1990s and earlier)
        MODERN (roughly 2000s)
        PHOTOREAL (roughly late 2010s).",https://production-media.paperswithcode.com/datasets/Sports10_Banner_Image.png,EditApache License 2.0,Image,,,,,,,,"Image Classification, Representation Learning","representation-learning-on-sports10, image-classification-on-sports10",,See all 1951 tasks,Representation Learning16 benc,Representation Learning16 benc
SYNTH-PEDES,SYNTH-PEDES Dataset,"SYNTH-PEDES is a large-scale person dataset with image-text pairs by far, which contains 312,321 identities, 4,791,711 images, and 12,138,157 textual descriptions.",https://github.com/zplusdragon/plip,EditMIT License,,,,,,711 images,,,Representation Learning,,,See all 1951 tasks,Representation Learning16 benc,Representation Learning16 benc
TYC_Dataset,TYC Dataset Dataset,"We introduce the trapped yeast cell (TYC) dataset, a novel dataset for understanding instance-level semantics and motions of cells in microstructures. We release $105$ dense annotated high-resolution brightfield microscopy images, including about $19$k instance masks. We also release $261$ curated video clips composed of $1293$ high-resolution microscopy images to facilitate unsupervised understanding of cell motions and morphology.",https://production-media.paperswithcode.com/datasets/179dbc32-c6f0-45c6-98eb-93a3a8b45ca7.gif,EditCC BY 4.0,Image,,,,,,,,"Unsupervised Pre-training, Instance Segmentation, Unsupervised Image Segmentation, Panoptic Segmentation, Cell Segmentation, Representation Learning",,,See all 1951 tasks,Representation Learning16 benc,Representation Learning16 benc
xView3-SAR,xView3-SAR Dataset,"Unsustainable fishing practices worldwide pose a major threat to marine resources and ecosystems. Identifying vessels that do not show up in conventional monitoring systems---known as ``dark vessels''---is key to managing and securing the health of marine environments. With the rise of satellite-based synthetic aperture radar (SAR) imaging and modern machine learning (ML), it is now possible to automate detection of dark vessels day or night, under all-weather conditions. SAR images, however, require a domain-specific treatment and are not widely accessible to the ML community. Maritime objects (vessels and offshore infrastructure) are relatively small and sparse, challenging traditional computer vision approaches. We present the largest labeled dataset for training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The images are annotated using a combination of automated and manual analysis. Co-located bathymetry and wind state rasters accompany every SAR image. We also provide an overview of the xView3 Computer Vision Challenge, an international competition using xView3-SAR for ship detection and characterization at large scale. We release the data  (\href{https://iuu.xview.us/}{https://iuu.xview.us/}) and code (\href{https://github.com/DIUx-xView}{https://github.com/DIUx-xView}) to support ongoing development and evaluation of ML approaches for this important application.",https://production-media.paperswithcode.com/datasets/40879ffc-551e-4dd1-a051-8c0f1797cbb8.jpg,EditCC BY-NC-SA 4.0,Image,,,,,,"training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images",,"Object Detection, Dense Object Detection, Holdout Set, regression, Decision Making Under Uncertainty, Representation Learning",holdout-set-on-xview3-sar,,See all 1951 tasks,Representation Learning16 benc,Representation Learning16 benc
ALCE,ALCE Dataset,ALCE is a benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations.,https://arxiv.org/pdf/2305.14627v1.pdf,EditMIT License,,,,,,,,,Retrieval,,,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
ARO,ARO Dataset,"Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO-Order & Flickr30k-Order, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases.",https://production-media.paperswithcode.com/datasets/09c174ea-97ca-404d-97dd-80e5e0992c57.png,EditUnknown,,,,,,,,,Retrieval,,,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
HotpotQA,HotpotQA Dataset,"HotpotQA is a question answering dataset collected on the English Wikipedia, containing about 113K crowd-sourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. 

A diverse range of reasoning strategies are featured in HotpotQA, including questions involving missing entities in the question, intersection questions (What satisfies property A and property B?), and comparison questions, where two entities are compared by a common attribute, among others. In the few-document distractor setting, the QA models are given ten paragraphs in which the gold paragraphs are guaranteed to be found; in the open-domain fullwiki setting, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F1, and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F1). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously.",https://arxiv.org/abs/1910.07000,EditCC BY-SA 4.0,Text,English,,,,,,,"Reading Comprehension, Text Retrieval, Retrieval, Question Answering","retrieval-on-hotpotqa, text-retrieval-on-hotpotqa, question-answering-on-hotpotqa",,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
InfoSeek,InfoSeek Dataset,"In this project, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training.",https://production-media.paperswithcode.com/datasets/2f8d4199-ea7b-4d02-b117-7f404ac67520.jpeg,EditApache-2.0,"Image, Text",English,,,,,,,"Open-Domain Question Answering, Visual Question Answering (VQA), Retrieval","retrieval-on-infoseek, visual-question-answering-vqa-on-infoseek",,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
Natural_Questions,Natural Questions Dataset,"The Natural Questions corpus is a question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples. Each example is comprised of a google.com query and a corresponding Wikipedia page. Each Wikipedia page has a passage (or long answer) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer. The long and the short answer annotations can however be empty. If they are both empty, then there is no answer on the page at all. If the long answer annotation is non-empty, but the short answer annotation is empty, then the annotated passage answers the question but no explicit short answer could be found. Finally 1% of the documents have a passage annotated with a short answer that is “yes” or “no”, instead of a list of short spans.",https://arxiv.org/abs/1901.08634,EditCC BY-SA 3.0,Text,English,,,,,"training examples, 7,830 development examples",,"Zero-shot Text Search, Passage Retrieval, Retrieval, Open-Domain Question Answering, Question Generation, Text Retrieval, Question Answering","zero-shot-text-search-on-nq, open-domain-question-answering-on-natural, retrieval-on-natural-questions, open-domain-question-answering-on-natural-1, question-generation-on-natural-questions, text-retrieval-on-natural-questions, question-answering-on-nq-beir, question-answering-on-natural-questions, passage-retrieval-on-natural-questions, question-answering-on-natural-questions-long",,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
OK-VQA,OK-VQA Dataset,"Outside Knowledge Visual Question Answering (OK-VQA) includes more than 14,000 questions that require external knowledge to answer.",https://arxiv.org/pdf/1906.00067v2.pdf,EditUnknown,"Image, Text",English,,,,,,,"Question Generation, Visual Question Answering (VQA), Retrieval, Question Answering","retrieval-on-ok-vqa, visual-question-answering-on-ok-vqa",,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
Polyvore,Polyvore Dataset,"This dataset contains 21,889 outfits from polyvore.com, in which 17,316 are for training, 1,497 for validation and 3,076 for testing.",https://github.com/xthan/polyvore-dataset,EditUnknown,,,,,,,,,"Slot Filling, Recommendation Systems, Retrieval","retrieval-on-polyvore, slot-filling-on-polyvore, recommendation-systems-on-polyvore",,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
PopQA,PopQA Dataset,"PopQA is an open-domain QA dataset with 14k QA pairs with fine-grained Wikidata entity ID, Wikipedia page views, and relationship type information.",https://arxiv.org/pdf/2212.10511v1.pdf,EditUnknown,Text,English,,,,,,,"Knowledge Probing, Memorization, Retrieval, Question Answering",question-answering-on-popqa,,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
QAMPARI,QAMPARI Dataset,"QAMPARI is an ODQA benchmark, where question answers are lists of entities, spread across many paragraphs. It was created by (a) generating questions with multiple answers from Wikipedia's knowledge graph and tables, (b) automatically pairing answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing questions and validating each answer.",https://paperswithcode.com/paper/qampari-an-open-domain-question-answering,EditUnknown,Text,English,,,,,,,"Answer Generation, Passage Retrieval, Retrieval, Open-Domain Question Answering, Natural Questions, Question Answering",,,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
Quora_Question_Pairs,Quora Question Pairs Dataset,"Quora Question Pairs (QQP) dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other.",/paper/bilateral-multi-perspective-matching-for,EditCustom (non-commercial),Text,English,,,,,,,"Community Question Answering, Paraphrase Identification within Bi-Encoder, Retrieval, Paraphrase Generation, Natural Language Inference, QQP, Text Retrieval, Paraphrase Identification, Question Answering","qqp-on-qqp, community-question-answering-on-quora, natural-language-inference-on-quora-question, text-retrieval-on-quora-question-pairs, retrieval-on-quora-question-pairs, question-answering-on-quora-question-pairs, paraphrase-identification-on-quora-question-1, paraphrase-generation-on-quora-question-pairs-1, paraphrase-identification-within-bi-encoder, paraphrase-identification-on-quora-question",,See all 1951 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
RF-based_Visual_Tracking_Dataset,RF-based Visual Tracking Dataset,,https://paperswithcode.com/dataset/rf-based-visual-tracking,,,,,,,,,,,,,See all 1951 tasks,RF-based Visual Tracking,RF-based Visual Tracking
RGB-T_Salient_Object_Detection6_papers_with_code_D,RGB-T Salient Object Detection6 papers with code Dataset,,https://paperswithcode.com/dataset/rgb-t-salient-object-detection,,,,,,,,,,,,,See all 1951 tasks,RGB-T Salient Object Detection,RGB-T Salient Object Detection
LasHeR,LasHeR Dataset,"LasHeR consists of 1224 visible and thermal infrared video pairs with more than 730K frame pairs in total. Each frame pair is spatially aligned and manually annotated with a bounding box, making the dataset well and densely annotated. LasHeR is highly diverse capturing from a broad range of object categories, camera viewpoints, scene complexities and environmental factors across seasons, weathers, day and night.",https://production-media.paperswithcode.com/datasets/7b603fe0-7be2-4ec5-bf12-a69ada423683.jpg,EditUnknown,"Image, Video",,,,,,,,Rgb-T Tracking,rgb-t-tracking-on-lasher,,See all 1951 tasks,Rgb-T Tracking4 benchmarks23 p,Rgb-T Tracking4 benchmarks23 p
RGBT234,RGBT234 Dataset,"The RGBT234 dataset is a comprehensive video dataset specifically designed for RGB-T (Red-Green-Blue and Thermal) tracking purposes. This dataset addresses the limitations of existing datasets like OSU-CT, LITIV, and GTOT in terms of size. RGBT234 consists of 234 RGB-T videos, each containing both an RGB video and a thermal video. The total number of frames in the dataset is approximately 234,000, with the largest video pair containing up to 8,000 frames.Each frame in the RGBT234 dataset is annotated with a minimum bounding box that covers the target for both the RGB and thermal modalities. The dataset also includes various environmental challenges such as rainy conditions, nighttime scenes, cold and hot weather scenarios. To analyze the performance of different tracking algorithms based on specific attributes, the RGBT234 dataset annotates 12 attributes and provides baseline trackers, including both deep learning and non-deep learning methods like structured SVM, sparse representation, and correlation filter-based trackers. Additionally, the dataset employs 5 metrics to evaluate the performance of RGB-T trackers effectively.",https://production-media.paperswithcode.com/datasets/7ba1268a-6186-49c8-ba59-e3fd389b25a9.png,EditUnknown,"Image, Video",,,,,,,,Rgb-T Tracking,rgb-t-tracking-on-rgbt234,,See all 1951 tasks,Rgb-T Tracking4 benchmarks23 p,Rgb-T Tracking4 benchmarks23 p
DUT-OMRON,DUT-OMRON Dataset,"The DUT-OMRON dataset is used for evaluation of Salient Object Detection task and it contains 5,168 high quality images. The images have one or more salient objects and relatively cluttered background.",https://arxiv.org/abs/2003.00651,EditUnknown,Image,,,,,,"valuation of Salient Object Detection task and it contains 5,168 high quality images",,"Salient Object Detection, RGB Salient Object Detection, Saliency Detection, Unsupervised Saliency Detection","unsupervised-saliency-detection-on-dut-omron, salient-object-detection-on-dut-omron, salient-object-detection-on-dut-omron-2, saliency-detection-on-dut-omron",,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
DUTS,DUTS Dataset,"DUTS is a saliency detection dataset containing 10,553 training images and 5,019 test images. All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set. Both the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects.",http://saliencydetection.net/duts/,EditUnknown,Image,,,,,,"training images and 5,019 test images",,"Saliency Detection, Salient Object Detection, RGB Salient Object Detection, Unsupervised Object Segmentation, Unsupervised Saliency Detection","unsupervised-saliency-detection-on-duts, salient-object-detection-on-duts-te-1, unsupervised-object-segmentation-on-duts, saliency-detection-on-duts-test, salient-object-detection-on-duts-te",,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
ECSSD,ECSSD Dataset,"The Extended Complex Scene Saliency Dataset (ECSSD) is comprised of complex scenes, presenting textures and structures common to real-world images. ECSSD contains 1,000 intricate images and respective ground-truth saliency maps, created as an average of the labeling of five human participants.",https://arxiv.org/abs/2003.04820,EditUnknown,Image,,,,,,,,"Saliency Detection, Salient Object Detection, RGB Salient Object Detection, Unsupervised Object Segmentation, Unsupervised Saliency Detection","salient-object-detection-on-ecssd-1, saliency-detection-on-ecssd, unsupervised-object-segmentation-on-ecssd, salient-object-detection-on-ecssd, unsupervised-saliency-detection-on-ecssd",,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
HKU-IS,HKU-IS Dataset,"HKU-IS is a visual saliency prediction dataset which contains 4447 challenging images, most of which have either low contrast or multiple salient objects.",https://arxiv.org/abs/1603.01976,EditUnknown,Image,,,,,,,,"Salient Object Detection, RGB Salient Object Detection, Saliency Detection","salient-object-detection-on-hku-is, saliency-detection-on-hku-is, salient-object-detection-on-hku-is-1",,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
HRSOD,HRSOD Dataset,"There exist several datasets for saliency detection, but none of them is specifically designed for high-resolution salient object detection. High-Resolution Salient Object Detection (HRSOD) dataset, containing 1610 training images and 400 test images. The total 2010 images are collected from the website of Flickr with the license of all creative commons. Pixel-level ground truths are manually annotated by 40 subjects. The shortest edge of each image in HRSOD is more than 1200 pixels.",https://production-media.paperswithcode.com/datasets/10d57177-03cb-4f1c-b0b9-ff022b29e2b2.png,Editunknown,Image,,2010,,,2010 images,training images and 400 test images,,"Salient Object Detection, RGB Salient Object Detection",rgb-salient-object-detection-on-hrsod,,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
ISTD,ISTD Dataset,"The Image Shadow Triplets dataset (ISTD) is a dataset for shadow understanding that contains 1870 image triplets of shadow image, shadow mask, and shadow-free image.",https://arxiv.org/pdf/1712.02478.pdf,"EditCustom (research-only, non-commercial)",Image,,,,,,,,"RGB Salient Object Detection, Shadow Removal","shadow-removal-on-istd, salient-object-detection-on-istd",,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
PASCAL-S,PASCAL-S Dataset,PASCAL-S is a dataset for salient object detection consisting of a set of 850 images from PASCAL VOC 2010 validation set with multiple salient objects on the scenes.,https://arxiv.org/abs/1909.04366,EditUnknown,Image,,2010,,,850 images,,,"Salient Object Detection, RGB Salient Object Detection, Saliency Detection","salient-object-detection-on-pascal-s-1, salient-object-detection-on-pascal-s, saliency-detection-on-pascal-s",,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
SBU___SBU-Refine,SBU / SBU-Refine Dataset,"SBU-Kinect-Interaction dataset version 2.0 comprises of RGB-D video sequences of humans performing interaction activities that are recording using the Microsoft Kinect sensor. This dataset was originally recorded for a class project, and it must be used only for the purposes of research. If you use this dataset in your work, please cite the following paper.
Kiwon Yun, Jean Honorio, Debaleena Chattopadhyay, Tamara L. Berg, and Dimitris Samaras, The 2nd International Workshop on Human Activity Understanding from 3D Data at Conference on Computer Vision and Pattern Recognition (HAU3D-CVPRW), CVPR 2012
SBU-Refine: SBU-Refine relabels the test set manually and refines the noise labels in training set by algorithm. H. Yang, T. Wang, X. Hu, and C.-W. Fu, “SILT: Shadow-aware iterative label tuning for learning to detect shadows from noisy labels,” in ICCV, 2023, pp. 12 687–12 698.",https://production-media.paperswithcode.com/datasets/0dd86abf-45c4-436e-a5d3-e1a912394128.gif,EditUnknown,"Image, Video",,2012,,,,,,"RGB Salient Object Detection, Shadow Detection, Skeleton Based Action Recognition, Human Interaction Recognition","salient-object-detection-on-sbu, skeleton-based-action-recognition-on-sbu, human-interaction-recognition-on-sbu, shadow-detection-on-sbu",,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
SOC,SOC Dataset,"SOC (Salient Objects in Clutter) is a dataset for Salient Object Detection (SOD). It includes images with salient and non-salient objects from daily object categories. Beyond object category annotations, each salient image is accompanied by attributes that reflect common challenges in real-world scenes.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-05-10_at_09.54.42.jpg,EditUnknown,Image,,,,,,,,"Salient Object Detection, RGB Salient Object Detection",salient-object-detection-on-soc,,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
VT5000,VT5000 Dataset,Includes 5000 spatially aligned RGBT image pairs with ground truth annotations. VT5000 has 11 challenges collected in different scenes and environments for exploring the robustness of algorithms.,/paper/rgbt-salient-object-detection-a-large-scale,EditUnknown,Image,,,,,,,,"Salient Object Detection, Object Detection, RGB Salient Object Detection",,,See all 1951 tasks,RGB Salient Object Detection33,RGB Salient Object Detection33
KITTI-C,KITTI-C Dataset,"🤖 Robo3D - The KITTI-C Benchmark
KITTI-C is an evaluation benchmark heading toward robust and reliable 3D object detection in autonomous driving. With it, we probe the robustness of 3D detectors under out-of-distribution (OoD) scenarios against corruptions that occur in the real-world environment. Specifically, we consider natural corruptions happen in the following cases:


Adverse weather conditions, such as fog, wet ground, and snow;
External disturbances that are caused by motion blur or result in LiDAR beam missing;
Internal sensor failure, including crosstalk, possible incomplete echo, and cross-sensor scenarios.

KITTI-C is part of the Robo3D benchmark. Visit our homepage to explore more details.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,"3D, Image",English,,,,,,,"Unsupervised Monocular Depth Estimation, Robust 3D Object Detection, 3D Object Detection","robust-3d-object-detection-on-kitti-c, unsupervised-monocular-depth-estimation-on-7",,See all 1951 tasks,Robust 3D Object Detection2 be,Robust 3D Object Detection2 be
nuScenes-C,nuScenes-C Dataset,"🤖 Robo3D - The nuScenes-C Benchmark
nuScenes-C is an evaluation benchmark heading toward robust and reliable 3D perception in autonomous driving. With it, we probe the robustness of 3D detectors and segmentors under out-of-distribution (OoD) scenarios against corruptions that occur in the real-world environment. Specifically, we consider natural corruptions happen in the following cases:


Adverse weather conditions, such as fog, wet ground, and snow;
External disturbances that are caused by motion blur or result in LiDAR beam missing;
Internal sensor failure, including crosstalk, possible incomplete echo, and cross-sensor scenarios.

SemanticKITTI-C is part of the Robo3D benchmark. Visit our homepage to explore more details.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,"3D, Image",English,,,,,,,"Robust 3D Object Detection, Robust 3D Semantic Segmentation, 3D Semantic Segmentation, Robust Camera Only 3D Object Detection, 3D Object Detection","robust-3d-semantic-segmentation-on-nuscenes-c, robust-3d-object-detection-on-nuscenes-c, robust-camera-only-3d-object-detection-on",,See all 1951 tasks,Robust 3D Object Detection2 be,Robust 3D Object Detection2 be
RoomEnv-v0,RoomEnv-v0 Dataset,"The Room environment - v0


We have released a challenging Gymnasium compatible
environment. The best strategy for this environment is to have both episodic and semantic
memory systems. See the paper for more information.

Prerequisites

A unix or unix-like x86 machine
python 3.10 or higher.
Running in a virtual environment (e.g., conda, virtualenv, etc.) is highly recommended so that you don't mess up with the system python.
This env is added to the PyPI server. Just run: pip install room-env

Data collection
Data is collected from querying ConceptNet APIs. For simplicity, we only collect triples
whose format is (head, atlocation, tail). Here head is one of the 80 MS COCO
dataset categories. This was kept in mind so that later on we can use images as well.

If you want to collect the data manually, then run below:

python collect_data.py

How does this environment work?
The Gymnasium-compatible Room environment is one big room with
N<sub>people</sub> number of people who can freely move
around. Each of them selects one object, among
N<sub>objects</sub>, and places it in one of the
N<sub>locations</sub> locations.
N<sub>agents</sub> number of agent(s) are also in this
room. They can only observe one human placing an object, one at a time;
x<sup>(t)</sup>. At the same time, they are given one question
about the location of an object; q<sup>(t)</sup>.
x<sup>(t)</sup> is given as a quadruple,
(h<sup>(t)</sup>,r<sup>(t)</sup>,t<sup>(t)</sup>,t),
For example, &lt;James’s laptop, atlocation, James’s desk, 42&gt; accounts
for an observation where an agent sees James placing his laptop on his
desk at t = 42. q<sup>(t)</sup> is given as a double,
(h,r). For example, &lt;Karen’s cat, atlocation&gt; is asking where
Karen’s cat is located. If the agent answers the question correctly, it
gets a reward of  + 1, and if not, it gets 0.

The reason why the observations and questions are given as
RDF-triple-like format is two folds. One is that this structured format
is easily readable / writable by both humans and machines. Second is
that we can use existing knowledge graphs, such as ConceptNet .

To simplify the environment, the agents themselves are not actually
moving, but the room is continuously changing. There are several random
factors in this environment to be considered:



With the chance of p<sub>commonsense</sub>,
   a human places an object in a commonsense location (e.g., a laptop
   on a desk). The commonsense knowledge we use is from ConceptNet.
   With the chance of
   1 − p<sub>commonsense</sub>, an object is
   placed at a non-commonsense random location (e.g., a laptop on the
   tree).



With the chance of
   p<sub>new_location</sub>, a human changes
   object location.



With the chance of p<sub>new_object</sub>, a
   human changes his/her object to another one.



With the chance of
   p<sub>switch_person</sub>, two people
   switch their locations. This is done to mimic an agent moving around
   the room.



All of the four probabilities account for the Bernoulli distributions.

Consider there is only one agent. Then this is a POMDP, where S<sub>t</sub> = (x<sup>(t)</sup>, q<sup>(t)</sup>), A<sub>t</sub> = (do something with x<sup>(t)</sup>, answer q<sup>(t)</sup>), and R<sub>t</sub> ∈ {0, 1}.

Currently there is no RL trained for this. We only have some heuristics. Take a look at the paper for more details.

RoomEnv-v0
```python
import gymnasium as gym

env = gym.make(""room_env:RoomEnv-v0"")
(observation, question), info = env.reset()
rewards = 0

while True:
    (observation, question), reward, done, truncated, info = env.step(""This is my answer!"")
    rewards += reward
    if done:
        break

print(rewards)
```

Every time when an agent takes an action, the environment will give you an observation
and a question to answer. You can try directly answering the question,
such as env.step(""This is my answer!""), but a better strategy is to keep the
observations in memory systems and take advantage of the current observation and the
history of them in the memory systems.

Take a look at this repo for an actual
interaction with this environment to learn a policy.

Contributing
Contributions are what make the open source community such an amazing place to be learn,
inspire, and create. Any contributions you make are greatly appreciated.


Fork the Project
Create your Feature Branch (git checkout -b feature/AmazingFeature)
Run make test &amp;&amp; make style &amp;&amp; make quality in the root repo directory,
   to ensure code quality.
Commit your Changes (git commit -m 'Add some AmazingFeature')
Push to the Branch (git push origin feature/AmazingFeature)
Open a Pull Request

Cite our paper
bibtex
@misc{https://doi.org/10.48550/arxiv.2204.01611,
  doi = {10.48550/ARXIV.2204.01611},
  url = {https://arxiv.org/abs/2204.01611},
  author = {Kim, Taewoon and Cochez, Michael and Francois-Lavet, Vincent and Neerincx,
  Mark and Vossen, Piek},
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences,
  FOS: Computer and information sciences},
  title = {A Machine With Human-Like Memory Systems},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

Authors

Taewoon Kim
Michael Cochez
Vincent Francois-Lavet
Mark Neerincx
Piek Vossen

License
MIT",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,,,2022,,,,,,"RoomEnv-v0, Multi-agent Reinforcement Learning",roomenv-v0-on-roomenv-v0,,See all 1951 tasks,RoomEnv-v01 benchmark1 papers ,RoomEnv-v01 benchmark1 papers 
CAT2000,CAT2000 Dataset,"Includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings.",https://arxiv.org/pdf/1505.03581v1.pdf,EditUnknown,"Image, Time Series",,,,,4000 images,,20,"Saliency Detection, Saliency Prediction","saliency-detection-on-cat2000, saliency-prediction-on-cat2000",,See all 1951 tasks,Saliency Detection27 benchmark,Saliency Detection27 benchmark
HS-SOD,HS-SOD Dataset,HS-SOD is a hyperspectral salient object detection dataset with a collection of 60 hyperspectral images with their respective ground-truth binary images and representative rendered colour images (sRGB).,/paper/hyperspectral-image-dataset-for-benchmarking,EditUnknown,Image,,,,,,,,"Object Detection, RGB Salient Object Detection, Saliency Detection",,,See all 1951 tasks,Saliency Detection27 benchmark,Saliency Detection27 benchmark
PASCAL_Context,PASCAL Context Dataset,"The PASCAL Context dataset is an extension of the PASCAL VOC 2010 detection challenge, and it contains pixel-wise labels for all training images. It contains more than 400 classes (including the original 20 classes plus backgrounds from PASCAL VOC segmentation), divided into three categories (objects, stuff, and hybrids). Many of the object categories of this dataset are too sparse and; therefore, a subset of 59 frequent classes are usually selected for use.",https://arxiv.org/abs/2001.05566,EditUnknown,"Image, Text",English,2010,,,,,400,"Boundary Detection, Saliency Detection, Surface Normals Estimation, Zero-Shot Learning, Human Parsing, Semantic Segmentation","saliency-detection-on-pascal-context, boundary-detection-on-pascal-context, surface-normals-estimation-on-pascal-context, zero-shot-learning-on-pascal-context, semantic-segmentation-on-pascal-context, human-parsing-on-pascal-context",,See all 1951 tasks,Saliency Detection27 benchmark,Saliency Detection27 benchmark
ReDWeb-S,ReDWeb-S Dataset,ReDWeb-S is a large-scale challenging dataset for Salient Object Detection. It has totally 3179 images with various real-world scenes and high-quality depth maps. The dataset is split into a training set with 2179 RGB-D image pairs and a testing set with the remaining 1000 image pairs.,https://github.com/nnizhang/SMAC,EditUnknown,Image,,,,,3179 images,,,"Object Detection, RGB-D Salient Object Detection, Saliency Detection",,,See all 1951 tasks,Saliency Detection27 benchmark,Saliency Detection27 benchmark
AViMoS,AViMoS Dataset,"A novel audio-visual mouse saliency (AViMoS) dataset with the following key-features:



Diverse content: movie, sports, live, vertical videos, etc.;



Large scale: 1500 videos with mean 19s duration;



High resolution: all streams are FullHD;



Audio track saved and played to observers;



Mouse fixations from >5000 observers (>70 per video);



License: CC-BY;",https://production-media.paperswithcode.com/datasets/cbc4e2a1-06ab-4a66-8467-fef5cb6311b9.jpg,EditCC-BY,"Image, Time Series, Video",,,,,,,,"Video Saliency Detection, Video Saliency Prediction, Saliency Detection, Saliency Prediction",,,See all 1951 tasks,Saliency Prediction12 benchmar,Saliency Prediction12 benchmar
CapMIT1003,CapMIT1003 Dataset,"The CapMIT1003 database contains captions and clicks collected for images from the MIT1003 database, for which reference eye scanpath are available. The database is distributed as a single SQLite3 database named capmit1003.db. For convenience, a lightweight Python class to access the database is provided in the official repository",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Time Series,,,,,,,,"Scanpath prediction, Saliency Prediction",scanpath-prediction-on-capmit1003,,See all 1951 tasks,Saliency Prediction12 benchmar,Saliency Prediction12 benchmar
MSU_Video_Saliency_Prediction,MSU Video Saliency Prediction Dataset,"The dataset presents open high-resolution test clips set with different types of content: movie fragments, sport streams, live caption clips.  Used clips of 1920×1080 resolution and with duration from 13 to 38 seconds. And Performed reliable data collection from 50 observers (19–24 y. o.) using 500 Hz SMI iViewXTM Hi-Speed 1250 eye-tracker. Also used cross-fade which ensures the independence of the received fixations between different clips. The final ground-truth saliency map was estimated as a Gaussian mixture with centers at the fixation points. A standard deviation for the Gaussians equal to 120 was chosen (this value matches 8 angular degrees, which is known to be the sector of sharp vision).",https://production-media.paperswithcode.com/datasets/5d07cfbd-004e-44ff-80b6-720a563513c9.png,EditUnknown,"Image, Time Series, Video",,1920,,,,,,"Video Saliency Detection, Saliency Prediction",video-saliency-detection-on-msu-video,,See all 1951 tasks,Saliency Prediction12 benchmar,Saliency Prediction12 benchmar
SALICON,SALICON Dataset,"The SALIency in CONtext (SALICON) dataset contains 10,000 training images, 5,000 validation images and 5,000 test images for saliency prediction. This dataset has been created by annotating saliency in images from MS COCO.
The ground-truth saliency annotations include fixations generated from mouse trajectories. To improve the data quality, isolated fixations with low local density have been excluded.
The training and validation sets, provided with ground truth, contain the following data fields: image, resolution and gaze.
The testing data contains only the image and resolution fields.",https://arxiv.org/abs/1510.02927,EditCreative Commons Attribution 4.0 License,Time Series,,,,,,"training images, 5,000 validation images",,"Few-Shot Transfer Learning for Saliency Prediction, Saliency Prediction","few-shot-transfer-learning-on-salicon, saliency-prediction-on-salicon, few-shot-transfer-learning-on-salicon-3, few-shot-transfer-learning-on-salicon-1, few-shot-transfer-learning-on-salicon-2",,See all 1951 tasks,Saliency Prediction12 benchmar,Saliency Prediction12 benchmar
Salient-KITTI,Salient-KITTI Dataset,Salient-KITTI is a saliency map prediction dataset based on KITTI.,https://arxiv.org/pdf/2012.11863.pdf,EditUnknown,Time Series,English,,,,,,,Saliency Prediction,,,See all 1951 tasks,Saliency Prediction12 benchmar,Saliency Prediction12 benchmar
Saliency_Ranking8_papers_with_code_Dataset,Saliency Ranking8 papers with code Dataset,,https://paperswithcode.com/dataset/saliency-ranking,,,,,,,,,,,,,See all 1951 tasks,Saliency Ranking8 papers with ,Saliency Ranking8 papers with 
EORSSD,EORSSD Dataset,"The Extended Optical Remote Sensing Saliency Detection (EORSSD) dataset is an extension of the ORSSD dataset. This new dataset is larger and more varied than the original. It contains 2,000 images and corresponding pixel-wise ground truth, which includes many semantically meaningful but challenging images.",https://github.com/rmcong/EORSSD-dataset,EditUnknown,Image,,,,,000 images,,,"Salient Object Detection, Object Detection",,,See all 1951 tasks,Salient Object Detection6 benc,Salient Object Detection6 benc
Sample_Probing1_papers_with_code_Dataset,Sample Probing1 papers with code Dataset,,https://paperswithcode.com/dataset/sample-probing,,,,,,,,,,,,,See all 1951 tasks,Sample Probing1 papers with co,Sample Probing1 papers with co
Sar_Image_Despeckling11_papers_with_code_Dataset,Sar Image Despeckling11 papers with code Dataset,,https://paperswithcode.com/dataset/sar-image-despeckling,,,,,,,,,,,,,See all 1951 tasks,Sar Image Despeckling11 papers,Sar Image Despeckling11 papers
3DSSG,3DSSG Dataset,"3DSSG provides 3D semantic scene graphs for 3RScan. A semantic scene graph is defined by a set of tuples between nodes and edges where nodes represent specific 3D object instances in a 3D scan. Nodes are defined by its semantics, a hierarchy of classes as well as a set of attributes that describe the visual and physical appearance of the object instance and their affordances. The edges in our graphs are the semantic relationships (predicates) between the nodes such as standing on, hanging on, more comfortable than or same material.",https://production-media.paperswithcode.com/datasets/f307b026-26b9-4d8f-971f-b0283536c9bc.png,EditUnknown,"3D, Graph, Text",English,,,,,,,"3D Scene Graph Alignment, Scene Graph Generation, 3d scene graph generation","3d-scene-graph-generation-on-3dssg, 3d-scene-graph-alignment-on-3dssg",,See all 1951 tasks,Scene Graph Generation9 benchm,Scene Graph Generation9 benchm
4D-OR,4D-OR Dataset,"4D-OR includes a total of 6734 scenes, recorded by six calibrated RGB-D Kinect sensors 1 mounted to the ceiling of the OR, with one frame-per-second, providing synchronized RGB and depth images. We provide fused point cloud sequences of entire scenes, automatically annotated human 6D poses and 3D bounding boxes for OR objects. Furthermore, we provide SSG annotations for each step of the surgery together with the clinical roles of all the humans in the scenes, e.g., nurse, head surgeon, anesthesiologist.",https://production-media.paperswithcode.com/datasets/ca3e6cbc-dcb8-44cd-86d2-ba89b038abf8.jpg,EditCC-BY-NC,"3D, Graph, Image, Text, Video",English,,,,,,,"2D Panoptic Segmentation, Scene Graph Generation, 3D Panoptic Segmentation, Panoptic Segmentation, 4D Panoptic Segmentation, 3D Human Pose Estimation, Video Panoptic Segmentation, 3D Object Detection","2d-panoptic-segmentation-on-4d-or, video-panoptic-segmentation-on-4d-or, scene-graph-generation-on-4d-or",,See all 1951 tasks,Scene Graph Generation9 benchm,Scene Graph Generation9 benchm
MM-OR,MM-OR Dataset,"Operating rooms (ORs) are complex, high-stakes environments requiring precise understanding of interactions among medical staff, tools, and equipment for enhancing surgical assistance, situational awareness, and patient safety. Current datasets fall short in scale, realism and do not capture the multimodal nature of OR scenes, limiting progress in OR modeling. To this end, we introduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR dataset, and the first dataset to enable multimodal scene graph generation. MM-OR captures comprehensive OR scenes containing RGB-D data, detail views, audio, speech transcripts, robotic logs, and tracking data and is annotated with panoptic segmentations, semantic scene graphs, and downstream task labels. Further, we propose MM2SG, the first multimodal large vision-language model for scene graph generation, and through extensive experiments, demonstrate its ability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG establish a new benchmark for holistic OR understanding, and open the path towards multimodal scene analysis in complex, high-stakes environments.

Paper: https://arxiv.org/abs/2503.02579",https://production-media.paperswithcode.com/datasets/c9f61b98-b234-418f-a96a-d9125c784a97.jpg,Editapache 2.0,"3D, Graph, Image, Text, Video",English,,,,,,,"2D Panoptic Segmentation, Surgical phase recognition, Scene Graph Generation, 3D Panoptic Segmentation, Action Anticipation, 4D Panoptic Segmentation, Video Segmentation, Video Panoptic Segmentation","2d-panoptic-segmentation-on-mm-or, scene-graph-generation-on-mm-or, video-panoptic-segmentation-on-mm-or",,See all 1951 tasks,Scene Graph Generation9 benchm,Scene Graph Generation9 benchm
PSG_Dataset,PSG Dataset Dataset,PSG dataset has 48749 images with 133 object classes (80 objects and 53 stuff) and 56 predicate classes. It annotates inter-segment relations based on COCO panoptic segmentation.,https://production-media.paperswithcode.com/datasets/60bf4f8e-6331-4c3d-8f97-ba6dc388a292.jpeg,EditCreative Commons Attribution 4.0 License,"Graph, Image, Text",English,,,,48749 images,,,"Panoptic Scene Graph Generation, Scene Graph Generation",panoptic-scene-graph-generation-on-psg,,See all 1951 tasks,Scene Graph Generation9 benchm,Scene Graph Generation9 benchm
SpaceSGG,SpaceSGG Dataset,"Scene Graph Generation (SGG) converts visual scenes into structured graph representations, providing deeper scene understanding for complex vision tasks. However, existing SGG models often overlook essential spatial relationships and struggle with generalization in open-vocabulary contexts. To address these limitations, we propose LLaVA-SpaceSGG, a multimodal large language model (MLLM) designed for open-vocabulary SGG with enhanced spatial relation modeling. To train it, we collect the SGG instruction-tuning dataset, named SpaceSGG. This dataset is constructed by combining publicly available datasets and synthesizing data using open-source models within our data construction pipeline. It combines object locations, object relations, and depth information, resulting in three data formats: spatial SGG description, question-answering, and conversation. To enhance the transfer of MLLMs' inherent capabilities to the SGG task, we introduce a two-stage training paradigm. Experiments show that LLaVA-SpaceSGG outperforms other open-vocabulary SGG methods, boosting recall by 8.6% and mean recall by 28.4% compared to the baseline. Our codebase, dataset, and trained models are publicly accessible on GitHub at the following URL: https://github.com/Endlinc/LLaVA-SpaceSGG.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache License,"Graph, Text",English,,,,,,,Scene Graph Generation,,,See all 1951 tasks,Scene Graph Generation9 benchm,Scene Graph Generation9 benchm
PGDP5K,PGDP5K Dataset,"PGDP5K is a dataset consisting of 5000 diagram samples composed of 16 shapes, covering 5 positional relations, 22 symbol types and 6 text types, labeled with more fine-grained annotations at primitive level, including primitive classes, locations and relationships, where 1,813 non-duplicated images are selected from the Geometry3K dataset and other 3,187 images are collected from three popular textbooks across grades 6-12 on mathematics curriculum websites by taking screenshots from PDF books.",https://production-media.paperswithcode.com/datasets/5752857f-98f8-4797-9a87-160a79aea19b.png,EditUnknown,"Image, Text",English,,,,187 images,,,"Multi-Task Learning, Visual Reasoning, Scene Parsing",scene-parsing-on-pgdp5k,,See all 1951 tasks,Scene Parsing68 benchmarks80 p,Scene Parsing68 benchmarks80 p
Stanford_Background,Stanford Background Dataset,The Stanford Background dataset contains 715 RGB images and the corresponding label images. Images are approximately 240×320 pixels in size and pixels are classified into eight different categories,https://arxiv.org/abs/1605.01368,EditCustom,"Image, Text",English,,,,,,,"Scene Labeling, Semantic Segmentation, Scene Parsing",,,See all 1951 tasks,Scene Parsing68 benchmarks80 p,Scene Parsing68 benchmarks80 p
UNDD,UNDD Dataset,"UNDD consists of 7125 unlabelled day and night images; additionally, it has 75 night images with pixel-level annotations having classes equivalent to Cityscapes dataset.",https://github.com/sauradip/night_image_semantic_segmentation,EditUnknown,"Image, Text",English,,,,,,,"Autonomous Driving, Semantic Segmentation, Scene Parsing",,,See all 1951 tasks,Scene Parsing68 benchmarks80 p,Scene Parsing68 benchmarks80 p
ADVANCE,ADVANCE Dataset,"The AuDio Visual Aerial sceNe reCognition datasEt (ADVANCE) is a brand-new multimodal learning dataset, which aims to explore the contribution of both audio and conventional visual messages to scene recognition. This dataset in summary contains 5075 pairs of geotagged aerial images and sounds, classified into 13 scene classes, i.e., airport, sports land, beach, bridge, farmland, forest, grassland, harbor, lake, orchard, residential area, shrub land, and train station.",https://akchen.github.io/ADVANCE-DATASET/,EditUnknown,Image,,,,,,,,Scene Recognition,,,See all 1951 tasks,Scene Recognition8 benchmarks6,Scene Recognition8 benchmarks6
AID,AID Dataset,"AID is a new large-scale aerial image dataset, by collecting sample images from Google Earth imagery. Note that although the Google Earth images are post-processed using RGB renderings from the original optical aerial images, it has proven that there is no significant difference between the Google Earth images with the real optical aerial images even in the pixel-level land use/cover mapping. Thus, the Google Earth images can also be used as aerial images for evaluating scene classification algorithms.

The new dataset is made up of the following 30 aerial scene types: airport, bare land, baseball field, beach, bridge, center, church, commercial, dense residential, desert, farmland, forest, industrial, meadow, medium residential, mountain, park, parking, playground, pond, port, railway station, resort, river, school, sparse residential, square, stadium, storage tanks and viaduct. All the images are labelled by the specialists in the field of remote sensing image interpretation, and some samples of each class are shown in Fig.1. In all, the AID dataset has a number of 10000 images within 30 classes.

The images in AID are actually multi-source, as Google Earth images are from different remote imaging sensors. This brings more challenges for scene classification than the single source images like UC-Merced dataset. Moreover, all the sample images per each class in AID are carefully chosen from different countries and regions around the world, mainly in China, the United States, England, France, Italy, Japan, Germany, etc., and they are extracted at different time and seasons under different imaging conditions, which increases the intra-class diversities of the data.",https://production-media.paperswithcode.com/datasets/aid-dataset.png,EditUnknown,Image,,,,,10000 images,,30,"Object Detection In Aerial Images, Scene Recognition, Remote Sensing Image Classification, Scene Classification, Transductive Zero-Shot Classification","scene-recognition-on-aid, transductive-zero-shot-classification-on-aid",,See all 1951 tasks,Scene Recognition8 benchmarks6,Scene Recognition8 benchmarks6
MIT_Indoor_Scenes,MIT Indoor Scenes Dataset,"Context
This is the Original data provided by MIT .

Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g., bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information.

Content
The database contains 67 Indoor categories, and a total of 15620 images. The number of images varies across categories, but there are at least 100 images per category. All images are in jpg format. The images provided here are for research purposes only.

Acknowledgements
Thanks to MIT
Thanks to Aude Oliva for helping to create the database of indoor scenes.
Funding for this research was provided by NSF Career award (IIS 0747120)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,15620 images,,,Scene Recognition,scene-recognition-on-mit-indoors-scenes,,See all 1951 tasks,Scene Recognition8 benchmarks6,Scene Recognition8 benchmarks6
SUN397,SUN397 Dataset,"The Scene UNderstanding (SUN) database contains 899 categories and 130,519 images. There are 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition.",https://production-media.paperswithcode.com/datasets/sun.png,EditUnknown,Image,,,,,519 images,,899,"Image Clustering, Scene Recognition, Fine-Grained Image Classification, Prompt Engineering, Few-Shot Learning, Zero-Shot Learning, Image Classification, Transductive Zero-Shot Classification","few-shot-learning-on-sun397, prompt-engineering-on-sun397, transductive-zero-shot-classification-on-4, zero-shot-learning-on-sun397, fine-grained-image-classification-on-sun397, image-clustering-on-sun397, scene-recognition-on-sun397, image-classification-on-sun397",,See all 1951 tasks,Scene Recognition8 benchmarks6,Scene Recognition8 benchmarks6
YUP__,YUP++ Dataset,A new and challenging video database of dynamic scenes that more than doubles the size of those previously available. This dataset is explicitly split into two subsets of equal size that contain videos with and without camera motion to allow for systematic study of how this variable interacts with the defining dynamics of the scene per se.,/paper/temporal-residual-networks-for-dynamic-scene,EditUnknown,"Image, Video",,,,,,,,"Action Recognition, Optical Flow Estimation, Scene Recognition",scene-recognition-on-yup,,See all 1951 tasks,Scene Recognition8 benchmarks6,Scene Recognition8 benchmarks6
COCO-Text,COCO-Text Dataset,"The COCO-Text dataset is a dataset for text detection and recognition. It is based on the MS COCO dataset, which contains images of complex everyday scenes. The COCO-Text dataset contains non-text images, legible text images and illegible text images. In total there are 22184 training images and 7026 validation images with at least one instance of legible text.",https://arxiv.org/abs/1702.05089,EditCreative Commons Attribution 4.0 License,"Image, Text",English,,,,,training images and 7026 validation images,,"Scene Text Detection, Scene Text Recognition","scene-text-detection-on-coco-text, scene-text-recognition-on-coco-text",,See all 1951 tasks,Scene Text Recognition15 bench,Scene Text Recognition15 bench
CUTE80,CUTE80 Dataset,"The CUTE80 dataset is a lightweight collection of images specifically designed for text detection in natural scene images. It contains a total of 13,000 annotated page images across five different popular categories: 1) Table 2) Figure 3) Natural image 4) Logo 5) ignature",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,Scene Text Recognition,scene-text-recognition-on-cute80,,See all 1951 tasks,Scene Text Recognition15 bench,Scene Text Recognition15 bench
HOST,HOST Dataset,The heavily occluded scene text (HOST) dataset is a dataset that contains images of text with occlusions. It is used to improve the recognition performance of occluded text in machine vision applications 1. The dataset is composed of 4832 images that are manually occluded in weak or heavy degrees.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,4832 images,,,Scene Text Recognition,scene-text-recognition-on-host,,See all 1951 tasks,Scene Text Recognition15 bench,Scene Text Recognition15 bench
ICDAR_2013,ICDAR 2013 Dataset,"The ICDAR 2013 dataset consists of 229 training images and 233 testing images, with word-level annotations provided. It is the standard benchmark dataset for evaluating near-horizontal text detection.",https://arxiv.org/abs/1709.00138,EditUnknown,"Image, Tabular, Text",English,2013,,,,training images and 233 testing images,,"Scene Text Detection, Table Detection, Scene Text Recognition, Handwritten Chinese Text Recognition","scene-text-detection-on-icdar-2013, table-detection-on-icdar2013-1, scene-text-recognition-on-icdar2013",,See all 1951 tasks,Scene Text Recognition15 bench,Scene Text Recognition15 bench
RCTW-17,RCTW-17 Dataset,"Features a large-scale dataset with 12,263 annotated images. Two tasks, namely text localization and end-to-end recognition, are set up. The competition took place from January 20 to May 31, 2017. 23 valid submissions were received from 19 teams.",/paper/icdar2017-competition-on-reading-chinese-text,EditUnknown,"Image, Text",English,2017,,,,,,"Scene Text Detection, Scene Text Recognition",,,See all 1951 tasks,Scene Text Recognition15 bench,Scene Text Recognition15 bench
SVT,SVT Dataset,"The Street View Text (SVT) dataset was harvested from Google Street View. Image text in this data exhibits high variability and often has low resolution. In dealing with outdoor street level imagery, we note two characteristics. (1) Image text often comes from business signage and (2) business names are easily available through geographic business searches. These factors make the SVT set uniquely suited for word spotting in the wild: given a street view image, the goal is to identify words from nearby businesses.

Note: the dataset has undergone revision since the time it was evaluated in this publication. Please consult the ICCV2011 paper for most up-to-date results.",http://vision.ucsd.edu/~kai/svt/,EditUnknown,"Image, Text",English,,,,,,,Scene Text Recognition,scene-text-recognition-on-svt,,See all 1951 tasks,Scene Text Recognition15 bench,Scene Text Recognition15 bench
SVTP,SVTP Dataset,"SVTP dataset stands for Scene Text Recognition Datasets. It is a collection of 4 popular Latin/English scene text recognition datasets, namely IIIT5K, SVT, SVTP, and CUTE-80. These datasets only provide case-insensitive annotations and no punctuation marks.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,Scene Text Recognition,scene-text-recognition-on-svtp,,See all 1951 tasks,Scene Text Recognition15 bench,Scene Text Recognition15 bench
WOST,WOST Dataset,"The Weakly Occluded Scene Text (WOST) dataset is a public dataset for scene text segmentation. It is used to generate pixel-level annotations in scene text images 1. The dataset is designed to contain weakly annotated images, which means that the images are not fully annotated with pixel-level labels.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,Scene Text Recognition,scene-text-recognition-on-wost,,See all 1951 tasks,Scene Text Recognition15 bench,Scene Text Recognition15 bench
3RScan,3RScan Dataset,"A novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans.",/paper/rio-3d-object-instance-re-localization-in,EditUnknown,"3D, Graph, Image, Text",English,,,,,,,"Predicate Classification, Scene Graph Generation, Point Cloud Registration, 3D Semantic Segmentation, 3D Object Classification, Semantic Segmentation, Scene Change Detection, Scene Understanding, 3D Object Detection","3d-object-detection-on-3rscan, point-cloud-registration-on-3rscan, scene-graph-generation-on-3r-scan-1, 3d-object-classification-on-3r-scan-1, predicate-classification-on-3r-scan-1",,See all 1951 tasks,Scene Understanding28 benchmar,Scene Understanding28 benchmar
KITTI_Road,KITTI Road Dataset,"KITTI Road is road and lane estimation benchmark that consists of 289 training and 290 test images. It contains three different categories of road scenes:
* uu - urban unmarked (98/100)
* um - urban marked (95/96)
* umm - urban multiple marked lanes (96/94)
* urban - combination of the three above
Ground truth has been generated by manual annotation of the images and is available for two different road terrain types: road - the road area, i.e, the composition of all lanes, and lane - the ego-lane, i.e., the lane the vehicle is currently driving on (only available for category ""um""). Ground truth is provided for training images only.",http://www.cvlibs.net/datasets/kitti/eval_road.php,EditCustom,Image,English,,,,,training and 290 test images,,"Autonomous Driving, Scene Understanding, Semantic Segmentation",,,See all 1951 tasks,Scene Understanding28 benchmar,Scene Understanding28 benchmar
MUSIC-AVQA,MUSIC-AVQA Dataset,"The large-scale MUSIC-AVQA dataset of musical performance contains 45,867 question-answer pairs, distributed in 9,288 videos for over 150 hours. All QA pairs types are divided into 3 modal scenarios, which contain 9 question types and 33 question templates. Finally, as an open-ended problem of our AVQA tasks, all 42 kinds of answers constitute a set for selection.",https://production-media.paperswithcode.com/datasets/c00fd96f-67b4-49b8-aa6c-75301d823cbf.png,EditMIT,"Audio, Image, Text",English,,,,,,,"Audio-visual Question Answering, Scene Understanding, Visual Question Answering (VQA), Question Answering",audio-visual-question-answering-on-music-avqa,,See all 1951 tasks,Scene Understanding28 benchmar,Scene Understanding28 benchmar
SensatUrban,SensatUrban Dataset,"The SensatUrbat dataset is an urban-scale photogrammetric point cloud dataset with nearly three billion richly annotated points, which is five times the number of labeled points than the existing largest point cloud dataset. The dataset consists of large areas from two UK cities, covering about 6 km^2 of the city landscape. In the dataset, each 3D point is labeled as one of 13 semantic classes, such as ground, vegetation, car, etc..",https://github.com/QingyongHu/SensatUrban,EditUnknown,"3D, Image",,,,,,,,"Scene Understanding, Semantic Segmentation, 3D Semantic Segmentation",3d-semantic-segmentation-on-sensaturban,,See all 1951 tasks,Scene Understanding28 benchmar,Scene Understanding28 benchmar
SUIM,SUIM Dataset,"The Segmentation of Underwater IMagery (SUIM) dataset contains over 1500 images with pixel annotations for eight object categories: fish (vertebrates), reefs (invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and sea-floor. The images have been rigorously collected during oceanic explorations and human-robot collaborative experiments, and annotated by human participants.",https://arxiv.org/abs/2004.01241,EditUnknown,"Image, Time Series",,,,,1500 images,,,"Unsupervised Semantic Segmentation, Semi-Supervised Semantic Segmentation, Semantic Segmentation, Saliency Prediction, Scene Understanding","semi-supervised-semantic-segmentation-on-suim, unsupervised-semantic-segmentation-on-suim",,See all 1951 tasks,Scene Understanding28 benchmar,Scene Understanding28 benchmar
Toronto-3D,Toronto-3D Dataset,"Toronto-3D is a large-scale urban outdoor point cloud dataset acquired by an MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of road and consists of about 78.3 million points. Point clouds has 10 attributes and classified in 8 labelled object classes.",https://github.com/WeikaiTan/Toronto-3D,EditUnknown,"3D, Image",,,,,,,,"Autonomous Driving, Scene Understanding, Semantic Segmentation, 3D Semantic Segmentation","3d-semantic-segmentation-on-toronto-3d, semantic-segmentation-on-toronto-3d-l002",,See all 1951 tasks,Scene Understanding28 benchmar,Scene Understanding28 benchmar
UAVid,UAVid Dataset,"UAVid is a high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. The UAV dataset consists of 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task.",/paper/the-uavid-dataset-for-video-semantic,EditUnknown,Image,,,,,300 images,,8,"Autonomous Driving, Scene Understanding, Semantic Segmentation, Scene Segmentation","semantic-segmentation-on-uavid, scene-segmentation-on-uavid",,See all 1951 tasks,Scene Understanding28 benchmar,Scene Understanding28 benchmar
BDD-X,BDD-X Dataset,"Berkeley Deep Drive-X (eXplanation) is a dataset is composed of over 77 hours of driving within 6,970 videos. The videos are taken in diverse driving conditions, e.g. day/night, highway/city/countryside, summer/winter etc. On average 40 seconds long, each video contains around 3-4 actions, e.g. speeding up, slowing down, turning right etc., all of which are annotated with a description and an explanation. Our dataset contains over 26K activities in over 8.4M frames.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-05-26_at_13.36.15.png,EditCustom,,,,,,,,,"Autonomous Driving, Self-Driving Cars, Behavioural cloning, Explainable artificial intelligence",,,See all 1951 tasks,Self-Driving Cars186 papers wi,Self-Driving Cars186 papers wi
DDAD,DDAD Dataset,"DDAD is a new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. It contains monocular videos and accurate ground-truth depth (across a full 360 degree field of view) generated from high-density LiDARs mounted on a fleet of self-driving cars operating in a cross-continental setting. DDAD contains scenes from urban settings in the United States (San Francisco, Bay Area, Cambridge, Detroit, Ann Arbor) and Japan (Tokyo, Odaiba).",https://github.com/TRI-ML/DDAD,EditUnknown,3D,,,,,,,,"Self-Driving Cars, Depth Estimation, Monocular Depth Estimation",monocular-depth-estimation-on-ddad,,See all 1951 tasks,Self-Driving Cars186 papers wi,Self-Driving Cars186 papers wi
ELAS,ELAS Dataset,"ELAS is a dataset for lane detection. It contains more than 20 different scenes (in more than 15,000 frames) and considers a variety of scenarios (urban road, highways, traffic, shadows, etc.). The dataset was manually annotated for several events that are of interest for the research community (i.e., lane estimation, change, and centering; road markings; intersections; LMTs; crosswalks and adjacent lanes).",https://arxiv.org/abs/1806.05984,EditUnknown,Image,,,,,,,,"Autonomous Driving, Self-Driving Cars, Lane Detection",,,See all 1951 tasks,Self-Driving Cars186 papers wi,Self-Driving Cars186 papers wi
HPD,HPD Dataset,"These images were generated using Blender and IEE-Simulator with different head-poses, where the images are labelled according to nine classes (straight, turned bottom-left, turned left, turned top-left, turned bottom-right, turned right, turned top-right, reclined, looking up). The dataset contains 16,013 training images and 2,825 testing images, in addition to 4,700 images for improvements.",https://production-media.paperswithcode.com/datasets/b9fdc7d0-71d4-4eec-9421-9fad4a9371d3.png,EditUnknown,"3D, Image",,,,,700 images,"training images and 2,825 testing images",,"Head Pose Estimation, Self-Driving Cars, Driver Attention Monitoring",,,See all 1951 tasks,Self-Driving Cars186 papers wi,Self-Driving Cars186 papers wi
Lost_and_Found,Lost and Found Dataset,Lost and Found is a novel lost-cargo image sequence dataset comprising more than two thousand frames with pixelwise annotations of obstacle and free-space and provide a thorough comparison to several stereo-based baseline methods. The dataset will be made available to the community to foster further research on this important topic.,/paper/lost-and-found-detecting-small-road-hazards,EditCustom,Image,,,,,,,,"Anomaly Detection, Self-Driving Cars, Autonomous Driving, Semantic Segmentation",anomaly-detection-on-lost-and-found,,See all 1951 tasks,Self-Driving Cars186 papers wi,Self-Driving Cars186 papers wi
OC,OC Dataset,"These images were generated using UnityEyes simulator, after including essential eyeball physiology elements and modeling binocular vision dynamics. The images are annotated with head pose and gaze direction information, besides 2D and 3D landmarks of eye's most important features. Additionally, the images are distributed into two classes denoting the status of the eye (Open for open eyes, Closed for closed eyes). This dataset was used to train a DNN model for detecting drowsiness status of a driver. The dataset contains 1,704 training images, 4,232 testing images and additional 4,103 images for improvements.",https://production-media.paperswithcode.com/datasets/315d9cca-96ef-4aac-8d16-6c6de359fd42.jpg,EditUnknown,,,,,,103 images,"train a DNN model for detecting drowsiness status of a driver. The dataset contains 1,704 training images",,"Driver Attention Monitoring, Self-Driving Cars, Citation Recommendation",citation-recommendation-on-oc,,See all 1951 tasks,Self-Driving Cars186 papers wi,Self-Driving Cars186 papers wi
Shifts,Shifts Dataset,"The Shifts Dataset is a dataset for evaluation of uncertainty estimates and robustness to distributional shift. The dataset, which has been collected from industrial sources and services, is composed of three tasks, with each corresponding to a particular data modality: tabular weather prediction, machine translation, and self-driving car (SDC) vehicle motion prediction. All of these data modalities and tasks are affected by real, `in-the-wild' distributional shifts and pose interesting challenges with respect to uncertainty estimation.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-07-19_at_09.11.04.png,EditMultiple licenses,"Text, Time Series",English,,,,,,,"Self-Driving Cars, Weather Forecasting, Machine Translation",weather-forecasting-on-shifts,,See all 1951 tasks,Self-Driving Cars186 papers wi,Self-Driving Cars186 papers wi
SOD,SOD Dataset,"Aiming
Detect small obstacles, like lost and found.

frames
3000+ picture.

3000+ claimed labelled.

1600 actually labelled.",https://github.com/LT1st/SmallObstacleDetection/tree/main/code#readme,Editnone,Image,,,,,,,,"Salient Object Detection, Object Detection, RGB Salient Object Detection, Self-Driving Cars","salient-object-detection-on-sod-1, salient-object-detection-on-sod",,See all 1951 tasks,Self-Driving Cars186 papers wi,Self-Driving Cars186 papers wi
AVA,AVA Dataset,"AVA is a project that provides audiovisual annotations of video for improving our understanding of human activity. Each of the video clips has been exhaustively annotated by human annotators, and together they represent a rich variety of scenes, recording conditions, and expressions of human activity. There are annotations for:


Kinetics (AVA-Kinetics) - a crossover between AVA and Kinetics. In order to provide localized action labels on a wider variety of visual scenes, authors provide AVA action labels on videos from Kinetics-700, nearly doubling the number of total annotations, and increasing the number of unique videos by over 500x. 
Actions (AvA Actions) - the AVA dataset densely annotates 80 atomic visual actions in 430 15-minute movie clips, where actions are localized in space and time, resulting in 1.62M action labels with multiple labels per human occurring frequently. 
Spoken Activity (AVA ActiveSpeaker, AVA Speech). AVA ActiveSpeaker: associates speaking activity with a visible face, on the AVA v1.0 videos, resulting in 3.65 million frames labeled across ~39K face tracks. AVA Speech densely annotates audio-based speech activity in AVA v1.0 videos, and explicitly labels 3 background noise conditions, resulting in ~46K labeled segments spanning 45 hours of data.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_10.47.15_AM.png,EditCC BY 4.0,"Audio, Image, Time Series, Video",,,,,,,,"Video Understanding, Action Detection, Audio-Visual Active Speaker Detection, Speech Enhancement, Speaker Diarization, Gaze Estimation, Aesthetics Quality Assessment, Action Recognition In Videos, Activity Detection, Spatio-Temporal Action Localization, Action Recognition, Self-Supervised Learning, Node Classification","activity-detection-on-ava-speech, action-recognition-on-ava-v2-2, audio-visual-active-speaker-detection-on-ava, node-classification-on-ava, action-recognition-in-videos-on-ava-v21, action-recognition-in-videos-on-ava-v2-2, action-recognition-in-videos-on-ava-v2-1, aesthetics-quality-assessment-on-ava, spatio-temporal-action-localization-on-ava",,See all 1951 tasks,Self-Supervised Learning10 ben,Self-Supervised Learning10 ben
CATER,CATER Dataset,"Rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning.",/paper/cater-a-diagnostic-dataset-for-compositional,EditApache License 2.0,"Image, Video",,,,,,,,"Visual Reasoning, Composite action recognition, Video Object Tracking, Atomic action recognition, Action Recognition, Self-Supervised Learning","video-object-tracking-on-cater, composite-action-recognition-on-cater, atomic-action-recognition-on-cater",,See all 1951 tasks,Self-Supervised Learning10 ben,Self-Supervised Learning10 ben
COCO_10__labeled_data,COCO 10% labeled data Dataset,Semi-Supervised Object Detection on COCO 10% labeled data,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,English,,,,,,,"Self-Supervised Learning, Semi-Supervised Instance Segmentation, Semi-Supervised Object Detection","semi-supervised-instance-segmentation-on-coco-7, semi-supervised-object-detection-on-coco-10",,See all 1951 tasks,Self-Supervised Learning10 ben,Self-Supervised Learning10 ben
MotionSense,MotionSense Dataset,"This dataset includes time-series data generated by accelerometer and gyroscope sensors (attitude, gravity, userAcceleration, and rotationRate). It is collected with an iPhone 6s kept in the participant's front pocket using SensingKit which collects information from Core Motion framework on iOS devices. All data is collected in 50Hz sample rate. A total of 24 participants in a range of gender, age, weight, and height performed 6 activities in 15 trials in the same environment and conditions: downstairs, upstairs, walking, jogging, sitting, and standing.",https://github.com/mmalekzadeh/motion-sense,EditUnknown,"Image, Video",,,,,,,,"Activity Recognition, Self-Supervised Learning, Imputation",,,See all 1951 tasks,Self-Supervised Learning10 ben,Self-Supervised Learning10 ben
ADE20K,ADE20K Dataset,"The ADE20K semantic segmentation dataset contains more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels. There are totally 150 semantic categories, which include stuffs like sky, road, grass, and discrete objects like person, car, bed.",https://arxiv.org/abs/1911.00679,"EditCustom (research-only, non-commercial)","3D, Audio, Image, Text",English,,,,,,,"Scene Recognition, Instance Segmentation, Overlapped 100-10, Reconstruction, Face Detection, Open Vocabulary Semantic Segmentation, Semantic Segmentation, Speech Prompted Semantic Segmentation, Sound Prompted Semantic Segmentation, Image-to-Image Translation, Semi-Supervised Semantic Segmentation, Unsupervised Semantic Segmentation with Language-image Pre-training, Zero-Shot Semantic Segmentation, Scene Understanding, Overlapped 25-25, Overlapped 100-5, Continual Semantic Segmentation, Pose Transfer, Open Vocabulary Panoptic Segmentation, Panoptic Segmentation, Weakly-Supervised Semantic Segmentation, Overlapped 50-50, Overlapped 100-50","zero-shot-semantic-segmentation-on-ade20k-847, overlapped-100-5-on-ade20k, speech-prompted-semantic-segmentation-on, unsupervised-semantic-segmentation-with-4, face-detection-on-ade20k, reconstruction-on-ade20k, open-vocabulary-panoptic-segmentation-on, scene-recognition-on-ade20k, weakly-supervised-semantic-segmentation-on-20, panoptic-segmentation-on-ade20k, open-vocabulary-semantic-segmentation-on-3, open-vocabulary-semantic-segmentation-on-2, continual-semantic-segmentation-on-ade20k, semantic-segmentation-on-ade20k, overlapped-100-50-on-ade20k, semi-supervised-semantic-segmentation-on-41, panoptic-segmentation-on-ade20k-val, overlapped-25-25-on-ade20k, pose-transfer-on-ade20k, scene-understanding-on-ade20k-val-1, overlapped-100-10-on-ade20k, image-to-image-translation-on-ade20k-labels, semantic-segmentation-on-ade20k-val, instance-segmentation-on-ade20k-val, image-to-image-translation-on-ade20k-outdoor, overlapped-50-50-on-ade20k, semi-supervised-semantic-segmentation-on-42, sound-prompted-semantic-segmentation-on",,See all 1951 tasks,Semantic Segmentation350 bench,Semantic Segmentation350 bench
DAVIS,DAVIS Dataset,"The Densely Annotation Video Segmentation dataset (DAVIS) is a high quality and high resolution densely annotated video segmentation dataset under two resolutions, 480p and 1080p. There are 50 video sequences with 3455 densely annotated frames in pixel level. 30 videos with 2079 frames are for training and 20 videos with 1376 frames are for validation.",https://arxiv.org/abs/2007.09943,EditUnknown,"Image, Time Series, Video",,2079,,,,,,"Interactive Video Object Segmentation, Interactive Segmentation, Video Prediction, Video Frame Interpolation, Semi-Supervised Video Object Segmentation, Visual Tracking, Semantic Segmentation, Video Inpainting, Video Object Segmentation, Video Denoising","interactive-video-object-segmentation-on, interactive-segmentation-on-davis, video-prediction-on-davis-2017, visual-tracking-on-davis, video-denoising-on-davis-sigma20, video-frame-interpolation-on-davis, video-denoising-on-davis-sigma10, video-inpainting-on-davis, video-object-segmentation-on-davis-2017, video-denoising-on-davis-sigma50, semi-supervised-video-object-segmentation-on-20, video-denoising-on-davis-sigma40, video-denoising-on-davis-sigma30",,See all 1951 tasks,Semantic Segmentation350 bench,Semantic Segmentation350 bench
EuroSAT,EuroSAT Dataset,"Eurosat is a dataset and deep learning benchmark for land use and land cover classification. The dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images.",https://github.com/phelber/eurosat,EditUnknown,Image,,,,,,,10,"Image Clustering, Prompt Engineering, Few-Shot Learning, Zero-Shot Learning, Classification, Cross-Domain Few-Shot, Semantic Segmentation, Image Classification, Transductive Zero-Shot Classification","image-classification-on-eurosat, prompt-engineering-on-eurosat, image-clustering-on-eurosat, classification-on-eurosat, transductive-zero-shot-classification-on-1, few-shot-learning-on-eurosat, zero-shot-learning-on-eurosat, cross-domain-few-shot-on-eurosat",,See all 1951 tasks,Semantic Segmentation350 bench,Semantic Segmentation350 bench
SYNTHIA,SYNTHIA Dataset,The SYNTHIA dataset is a synthetic dataset that consists of 9400 multi-viewpoint photo-realistic frames rendered from a virtual city and comes with pixel-level semantic annotations for 13 classes. Each frame has resolution of 1280 × 960.,https://arxiv.org/abs/1907.12849,EditCC BY-NC-SA 3.0,"Image, Text",English,,,,,,13,"Image-to-Image Translation, One-shot Unsupervised Domain Adaptation, Source-Free Domain Adaptation, Novel View Synthesis, Domain Adaptation, Synthetic-to-Real Translation, Semantic Segmentation, Unsupervised Domain Adaptation","semantic-segmentation-on-synthia-to, novel-view-synthesis-on-synthia-novel-view, semantic-segmentation-on-synthia, image-to-image-translation-on-synthia-fall-to, semantic-segmentation-on-synthia-cvpr16, image-to-image-translation-on-synthia-to, one-shot-unsupervised-domain-adaptation-on-1, synthetic-to-real-translation-on-synthia-to-1, source-free-domain-adaptation-on-synthia-to, domain-adaptation-on-synthia-to-cityscapes, unsupervised-domain-adaptation-on-synthia-to",,See all 1951 tasks,Semantic Segmentation350 bench,Semantic Segmentation350 bench
AuxAD,AuxAD Dataset,AuxAD is a a distantly supervised dataset for acronym disambiguation.,https://github.com/PrimerAI/sdu-data,EditUnknown,,,,,,,,,Sentence Embeddings,,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
AuxAI,AuxAI Dataset,AuxAI is a distantly supervised dataset for acronym identification.,https://github.com/PrimerAI/sdu-data,EditUnknown,,,,,,,,,Sentence Embeddings,,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
COSTRA_1.0,COSTRA 1.0 Dataset,"COSTRA 1.0 is a dataset of complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing. The first version of the dataset is limited to sentences in Czech but the construction method is universal and the authors plan to use it also for other languages. The dataset consist of 4,262 unique sentences with average length of 10 words, illustrating 15 types of modifications such as simplification, generalization, or formal and informal language variation.",/paper/costra-10-a-dataset-of-complex-sentence,EditUnknown,,,,,,,,,"Sentence Embedding, Sentence Embeddings",,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
Discovery,Discovery Dataset,"The Discovery datasets consists of adjacent sentence pairs (s1,s2) with a discourse marker (y) that occurred at the beginning of s2. They were extracted from the depcc web corpus.

Markers prediction can be used in order to train a sentence encoders. Discourse markers can be considered as noisy labels for various semantic tasks, such as entailment (y=therefore), subjectivity analysis (y=personally) or sentiment analysis (y=sadly), similarity (y=similarly), typicality, (y=curiously) ...

The specificity of this dataset is the diversity of the markers, since previously used data used only ~10 imbalanced classes. The author of the dataset provide:


a list of the 174 discourse markers
a Base version of the dataset with 1.74 million pairs (10k examples per marker)
a Big version with 3.4 million pairs
a Hard version with 1.74 million pairs where the connective couldn't be predicted with a fastText linear model",https://github.com/synapse-developpement/Discovery,EditApache 2.0,"Graph, Image",,,,,10k examples,,,"Sentence Embeddings, Relation Classification",relation-classification-on-discovery-dataset,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
French_CASS_dataset,French CASS dataset Dataset,Composed of judgments from the French Court of cassation and their corresponding summaries.,/paper/strass-a-light-and-effective-method-for,EditUnknown,Text,English,,,,,,,"Sentence Embedding, Sentence Embeddings, Document Embedding",,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
GeoCoV19,GeoCoV19 Dataset,"GeoCoV19 is a large-scale Twitter dataset containing more than 524 million multilingual tweets. The dataset contains around 378K geotagged tweets and 5.4 million tweets with Place information. The annotations include toponyms from the user location field and tweet content and resolve them to geolocations such as country, state, or city level. In this case, 297 million tweets are annotated with geolocation using the user location field and 452 million tweets using tweet content.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Misinformation, Sentence Embeddings, Sentiment Analysis",,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
PIT,PIT Dataset,"Paraphrase and Semantic Similarity in Twitter (PIT) presents a constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs.",https://www.aclweb.org/anthology/S15-2001.pdf,EditUnknown,,,,,,,,,"Sentence Embeddings, Paraphrase Identification, Semantic Textual Similarity",paraphrase-identification-on-pit,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
ScanRefer_Dataset,ScanRefer Dataset Dataset,"Contains 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.",/paper/scanrefer-3d-object-localization-in-rgb-d,EditUnknown,Image,,,,,,,,"Object Localization, Object Detection, Sentence Embeddings",,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
SemEval-2014_Task-10,SemEval-2014 Task-10 Dataset,"SemEval 2014 is a collection of datasets used for the Semantic Evaluation (SemEval) workshop, an annual event that focuses on the evaluation and comparison of systems that can analyze diverse semantic phenomena in text. The datasets from SemEval 2014 are used for various tasks, including but not limited to:


Aspect-Based Sentiment Analysis (ABSA): This task is based on laptop and restaurant reviews. It involves identifying the aspects or features mentioned in a review and determining the sentiment expressed towards each aspect.
Text Classification: This task involves classifying text into predefined categories. Sub-tasks include text-scoring, natural language inference, and semantic-similarity-scoring.",https://production-media.paperswithcode.com/datasets/STS_2014-0000003579-a222dcdb.jpg,EditUnknown,,,2014,,,,,,"Word Embeddings, Sentence Embeddings, Semantic Textual Similarity",,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
WikiMatrix,WikiMatrix Dataset,"WikiMatrix is a dataset of parallel sentences in the textual content of Wikipedia for all possible language pairs. The mined data consists of:


85 different languages, 1620 language pairs
134M parallel sentences, out of which 34M are aligned with English",/paper/wikimatrix-mining-135m-parallel-sentences-in,EditCC BY-SA 4.0,,,,,,,,,"Word Embeddings, Sentence Embeddings",,,See all 1951 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
ASTD,ASTD Dataset,"Arabic Sentiment Tweets Dataset (ASTD) is an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classified as objective, subjective positive, subjective negative, and subjective mixed.",/paper/astd-arabic-sentiment-tweets-dataset,EditGPL-2.0,Text,English,,,,,,,Sentiment Analysis,sentiment-analysis-on-astd,,See all 1951 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
MPQA_Opinion_Corpus,MPQA Opinion Corpus Dataset,"The MPQA Opinion Corpus contains 535 news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).",http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf,EditCustom (research-only),"Image, Text",English,,,,,,,"Sentiment Analysis, Fine-Grained Opinion Analysis, Opinion Mining, Keyword Extraction, Document Classification","fine-grained-opinion-analysis-on-mpqa, sentiment-analysis-on-mpqa, document-classification-on-mpqa",,See all 1951 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
Multi-Domain_Sentiment,Multi-Domain Sentiment Dataset,The Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains). Some domains (books and dvds) have hundreds of thousands of reviews. Others (musical instruments) have only a few hundred. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed.,https://www.cs.jhu.edu/~mdredze/datasets/sentiment/,EditUnknown,Text,English,,,,,,,Sentiment Analysis,sentiment-analysis-on-multi-domain-sentiment,,See all 1951 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
ReDial,ReDial Dataset,"ReDial (Recommendation Dialogues) is an annotated dataset of dialogues, where users recommend movies to each other. The dataset consists of over 10,000 conversations centered around the theme of providing movie recommendations.",/paper/towards-deep-conversational-recommendations,EditCC BY 4.0,Text,English,,,,,,,"Text Generation, Knowledge Graphs, Recommendation Systems, Sentiment Analysis","text-generation-on-redial, recommendation-systems-on-redial",,See all 1951 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
SST-5,SST-5 Dataset,"The SST-5, also known as the Stanford Sentiment Treebank with 5 labels, is a dataset used for sentiment analysis. The SST-5 dataset consists of 11,855 single sentences extracted from movie reviews¹. It includes a total of 215,154 unique phrases from parse trees, each annotated by 3 human judges¹. Each phrase is labeled as either negative, somewhat negative, neutral, somewhat positive, or positive. This is why it's referred to as SST-5 or SST fine-grained.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,5,"Few-Shot Text Classification, Explanation Fidelity Evaluation, Sentiment Analysis","explanation-fidelity-evaluation-on-sst-5, sentiment-analysis-on-sst-5-fine-grained, few-shot-text-classification-on-sst-5",,See all 1951 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
TweetEval,TweetEval Dataset,TweetEval introduces an evaluation framework consisting of seven heterogeneous Twitter-specific classification tasks.,/paper/tweeteval-unified-benchmark-and-comparative,EditUnknown,Text,English,,,,,,,"Language Modelling, Sentiment Analysis",sentiment-analysis-on-tweeteval,,See all 1951 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
Doc3DShade,Doc3DShade Dataset,Doc3DShade extends Doc3D with realistic lighting and shading. Follows a similar synthetic rendering procedure using captured document 3D shapes but final image generation step combines real shading of different types of paper materials under numerous illumination conditions.,https://github.com/cvlab-stonybrook/DocIIW,EditUnknown,Image,,,,,,,,"Shadow Removal, Intrinsic Image Decomposition, Optical Character Recognition (OCR)",,,See all 1951 tasks,Shadow Removal6 benchmarks68 p,Shadow Removal6 benchmarks68 p
INS_Dataset,INS Dataset Dataset,"A significant challenge in removing shadows from indoor scenes is obtaining shadow-free images. To overcome this challenge, we propose a novel rendering pipeline for generating shadowed and shadow-free images under direct and indirect illumination, and create a comprehensive synthetic dataset that contains over 30,000 image pairs, covering various object types and lighting conditions.

We implemented a direct/indirect shadow and shadow-free rendering pipeline using Blender Cycles engine, with the help of Open Shading Language (OSL). The resulting collection of shadow and shadow-free images is referred to as the “INS dataset”. The dataset includes 30,000 training and 2,000 testing images, all with a resolution of 512 × 512. The training and testing images are generated from distinct scenes with different objects and materials.",https://production-media.paperswithcode.com/datasets/42e85039-3a55-46aa-84b9-5271063e03f6.png,"EditCustom (research-only, non-commercial)",Image,,,,,,"training and 2,000 testing images",,"Shadow Detection, Image Shadow Removal, Shadow Removal","shadow-removal-on-ins-dataset, image-shadow-removal-on-ins-dataset",,See all 1951 tasks,Shadow Removal6 benchmarks68 p,Shadow Removal6 benchmarks68 p
ISTD_,ISTD+ Dataset,"ISTD+ consists of shadow images, shadow-free images, and shadow masks, with 1,330 training images and 540 testing images from 135 unique background scenes. ISTD suffers from color and luminosity inconsistencies between shadow and shadow-free images, which ISTD+ corrects with a color compensation mechanism to ensure uniform pixel colors across the ground-truth images.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,training images and 540 testing images,,"2D Semantic Segmentation, Shadow Removal",shadow-removal-on-istd-1,,See all 1951 tasks,Shadow Removal6 benchmarks68 p,Shadow Removal6 benchmarks68 p
Jung,Jung Dataset,Dataset for document shadow removal,https://production-media.paperswithcode.com/datasets/d9204900-871a-42d5-b4ee-8d8f04e0447d.jpg,EditUnknown,Text,English,,,,,,,"Document Shadow Removal, Shadow Removal",,,See all 1951 tasks,Shadow Removal6 benchmarks68 p,Shadow Removal6 benchmarks68 p
Kligler,Kligler Dataset,Dataset for document shadow removal,https://production-media.paperswithcode.com/datasets/d6c6555c-a87c-4c09-bc2c-7580e3c9257a.jpg,EditUnknown,Text,English,,,,,,,"Document Shadow Removal, Shadow Removal",,,See all 1951 tasks,Shadow Removal6 benchmarks68 p,Shadow Removal6 benchmarks68 p
SD7K,SD7K Dataset,"SD7K is the only large-scale high-resolution dataset that satisfies all important data features about document shadow currently, which covers a large number of document shadow images. Mean resolution is $2462 \times 3699$",https://production-media.paperswithcode.com/datasets/2d2530b2-3059-47c9-a655-d2faa0961df4.png,EditUnknown,"Image, Text",English,,,,,,,"Document Shadow Removal, Image Shadow Removal, Shadow Removal",,,See all 1951 tasks,Shadow Removal6 benchmarks68 p,Shadow Removal6 benchmarks68 p
SRD,SRD Dataset,SRD is a dataset for shadow removal that contains 3088 shadow and shadow-free image pairs.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,,,,,,,Shadow Removal,shadow-removal-on-srd,,See all 1951 tasks,Shadow Removal6 benchmarks68 p,Shadow Removal6 benchmarks68 p
WSRD_,WSRD+ Dataset,A version of the WSRD Dataset will be used as a benchmark for the NTIRE24 Challenge on Image Shadow Removal.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,,,,,,,Shadow Removal,shadow-removal-on-wsrd,,See all 1951 tasks,Shadow Removal6 benchmarks68 p,Shadow Removal6 benchmarks68 p
Bonn_RGB-D_Dynamic,Bonn RGB-D Dynamic Dataset,"Bonn RGB-D Dynamic is a dataset for RGB-D SLAM, containing highly dynamic sequences. We provide 24 dynamic sequences, where people perform different tasks, such as manipulating boxes or playing with balloons, plus 2 static sequences. For each scene we provide the ground truth pose of the sensor, recorded with an Optitrack Prime 13 motion capture system. The sequences are in the same format as the TUM RGB-D Dataset, so that the same evaluation tools can be used. Furthermore, we provide a ground truth 3D point cloud of the static environment recorded using a Leica BLK360 terrestrial laser scanner.",https://production-media.paperswithcode.com/datasets/lab_groundtruth.jpeg,EditUnknown,Image,,,,,,,,Simultaneous Localization and Mapping,,,See all 1951 tasks,Simultaneous Localization and ,Simultaneous Localization and 
Endomapper,Endomapper Dataset,"The Endomapper dataset is the first collection of complete endoscopy sequences acquired during regular medical practice, including slow and careful screening explorations, making secondary use of medical data. Its original purpose is to facilitate the development and evaluation of VSLAM (Visual Simultaneous Localization and Mapping) methods in real endoscopy data. The first release of the dataset is composed of 50 sequences with a total of more than 13 hours of video. It is also the first endoscopic dataset that includes both the computed geometric and photometric endoscope calibration as well as the original calibration videos. Meta-data and annotations associated to the dataset varies from anatomical landmark and description of the procedure labeling, tools segmentation masks, COLMAP 3D reconstructions, simulated sequences with groundtruth and meta-data related to special cases, such as sequences from the same patient. This information will improve the research in endoscopic VSLAM, as well as other research lines, and create new research lines.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCustom,Image,,,,,,,,Simultaneous Localization and Mapping,,,See all 1951 tasks,Simultaneous Localization and ,Simultaneous Localization and 
Hilti_SLAM_Challenge,Hilti SLAM Challenge Dataset,"Hilti SLAM Challenge is a dataset for Simultaneous Localization and Mapping (SLAM) algorithms due to sparsity, varying illumination conditions, and dynamic objects. The sensor platform used to collect this dataset contains a number of visual, lidar and inertial sensors which have all been rigorously calibrated. All data is temporally aligned to support precise multi-sensor fusion. Each dataset includes accurate ground truth to allow direct testing of SLAM results. Raw data as well as intrinsic and extrinsic sensor calibration data from twelve datasets in various environments is provided. Each environment represents common scenarios found in building construction sites in various stages of completion.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Simultaneous Localization and Mapping,,,See all 1951 tasks,Simultaneous Localization and ,Simultaneous Localization and 
InteriorNet,InteriorNet Dataset,"InteriorNet is a RGB-D for large scale interior scene understanding and mapping. The dataset contains 20M images created by pipeline:


(A) the authors collected around 1 million CAD models provided by world-leading furniture manufacturers.
(B) based on those models, around 1,100 professional designers create around 22 million interior layouts. Most of such layouts have been used in real-world decorations.
(C) For each layout, authors generate a number of configurations to represent different random lightings and simulation of scene change over time in daily life.
(D) Authors provide an interactive simulator (ViSim) to help for creating ground truth IMU, events, as well as monocular or stereo camera trajectories including hand-drawn, random walking and neural network based realistic trajectory.
(E) All supported image sequences and ground truth.",/paper/interiornet-mega-scale-multi-sensor-photo,EditCC BY-NC-ND 4.0,Image,,,,,20M images,,,"Object Detection, Semantic Segmentation, Simultaneous Localization and Mapping",,,See all 1951 tasks,Simultaneous Localization and ,Simultaneous Localization and 
New_College,New College Dataset,"The New College Data is a freely available dataset collected from a robot completing several loops outdoors around the New College campus in Oxford. The data includes odometry, laser scan, and visual information. The dataset URL is not working anymore.",https://www.ros.org/news/2010/07/new-college-dataset-parser-for-ros.html,EditUnknown,Image,,,,,,,,"Simultaneous Localization and Mapping, Loop Closure Detection, Visual Odometry",,,See all 1951 tasks,Simultaneous Localization and ,Simultaneous Localization and 
S3E,S3E Dataset,"S3E is a novel large-scale multimodal dataset captured by a fleet of unmanned ground vehicles along four designed collaborative trajectory paradigms. S3E consists of 7 outdoor and 5 indoor scenes that each exceed 200 seconds, consisting of well synchronized and calibrated high-quality stereo camera, LiDAR, and high-frequency IMU data.",https://arxiv.org/pdf/2210.13723v1.pdf,EditApache-2.0 license,Image,,,,,,,,Simultaneous Localization and Mapping,,,See all 1951 tasks,Simultaneous Localization and ,Simultaneous Localization and 
TUM_monoVO,TUM monoVO Dataset,"TUM monoVO is a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes.
All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence.
In contrast to existing datasets, all sequences are photometrically calibrated: the dataset creators provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting).",https://vision.in.tum.de/data/datasets/mono-dataset,EditUnknown,Image,,,,,,,,"Simultaneous Localization and Mapping, Monocular Visual Odometry, Visual Odometry",,,See all 1951 tasks,Simultaneous Localization and ,Simultaneous Localization and 
Virtual_KITTI_2,Virtual KITTI 2 Dataset,"Virtual KITTI 2 is an updated version of the well-known Virtual KITTI dataset which consists of 5 sequence clones from the KITTI tracking benchmark. In addition, the dataset provides different variants of these sequences such as modified weather conditions (e.g. fog, rain) or modified camera configurations (e.g. rotated by 15◦). For each sequence we provide multiple sets of images containing RGB, depth, class segmentation, instance segmentation, flow, and scene flow data. Camera parameters and poses as well as vehicle locations are available as well. In order to showcase some of the dataset’s capabilities, we ran multiple relevant experiments using state-of-the-art algorithms from the field of autonomous driving. The dataset is available for download at https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-07-21_at_17.24.19_hRZ24UH.png,EditCreative Commons Attribution-NonCommercial-ShareAlike 3.0,"3D, Image, Video",English,,,,,,,"Monocular Depth Estimation, Visual Odometry, Monocular 3D Object Detection, Depth Estimation, Multi-Object Tracking, Object Tracking, Stereo Matching, Semantic Segmentation, Simultaneous Localization and Mapping","monocular-depth-estimation-on-virtual-kitti-2, monocular-3d-object-detection-on-virtual",,See all 1951 tasks,Simultaneous Localization and ,Simultaneous Localization and 
NTU_RGB_D_120,NTU RGB+D 120 Dataset,"NTU RGB+D 120 is a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities.",https://arxiv.org/pdf/1905.04757v2.pdf,EditCustom (research-only),"3D, Image, Text, Video",English,,,,,,,"Zero Shot Skeletal Action Recognition, Skeleton Based Action Recognition, Generalized Zero Shot skeletal action recognition, Human Interaction Recognition, Self-supervised Skeleton-based Action Recognition, Few-Shot Skeleton-Based Action Recognition, Self-Supervised Human Action Recognition, One-Shot 3D Action Recognition, Action Recognition, Unsupervised Skeleton Based Action Recognition, Human action generation","skeleton-based-action-recognition-on-ntu-rgbd-1, one-shot-3d-action-recognition-on-ntu-rgbd, human-interaction-recognition-on-ntu-rgb-d-1, few-shot-skeleton-based-action-recognition-on, self-supervised-skeleton-based-action-1, action-recognition-in-videos-on-ntu-rgbd-120, self-supervised-human-action-recognition-on, generalized-zero-shot-skeletal-action-1, human-action-generation-on-ntu-rgb-d-120, zero-shot-skeletal-action-recognition-on-ntu-1, unsupervised-skeleton-based-action-1",,See all 1951 tasks,Skeleton Based Action Recognit,Skeleton Based Action Recognit
Penn_Action,Penn Action Dataset,The Penn Action Dataset contains 2326 video sequences of 15 different actions and human joint annotations for each sequence.,http://dreamdragon.github.io/PennAction/,EditUnknown,"3D, Image, Video",,,,,,,,"Skeleton Based Action Recognition, Action Recognition, Pose Estimation, Video Alignment","pose-estimation-on-upenn-action, video-alignment-on-upenn-action, skeleton-based-action-recognition-on-upenn, action-recognition-on-penn-action",,See all 1951 tasks,Skeleton Based Action Recognit,Skeleton Based Action Recognit
PKU-MMD,PKU-MMD Dataset,"The PKU-MMD dataset is a large skeleton-based action detection dataset. It contains 1076 long untrimmed video sequences performed by 66 subjects in three camera views. 51 action categories are annotated, resulting almost 20,000 action instances and 5.4 million frames in total. Similar to NTU RGB+D, there are also two recommended evaluate protocols, i.e. cross-subject and cross-view.",https://arxiv.org/abs/1804.06055,EditUnknown,"Image, Video",,,,,,,,"Zero Shot Skeletal Action Recognition, Skeleton Based Action Recognition, Generalized Zero Shot skeletal action recognition, Action Recognition In Videos, Unsupervised Skeleton Based Action Recognition","zero-shot-skeletal-action-recognition-on-pku, generalized-zero-shot-skeletal-action-2, unsupervised-skeleton-based-action-2, skeleton-based-action-recognition-on-pku-mmd, action-recognition-in-videos-on-pku-mmd",,See all 1951 tasks,Skeleton Based Action Recognit,Skeleton Based Action Recognit
UT-Kinect,UT-Kinect Dataset,"The UT-Kinect dataset is a dataset for action recognition from depth sequences. The videos were captured using a single stationary Kinect. There are 10 action types: walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, clap hands. There are 10 subjects, Each subject performs each actions twice. Three channels were recorded: RGB, depth and skeleton joint locations. The three channel are synchronized. The framerate is 30f/s.",https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html,EditUnknown,"Image, Video",,,,,,,,"Skeleton Based Action Recognition, Multimodal Activity Recognition","multimodal-activity-recognition-on-ut-kinect, skeleton-based-action-recognition-on-ut",,See all 1951 tasks,Skeleton Based Action Recognit,Skeleton Based Action Recognit
Chairs,Chairs Dataset,The Chairs dataset contains rendered images of around 1000 different three-dimensional chair models.,https://arxiv.org/abs/2001.04761,EditCustom (non-commercial),Image,,,,,,,,Sketch-Based Image Retrieval,sketch-based-image-retrieval-on-chairs,,See all 1951 tasks,Sketch-Based Image Retrieval3 ,Sketch-Based Image Retrieval3 
QuickDraw-Extended,QuickDraw-Extended Dataset,"Consists of 330,000 sketches and 204,000 photos spanning across 110 categories.",/paper/doodle-to-search-practical-zero-shot-sketch,EditUnknown,Image,,,,,,,110,"Sketch-Based Image Retrieval, Image Retrieval",,,See all 1951 tasks,Sketch-Based Image Retrieval3 ,Sketch-Based Image Retrieval3 
Quick__Draw__Dataset,"Quick, Draw! Dataset Dataset","The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located.",https://github.com/googlecreativelab/quickdraw-dataset,EditCC BY 4.0,Image,,,,,,,345,"Sketch-Based Image Retrieval, Sketch Recognition, Image Retrieval",,,See all 1951 tasks,Sketch-Based Image Retrieval3 ,Sketch-Based Image Retrieval3 
ShoeV2,ShoeV2 Dataset,"ShoeV2 is a dataset of 2,000 photos and 6648 sketches of shoes. The dataset is designed for fine-grained sketch-based image retrieval.",/paper/sketch-me-that-shoe,EditUnknown,Image,,,,,,,,"Sketch-Based Image Retrieval, Sketch Recognition, Image Retrieval",,,See all 1951 tasks,Sketch-Based Image Retrieval3 ,Sketch-Based Image Retrieval3 
University_of_Waterloo_skin_cancer_database,University of Waterloo skin cancer database Dataset,"The dataset is maintained by VISION AND IMAGE PROCESSING LAB, University of Waterloo.
The images of the dataset were extracted from the public databases DermIS and DermQuest, along with manual segmentations of the lesions.

The dataset was used in the following journal publication.
[1] Glaister, J., A. Wong, and D. A. Clausi, ""Automatic segmentation of skin lesions from dermatological photographs using a joint probabilistic texture distinctiveness approach"", IEEE Transactions on Biomedical Engineering
[2] Amelard, R., J. Glaister, A. Wong, and D. A. Clausi, ""High-level intuitive features (HLIFs) for intuitive skin lesion descriptionpdf"", IEEE Transactions on Biomedical Engineering, vol. 62, issue 3, pp. 820-831, October, 2015.
[3] Glaister, J., R. Amelard, A. Wong, and D. A. Clausi, ""MSIM: Multi-Stage Illumination Modeling of Dermatological Photographs for Illumination-Corrected Skin Lesion Analysis"", IEEE Transactions on Biomedical Engineering, vol. 60, issue 7, pp. 1873 - 1883, November, 2013.",https://production-media.paperswithcode.com/datasets/eb301422-9fd2-49b5-b4b6-6ba9910858d3.jpg,EditUnknown,Image,,2015,,,,,,"Lesion Segmentation, Local Color Enhancement, Skin Lesion Segmentation","local-color-enhancement-on-university-of, skin-lesion-segmentation-on-university-of, lesion-segmentation-on-university-of-waterloo",,See all 1951 tasks,Skin Lesion Segmentation3 benc,Skin Lesion Segmentation3 benc
Slice_Discovery5_papers_with_code_Dataset,Slice Discovery5 papers with code Dataset,,https://paperswithcode.com/dataset/slice-discovery,,,,,,,,,,,,,See all 1951 tasks,Slice Discovery5 papers with c,Slice Discovery5 papers with c
ACDC__Adverse_Conditions_Dataset_with_Corresponden,ACDC (Adverse Conditions Dataset with Correspondences) Dataset,"We introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing semantic segmentation methods on adverse visual conditions. It comprises a large set of 4006 images which are evenly distributed between fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality fine pixel-level semantic annotation, a corresponding image of the same scene taken under normal conditions and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content.

ACDC supports two tasks:
1. standard semantic segmentation
2. uncertainty-aware semantic segmentation",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,4006 images,training and testing semantic segmentation methods on adverse visual conditions. It comprises a large set of 4006 images,,"Unsupervised Semantic Segmentation, Foggy Scene Segmentation, Source-Free Domain Adaptation, Domain Adaptation","source-free-domain-adaptation-on-cityscapes, unsupervised-semantic-segmentation-on-acdc, foggy-scene-segmentation-on-acdc-adverse, domain-adaptation-on-cityscapes-to-acdc",,See all 1951 tasks,Source-Free Domain Adaptation1,Source-Free Domain Adaptation1
Dark_Zurich,Dark Zurich Dataset,"Dark Zurich is an image dataset containing a total of 8779 images captured at nighttime, twilight, and daytime, along with the respective GPS coordinates of the camera for each image. These GPS annotations are used to construct cross-time-of-day correspondences, i.e., to match each nighttime or twilight image to its daytime counterpart.

These attributes allow the usage of Dark Zurich as a dataset to build models and systems that perform:

1) domain adaptation (unsupervised, weakly supervised or semi-supervised), e.g. for semantic segmentation or object detection,

2) image translation / style transfer to different times of day,

3) robust image matching / visual localization across diverse domains, and

4) other visual perception tasks that are central for autonomous vehicles and other robotic applications.",https://production-media.paperswithcode.com/datasets/Dark_Zurich_train.png,EditUnknown,Image,,,,,8779 images,,,"Unsupervised Semantic Segmentation, Semantic Segmentation, Source-Free Domain Adaptation","semantic-segmentation-on-dark-zurich, unsupervised-semantic-segmentation-on-dark, source-free-domain-adaptation-on-cityscapes-1",,See all 1951 tasks,Source-Free Domain Adaptation1,Source-Free Domain Adaptation1
VisDA-2017,VisDA-2017 Dataset,"VisDA-2017 is a simulation-to-real dataset for domain adaptation with over 280,000 images across 12 categories in the training, validation and testing domains. The training images are generated from the same object under different circumstances, while the validation images are collected from MSCOCO..",https://arxiv.org/abs/2003.13183,EditCustom,Image,,2017,,,000 images,"training, validation and testing domains. The training images",12,"Partial Domain Adaptation, Universal Domain Adaptation, Source-Free Domain Adaptation, Semi-supervised Domain Adaptation, Domain Adaptation, Semantic Segmentation, Unsupervised Domain Adaptation","domain-adaptation-on-visda2017, partial-domain-adaptation-on-visda2017, universal-domain-adaptation-on-visda2017, source-free-domain-adaptation-on-visda-2017, semi-supervised-domain-adaptation-on, unsupervised-domain-adaptation-on-visda-2017-1, unsupervised-domain-adaptation-on-visda2017",,See all 1951 tasks,Source-Free Domain Adaptation1,Source-Free Domain Adaptation1
JRDB-Act,JRDB-Act Dataset,"JRDB-Act is an extension of the JRDB dataset to create a large-scale multi-modal dataset for spatio-temporal action, social group and activity detection. 

JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M action labels, constituting a large-scale spatio-temporal action detection dataset. Each human bounding box is labelled with one pose-based action label and multiple (optional) interaction-based action labels. Moreover JRDB-Act comes with social group identification annotations conducive to the task of grouping individuals based on their interactions in the scene to infer their social activities (common activities in each social group).",https://github.com/scubers/JRDB,EditUnknown,"Image, Time Series, Video",,,,,,,,Spatio-Temporal Action Localization,,,See all 1951 tasks,Spatio-Temporal Action Localiz,Spatio-Temporal Action Localiz
LIRIS_human_activities_dataset,LIRIS human activities dataset Dataset,"The LIRIS human activities dataset contains (gray/rgb/depth) videos showing people performing various activities taken from daily life (discussing, telphone calls, giving an item etc.). The dataset is fully annotated, where the annotation not only contains information on the action class but also its spatial and temporal positions in the video. It was originally shot for the ICPR-HARL 2012 competition.

The dataset has been shot with two different cameras:

Subset D1 has been shot with a MS Kinect module mounted on a remotely controlled Wany robotics Pekee II mobile robot which is part of the LIRIS-VOIR platform.
Subset D2 has been shot with a sony consumer camcorder",https://production-media.paperswithcode.com/datasets/974f3c80-2ec9-4918-86ed-e441e6bf5a7a.png,EditUnknown,"Image, Time Series, Video",,2012,,,,,,"Spatio-Temporal Action Localization, Action Detection",,,See all 1951 tasks,Spatio-Temporal Action Localiz,Spatio-Temporal Action Localiz
MultiSports,MultiSports Dataset,"Spatio-temporal action detection is an important and challenging problem in video understanding. The existing action detection benchmarks are limited in aspects of small numbers of instances in a trimmed video or low-level atomic actions. This paper aims to present a new multi-person dataset of spatio-temporal localized sports actions, coined as MultiSports. We first analyze the important ingredients of constructing a realistic and challenging dataset for spatio-temporal action detection by proposing three criteria: (1) multi-person scenes and motion dependent identification, (2) with well-defined boundaries, (3) relatively fine-grained classes of high complexity. Based on these guidelines, we build the dataset of MultiSports v1.0 by selecting 4 sports classes, collecting 3200 video clips, and annotating 37701 action instances with 902k bounding boxes. Our dataset is characterized with important properties of high diversity, dense annotation, and high quality. Our MultiSports, with its realistic setting and detailed annotations, exposes the intrinsic challenges of spatio-temporal action detection. We hope our MultiSports can serve as a standard benchmark for spatio-temporal action detection in the future.",https://production-media.paperswithcode.com/datasets/a1774555-22c6-4889-aa7e-bdaefcf79e1a.gif,EditCC BY_NC 4.0,"Image, Time Series, Video",,,,,,,,"Action Recognition, Spatio-Temporal Action Localization, Action Detection, Fine-Grained Action Detection",action-detection-on-multisports,,See all 1951 tasks,Spatio-Temporal Action Localiz,Spatio-Temporal Action Localiz
VidHOI,VidHOI Dataset,VidHOI is a video-based human-object interaction detection benchmark. VidHOI is based on VidOR which is densely annotated with all humans and predefined objects showing up in each frame. VidOR is also more challenging as the videos are non-volunteering user-generated and thus jittery at times.,https://production-media.paperswithcode.com/datasets/8540312536.gif,EditUnknown,"Image, Time Series, Video",,,,,,,,"Spatio-Temporal Action Localization, Action Detection, Human-Object Interaction Anticipation, Human-Object Interaction Detection","human-object-interaction-anticipation-on, human-object-interaction-detection-on-vidhoi",,See all 1951 tasks,Spatio-Temporal Action Localiz,Spatio-Temporal Action Localiz
ARAD-1K,ARAD-1K Dataset,The dataset used for NTIRE 2022 Spectral Recovery Challenge,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://github.com/boazarad/ARAD_1K,3D,,2022,,,,,,Spectral Reconstruction,spectral-reconstruction-on-arad-1k,,See all 1951 tasks,Spectral Reconstruction4 bench,Spectral Reconstruction4 bench
CAVE,CAVE Dataset,Multispectral imaging using multiplexed illumination.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://ieeexplore.ieee.org/document/4409090,3D,,,,,,,,Spectral Reconstruction,spectral-reconstruction-on-cave,,See all 1951 tasks,Spectral Reconstruction4 bench,Spectral Reconstruction4 bench
KAIST,KAIST Dataset,High-quality hyperspectral reconstruction using a spectral prior,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://zaguan.unizar.es/record/75680,3D,,,,,,,,Spectral Reconstruction,spectral-reconstruction-on-kaist,,See all 1951 tasks,Spectral Reconstruction4 bench,Spectral Reconstruction4 bench
Real_HSI,Real HSI Dataset,End-to-End Low Cost Compressive Spectral Imaging with Spatial-Spectral Self-Attention,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://github.com/mengziyi64/TSA-Net,3D,,,,,,,,Spectral Reconstruction,spectral-reconstruction-on-real-hsi,,See all 1951 tasks,Spectral Reconstruction4 bench,Spectral Reconstruction4 bench
EmoDB_Dataset,EmoDB Dataset Dataset,"The EMODB database is the freely available German emotional database. The database is created by the Institute of Communication Science, Technical University, Berlin, Germany. Ten professional speakers (five males and five females) participated in data recording. The database contains a total of 535 utterances. The EMODB database comprises of seven emotions: 1) anger; 2) boredom; 3) anxiety; 4) happiness; 5) sadness; 6) disgust; and 7) neutral. The data was recorded at a 48-kHz sampling rate and then down-sampled to 16-kHz.

Citation:
Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter Sendlmeier und Benjamin Weiss
A Database of German Emotional Speech
Proceedings Interspeech 2005, Lissabon, Portugal",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Image, Text",English,2005,,,,,,"Emotional Speech Synthesis, Speech Emotion Recognition, Speech Recognition",speech-emotion-recognition-on-emodb-dataset,,See all 1951 tasks,Speech Emotion Recognition19 b,Speech Emotion Recognition19 b
IEMOCAP,IEMOCAP Dataset,"Multimodal Emotion Recognition IEMOCAP The IEMOCAP dataset consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. The dataset is recorded across 5 sessions with 5 pairs of speakers.",https://arxiv.org/abs/1802.00923,EditCustom (non-commercial),"Audio, Image",,,,,,,,"Emotion Recognition in Conversation, Speech Emotion Recognition, Multimodal Emotion Recognition","multimodal-emotion-recognition-on-iemocap, speech-emotion-recognition-on-iemocap, emotion-recognition-in-conversation-on",,See all 1951 tasks,Speech Emotion Recognition19 b,Speech Emotion Recognition19 b
LSSED,LSSED Dataset,"LSSED, a challenging large-scale english dataset for speech emotion recognition. It contains 147,025 sentences (206 hours and 25 minutes in total) spoken by 820 people. Each segment is annotated for the presence of 11 emotions (angry, neutral, fear, happy, sad, disappointed, bored, disgusted, excited, surprised, fear and other)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Image",,,,,025 sentences,,,Speech Emotion Recognition,speech-emotion-recognition-on-lssed,,See all 1951 tasks,Speech Emotion Recognition19 b,Speech Emotion Recognition19 b
MELD,MELD Dataset,"Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been labeled by any of these seven emotions -- Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. MELD also has sentiment (positive, negative and neutral) annotation for each utterance.",https://affective-meld.github.io/,EditUnknown,"Audio, Image",,,,,,,,"Emotion Recognition in Conversation, Speech Emotion Recognition, Facial Expression Recognition, Multimodal Emotion Recognition","emotion-recognition-in-conversation-on-meld, facial-expression-recognition-on-meld, multimodal-emotion-recognition-on-meld",,See all 1951 tasks,Speech Emotion Recognition19 b,Speech Emotion Recognition19 b
MSP-Podcast,MSP-Podcast Dataset,"The MSP-Podcast corpus contains speech segments from podcast recordings which are perceptually annotated using crowdsourcing. The collection of this corpus is an ongoing process. Version 1.7 of the corpus has 62,140 speaking turns (100hrs).

Key features of this corpus:


We download available audio recordings with common license. We only use the podcasts that have less restrictive licenses, so we can modify, sell and distribute the corpus (you can use it for commercial product!). 
Most of the segments in a regular podcasts are neutral. We use machine learning techniques trained with available data to retrieve candidate segments. These segments are emotionally annotated with crowdsourcing. This approach allows us to spend our resources on speech segments that are likely to convey emotions.
We annotate categorical emotions and attribute based labels at the speaking turn label
This is an ongoing effort, where we currently have 62,140 speaking turns (100h). We collect approximately 10,000-13,000 new speaking turns per year. Our goal is to reach 400 hours.",https://production-media.paperswithcode.com/datasets/8780da01-adf3-41a1-a159-5682b0e804f0.png,EditUnknown,"Audio, Image",,,,,,,,"Emotion Recognition, Speech Emotion Recognition","speech-emotion-recognition-on-msp-podcast, emotion-recognition-on-msp-podcast, speech-emotion-recognition-on-msp-podcast-1, speech-emotion-recognition-on-msp-podcast-2",,See all 1951 tasks,Speech Emotion Recognition19 b,Speech Emotion Recognition19 b
RAVDESS,RAVDESS Dataset,"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7,356 files (total size: 24.8 GB). The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.

Paper: The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",https://zenodo.org/record/1188976#.YFZuJ0j7SL8,EditAttribution-NonCommercial-ShareAlike 4.0 International,"Audio, Image, Video",,,,,,,,"Audio Classification, Speech Emotion Recognition, Video Emotion Recognition, Emotion Recognition, Facial Expression Recognition (FER), Facial Emotion Recognition, Music Emotion Recognition, Emotion Classification","emotion-recognition-on-ravdess, emotion-classification-on-ravdess, speech-emotion-recognition-on-ravdess, audio-classification-on-ravdess, facial-expression-recognition-on-ravdess, facial-emotion-recognition-on-ravdess",,See all 1951 tasks,Speech Emotion Recognition19 b,Speech Emotion Recognition19 b
SEWA_DB,SEWA DB Dataset,"A database of more than 2000 minutes of audio-visual data of 398 people coming from six cultures, 50% female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies.",/paper/sewa-db-a-rich-database-for-audio-visual,EditUnknown,"Audio, Image",,2000,,,,"valued valence, arousal, liking, agreement, and prototypic examples",,"Emotion Recognition, Speech Emotion Recognition",,,See all 1951 tasks,Speech Emotion Recognition19 b,Speech Emotion Recognition19 b
ShEMO,ShEMO Dataset,"The database includes 3000 semi-natural utterances, equivalent to 3 hours and 25 minutes of speech data extracted from online radio plays. The ShEMO covers speech samples of 87 native-Persian speakers for five basic emotions including anger, fear, happiness, sadness and surprise, as well as neutral state.",/paper/shemo-a-large-scale-validated-database-for,EditUnknown,"Audio, Image",,,,,,valent to 3 hours and 25 minutes of speech data extracted from online radio plays. The ShEMO covers speech samples,,Speech Emotion Recognition,speech-emotion-recognition-on-shemo,,See all 1951 tasks,Speech Emotion Recognition19 b,Speech Emotion Recognition19 b
GUISS_dataset,GUISS dataset Dataset,"We provide all the expected data inputs to GUISS such as meshes, texture images, and blend files. Generated datasets used in our experiments along with the stereo depth estimations can be downloaded. We have defined seven dataset types: scene_reconstructions, texture_variation, gaea_texture_variation, generative_texture, terrain_variation, rocks, and generative_texture_snow. Each dataset type contains renderings with varying values of different parameters such as lighting angle, texture imgs, albedo, etc. Position each dataset type folder under data/dataset/.

Details and links at: https://github.com/nasa-jpl/guiss",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"3D, Image",,,,,,,,"Stereo Depth Estimation, Instance Segmentation, Stereo Matching, Semantic Segmentation, Stereo Disparity Estimation",,,See all 1951 tasks,Stereo Depth Estimation6 bench,Stereo Depth Estimation6 bench
Helvipad,Helvipad Dataset,"The Helvipad dataset is a real-world stereo dataset designed for omnidirectional depth estimation. It comprises 39,553 paired equirectangular images captured using a top-bottom 360° camera setup and corresponding pixel-wise depth and disparity labels derived from LiDAR point clouds.  The dataset spans diverse indoor and outdoor scenes under varying lighting conditions, including night-time environments.",https://production-media.paperswithcode.com/datasets/b2dc6125-0ce9-4923-9cbf-c03949297c66.png,EditCC0,3D,,,,,,,,"Stereo Depth Estimation, Omnnidirectional Stereo Depth Estimation, Stereo Matching",omnnidirectional-stereo-depth-estimation-on,,See all 1951 tasks,Stereo Depth Estimation6 bench,Stereo Depth Estimation6 bench
Spring,Spring Dataset,"Spring is a large, high-resolution and high-detail, computer-generated benchmark for scene flow, optical flow, and stereo. Based on rendered scenes from the open-source Blender movie ""Spring"", it provides photo-realistic HD datasets with state-of-the-art visual effects and ground truth training data.",https://arxiv.org/pdf/2303.01943v1.pdf,EditCC BY 4.0,"3D, Video",,,,,,,,"Stereo Depth Estimation, Optical Flow Estimation, Scene Flow Estimation, Stereo Disparity Estimation","optical-flow-estimation-on-spring, scene-flow-estimation-on-spring, stereo-depth-estimation-on-spring",,See all 1951 tasks,Stereo Depth Estimation6 bench,Stereo Depth Estimation6 bench
VBR,VBR Dataset,"This dataset presents a vision and perception research dataset collected in Rome, featuring RGB data, 3D point clouds, IMU, and GPS data. We introduce a new benchmark targeting visual odometry and SLAM, to advance the research in autonomous robotics and computer vision. This work complements existing datasets by simultaneously addressing several issues, such as environment diversity, motion patterns, and sensor frequency. It uses up-to-date devices and presents effective procedures to accurately calibrate the intrinsic and extrinsic of the sensors while addressing temporal synchronization. During recording, we cover multi-floor buildings, gardens, urban and highway scenarios. Combining handheld and car-based data collections, our setup can simulate any robot (quadrupeds, quadrotors, autonomous vehicles). The dataset includes an accurate 6-dof ground truth based on a novel methodology that refines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment. All sequences divided in training and testing are accessible through our website.",https://production-media.paperswithcode.com/datasets/2b4c0345-1e45-4a0d-9a08-4ae1ca4abbca.png,EditCC BY-SA 4.0 DEED,"3D, Image, Video",,,,,,,,"Stereo Depth Estimation, Visual Place Recognition, Visual Odometry, Image Registration, Pose Tracking, Visual Localization, Stereo-LiDAR Fusion, Optical Flow Estimation, Monocular Visual Odometry, 3D Pose Estimation, 3D Place Recognition, Novel View Synthesis, Point Cloud Completion, Stereo Matching, Stereo Image Super-Resolution, Image to 3D, Pose Estimation, Monocular Depth Estimation, Point Cloud Super Resolution, Point Cloud Registration, Vehicle Speed Estimation, Stereo Disparity Estimation, Motion Estimation, Vehicle Pose Estimation, Scene Flow Estimation, Novel LiDAR View Synthesis, Visual Tracking, 3D Reconstruction, Image to Point Cloud Registration",,,See all 1951 tasks,Stereo Depth Estimation6 bench,Stereo Depth Estimation6 bench
Style_Generalization5_papers_with_code_Dataset,Style Generalization5 papers with code Dataset,,https://paperswithcode.com/dataset/style-generalization,,,,,,,,,,,,,See all 1951 tasks,Style Generalization5 papers w,Style Generalization5 papers w
DukeMTMC-reID,DukeMTMC-reID Dataset,"The DukeMTMC-reID (Duke Multi-Tracking Multi-Camera ReIDentification) dataset is a subset of the DukeMTMC for image-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian image datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images.

NOTE: This dataset has been retracted.",https://arxiv.org/abs/1804.11027,EditUnknown,Image,,,,,,"training images of 702 identities, 2,228 query images",,"Unsupervised Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification, Style Transfer",,,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
GYAFC,GYAFC Dataset,"Grammarly’s Yahoo Answers Formality Corpus (GYAFC) is the largest dataset for any style containing a total of 110K informal / formal sentence pairs.

Yahoo Answers is a question answering forum, contains a large number of informal sentences and allows redistribution of data. The authors used the Yahoo Answers L6 corpus to create the GYAFC dataset of informal and formal sentence pairs. In order to ensure a uniform distribution of data, they removed sentences that are questions, contain URLs, and are shorter than 5 words or longer than 25. After these preprocessing steps, 40 million sentences remain. 

The Yahoo Answers corpus consists of several different domains like Business, Entertainment & Music, Travel, Food, etc. Pavlick and Tetreault formality classifier (PT16) shows that the formality level varies significantly
across different genres. In order to control for this variation, the authors work with two specific domains that contain the most informal sentences and show results on training and testing within those categories. The authors use the formality classifier from PT16 to identify informal sentences and train this classifier on the Answers genre of the PT16 corpus
which consists of nearly 5,000 randomly selected sentences from Yahoo Answers manually annotated on a scale of -3 (very informal) to 3 (very formal). They find that the domains of Entertainment & Music and Family & Relationships contain the most informal sentences and create the GYAFC dataset using these domains.",https://arxiv.org/pdf/1803.06535v2.pdf,EditCustom (research-only),Text,English,,,,,,,"Unsupervised Text Style Transfer, Formality Style Transfer, Style Transfer","formality-style-transfer-on-gyafc, unsupervised-text-style-transfer-on-gyafc, style-transfer-on-gyafc",,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
iKala,iKala Dataset,"The iKala dataset is a singing voice separation dataset that comprises of 252 30-second excerpts sampled from 206 iKala songs (plus 100 hidden excerpts reserved for MIREX data mining contest). The music accompaniment and the singing voice are recorded at the left and right channels respectively. Additionally, the human-labeled pitch contours and timestamped lyrics are provided.

This dataset is not available anymore.


""T"" as a sofa:

The ""T"" horizontal strip can mimic the back of a sofa with a delicate cushion or details of the uphols or appliances with the color button.

The ""T"" vertical strip can show a feet or arm of the sofa, shiny, yet firm.


Merge ""P"":

Put ""P"" next to ""T"", your curve to delicately with the top ""T."" It is intertwined. The circular part of ""P"" can show a cushion or a curved chair and synchronize the subject of furniture.

Make sure ""P"" is visually relying on ""T"", which reflects the relationship of cohesion and balance.


Coherence of ""B"" and ""I"":

""B"" can be aligned as a pair of cushions or a modern chair, with mild curves with glossy and modern aesthetics.

""I"" can be a symbol of a shiny furniture or a vertical light bar and completes the shapes without overburdess them.

Color palette 4:

Includes soft soil colors such as beige, top and gray shades, along with silent or silver gold tips to touch elegance.

Consider a slope effect to enhance modernity, to keep colors elegant and complex.


Connect the letters:

Use the overlap or intertwined edges that the letters meet for the symbol of unity.

The plan should allow viewers to distinguish each letter while feeling part of the same ""structure"".


Background patterns:

Use delicate geometric patterns or textures that mimic fabrics or furniture materials such as wood seeds or woven fibers.

These patterns must remain minimalist and focus on highlighting the logo, while maintaining communication.

While it deals with the subject of furniture and design, this concept conveys modernity, creativity and professional. If you like, I can create a draft design for better visualization.",http://mac.citi.sinica.edu.tw/ikala/,EditUnknown,Audio,,,,,,,,"Speech Separation, Style Transfer, Music Information Retrieval, Information Retrieval",speech-separation-on-ikala,,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
LaMem,LaMem Dataset,"An annotated image memorability dataset to date (with 60,000 labeled images from a diverse array of sources).",/paper/understanding-and-predicting-image,EditUnknown,"Image, Text",English,,,,,,,"Image Classification, Image-to-Image Translation, Style Transfer",,,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
MPIIGaze,MPIIGaze Dataset,"MPIIGaze is a dataset for appearance-based gaze estimation in the wild. It contains 213,659 images collected from 15 participants during natural everyday laptop use over more than three months. It has a large variability in appearance and illumination.",https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild,EditCC BY-NC-SA 4.0,Image,English,,,,659 images,,,"Style Transfer, Gaze Estimation","gaze-estimation-on-mpii-gaze, gaze-estimation-on-mpiigaze-1",,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
MPI_Sintel,MPI Sintel Dataset,MPI (Max Planck Institute) Sintel is a dataset for optical flow evaluation that has 1064 synthesized stereo images and ground truth data for disparity. Sintel is derived from open-source 3D animated short film Sintel. The dataset has 23 different scenes. The stereo images are RGB while the disparity is grayscale. Both have resolution of 1024×436 pixels and 8-bit per channel.,https://arxiv.org/abs/1805.07499,EditUnknown,"Image, Time Series, Video",,,,,,valuation that has 1064 synthesized stereo images,,"Optical Flow Estimation, Video Prediction, Intrinsic Image Decomposition, Style Transfer, Temporal View Synthesis","optical-flow-estimation-on-sintel-final-2, optical-flow-estimation-on-sintel-final, video-prediction-on-mpi-sintel, optical-flow-estimation-on-sintel-clean-2, temporal-view-synthesis-on-mpi-sintel, optical-flow-estimation-on-sintel-clean",,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
POP909,POP909 Dataset,"POP909 is a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, annotations are provided of tempo, beat, key, and chords, where the tempo curves are hand-labelled and others are done by MIR algorithms.",https://arxiv.org/pdf/2008.07142.pdf,EditUnknown,"Audio, Text",English,,,,,,,"Music Generation, Style Transfer",,,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
TextSeg,TextSeg Dataset,"TextSeg is a large-scale fine-annotated and multi-purpose text detection and segmentation dataset, collecting scene and design text with six types of annotations: word- and character-wise bounding polygons, masks and transcriptions.",https://github.com/SHI-Labs/Rethinking-Text-Segmentation,EditUnknown,"Image, Text",English,,,,,,,"Text Style Transfer, Style Transfer, self-supervised scene text recognition",self-supervised-scene-text-recognition-on-1,,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
Touchdown_Dataset,Touchdown Dataset Dataset,"Touchdown is a corpus for executing navigation instructions and resolving spatial descriptions in visual real-world environments. The task is to follow instruction to a goal position and there find a hidden object, Touchdown the bear.",https://github.com/lil-lab/touchdown,EditUnknown,Text,English,,,,,,,"Text Style Transfer, Style Transfer, Vision and Language Navigation",vision-and-language-navigation-on-touchdown,,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
WikiArt,WikiArt Dataset,WikiArt contains painting from 195 different artists. The dataset has 42129 images for training and 10628 images for testing.,https://arxiv.org/abs/1805.11119,EditCustom (non-commercial),,,,,,42129 images,training and 10628 images,,"Continual Learning, Style Transfer","style-transfer-on-wikiart, continual-learning-on-wikiart-fine-grained-6",,See all 1951 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
CATS,CATS Dataset,"A dataset consisting of stereo thermal, stereo color, and cross-modality image pairs with high accuracy ground truth (< 2mm) generated from a LiDAR. The authors scanned 100 cluttered indoor and 80 outdoor scenes featuring challenging environments and conditions. CATS contains approximately 1400 images of pedestrians, vehicles, electronics, and other thermally interesting objects in different environmental conditions, including nighttime, daytime, and foggy scenes.",/paper/cats-a-color-and-thermal-stereo-benchmark,EditUnknown,"3D, Image, Text",English,,,,1400 images,,,"Super-Resolution, Multimodal Unsupervised Image-To-Image Translation, Stereo Matching, Anomaly Detection, Stereo Matching Hand","anomaly-detection-on-cats-and-dogs, multimodal-unsupervised-image-to-image",,See all 1951 tasks,Super-Resolution248 benchmarks,Super-Resolution248 benchmarks
DRealSR,DRealSR Dataset,"DRealSR establishes a Super Resolution (SR) benchmark with diverse real-world degradation processes, mitigating the limitations of conventional simulated image degradation. 

It has been collected from five DSLR cameras in natural scenes and cover indoor and outdoor scenes avoiding moving objects, e.g., advertising posters, plants, offices, buildings. The training images are cropped into 380×380, 272×272 and 192×192 patches, resulting in 31,970 patches.",https://arxiv.org/abs/2008.01928,EditUnknown,Image,,,,,,,,"Super-Resolution, Image Super-Resolution, SSIM, Blind Super-Resolution",blind-super-resolution-on-drealsr,,See all 1951 tasks,Super-Resolution248 benchmarks,Super-Resolution248 benchmarks
Holopix50k,Holopix50k Dataset,"An in-the-wild stereo image dataset, comprising 49,368 image pairs contributed by users of the Holopix mobile social platform.",/paper/holopix50k-a-large-scale-in-the-wild-stereo-1,Editnon-commercial,3D,,,,,,,,"Super-Resolution, Depth Estimation, Monocular Depth Estimation",,,See all 1951 tasks,Super-Resolution248 benchmarks,Super-Resolution248 benchmarks
MSU_SR-QA_Dataset,MSU SR-QA Dataset Dataset,"Our dataset was made of videos from MSU Video Upscalers Benchmark Dataset, MSU Video Super-Resolution Benchmark Dataset and MSU Super-Resolution for Video Compression Benchmark Dataset. Dataset consists of real videos (were filmed with 2 cameras), video games footages, movies, cartoons, dynamic ads. 

How we brought our dataset closer to completeness?
* The dataset covers a large number of use cases in the field of SR due to the large number of content types
* The dataset contains videos with completely different resolutions, FPS values: 8, 24, 25, 30, 60, as well as high and low spatio-temporal complexity
* Distorted videos were obtained using 46 SR methods, some of them were preprocessed with 5 codecs: aomenc, vvenc, x264, x265, uavs3es with different bitrates and qp values
* The dataset was manually checked for redundancy

Videos from benchmarks are FullHD video crops, since the subjective comparison was made on crops. Therefore, the resolution of all videos in the received dataset is low.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,,,,"Super-Resolution, Image Super-Resolution, Video Quality Assessment, Video Super-Resolution, Image Quality Assessment",video-quality-assessment-on-msu-sr-qa-dataset,,See all 1951 tasks,Super-Resolution248 benchmarks,Super-Resolution248 benchmarks
OST300,OST300 Dataset,"OST300 is an outdoor scene dataset with 300 test images of outdoor scenes, and a training set of 7 categories of images with rich textures.",https://paperswithcode.com/paper/recovering-realistic-texture-in-image-super,EditUnknown,Image,,,,,,"test images of outdoor scenes, and a training set of 7 categories of images",7,"Super-Resolution, Image Super-Resolution, Semantic Segmentation",,,See all 1951 tasks,Super-Resolution248 benchmarks,Super-Resolution248 benchmarks
PROBA-V,PROBA-V Dataset,"The PROBA-V Super-Resolution dataset is the official dataset of ESA's Kelvins competition for ""PROBA-V Super Resolution"". It contains satellite data from 74 hand-selected regions around the globe at different points in time. The data is composed of radiometrically and geometrically corrected Top-Of-Atmosphere (TOA) reflectances for the RED and NIR spectral bands at 300m and 100m resolution in Plate Carrée projection. The 300m resolution data is delivered as 128x128 grey-scale pixel images, the 100m resolution data as 384x384 grey-scale pixel images. Additionally, a quality map is provided for each pixel, indicating whether the pixels are concealed (i.e. by clouads, ice, water, missing information, etc.).

The goal of the challenge can be described as Multi-Image Super-resolution: Construct a single high-resolution image out of a series of more frequent low resolution images.

Detailed information about the related competition can be found at https://kelvins.esa.int/proba-v-super-resolution.",https://production-media.paperswithcode.com/datasets/ebb11bad-3402-47cf-974b-b464067278cd.jpg,EditCreative Commons Attribution 4.0 International,Image,,,,,,,,"Super-Resolution, Image Super-Resolution, satellite image super-resolution, Multi-Frame Super-Resolution",multi-frame-super-resolution-on-proba-v,,See all 1951 tasks,Super-Resolution248 benchmarks,Super-Resolution248 benchmarks
RealSRSet,RealSRSet Dataset,20 real low-resolution images selected from existing datasets or downloaded from internet,https://production-media.paperswithcode.com/datasets/realsrset.jpg,EditUnknown,,,,,,,,,Super-Resolution,,,See all 1951 tasks,Super-Resolution248 benchmarks,Super-Resolution248 benchmarks
Stanford_Light_Field,Stanford Light Field Dataset,The Stanford Light Field Archive is a collection of several light fields for research in computer graphics and vision.,http://lightfield.stanford.edu/,EditUnknown,3D,,,,,,,,"Super-Resolution, Depth Estimation",,,See all 1951 tasks,Super-Resolution248 benchmarks,Super-Resolution248 benchmarks
TextZoom,TextZoom Dataset,TextZoom is a super-resolution dataset that consists of paired Low Resolution – High Resolution scene text images. The images are captured by cameras with different focal length in the wild.,https://github.com/JasonBoy1/TextZoom,EditUnknown,"Image, Text",English,,,,,,,"Super-Resolution, Image Super-Resolution, self-supervised scene text recognition",self-supervised-scene-text-recognition-on,,See all 1951 tasks,Super-Resolution248 benchmarks,Super-Resolution248 benchmarks
BTAD,BTAD Dataset,The BTAD ( beanTech Anomaly Detection) dataset is a real-world industrial anomaly dataset. The dataset contains a total of 2830 real-world images of 3 industrial products showcasing body and surface defects.,https://production-media.paperswithcode.com/datasets/Screenshot_2021-04-22_at_14.21.48.png,EditUnknown,Image,,,,,,,,"Anomaly Detection, Supervised Anomaly Detection","supervised-anomaly-detection-on-btad, anomaly-detection-on-btad",,See all 1951 tasks,Supervised Anomaly Detection2 ,Supervised Anomaly Detection2 
CHAD,CHAD Dataset,"CHAD: Charlotte Anomaly Dataset
CHAD is high-resolution, multi-camera dataset for surveillance video anomaly detection. It includes bounding box, Re-ID, and pose annotations, as well as frame-level anomaly labels, dividing all frames into two groups of anomalous or normal. You can find the paper with all the details in the following link: CHAD: Charlotte Anomaly Dataset. Please refer to the page of the dataset for more information.",https://production-media.paperswithcode.com/datasets/87103313-a6b1-4dc3-aecd-42e4a0ab1a11.jpg,EditUnknown,"Image, Video",,,,,,,,"Unsupervised Anomaly Detection, Video Anomaly Detection, Group Anomaly Detection, Anomaly Detection, Supervised Anomaly Detection",video-anomaly-detection-on-chad,,See all 1951 tasks,Supervised Anomaly Detection2 ,Supervised Anomaly Detection2 
ISP-AD,ISP-AD Dataset,"The ISP-AD Dataset is a large-scale anomaly detection dataset, representing a real-world industrial use case.                 It contains 312,674 fault-free and 246,375 defective samples, including 245,664 synthetic defects and   711 real defects collected on the factory floor.

Designed to advance research in unsupervised, self-supervised, and supervised anomaly detection,
ISP-AD serves as a benchmark for evaluating defect detection methods under realistic industrial conditions.",https://production-media.paperswithcode.com/datasets/36f7228c-e615-4a8d-baa5-7ce42724f134.png,EditCC BY-NC-SA 4.0,Image,,,,,,,,"Supervised Defect Detection, Defect Detection, Unsupervised Anomaly Detection, Self-Supervised Anomaly Detection, Anomaly Detection, Weakly Supervised Defect Detection, Supervised Anomaly Detection",,,See all 1951 tasks,Supervised Anomaly Detection2 ,Supervised Anomaly Detection2 
Supervised_dimensionality_reduction17_papers_with_,Supervised dimensionality reduction17 papers with code Dataset,,https://paperswithcode.com/dataset/supervised-dimensionality-reduction,,,,,,,,,,,,,See all 1951 tasks,Supervised dimensionality redu,Supervised dimensionality redu
Survey807_papers_with_code_Dataset,Survey807 papers with code Dataset,,https://paperswithcode.com/dataset/survey,,,,,,,,,,,,,See all 1951 tasks,Survey807 papers with code,Survey807 papers with code
CompMix-IR,CompMix-IR Dataset,"CompMix-IR Dataset Overview:

Characteristics: CompMix-IR is a heterogeneous knowledge retrieval benchmark dataset, featuring four knowledge types (text, knowledge graphs, tables, and infoboxes), 9,400+ QA pairs, and a corpus of 10 million entries. It supports two retrieval scenarios: retrieving across all knowledge types or retrieving specific types based on user instructions.

Motivation: It addresses the limitations of existing benchmarks by providing a more comprehensive and realistic dataset that reflects real-world retrieval needs with diverse knowledge sources and user intents.

Potential Use Cases: Ideal for developing and evaluating heterogeneous IR models, instruction-aware retrieval systems, and open-domain QA systems. It can also be used for benchmarking, cross-domain IR research, and enhancing the adaptability and robustness of retrieval models.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Editcc-by-4.0,"Tabular, Text",English,,,,,,,"Table Retrieval, RAG, Knowledge Graphs, Information Retrieval, Text Retrieval, Question Answering",,,See all 1951 tasks,Table Retrieval1 benchmark14 p,Table Retrieval1 benchmark14 p
Statcan_Dialogue_Dataset,Statcan Dialogue Dataset Dataset,"The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents

Xing Han Lu, Siva Reddy, Harm de Vries

EACL 2023


| | | | | |
| :--: | :--: | :--: | :--: | :--: |
| Code | Huggingface | Request on Dataverse | Paper | Website |",https://production-media.paperswithcode.com/datasets/b40d683a-08bd-4420-80ce-a40176c36c5e.png,EditCustom,"Tabular, Text",English,2023,,,,,,"Dialogue Generation, Table Retrieval, Retrieval",table-retrieval-on-statcan-dialogue-dataset,,See all 1951 tasks,Table Retrieval1 benchmark14 p,Table Retrieval1 benchmark14 p
Charades,Charades Dataset,"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.",https://arxiv.org/abs/1908.09995,EditCustom (non-commercial),"Image, Time Series, Video",,,,,,,104,"Zero-Shot Action Recognition, Temporal Action Localization, Video Understanding, Action Detection, Video Classification, Action Classification, Weakly Supervised Object Detection, Action Recognition","weakly-supervised-object-detection-on-4, action-classification-on-charades, action-detection-on-charades, video-classification-on-charades, action-recognition-in-videos-on-charades, zero-shot-action-recognition-on-charades-1",,See all 1951 tasks,Temporal Action Localization96,Temporal Action Localization96
COIN,COIN Dataset,"The COIN dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.",/paper/coin-a-large-scale-dataset-for-comprehensive,EditCustom,"Image, Time Series, Video",,,,,,,,"Temporal Action Localization, Video Classification, Action Localization, Action Recognition, Action Segmentation","action-segmentation-on-coin, video-classification-on-coin-1",,See all 1951 tasks,Temporal Action Localization96,Temporal Action Localization96
HMDB51,HMDB51 Dataset,"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as “jump”, “kiss” and “laugh”), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.",https://arxiv.org/abs/1505.04868,EditCC BY 4.0,"Image, Time Series, Video",,,,,,,,"Skeleton Based Action Recognition, Self-Supervised Action Recognition Linear, Zero-Shot Action Recognition, Temporal Action Localization, Human Activity Recognition, Self-supervised Video Retrieval, Action Classification, Few Shot Action Recognition, Action Recognition In Videos, Action Recognition, Self-Supervised Action Recognition","human-activity-recognition-on-hmdb51, self-supervised-video-retrieval-on-hmdb51, self-supervised-action-recognition-on-hmdb51-1, zero-shot-action-recognition-on-hmdb51, action-recognition-in-videos-on-hmdb51, few-shot-action-recognition-on-hmdb51, action-classification-on-hmdb51, action-recognition-in-videos-on-hmdb-51, self-supervised-action-recognition-on-hmdb51, self-supervised-action-recognition-linear-on-1, action-recognition-in-videos-on-hmdb-51-1, skeleton-based-action-recognition-on-hmdb51",,See all 1951 tasks,Temporal Action Localization96,Temporal Action Localization96
KTH,KTH Dataset,"The efforts to create a non-trivial and publicly available dataset for action recognition was initiated at the KTH Royal Institute of Technology in 2004. The KTH dataset is one of the most standard datasets, which contains six actions: walk, jog, run, box, hand-wave, and hand clap. To account for performance nuance, each action is performed by 25 different individuals, and the setting is systematically altered for each action per actor. Setting variations include: outdoor (s1), outdoor with scale variation (s2), outdoor with different clothes (s3), and indoor (s4). These variations test the ability of each algorithm to identify actions independent of the background, appearance of the actors, and the scale of the actors.",https://arxiv.org/abs/1610.06906,"EditCustom (non-commercial, attribution)","Image, Time Series, Video",,2004,,,,,,"Temporal Action Localization, Action Recognition, Video Prediction","action-recognition-on-kth, video-prediction-on-kth",,See all 1951 tasks,Temporal Action Localization96,Temporal Action Localization96
THUMOS14,THUMOS14 Dataset,"The THUMOS14 (THUMOS 2014) dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.",https://arxiv.org/abs/2008.13705,EditUnknown,"Image, Text, Time Series, Video",English,2014,,,,,20,"Zero-Shot Action Recognition, Temporal Action Localization, Action Detection, Weakly-supervised Temporal Action Localization, Semi-Supervised Action Detection, Action Classification, Temporal Action Proposal Generation, Zero-Shot Action Detection, Weakly Supervised Temporal Action Localization, Action Recognition In Videos, Online Action Detection, Action Recognition, Few Shot Temporal Action Localization, Weakly Supervised Action Localization","online-action-detection-on-thumos-14, weakly-supervised-temporal-action, action-recognition-on-thumos14, action-recognition-in-videos-on-thumos14-1, action-detection-on-thumos-14, temporal-action-localization-on-thumos14-2, few-shot-temporal-action-localization-on-1, action-recognition-in-videos-on-thumos14, weakly-supervised-temporal-action-5, zero-shot-action-recognition-on-thumos-14, action-classification-on-thumos14, weakly-supervised-action-localization-on-4, temporal-action-localization-on-thumos-14, zero-shot-action-detection-on-thumos-14, temporal-action-proposal-generation-on-thumos, weakly-supervised-action-localization-on-5, weakly-supervised-action-localization-on-8, weakly-supervised-action-localization-on, action-classification-on-thumos-14, semi-supervised-action-detection-on-thumos-14, temporal-action-localization-on-thumos14",,See all 1951 tasks,Temporal Action Localization96,Temporal Action Localization96
DADE,DADE Dataset,"The DADE dataset, short for Driving Agents in Dynamic Environments, is a synthetic dataset designed for the training and evaluation of methods for the task of semantic segmentation in the context of autonomous driving agents navigating dynamic environments and weather conditions.

This dataset was generated using the CARLA simulator (version 0.9.14) to provide perfect sensor synchronization and calibration, as well as precise semantic segmentation ground truths. All data were collected within the Town12 map in CARLA.

DADE dataset is divided into two sub-datasets. For both subsets, each sequence is acquired by one agent (one ego vehicle) running for some time within a 5-hour time frame, amounting to a total of 990k frames for the entire dataset. The agents travel various locations such as forest, countryside, rural farmland, highway, low density residential, community buildings, and high density residential.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC-BY-4.0 license,Image,,,,,,,,"Test-time Adaptation, Semantic Segmentation",,,See all 1951 tasks,Test-time Adaptation1 benchmar,Test-time Adaptation1 benchmar
Conceptual_Captions,Conceptual Captions Dataset,"Automatic image captioning is the task of producing a natural-language utterance (usually a sentence) that correctly reflects the visual content of an image. Up to this point, the resource most used for this task was the MS-COCO dataset, containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).

Google's Conceptual Captions dataset has more than 3 million images, paired with natural-language captions. In contrast with the curated style of the MS-COCO images, Conceptual Captions images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. The raw descriptions are harvested from the Alt-text HTML attribute associated with web images. The authors developed an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.",https://github.com/google-research-datasets/conceptual-captions,EditCustom,"Image, Text",English,,,,000 images,,,"Text-to-Image Generation, Visual Question Answering (VQA), Image Captioning, Question Answering","image-captioning-on-conceptual-captions, text-to-image-generation-on-conceptual",,See all 1951 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
CUB-200-2011,CUB-200-2011 Dataset,"The Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from Reed et al.. They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.",https://arxiv.org/abs/1709.00340,EditUnknown,"3D, Audio, Graph, Image, Text",English,2011,,,788 images,,,"Long-tail learning with class descriptors, Multimodal Deep Learning, Cross-Domain Few-Shot, Dataset Distillation - 1IPC, Multi-Modal Document Classification, Few-Shot Class-Incremental Learning, Image Clustering, Interpretable Machine Learning, Concept-based Classification, Fine-Grained Image Classification, Text-to-Image Generation, Error Understanding, Document Text Classification, Generalized Zero-Shot Learning, Zero-Shot Learning, Bird Species Classification With Audio-Visual Data, Image Attribution, Small Data Image Classification, Image Classification, Weakly-Supervised Object Localization, Image Retrieval, Few-Shot Learning, Graph Matching, Single-View 3D Reconstruction, Unsupervised Keypoint Estimation, Fine-Grained Image Recognition, Semantic correspondence, Generalized Few-Shot Learning, Point-interactive Image Colorization, Fine-Grained Visual Recognition, Multimodal Text and Image Classification, Few-Shot Image Classification, Metric Learning, Image Generation, Transductive Zero-Shot Classification","zero-shot-learning-on-cub-200-2011, few-shot-image-classification-on-cub-200-50, image-clustering-on-cub-birds, concept-based-classification-on-cub-200-2011, long-tail-learning-with-class-descriptors-on, few-shot-image-classification-on-cub-200-2011-1, unsupervised-keypoint-estimation-on-cub, image-clustering-on-cub-200-2011, transductive-zero-shot-classification-on-cub, few-shot-image-classification-on-cub-200-5, image-classification-on-cub, image-classification-on-cub-200-2011-3, cross-domain-few-shot-on-cub, fine-grained-image-classification-on, dataset-distillation-1ipc-on-cub-200-2011, few-shot-image-classification-on-cub-200-0, fine-grained-image-recognition-on-cub-birds, zero-shot-learning-on-cub-200-0-shot-learning-1, point-interactive-image-colorization-on-cub, fine-grained-visual-recognition-on-cub-200-1, fine-grained-image-classification-on-cub-200, multi-modal-document-classification-on-cub, small-data-on-cub-200-2011-5-samples-per-1, generalized-zero-shot-learning-on-cub-200, multimodal-deep-learning-on-cub-200-2011, interpretable-machine-learning-on-cub-200, image-classification-on-imbalanced-cub-200, image-attribution-on-cub-200-2011-1, weakly-supervised-object-localization-on-cub, error-understanding-on-cub-200-2011-1, metric-learning-on-cub-200-2011-4, fine-grained-image-recognition-on-cub-200, weakly-supervised-object-localization-on-cub-2, generalized-few-shot-learning-on-cub, graph-matching-on-cub, few-shot-image-classification-on-cub-200-2011-2, few-shot-image-classification-on-cub-200-2011, few-shot-class-incremental-learning-on-cub, image-generation-on-cub-128-x-128, image-retrieval-on-cub-200-2011, weakly-supervised-object-localization-on-cub-1, metric-learning-on-cub-200-2011, single-view-3d-reconstruction-on-cub-200-2011, small-data-on-cub-200-2011-30-samples-per-1, document-text-classification-on-cub-200-2011, multimodal-text-and-image-classification-on, text-to-image-generation-on-cub, semantic-correspondence-on-cub-200-2011, few-shot-image-classification-on-cub-200-5-1, fine-grained-image-classification-on-cub-200-1",,See all 1951 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Fashion-Gen,Fashion-Gen Dataset,"Fashion-Gen consists of 293,008 high definition (1360 x 1360 pixels) fashion images paired with item descriptions provided by professional stylists. Each item is photographed from a variety of angles.",https://arxiv.org/pdf/1806.08317v2.pdf,EditCustom,"Image, Text",English,,,,,,,"Image Inpainting, Image Generation, Text-to-Image Generation, Image Retrieval",,,See all 1951 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Human-Art,Human-Art Dataset,"Human-Art is a versatile human-centric dataset to bridge the gap between natural and artificial scenes. It includes twenty high-quality human scenes, including natural and artificial humans in both 2D representation and 3D representation. It includes 50,000 images including more than 123,000 human figures in 20 scenarios, with annotations of human bounding box, 21 2D human keypoints, human self-contact keypoints, and description text.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://docs.google.com/document/d/19j-6GFOCYBDU4CxwRSKgORndse_j5iHGK0RCJ2TvXNQ/edit?usp=sharing,"3D, Image, Text",English,,,,000 images,,,"Conditional Image Generation, Text-to-Image Generation, 2D Human Pose Estimation, 3D Human Pose Estimation",2d-human-pose-estimation-on-human-art,,See all 1951 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
LHQ,LHQ Dataset,"A dataset of 90,000 high-resolution nature landscape images, crawled from Unsplash and Flickr and preprocessed with Mask R-CNN and Inception V3.",https://production-media.paperswithcode.com/datasets/lhq.jpg,EditCC BY 2.0,"Image, Text",English,,,,,,,"Text-to-Image Generation, Infinite Image Generation, Image Generation, Perpetual View Generation, Image Outpainting","perpetual-view-generation-on-lhq, text-to-image-generation-on-lhqc, image-outpainting-on-lhqc, infinite-image-generation-on-lhq",,See all 1951 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Multi-Modal_CelebA-HQ,Multi-Modal CelebA-HQ Dataset,"Multi-Modal-CelebA-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has high-quality segmentation mask, sketch, descriptive text, and image with transparent background.

Multi-Modal-CelebA-HQ can be used to train and evaluate algorithms of text-to-image-generation, text-guided image manipulation, sketch-to-image generation, and GANs for face generation and editing.",https://github.com/weihaox/Multi-Modal-CelebA-HQ-Dataset,EditUnknown,"Image, Text",English,,,,,,,"Text-to-Image Generation, Image Generation, Face Sketch Synthesis, multimodal generation","face-sketch-synthesis-on-multi-modal-celeba, text-to-image-generation-on-multi-modal, multimodal-generation-on-multi-modal-celeba",,See all 1951 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Oxford_102_Flower,Oxford 102 Flower Dataset,"Oxford 102 Flower is an image classification dataset consisting of 102 flower categories. The flowers chosen to be flower commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images.

The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories.",https://production-media.paperswithcode.com/datasets/flowers.jpg,EditUnknown,"Image, Text",English,,,,258 images,,,"Point-interactive Image Colorization, Image Clustering, Fine-Grained Image Classification, Prompt Engineering, Unsupervised Image Segmentation, Few-Shot Image Classification, Text-to-Image Generation, Few-Shot Learning, Generalized Zero-Shot Learning, Zero-Shot Learning, Continual Learning, Image Generation, Neural Architecture Search, Image Classification, Transductive Zero-Shot Classification","point-interactive-image-colorization-on-1, generalized-zero-shot-learning-on-oxford-102-1, prompt-engineering-on-oxford-102-flower, fine-grained-image-classification-on-oxford, few-shot-image-classification-on-oxford-102, image-classification-on-flowers-102, continual-learning-on-flowers-fine-grained-6, image-generation-on-oxford-102-flowers-256-x, few-shot-learning-on-flowers-102, zero-shot-learning-on-oxford-102-flower, image-clustering-on-flowers-102, text-to-image-generation-on-oxford-102, unsupervised-image-segmentation-on-flowers, neural-architecture-search-on-oxford-102, few-shot-image-classification-on-flowers-102-1, transductive-zero-shot-classification-on-2, zero-shot-learning-on-flowers-102",,See all 1951 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Pick-a-Pic,Pick-a-Pic Dataset,"Pick-a-Pic dataset was created by logging user interactions with the Pick-a-Pic web application for text-to image generation. Overall, the Pick-a-Pic dataset contains over 500,000 examples and 35,000 distinct prompts. Each example contains a prompt, two generated images, and a label for which image is preferred, or if there is a tie when no image is significantly preferred over the other.",https://arxiv.org/pdf/2305.01569v1.pdf,EditUnknown,"Image, Text",English,,,,000 examples,,,Text-to-Image Generation,,,See all 1951 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
T2I-CompBench,T2I-CompBench Dataset,"T2I-CompBench is a comprehensive benchmark for open-world compositional text-to-image generation, consisting of 6,000 compositional textual prompts from 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions).",https://production-media.paperswithcode.com/datasets/6947ca2f-be02-4895-ac5e-32b045d0ec0a.jpeg,EditMIT License,"Image, Text",English,,,,,,3,Text-to-Image Generation,,,See all 1951 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Text_Augmentation40_papers_with_code_Dataset,Text Augmentation40 papers with code Dataset,,https://paperswithcode.com/dataset/text-augmentation,,,,,,,,,,,,,See all 1951 tasks,Text Augmentation40 papers wit,Text Augmentation40 papers wit
AG_News,AG News Dataset,"AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.",https://arxiv.org/pdf/1509.01626.pdf,EditCustom (non-commercial),"Image, Text",English,,,,,"training and 1,900 test samples",,"Zero-Shot Text Classification, Topic Models, Short Text Clustering, Stochastic Optimization, Continual Pretraining, Semi-Supervised Text Classification, Anomaly Detection, Unsupervised Text Classification, Text Classification","semi-supervised-text-classification-on-ag-1, unsupervised-text-classification-on-ag-news, text-classification-on-ag-news, zero-shot-text-classification-on-ag-news, stochastic-optimization-on-ag-news, short-text-clustering-on-ag-news, topic-models-on-ag-news, anomaly-detection-on-ag-news, continual-pretraining-on-ag-news",,See all 1951 tasks,Text Classification172 benchma,Text Classification172 benchma
CoNLL_2003,CoNLL 2003 Dataset,"CoNLL-2003 is a named entity recognition dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition.
The data consists of eight files covering two languages: English and German.
For each of the languages there is a training file, a development file, a test file and a large file with unannotated data.

The English data was taken from the Reuters Corpus. This corpus consists of Reuters news stories between August 1996 and August 1997.
For the training and development set, ten days worth of data were taken from the files representing the end of August 1996.
For the test set, the texts were from December 1996. The preprocessed raw data covers the month of September 1996.

The text for the German data was taken from the ECI Multilingual Text Corpus. This corpus consists of texts in many languages. The portion of data that
was used for this task, was extracted from the German newspaper Frankfurter Rundshau. All three of the training, development and test sets were taken
from articles written in one week at the end of August 1992.
The raw data were taken from the months of September to December 1992.

| English      data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |
|-------------------|----------|-----------|---------|------|------|------|------|
| Training     set  | 946      | 14,987    | 203,621 | 7140 | 3438 | 6321 | 6600 |
| Development  set  | 216      | 3,466     | 51,362  | 1837 | 922  | 1341 | 1842 |
| Test         set  | 231      | 3,684     | 46,435  | 1668 | 702  | 1661 | 1617 |

Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in English data files.

| German       data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |
|-------------------|----------|-----------|---------|------|------|------|------|
| Training     set  | 553      | 12,705    | 206,931 | 4363 | 2288 | 2427 | 2773 |
| Development  set  | 201      | 3,068     | 51,444  | 1181 | 1010 | 1241 | 1401 |
| Test         set  | 155      | 3,160     | 51,943  | 1035 | 670  | 773  | 1195 |

Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in German data files.",https://huggingface.co/datasets/conll2003,EditUnknown,"Image, Text",English,2003,,,,,,"FG-1-PG-1, Weakly-Supervised Named Entity Recognition, Named Entity Recognition (NER), Chunking, Token Classification, Cross-Lingual NER, Semantic Similarity, Low Resource Named Entity Recognition, Knowledge Distillation, POS, UIE, Text Classification, Named Entity Recognition, NER","chunking-on-conll-2003-english, text-classification-on-unknown, fg-1-pg-1-on-conll2003, named-entity-recognition-on-conll-2003-german-1, semantic-similarity-on-unknown, named-entity-recognition-on-conll03, named-entity-recognition-on-conll-2003-german, uie-on-conll-2003, ner-on-conll-2003-1, named-entity-recognition-ner-on-conll-2003, cross-lingual-ner-on-conll-2003, token-classification-on-conll2003, low-resource-named-entity-recognition-on-4, knowledge-distillation-on-unknown, named-entity-recognition-on-conll-2003-3, weakly-supervised-named-entity-recognition-on, chunking-on-conll-2003-german, pos-on-conll-2003, chunking-on-conll-2003",,See all 1951 tasks,Text Classification172 benchma,Text Classification172 benchma
DBpedia,DBpedia Dataset,"DBpedia (from ""DB"" for ""database"") is a project aiming to extract structured content from the information created in the Wikipedia project. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.",https://en.wikipedia.org/wiki/DBpedia,EditCC BY-SA 3.0,"Image, Text",English,,,,,,,"Text Retrieval, Text Classification, Zero-shot Text Search, Open Intent Discovery","text-classification-on-dbpedia, text-retrieval-on-dbpedia, zero-shot-text-search-on-dbpedia, open-intent-discovery-on-dbpedia",,See all 1951 tasks,Text Classification172 benchma,Text Classification172 benchma
IMDb_Movie_Reviews,IMDb Movie Reviews Dataset,"The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.",http://nlpprogress.com/english/sentiment_analysis.html,EditUnknown,"Graph, Image, Text, Time Series",English,,,,,,,"Node Clustering, Link Prediction, Opinion Mining, SQL Parsing, Language Modelling, Graph Similarity, Text Classification, Paraphrase Identification, Sentiment Analysis","paraphrase-identification-on-imdb, graph-similarity-on-imdb, text-classification-on-imdb-movie-reviews-1, opinion-mining-on-imdb-movie-reviews, sql-parsing-on-imdb, node-clustering-on-imdb, sentiment-analysis-on-imdb, sentiment-analysis-on-user-and-product, sentiment-analysis-on-imdb-movie-reviews-1, text-classification-on-imdb, link-prediction-on-imdb",,See all 1951 tasks,Text Classification172 benchma,Text Classification172 benchma
OpenWebText,OpenWebText Dataset,OpenWebText is an open-source recreation of the WebText corpus. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).,https://arxiv.org/abs/1907.11692,EditCustom,"Image, Text",English,,,,,,,"Text Generation, Language Modelling, Text Classification","text-generation-on-openwebtext, language-modelling-on-openwebtext",,See all 1951 tasks,Text Classification172 benchma,Text Classification172 benchma
RCV1,RCV1 Dataset,"The RCV1 dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.",https://arxiv.org/abs/1211.6085,EditCustom,"Image, Text",English,1996,,,,,,"Text Classification, Hierarchical Multi-label Classification, Multi-Label Text Classification, Cross-Lingual Document Classification","hierarchical-multi-label-classification-on-17, text-classification-on-rcv1, multi-label-text-classification-on-rcv1, multi-label-text-classification-on-rcv1-v2-1, cross-lingual-document-classification-on-12, cross-lingual-document-classification-on-13",,See all 1951 tasks,Text Classification172 benchma,Text Classification172 benchma
FEVER,FEVER Dataset,"FEVER is a publicly available dataset for fact extraction and verification against textual sources.

It consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGHINFO. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim.

The claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering. The verification of each claim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was
extracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.",https://arxiv.org/pdf/1803.05355v3.pdf,EditCustom,Text,English,,,,,,,"Fact Verification, Text Retrieval, Zero-shot Text Search, Question Answering","fact-verification-on-fever, question-answering-on-fever, zero-shot-text-search-on-fever, text-retrieval-on-fever",,See all 1951 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
MS_MARCO,MS MARCO Dataset,"The MS MARCO (Microsoft MAchine Reading Comprehension) is a collection of datasets focused on deep learning in search.
The first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. Over time the collection was extended with a 1,000,000 question dataset, a natural language generation dataset, a passage ranking dataset, keyphrase extraction dataset, crawling dataset, and a conversational search.",https://microsoft.github.io/msmarco/,"EditCustom (research-only, non-commercial)",Text,English,,,,,,,"Passage Ranking, TREC 2019 Passage Ranking, Passage Retrieval, Passage Re-Ranking, Information Retrieval, Reading Comprehension, Text Retrieval, Question Answering","passage-ranking-on-ms-marco, passage-retrieval-on-ms-marco-1, information-retrieval-on-msmarco, question-answering-on-ms-marco, information-retrieval-on-ms-marco, text-retrieval-on-ms-marco, passage-re-ranking-on-ms-marco, trec-2019-passage-ranking-on-msmarco",,See all 1951 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
MTEB,MTEB Dataset,"MTEB is a benchmark that spans 8 embedding tasks covering a total of 56 datasets and 112 languages. The 8 task types are Bitext mining, Classification, Clustering, Pair Classification, Reranking, Retrieval, Semantic Textual Similarity and Summarisation. The 56 datasets contain varying text lengths and they are grouped into three categories: Sentence to sentence, Paragraph to paragraph, and Sentence to paragraph.

Check the latest leaderboards at HuggingFace.",https://production-media.paperswithcode.com/datasets/5c793a66-bcc1-4aae-afed-061d31ee66fc.png,EditApache-2.0 license,"Image, Text",English,,,,,,,"Text Summarization, STS, Text Reranking, Semantic Textual Similarity, Text Pair Classification, Text Clustering, Information Retrieval, Text Classification, Text Retrieval","semantic-textual-similarity-on-mteb, text-reranking-on-mteb, information-retrieval-on-mteb, text-classification-on-mteb, text-clustering-on-mteb, text-pair-classification-on-mteb, text-summarization-on-mteb, text-retrieval-on-mteb",,See all 1951 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
Reuters-21578,Reuters-21578 Dataset,"The Reuters-21578 dataset is a collection of documents with news articles. The original corpus has 10,369 documents and a vocabulary of 29,930 words.",https://arxiv.org/abs/1604.00783,"EditCustom (research-only, attribution)","Image, Text",English,,,,369 documents,,,"Unsupervised Anomaly Detection, Multi-Modal Document Classification, Text Retrieval, Multi-Label Text Classification, Supervised Text Retrieval, Document Classification","text-retrieval-on-reuters-21578, multi-label-text-classification-on-reuters-1, multi-modal-document-classification-on-1, supervised-text-retrieval-on-reuters-21578, unsupervised-anomaly-detection-on-reuters-1, document-classification-on-reuters-21578",,See all 1951 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
RSICD,RSICD Dataset,"The Remote Sensing Image Captioning Dataset (RSICD) is a dataset for remote sensing image captioning task. It contains more than ten thousands remote sensing images which are collected from Google Earth, Baidu Map, MapABC and Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images is 10921, with five sentences descriptions per image.",https://github.com/201528014227051/RSICD_optimal,EditUnknown,"Image, Text",English,,,,,,,"Cross-Modal Retrieval, Image-to-Text Retrieval, Image Captioning, Scene Classification, Text Retrieval","text-retrieval-on-rsicd, cross-modal-retrieval-on-rsicd, image-to-text-retrieval-on-rsicd",,See all 1951 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
SciFact,SciFact Dataset,"SciFact is a dataset of 1.4K expert-written claims, paired with evidence-containing abstracts annotated with veracity labels and rationales.",https://huggingface.co/datasets/allenai/scifact_entailment,EditCC BY-NC 2.0,Text,English,,,,,,,"Text Retrieval, Zero-shot Text Search","text-retrieval-on-scifact, zero-shot-text-search-on-scifact",,See all 1951 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
TREC-COVID,TREC-COVID Dataset,"TREC-COVID is a community evaluation designed to build a test collection that captures the information needs of biomedical researchers using the scientific literature during a pandemic. One of the key characteristics of pandemic search is the accelerated rate of change: the topics of interest evolve as the pandemic progresses and the scientific literature in the area explodes. The COVID-19 pandemic provides an opportunity to capture this progression as it happens. TREC-COVID, in creating a test collection around COVID-19 literature, is building infrastructure to support new research and technologies in pandemic search.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Text Retrieval, Zero-shot Text Search, Information Retrieval","text-retrieval-on-trec-covid, zero-shot-text-search-on-trec-covid",,See all 1951 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
StyleGallery,StyleGallery Dataset,"We construct a style-balanced dataset, called StyleGallery, covering several open source datasets. Specifically, StyleGallery includes JourneyDB, a dataset comprising a broad spectrum of diverse styles derived from MidJourney, and WIKIART, with extensive fine-grained painting styles, such as pointillism and ink drawing, and a subset of stylized images from LAION-Aesthetics.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text, Video",English,,,,,,,"Text Style Transfer, Image Generation, Video Style Transfer, Style Transfer",style-transfer-on-stylebench,,See all 1951 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
StylePTB,StylePTB Dataset,"StylePTB is a fine-grained text style transfer benchmark. It consists of paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as compositions of multiple transfers which allow modelling of fine-grained stylistic changes as building blocks for more complex, high-level transfers.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-04-14_at_10.05.07.png,EditUnknown,Text,English,,,,,,,Text Style Transfer,,,See all 1951 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
TextBox_2.0,TextBox 2.0 Dataset,"TextBox 2.0 is a comprehensive and unified library for text generation, focusing on the use of pre-trained language models (PLMs). The library covers 13 common text generation tasks and their corresponding 83 datasets and further incorporates 45 PLMs covering general, translation, Chinese, dialogue, controllable, distilled, prompting, and lightweight PLMs.",https://arxiv.org/pdf/2212.13005v1.pdf,EditMIT License,Text,English,,,,,,,"Text Summarization, Text Generation, Text Simplification, Question Generation, Paraphrase Generation, Text Style Transfer, Machine Translation, Question Answering",,,See all 1951 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
XFORMAL,XFORMAL Dataset,"XFORMAL is a multilingual formal style transfer benchmark of multiple formal reformulations of informal text in Brazilian Portuguese, French, and Italian.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-04-14_at_10.57.58.png,EditUnknown,Text,English,,,,,,,Text Style Transfer,,,See all 1951 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
Yelp,Yelp Dataset,"The Yelp Dataset is a valuable resource for academic research, teaching, and learning. It provides a rich collection of real-world data related to businesses, reviews, and user interactions. Here are the key details about the Yelp Dataset:
Reviews: A whopping 6,990,280 reviews from users.
Businesses: Information on 150,346 businesses.
Pictures: A collection of 200,100 pictures.
Metropolitan Areas: Data from 11 metropolitan areas.
Tips: Over 908,915 tips provided by 1,987,897 users.
Business Attributes: Details like hours, parking availability, and ambiance for more than 1.2 million businesses.
Aggregated Check-ins: Historical check-in data for each of the 131,930 businesses.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Graph, Image, Text, Time Series",English,,,,,,,"Sentiment Classification, Sequential Recommendation, Anomaly Detection, Sentiment Analysis, Collaborative Filtering, Unsupervised Opinion Summarization, Paraphrase Identification, Fraud Detection, Aspect Extraction, SQL Parsing, Text Style Transfer, Text Classification, Node Classification, Unsupervised Text Style Transfer, Link Prediction, Graph Mining, Cross-Domain Document Classification, Recommendation Systems, Multibehavior Recommendation, Document Classification","text-classification-on-yelp-2, sentiment-analysis-on-yelp-binary, fraud-detection-on-yelp-fraud, link-prediction-on-yelp, multibehavior-recommendation-on-yelp, sql-parsing-on-yelp, unsupervised-opinion-summarization-on-yelp, text-style-transfer-on-yelp-review-dataset, cross-domain-document-classification-on-yelp, sentiment-analysis-on-yelp-fine-grained, document-classification-on-yelp-14, unsupervised-text-style-transfer-on-yelp2018, paraphrase-identification-on-yelp, node-classification-on-yelpchi, text-style-transfer-on-yelp-review-dataset-1, recommendation-systems-on-yelp2018, aspect-extraction-on-yaso-yelp, unsupervised-text-style-transfer-on-yelp, recommendation-systems-on-yelp, sequential-recommendation-on-yelp, collaborative-filtering-on-yelp2018, text-classification-on-yelp-5",,See all 1951 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
Text_to_Speech370_papers_with_code_Dataset,Text to Speech370 papers with code Dataset,,https://paperswithcode.com/dataset/text-to-speech,,,,,,,,,,,,,See all 1951 tasks,Text to Speech370 papers with ,Text to Speech370 papers with 
Charlotte-ThermalFace,Charlotte-ThermalFace Dataset,"Charlotte-ThermalFace is a thermal face dataset. The data is fully annotated with the facial landmarks, ambient temperature, relative humidity, the air speed of the room, distance to the camera, and subject thermal sensation at the time of capturing each image.

There are approximately 10,000 infrared thermal images from 10 subjects in varying thermal conditions, at several distances from the camera, and at changing head positions. We have also controlled the air temperature to change from 20.5°C ( 69°F) to 26.5 °C( 80°F). Images are available in four different temperatures, 10 relative distances from the camera, starting at 1m ( 3.3 ft) to 6.6m( 21.6 ft), and 25 head positions.

• The first public facial thermal dataset annotated with the environmental properties including air temperature, relative humidity, airspeed, distance from the camera, and subjective thermal sensation of each person at the time.

• All the images are manually annotated with 72 or 43 facial landmarks.

• We are publishing the data in the original 16-bit radiometric TemperatureLinear format, which has the thermal value of each pixel.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,Thermal Image Segmentation,,,See all 1951 tasks,Thermal Image Segmentation7 be,Thermal Image Segmentation7 be
MFNet,MFNet Dataset,"The first RGB-Thermal urban scene image dataset with pixel-level annotation. We published this new RGB-Thermal semantic segmentation dataset in support of further development of autonomous vehicles in the future. This dataset contains 1569 images (820 taken at daytime and 749 taken at nighttime). Eight classes of obstacles commonly encountered during driving
(car, person, bike, curve, car stop, guardrail, color cone, and bump) are labeled in this dataset.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,1569 images,,,Thermal Image Segmentation,thermal-image-segmentation-on-mfn-dataset,,See all 1951 tasks,Thermal Image Segmentation7 be,Thermal Image Segmentation7 be
Multi-Spectral_Stereo_Dataset___RGB__NIR__thermal_,"Multi-Spectral Stereo Dataset  (RGB, NIR, thermal images, LiDAR, GPS/IMU) Dataset","Abstract: 
We introduce the multi-spectral stereo (MS2) outdoor dataset, including stereo RGB, stereo NIR, stereo thermal, stereo LiDAR data, and GPS/IMU information. Our dataset provides rectified and synchronized 184K data pairs taken from city, residential, road, campus, and suburban areas in the morning, daytime, and nighttime under clear-sky, cloudy, and rainy conditions. We designed the dataset to explore various computer vision algorithms from multi-spectral sensor data to achieve high-level performance, reliability, and robustness against challenging environments.

MS2 dataset provides:
* 1. (Synchronized) Stereo RGB images / Stereo NIR images / Stereo thermal images
* 2. (Synchronized) Stereo LiDAR scans / GPS/IMU navigation data
* 3. Projected depth map (in RGB, NIR, thermal image planes)
* 4. Odometry data (in RGB, NIR, thermal cameras, and LiDAR coordinates)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-NonCommercial-ShareAlike 3.0 License,"3D, Image, Time Series",,,,,,,,"Stereo Depth Estimation, Depth Completion, Visual Odometry, Depth Prediction, Depth Estimation, Thermal Image Segmentation",,,See all 1951 tasks,Thermal Image Segmentation7 be,Thermal Image Segmentation7 be
PST900,PST900 Dataset,PST900 is a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge.,https://arxiv.org/abs/1909.10980,EditUnknown,"3D, Image, Video",,,,,,,,"Thermal Image Segmentation, Stereo Matching, Semantic Segmentation, Motion Estimation",thermal-image-segmentation-on-pst900,,See all 1951 tasks,Thermal Image Segmentation7 be,Thermal Image Segmentation7 be
TiROD,TiROD Dataset,"Dataset to benchmark Continual Learning for Object Detection in a Tiny Robotics settings.


10 Continual learning tasks
5 environments (1 indoor and 4 outdoor)
2 illumination conditions
14 object categories

Dataset Website

Dataset video

Code

Paper",https://production-media.paperswithcode.com/datasets/3237395a-b681-434a-aeb6-1d812472f63e.png,EditCC BY 4.0,Image,,,,,,,,"Object Detection, 2D Object Detection, TiROD, Continual Learning",tirod-on-tirod,,See all 1951 tasks,TiROD1 benchmark1 papers with ,TiROD1 benchmark1 papers with 
AutoPET,AutoPET Dataset,"A whole-body FDG-PET/CT dataset with manually annotated tumor lesions (FDG-PET-CT-Lesions)
1,014 studies (900 patients)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Lesion Detection, Lesion Segmentation, Tumor Segmentation",,,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
BraTS_2014,BraTS 2014 Dataset,BRATS 2014 is a brain tumor segmentation dataset.,https://arxiv.org/abs/1908.06965,EditUnknown,Image,,2014,,,,,,"Brain Tumor Segmentation, Tumor Segmentation, Semantic Segmentation",brain-tumor-segmentation-on-brats-2014,,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
BraTS_2015,BraTS 2015 Dataset,"The BraTS 2015 dataset is a dataset for brain tumor image segmentation. It consists of 220 high grade gliomas (HGG) and 54 low grade gliomas (LGG) MRIs. The four MRI modalities are T1, T1c, T2, and T2FLAIR. Segmented “ground truth” is provide about four intra-tumoral classes, viz. edema, enhancing tumor, non-enhancing tumor, and necrosis.",https://arxiv.org/abs/1910.02717,EditCustom,Image,,2015,,,,,,"Brain Tumor Segmentation, Tumor Segmentation, Semantic Segmentation",brain-tumor-segmentation-on-brats-2015,,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
BraTS_2016,BraTS 2016 Dataset,"BRATS 2016 is a brain tumor segmentation dataset. It shares the same training set as BRATS 2015, which consists of 220 HHG and 54 LGG. Its testing dataset consists of 191 cases with unknown grades.",https://production-media.paperswithcode.com/datasets/BraTS_2016-0000003751-78639289.jpg,EditCustom,Image,,2016,,,,,,"Data Augmentation, Tumor Segmentation, Semantic Segmentation",,,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
CARE,CARE Dataset,https://drive.google.com/file/d/1X_JTfD8Ch-IxmG5VHtKk_xGZT336Fl1Q/view?usp=drive_link,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://drive.google.com/file/d/1X_JTfD8Ch-IxmG5VHtKk_xGZT336Fl1Q/view?usp=drive_link,Image,,,,,,,,Tumor Segmentation,,,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
CSAW-S,CSAW-S Dataset,CSAW-S is a dataset of mammography images which includes expert annotations of tumors and non-expert annotations of breast anatomy and artifacts in the image.,https://arxiv.org/pdf/2008.00807.pdf,EditUnknown,Image,,,,,,,,Tumor Segmentation,,,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
DigestPath,DigestPath Dataset,"Introduced by Da et al. in DigestPath: a Benchmark Dataset with Challenge Review for the Pathological Detection and Segmentation of Digestive-System

Grand-Challenge Page
1. Signet ring cell dataset
Signet ring cell carcinoma is a type of rare adenocarcinoma with poor prognosis. Early detection of such cells leads to huge improvement of patients' survival rate. However, there is no existing public dataset with annotations for studying the problem of signet ring cell detection.

This dataset has positive samples and negative samples. Training positive samples contain 77 images from 20 WSIs, with cell bounding boxes written in xml. Training negative samples contain 378 images from 79 WSIs.These negative WSIs have no signet ring, but could contain other kinds of tumor cells. Each signet ring cell is labeled by experienced pathologists with a rectangle bounding box tightly surrounding the cell. Each image is of size 2000X2000. The training images are from 2 organs, including gastric mucosa and intestine. Because of the difficulty of manual annotation, there exist some signet ring cells who are missed by pathologists. In other words, this dataset is  a noisy dataset with its positive images not fully annotated. 

All whole slide images were stained by hematoxylin and eosin and scanned at X40. 

2. Colonoscopy tissue segment dataset
Colonoscopy pathology examination can find cells of early-stage colon tumor from small tissue slices. Pathologists need to daily examine hundreds of tissue slices, which is a time consuming and exhausting work. Here we propose a challenge task on automatic colonoscopy tissue segmentation and screening, aiming at automatic lesion segmentation and classification of the whole tissue (benign vs. malignant).

This dataset has positive samples and negative samples. Training positive samples contain 250 images of tissue from 93 WSIs, with pixel-level annotation in jpg format, where 0 means background and 255 for foreground (malignant lesion). You could simply get binary mask by a threshold 128. Training negative samples contain 410 images of tissue from 231 WSI. This negative images have no annotation because they don't have any malignant lesion.

The average size of all images are of 5000x5000 pixels, some of them are extremely huge. We will also provide another 152 patients' 212 tissues as the testing set, in which 90 images from 65 patients contain lesion. All whole slide images were stained by hematoxylin and eosin and scanned at X20.

Sign the DATABASE USE AGREEMENT first and download the dataset at the homepage!

```

Da Q, Huang X, Li Z, et al. DigestPath: a Benchmark Dataset with Challenge Review for the 
Pathological Detection and Segmentation of Digestive-System[J]. 
Medical Image Analysis, 2022: 102485.

（https://doi.org/10.1016/j.media.2022.102485）

```",https://production-media.paperswithcode.com/datasets/e09f6df7-adee-4af9-a90d-214fc75247c3.png,Edithttps://digestpath2019.grand-challenge.org/Home/,Image,,2022,,,77 images,Training positive samples contain 77 images,,"Medical Image Segmentation, Lesion Segmentation, Tumor Segmentation, Semantic Segmentation, Cell Segmentation",tumor-segmentation-on-digestpath,,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
HECKTOR,HECKTOR Dataset,Head and Neck Tumor Segmentation,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Lesion Detection, Lesion Segmentation, Tumor Segmentation",,,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
LiTS17,LiTS17 Dataset,LiTS17 is a liver tumor segmentation benchmark. The data and segmentations are provided by various clinical sites around the world. The training data set contains 130 CT scans and the test data set 70 CT scans.,https://production-media.paperswithcode.com/datasets/LiTS17-0000003748-16d273a7.jpg,EditUnknown,Image,,,,,,,,"Medical Image Segmentation, Computed Tomography (CT), Tumor Segmentation, Liver Segmentation","liver-segmentation-on-lits2017, tumor-segmentation-on-lits17, medical-image-segmentation-on-lits2017",,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
The_ULS23_Challenge_Test_Set,The ULS23 Challenge Test Set Dataset,"The ULS23 test set contains 725 lesions from 284 patients of the Radboudumc and JBZ hospitals in the Netherlands. It is intended to be used to measure the performance of 3D universal lesion segmentation models for Computed Tomography (CT). To prepare the data, radiological reports from both participating institutions where searched using NLP tools identifying patients with measurable target lesions, indicating that these lesions were clinically relevant. A random sample of patients was selected, 56.3% of which were male and with diverse scanner manufacturers. The lesions were annotated in 3D by expert radiologists with over 10 years of experience in reading oncological scans. ULS23 is an open benchmark, and we invite ongoing submissions to advance the development of future ULS models.",https://production-media.paperswithcode.com/datasets/9c0a55cf-f03b-499a-932a-01eb8c412475.png,EditUnknown,Image,,,,,,,,"Tumor Segmentation, Segmentation",tumor-segmentation-on-the-uls23-challenge,,See all 1951 tasks,Tumor Segmentation4 benchmarks,Tumor Segmentation4 benchmarks
ArSen-20,ArSen-20 Dataset,"Sentiment detection remains a pivotal task in natural language processing, yet its development in Arabic lags due to a scarcity of training materials compared to English. Addressing this gap, we present ArSen-20, a benchmark dataset tailored to propel Arabic sentiment detection forward. ArSen-20 comprises 20,000 professionally labeled tweets sourced from Twitter, focusing on the theme of COVID-19 and spanning the period from 2020 to 2023. Beyond tweet content, the dataset incorporates metadata associated with the user, enriching the contextual understanding. ArSen-20 offers a comprehensive resource to foster advancements in Arabic sentiment analysis and facilitate research in this critical domain.

The ArSen-20 dataset statistics:

| Statistics   |   Num  | 
|:-------------:|:-----:|
| Training set size | 16000 |
| Validation set size| 2000 |
| Testing set size | 2000 |
| Neutral | 17262 |
| Positive | 878 |
| Negative | 1860 |

Features
The dataset has the following features:

| Field   |  Type  |  Description  |
|:-----------:| :--------: |:----------------: |
| tweet id     | string     | The unique identifier of the requested Tweet.     |
| label   | string     | Sentiment Classification of this tweet.     |
| author id   | string    |The unique identifier of this user.     |
| created_at  | data     | Creation time of the Tweet.    |
| lang  | string     | Language of the Tweet, if detected by Twitter.    |
| like_count  | int     |The number of likes on this tweet.|
|quote_count  | int    | The number of times this tweet has been quoted.    |
| reply_count   | int     | The number of replies to this tweet.    |
| retweet_count| int    | The number of retweets to this tweet.    |
| tweet   | string     | The actual UTF-8 text of the Tweet.    |
|user_verified  | boolean     | Indicates if this user is a verified Twitter User.     |
|followers_count  | int     |The number of followers of the author.     |
| following_count  | int     | The number of following of the author.    |
| tweet_count  | int     | Total number of tweets by the author.    |
| listed_count | int     |The number of public lists that this user is a member of.    |
|name | string     | The name of the user.    |
| username   | string     | The Twitter screen name, handle, or alias.    |
| user_created_at| data     | The UTC datetime that the user account was created.     |
| description  | string     | The text of this user’s profile description (bio).     |

DownLoad
You can download the dataset from here.



ArSen-20_publish.csv - Contains all features.



ArSen-20_id_only.csv - Contains only tweets and their author's id.



Citation
If you use this dataset in your research, please cite the following papers:
bibtex
@inproceedings{fang2024arsen,
title={ArSen-20: A New Benchmark for Arabic Sentiment Detection},
author={Yang Fang and Cheng Xu},
booktitle={5th Workshop on African Natural Language Processing},
year={2024},
url={https://openreview.net/forum?id=GgsRUF5kJt}
}

bibtex
@inproceedings{fang2024advancing,
    title = ""Advancing {A}rabic Sentiment Analysis: {A}r{S}en Benchmark and the Improved Fuzzy Deep Hybrid Network"",
    author = ""Fang, Yang  and
      Xu, Cheng  and
      Guan, Shuhao  and
      Yan, Nan  and
      Mei, Yuke"",
    editor = ""Barak, Libby  and
      Alikhani, Malihe"",
    booktitle = ""Proceedings of the 28th Conference on Computational Natural Language Learning"",
    month = nov,
    year = ""2024"",
    address = ""Miami, FL, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.conll-1.39"",
    pages = ""507--516"",
}

contact
If you have any questions or comments about the dataset, please contact Yang Fang (20211209024@chnu.edu.cn).

Potential cooperation in related fields is also welcome. :)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache-2.0 license,"Image, Text",English,2020,,,,,,"Arabic Sentiment Analysis, Twitter Sentiment Analysis, Sentiment Classification, Sentiment Analysis",,,See all 1951 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
Crypto_related_tweets_from_10.10.2020_to_3.3.2021,Crypto related tweets from 10.10.2020 to 3.3.2021 Dataset,The dataset contains 30 million cryptocurrency-related tweets from 10.10.2020 to 3.3.2021. See https://github.com/meakbiyik/ask-who-not-what for more details.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY 4.0,"Image, Text",English,2020,,,,,,"Twitter Bot Detection, Twitter Event Detection, Twitter Sentiment Analysis",,,See all 1951 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
RETWEET,RETWEET Dataset,"RETWEET is a dataset of tweets and overall predominant sentiment of their replies.

SUMMARY
WHAT: Message-level Polarity Classification.

GOAL: To predict the predominant sentiment among (potential) first-order replies to a given tweet.

IDEA: Mitigate the problem of lacking labeled training data wi treating the unsupervised nature of the problem as a supervised learning case.

APPROACH:

Train a tweet classifier. 
Automatically label the replies using the classifier trained in the first part.
Choose a final label representing the general predominant sentiment of the replies of every tweet.

DATA COLLECTION
To download all of the replies to a tweet, the Search API should be used. However, the Search API is limited to 75000 requests per hour, which causes the mining and downloading process to be slow.
Furthermore, using the Twitter API, there is no possibility of downloading absolute random data. Therefore, we try to make the procedure as random as possible by utilizing two different strategies for data downloading and using them in an intermixed manner.



Our first strategy is based on a sample of English tweets obtained by filtering the Twitter stream via a list of cultural keywords. This list consists of 147 words that are deemed to play a ""pivotal role in discussions of culture and society"", covering diverse words such as aesthetics, environment, feminism, power, tourism, or youth. We extracted all tweets in 2019 that have a minimum of 20 first-order replies in the dataset. The data come with an obvious caveat: Both the source tweet as well as all the replies must contain at least one word from the list of keywords. Therewith, it is highly unlikely that the list of replies for any given source is exhaustive, i.e. there might be many more first-order replies to the source tweet that are not in the dataset.



As our second approach, we use the GetOldTweets3 library to download all the replies corresponding to every tweet. We define few restrictions to add randomization to the process. Firstly, every tweet and also every reply should contain at least 20 strings. This is due to the fact that our automatic tweet classifier, explsined in the paper, is optimized based on the message-level classification paradigm. Therefore, it operates optimal when the input contains at least a sufficient number of words. The second constraint is that every tweet should contain at least 20 first-order replies. In order to increase randomness, in this strategy, instead of referencing to a list of keywords, we manually choose some keywords, which are most likely to include long discussions, such as Coronavirus and football or the ones, which are most likely to include strong opinions such as birthday, war, or racism in order to account for the easy-to-guess examples. 



MANUAL ANNOTATIONS FOR THE RETWEET (TEST GOLD DATASET)
5,015 tweets with their corresponding replies, collected as a combination of the two different collection strategies, were given to three different students. Each of them had to read all the replies corresponding to every tweet, without observing the original tweet in order to avoid having a prior knowledge, and decide on ONE final sentiment for the replies. The assigned sentiment can only be one of the positive, negative, or neutral labels.

Considering the fact that this is a really challenging task for the machine, to prevent human mistakes, we correlated the results of the three annotators and only chose the tweets, in which all of the annotators had the same opinion on the labels, as the final gold standard test data. Therefore, we finally, ended up with a test set consisting of 1,519 human labeled tweets, with the labels being the sentiment of the replies of a tweet and not the tweet itself. 

DATASET CONTENTS
1. Training raw dataset: 34,953 unique tweets in total and individual automatic labels for all of their corresponding replies (1,519,504 total replies). Including,


./RETWEET_data/train_reply_labels_set1.txt
./RETWEET_data/train_reply_labels_set2.txt

2. Training autamtically-labeled dataset: 34,953 unique tweets and ONE final automatic label (chosen based on the algorithm 1 of our paper) for every tweet. Including,


./RETWEET_data/train_final_label.txt

3. Gold standard test dataset (RETWEET): 1,519 unique tweets with their manual labels for replies. ONE final label, which states the predominant overall polarity of all its replies, is assigned to every tweet. Including,


./RETWEET_data/test_gold.txt

NOTES


Please note that by downloading the Twitter data you agree to abide by the Twitter terms of service, and in particular you agree not to redistribute the data and to delete tweets that are marked deleted in the future.



The ""neutral"" label in the annotations stands for objective or neutral.



The distribution consists of a set of Twitter unique tweet IDs with annotations (overall polarity of replies). As for data privacy, the texts of the tweets and replies are not distributed. But as all the utilized resources in this dataset are taken from public tweets, having the tweet unique IDs, you can download the tweet and its replies.
You can use the Semeval Twitter data downloading script to obtain the corresponding tweets:  

https://github.com/seirasto/twitter_download/



The dataset URL:

https://kaggle.com/soroosharasteh/retweet/ 



LICENSE
The accompanying dataset is released under a Creative Commons Attribution 4.0 International License.

SOURCE CODE
The official source code of the paper: https://github.com/starasteh/retweet

In case you use this dataset, please cite the original paper:
S. Tayebi Arasteh, M. Monajem, V. Christlein, P. Heinrich, A. Nicolaou, H.N. Boldaji, M. Lotfinia,  S. Evert. ""How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies"". Proceedings of the 2021 IEEE 15th International Conference on Semantic Computing (ICSC), Laguna Hills, CA, USA, January 2021.

BibTex
@inproceedings{RETWEET,
  title = ""How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies"",
  author = ""Tayebi Arasteh, Soroosh and Monajem, Mehrpad and Christlein, Vincent and
  Heinrich, Philipp and Nicolaou, Anguelos and Naderi Boldaji, Hamidreza and Lotfinia, Mahshad and Evert, Stefan"",
  booktitle = ""Proceedings of the 2021 IEEE 15th International Conference on Semantic Computing (ICSC)"",
  address = ""Laguna Hills, CA, USA"",
  pages = ""370-373"",
  doi = ""10.1109/ICSC50631.2021.00068"",
  url = ""https://ieeexplore.ieee.org/document/9364527/"",
  month = ""01"",       
  year = ""2021""
  }


Dataset DOI: 10.34740/kaggle/ds/736988
Paper: https://ieeexplore.ieee.org/document/9364527
Paper DOI: 10.1109/ICSC50631.2021.00068

CONTACT
E-mail: soroosh.arasteh@fau.de

DATA FORMAT FOR ALL THE FILES
label TAB id

where, ""label"" can be positive, neutral or negative, corresponding to the overall message-level polarity of the replies of the tweet and ""id"" corresponds to the Twitter unique ID for the tweets.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY 4.0,"Image, Text",English,2019,,,,"traint is that every tweet should contain at least 20 first-order replies. In order to increase randomness, in this strategy, instead of referencing to a list of keywords, we manually choose some keywords, which are most likely to include long discussions, such as Coronavirus and football or the ones, which are most likely to include strong opinions such as birthday, war, or racism in order to account for the easy-to-guess examples",,"Multi-Domain Sentiment Classification, Point Processes, Tweet-Reply Sentiment Analysis, Twitter Sentiment Analysis, Sentiment Analysis","point-processes-on-retweet, tweet-reply-sentiment-analysis-on-retweet",,See all 1951 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
SentimentArcs__Sentiment_Reference_Corpus_for_Nove,SentimentArcs: Sentiment Reference Corpus for Novels Dataset,"SentimentArcs’ reference corpus for novels consists of 25 narratives selected to create a diverse set of well recognized novels that can serve as a benchmark for future studies. The composition of the corpora was limited by the effect of copyright laws as well as historical imbalances. Most works were obtained from US and Australian Gutenberg Projects. The corpora is expected to grow in size and diversity over time.  

Several dimensions of diversity were considered for inclusion including popularity, period, genre, topic, style and author diversity. The first version of our corpus includes only English, although Proust and Homer are included in translation. SentimentArcs has processed a larger set of novels, including some in foreign languages. The initial reference corpus is in English since performance across all ensemble models was uneven in less resourced languages

In sum, the corpora includes (1) the two most popular novels on Gutenberg.org (Project Gutenberg, 2021b), (2) eight of the fifteen most assigned novels at top US universities (EAB, 2021), and (3) three works that have sold over 20 million copies (Books, 2021). There are eight works by women, two by African-Americans and five works by two LGBTQ authors. Britain leads with 15 authors followed by 6 Americans and one each from France, Russia, North Africa and Ancient Greece.",https://production-media.paperswithcode.com/datasets/14bd0ec6-6779-49ee-a7cf-c9989d3410b0.jpg,EditMIT,Text,English,2021,,,,,,"Twitter Sentiment Analysis, Sentiment Analysis",,,See all 1951 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
_chinahate,#chinahate Dataset,"#chinahate dataset contains a total of 2,172,333 tweets hashtagged #china posted during the time it was collected. It is designed for the task of hate speech detection.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Image, Text",English,,,,,,,"Twitter Sentiment Analysis, Hate Speech Detection",,,See all 1951 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
ACE_2004,ACE 2004 Dataset,"ACE 2004 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2004 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities and relations and was created by Linguistic Data Consortium with support from the ACE Program, with additional assistance from the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) Program.
The objective of the ACE program is to develop automatic content extraction technology to support automatic processing of human language in text form. In September 2004, sites were evaluated on system performance in six areas: Entity Detection and Recognition (EDR), Entity Mention Detection (EMD), EDR Co-reference, Relation Detection and Recognition (RDR), Relation Mention Detection (RMD), and RDR given reference entities. All tasks were evaluated in three languages: English, Chinese and Arabic.",https://catalog.ldc.upenn.edu/LDC2005T09,EditCustom,"Graph, Image, Text",English,2004,,,,,,"Nested Mention Recognition, Nested Named Entity Recognition, Named Entity Recognition (NER), Relation Extraction, Entity Disambiguation, UIE","relation-extraction-on-ace-2004, entity-disambiguation-on-ace2004, nested-named-entity-recognition-on-ace-2004, uie-on-ace-2004, nested-mention-recognition-on-ace-2004, named-entity-recognition-on-ace-2004",,See all 1951 tasks,UIE28 benchmarks34 papers with,UIE28 benchmarks34 papers with
BC5CDR,BC5CDR Dataset,"BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions.",https://www.ncbi.nlm.nih.gov/research/bionlp/Data/,EditCustom,"Image, Text",English,,,,,,,"Weakly-Supervised Named Entity Recognition, UIE, Named Entity Recognition (NER)","weakly-supervised-named-entity-recognition-on-5, weakly-supervised-named-entity-recognition-on-8, weakly-supervised-named-entity-recognition-on-6, named-entity-recognition-ner-on-bc5cdr, named-entity-recognition-on-bc5cdr-disease, named-entity-recognition-on-bc5cdr-chemical, uie-on-bc5cdr",,See all 1951 tasks,UIE28 benchmarks34 papers with,UIE28 benchmarks34 papers with
GENIA,GENIA Dataset,"The GENIA corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology.

The corpus contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms “human”, “blood cells”, and “transcription factors”. The corpus has been annotated with various levels of linguistic and semantic information.

The primary categories of annotation in the GENIA corpus and the corresponding subcorpora are:


Part-of-Speech annotation
Constituency (phrase structure) syntactic annotation
Term annotation
Event annotation
Relation annotation
Coreference annotation",http://www.geniaproject.org/genia-corpus,EditUnknown,"Image, Text",English,,,,,,,"Nested Named Entity Recognition, Named Entity Recognition (NER), UIE, Dependency Parsing, Event Extraction","event-extraction-on-genia, named-entity-recognition-on-genia, event-extraction-on-genia-2013, dependency-parsing-on-genia-uas, dependency-parsing-on-genia-las, nested-named-entity-recognition-on-genia, uie-on-genia",,See all 1951 tasks,UIE28 benchmarks34 papers with,UIE28 benchmarks34 papers with
NCBI_Disease,NCBI Disease Dataset,"The NCBI Disease corpus consists of 793 PubMed abstracts, which are separated into training (593), development (100) and test (100) subsets. The NCBI Disease corpus is annotated with disease mentions, using concept identifiers from either MeSH or OMIM.",https://arxiv.org/abs/1812.06081,EditUnknown,"Image, Text",English,,,,,,,"UIE, Named Entity Recognition, Named Entity Recognition (NER)","named-entity-recognition-ner-on-ncbi-disease, uie-on-ncbi-disease, named-entity-recognition-on-ncbi-disease, named-entity-recognition-on-ncbi-disease-1",,See all 1951 tasks,UIE28 benchmarks34 papers with,UIE28 benchmarks34 papers with
New_York_Times_Annotated_Corpus,New York Times Annotated Corpus Dataset,"The New York Times Annotated Corpus contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. The corpus includes:


Over 1.8 million articles (excluding wire services articles that appeared during the covered period).
Over 650,000 article summaries written by library scientists.
Over 1,500,000 articles manually tagged by library scientists with tags drawn from a normalized indexing vocabulary of people, organizations, locations and topic descriptors.
Over 275,000 algorithmically-tagged articles that have been hand verified by the online production staff at nytimes.com.
As part of the New York Times' indexing procedures, most articles are manually summarized and tagged by a staff of library scientists. This collection contains over 650,000 article-summary pairs which may prove to be useful in the development and evaluation of algorithms for automated document summarization. Also, over 1.5 million documents have at least one tag. Articles are tagged for persons, places, organizations, titles and topics using a controlled vocabulary that is applied consistently across articles. For instance if one article mentions ""Bill Clinton"" and another refers to ""President William Jefferson Clinton"", both articles will be tagged with ""CLINTON, BILL"".",https://catalog.ldc.upenn.edu/LDC2008T19,"EditCustom (research-only, non-commercial)","Graph, Image, Text",English,1987,,,,,,"Text Summarization, Hierarchical Multi-label Classification, Topic Models, Document Dating, Relationship Extraction (Distant Supervised), Relation Extraction, Joint Entity and Relation Extraction, UIE, Open Information Extraction, Document Summarization, Abstractive Text Summarization","uie-on-nyt, hierarchical-multi-label-classification-on-18, relationship-extraction-distant-supervised-on-2, document-dating-on-nyt, relation-extraction-on-nyt, joint-entity-and-relation-extraction-on-nyt, open-information-extraction-on-nyt, relationship-extraction-distant-supervised-on, topic-models-on-nyt",,See all 1951 tasks,UIE28 benchmarks34 papers with,UIE28 benchmarks34 papers with
SciERC,SciERC Dataset,"SciERC dataset is a collection of 500 scientific abstract annotated with scientific entities, their relations, and coreference clusters. The abstracts are taken from 12 AI conference/workshop proceedings in four AI communities, from the Semantic Scholar Corpus. SciERC extends previous datasets in scientific articles SemEval 2017 Task 10 and SemEval 2018 Task 7 by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links.",http://nlp.cs.washington.edu/sciIE/,EditUnknown,"Graph, Image, Text",English,2017,,,,,,"Named Entity Recognition (NER), Continual Pretraining, Relation Extraction, Joint Entity and Relation Extraction, Few-Shot Relation Classification, UIE, Named Entity Recognition","named-entity-recognition-ner-on-scierc, named-entity-recognition-on-scierc, uie-on-scierc, few-shot-relation-classification-on-scierc, continual-pretraining-on-scierc, relation-extraction-on-scierc, joint-entity-and-relation-extraction-on, relation-extraction-on-scierc-sent",,See all 1951 tasks,UIE28 benchmarks34 papers with,UIE28 benchmarks34 papers with
WikiANN,WikiANN Dataset,"WikiANN, also known as PAN-X, is a multilingual named entity recognition dataset. It consists of Wikipedia articles that have been annotated with LOC (location), PER (person), and ORG (organization) tags in the IOB2 format¹². This dataset serves as a valuable resource for training and evaluating named entity recognition models across various languages.

For instance, it includes information about notable individuals, places, and organizations mentioned in Wikipedia articles. Researchers and practitioners can use WikiANN to develop and improve natural language processing systems that identify and classify named entities in text.

(1) wikiann · Datasets at Hugging Face. https://huggingface.co/datasets/wikiann.
(2) wikiann | TensorFlow Datasets. https://tensorflow.google.cn/datasets/catalog/wikiann.
(3) wikiann · Datasets at Hugging Face. https://huggingface.co/datasets/wikiann/viewer/en.
(4) WikiAnn Dataset | Papers With Code. https://paperswithcode.com/dataset/wikiann-1.",https://huggingface.co/datasets/unimelb-nlp/wikiann,EditUnknown,"Image, Text",English,,,,,,,"Named Entity Recognition (NER), Token Classification, Cross-Lingual Transfer, Word Embeddings, Cross-Lingual NER, UIE","token-classification-on-wikiann, uie-on-wikiann, cross-lingual-ner-on-wikiann-ner",,See all 1951 tasks,UIE28 benchmarks34 papers with,UIE28 benchmarks34 papers with
WNUT_2017,WNUT 2017 Dataset,"This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarisation), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet “so.. kktny in 30 mins?” - even human experts find entity kktny hard to detect and resolve. This task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text.

The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities.",https://huggingface.co/datasets/leondz/wnut_17,EditCC-BY 4.0,"Image, Text",English,,,,,,,"UIE, Named Entity Recognition (NER), Few-shot NER","named-entity-recognition-on-wnut-2017, uie-on-wnut-2017",,See all 1951 tasks,UIE28 benchmarks34 papers with,UIE28 benchmarks34 papers with
LSUI,LSUI Dataset,"We released a large-scale underwater image (LSUI) dataset including 5004 image pairs, which involve richer underwater scenes (lighting conditions, water types and target categories) and better visual quality reference images than the existing ones.",https://production-media.paperswithcode.com/datasets/06d78f7b-709e-4c6e-8182-cb703fd221aa.jpg,EditUnknown,Image,,,,,,,,Underwater Image Restoration,underwater-image-restoration-on-lsui,,See all 1951 tasks,Underwater Image Restoration1 ,Underwater Image Restoration1 
MVK,MVK Dataset,"The dataset contains single-shot videos taken from moving cameras in underwater environments. The first shard of a new Marine Video Kit dataset is presented to serve for video retrieval and other computer vision challenges. In addition to basic meta-data statistics, we present several insights based on low-level features as well as semantic annotations of selected keyframes. 
1379 videos with a length from 2 s to 4.95 min, with the mean and median duration of each video is 29.9 s, and 25.4 s, respectively.
We capture data from 11 diﬀerent regions and countries during the time from 2011 to 2022.",https://production-media.paperswithcode.com/datasets/a38cd39b-38d9-4cd2-b9b8-e10ee4d7d39f.gif,EditUnknown,"Image, Text, Video",English,2011,,,,,,"Retrieval, Text to Video Retrieval, Multimodal Deep Learning, Underwater Image Restoration, Video Retrieval",retrieval-on-mvk,,See all 1951 tasks,Underwater Image Restoration1 ,Underwater Image Restoration1 
Underwater_Object_Detection_Dataset,Underwater Object Detection Dataset Dataset,"Description:

<a href=""https://gts.ai/dataset-download/underwater-object-detection-dataset/"" target=""_blank"">👉 Download the dataset here</a>

This dataset is designed for advanced underwater object detection and classification. It provides a comprehensive collection of images featuring underwater objects, each precisely annotated with bounding boxes. The dataset aims to support a wide range of research applications, from environmental monitoring to underwater robotics.

Download Dataset

Classes:

Fish (individual and grouped)

Crab

Human Diver

Trash (marine pollution)

Jellyfish

Coral Reef

Sea Turtle

Starfish

Dataset Structure:

Training Set (70%): A robust sample for building detection models.

Validation Set (10%): Used to fine-tune model performance.

Test Set (20%): A carefully selected set of images for evaluating model accuracy.

Pre-processing Techniques:

Auto-Orientation: Ensures all images are correctly aligned.

Resizing: Images are scaled to 640×640 pixels for uniformity.

Brightness Normalization: Corrects for underwater lighting conditions.

Contrast Stretching: Enhances visibility for objects in murky or low-contrast scenes.

New Annotation Techniques:

Polygonal Segmentation: Introduces more precise segmentation for irregular shapes such as coral reefs.

3D Depth Mapping: For enhanced understanding of object placement in underwater space.

Dataset Use Cases:

Marine Ecology: Assessing species diversity and tracking the impact of environmental changes.

Pollution Analysis: Detecting and classifying marine trash, aiding in cleanup efforts.

Underwater Robotics: Training AUVs to recognize and navigate around complex underwater structures like coral reefs or large groups of fish.

Conclusion:

The expanded Underwater Object Detection provides a rich resource for researchers, environmentalists, and engineers working on underwater object detection and classification. Its enhanced classes, precise annotations, and preprocessing techniques make it a valuable asset for developing robust models in marine exploration and conservation.

This dataset is sourced from Kaggle.",https://production-media.paperswithcode.com/datasets/ad31f355-7767-45ea-bffa-5ee6b34bac2d.png,EditUnknown,Image,,,,,,Test Set (20%): A carefully selected set of images,,"Underwater Image Restoration, object-detection",,,See all 1951 tasks,Underwater Image Restoration1 ,Underwater Image Restoration1 
UNET_Quantization1_papers_with_code_Dataset,UNET Quantization1 papers with code Dataset,,https://paperswithcode.com/dataset/unet-quantization,,,,,,,,,,,,,See all 1951 tasks,UNET Quantization1 papers with,UNET Quantization1 papers with
universal_meta-learning1_papers_with_code_Dataset,universal meta-learning1 papers with code Dataset,,https://paperswithcode.com/dataset/universal-meta-learning,,,,,,,,,,,,,See all 1951 tasks,universal meta-learning1 paper,universal meta-learning1 paper
KolektorSDD2,KolektorSDD2 Dataset,"KolektorSDD2 is a surface-defect detection dataset with over 3000 images containing several types of defects, obtained while addressing a real-world industrial problem.

The dataset consists of:


356 images with visible defects
2979 images without any defect
image sizes of approximately 230 x 630 pixels
train set with 246 positive and 2085 negative images
test set with 110 positive and 894 negative images
several different types of defects (scratches, minor spots, surface imperfections, etc.)",https://production-media.paperswithcode.com/datasets/Kolektor-SDD2.png,EditUnknown,Image,,2085,,,3000 images,train set with 246 positive and 2085 negative images,,"Supervised Defect Detection, Defect Detection, Unsupervised Anomaly Detection, Self-Supervised Anomaly Detection, Weakly Supervised Defect Detection","defect-detection-on-kolektorsdd2, unsupervised-anomaly-detection-on, self-supervised-anomaly-detection-on, supervised-defect-detection-on-kolektorsdd2, weakly-supervised-defect-detection-on-3",,See all 1951 tasks,Unsupervised Anomaly Detection,Unsupervised Anomaly Detection
MIMII,MIMII Dataset,"Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection (MIMII) is a sound dataset
of industrial machine sounds.",/paper/mimii-dataset-sound-dataset-for,EditCC BY-SA 4.0,Image,,,,,,,,"Anomaly Detection, Unsupervised Anomaly Detection, Open Set Learning",,,See all 1951 tasks,Unsupervised Anomaly Detection,Unsupervised Anomaly Detection
MVTec_LOCO_AD,MVTec LOCO AD Dataset,"MVTec Logical Constraints Anomaly Detection (MVTec LOCO AD) dataset is intended for the evaluation of unsupervised anomaly localization algorithms. The dataset includes both structural and logical anomalies. It contains 3644 images from five different categories inspired by real-world industrial inspection scenarios. Structural anomalies appear as scratches, dents, or contaminations in the manufactured products. Logical anomalies violate underlying constraints, e.g., a permissible object being present in an invalid location or a required object not being present at all. The dataset also includes pixel-precise ground truth data for each anomalous region.",https://www.mvtec.com/company/research/datasets/mvtec-loco,EditCC BY-NC-SA 4.0 license,Image,,,,,3644 images,traints Anomaly Detection (MVTec LOCO AD) dataset is intended for the evaluation of unsupervised anomaly localization algorithms. The dataset includes both structural and logical anomalies. It contains 3644 images,,"Anomaly Detection, Unsupervised Anomaly Detection, Semi-supervised Anomaly Detection",anomaly-detection-on-mvtec-loco-ad,,See all 1951 tasks,Unsupervised Anomaly Detection,Unsupervised Anomaly Detection
SMAP,SMAP Dataset,"Soil Moisture Active Passive (SMAP) dataset is a dataset of soil samples and telemetry information using the Mars rover by NASA. Originally published in https://arxiv.org/abs/1802.04431 and used for the unsupervised anomaly detection task in time series data. Later it was used in many popular anomaly detection methods and benchmarks that distribute it in their repositories e.g., https://github.com/OpsPAI/MTAD",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Time Series",,,,,,,,"Time Series Anomaly Detection, Unsupervised Anomaly Detection","unsupervised-anomaly-detection-on-smap, time-series-anomaly-detection-on-smap",,See all 1951 tasks,Unsupervised Anomaly Detection,Unsupervised Anomaly Detection
unsupervised_class-incremental_learning2_papers_wi,unsupervised class-incremental learning2 papers with code Dataset,,https://paperswithcode.com/dataset/unsupervised-class-incremental-learning,,,,,,,,,,,,,See all 1951 tasks,unsupervised class-incremental,unsupervised class-incremental
ImageNet-A,ImageNet-A Dataset,"The ImageNet-A dataset consists of real-world, unmodified, and naturally occurring examples that are misclassified by ResNet models.",https://arxiv.org/abs/2007.08558,EditUnknown,Image,English,,,,,,,"Zero-Shot Transfer Image Classification, Prompt Engineering, Domain Generalization, Adversarial Robustness, Unsupervised Domain Adaptation","zero-shot-transfer-image-classification-on-5, unsupervised-domain-adaptation-on-imagenet-a, prompt-engineering-on-imagenet-a, adversarial-robustness-on-imagenet-a, domain-generalization-on-imagenet-a",,See all 1951 tasks,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation
ImageNet-C,ImageNet-C Dataset,"ImageNet-C is an open source data set that consists of algorithmically generated corruptions (blur, noise) applied to the ImageNet test-set.",https://arxiv.org/abs/1911.05248,EditCC BY 4.0,,English,,,,,,,"Test-time Adaptation, Domain Generalization, Unsupervised Domain Adaptation, Adversarial Robustness","domain-generalization-on-imagenet-c, test-time-adaptation-on-imagenet-c, unsupervised-domain-adaptation-on-imagenet-c, adversarial-robustness-on-imagenet-c",,See all 1951 tasks,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation
ImageNet-R,ImageNet-R Dataset,"ImageNet-R(endition) contains art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes.

ImageNet-R has renditions of 200 ImageNet classes resulting in 30,000 images.",/paper/the-many-faces-of-robustness-a-critical,EditUnknown,Image,English,,,,000 images,,,"Zero-Shot Transfer Image Classification, Prompt Engineering, Zero-Shot Composed Image Retrieval (ZS-CIR), Domain Generalization, Zero-shot Image Retrieval, Unsupervised Domain Adaptation","domain-generalization-on-imagenet-r, zero-shot-transfer-image-classification-on-4, unsupervised-domain-adaptation-on-imagenet-r, zero-shot-image-retrieval-on-imagenet-r, prompt-engineering-on-imagenet-r, zero-shot-composed-image-retrieval-zs-cir-on-6",,See all 1951 tasks,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation
Market-1501,Market-1501 Dataset,"Market-1501 is a large-scale public benchmark dataset for person re-identification. It contains 1501 identities which are captured by six different cameras, and 32,668 pedestrian image bounding-boxes obtained using the Deformable Part Models pedestrian detector. Each person has 3.6 images on average at each viewpoint. The dataset is split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images are selected as probe set to find the correct match across 19,732 reference gallery images.",https://arxiv.org/abs/1907.02547,EditUnknown,"3D, Image",,,,,6 images,"split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images",,"Pose Transfer, Person Re-Identification, Generalizable Person Re-identification, Unsupervised Person Re-Identification, Unsupervised Domain Adaptation","unsupervised-domain-adaptation-on-market-to, unsupervised-domain-adaptation-on-market-to-1, unsupervised-domain-adaptation-on-duke-to, person-re-identification-on-market-1501, pose-transfer-on-market-1501, unsupervised-person-re-identification-on-1, person-re-identification-on-dukemtmc-reid-1, generalizable-person-re-identification-on-21, unsupervised-person-re-identification-on-4",,See all 1951 tasks,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation
Unsupervised_Few-Shot_Learning13_papers_with_code_,Unsupervised Few-Shot Learning13 papers with code Dataset,,https://paperswithcode.com/dataset/unsupervised-few-shot-learning,,,,,,,,,,,,,See all 1951 tasks,Unsupervised Few-Shot Learning,Unsupervised Few-Shot Learning
ClevrTex,ClevrTex Dataset,"ClevrTex is a new benchmark designed as the next challenge to compare, evaluate and analyze algorithms for unsupervised multi-object segmentation. ClevrTex features synthetic scenes with diverse shapes, textures and photo-mapped materials, created using physically based rendering techniques.",https://production-media.paperswithcode.com/datasets/3cbded20-1d12-4b36-8ebe-580bda298157.png,EditCC-BY,Image,,,,,,,,Unsupervised Object Segmentation,unsupervised-object-segmentation-on-clevrtex,,See all 1951 tasks,Unsupervised Object Segmentati,Unsupervised Object Segmentati
Multi-dSprites,Multi-dSprites Dataset,,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Image Generation, Unsupervised Object Segmentation",image-generation-on-multi-dsprites,,See all 1951 tasks,Unsupervised Object Segmentati,Unsupervised Object Segmentati
ObjectsRoom,ObjectsRoom Dataset,"The ObjectsRoom dataset is based on the MuJoCo environment used by the Generative Query Network [4] and is a multi-object extension of the 3d-shapes dataset. The training set contains 1M scenes with up to three objects. We also provide ~1K test examples for the following variants:

2.1 Empty room: scenes consist of the sky, walls, and floor only.

2.2 Six objects: exactly 6 objects are visible in each image.

2.3 Identical color: 4-6 objects are placed in the room and have an identical, randomly sampled color.

Datapoints consist of an image and fixed number of masks. The first four masks correspond to the sky, floor, and two halves of the wall respectively. The remaining masks correspond to the foreground objects.",https://github.com/deepmind/multi_object_datasets,EditUnknown,"Image, Text",English,,,,,training set contains 1M scenes with up to three objects. We also provide ~1K test examples,,"Image Generation, Unsupervised Object Segmentation","unsupervised-object-segmentation-on-1, image-generation-on-objectsroom",,See all 1951 tasks,Unsupervised Object Segmentati,Unsupervised Object Segmentati
ShapeStacks,ShapeStacks Dataset,"A simulation-based dataset featuring 20,000 stack configurations composed of a variety of elementary geometric primitives richly annotated regarding semantics and structural stability.",https://arxiv.org/pdf/1804.08018v2.pdf,EditUnknown,"Image, Text",English,,,,,,,"Scene Understanding, Image Generation, Unsupervised Object Segmentation, Object Discovery","unsupervised-object-segmentation-on, image-generation-on-shapestacks",,See all 1951 tasks,Unsupervised Object Segmentati,Unsupervised Object Segmentati
DukeMTMC-VideoReID,DukeMTMC-VideoReID Dataset,"The DukeMTMC-VideoReID (Duke Multi-Tracking Multi-Camera Video-based ReIDentification) dataset is a subset of the DukeMTMC for video-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian video datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 4832 tracklets of 1812 identities in total, and each tracklet has 168 frames on average.

NOTE: This dataset has been retracted.",https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf,EditUnknown,"Image, Video",,,,,,,,"Unsupervised Person Re-Identification, Person Re-Identification, Video-Based Person Re-Identification","unsupervised-person-re-identification-on-11, person-re-identification-on-dukemtmc",,See all 1951 tasks,Unsupervised Person Re-Identif,Unsupervised Person Re-Identif
iLIDS-VID,iLIDS-VID Dataset,"The iLIDS-VID dataset is a person re-identification dataset which involves 300 different pedestrians observed across two disjoint camera views in public open space. It comprises 600 image sequences of 300 distinct individuals, with one pair of image sequences from two camera views for each person. Each image sequence has variable length ranging from 23 to 192 image frames, with an average number of 73. The iLIDS-VID dataset is very challenging due to clothing similarities among people, lighting and viewpoint variations across camera views, cluttered background and random occlusions.",http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html,EditUnknown,"Image, Video",,,,,,,,"Unsupervised Person Re-Identification, Person Re-Identification, Video-Based Person Re-Identification","unsupervised-person-re-identification-on-10, person-re-identification-on-ilids-vid",,See all 1951 tasks,Unsupervised Person Re-Identif,Unsupervised Person Re-Identif
LTCC,LTCC Dataset,"LTCC contains 17,119 person images of 152 identities, and each identity is captured by at least two cameras.  The dataset can be divided into two subsets: one cloth-change set where 91 persons appear with 416 different sets of outfits in 14,783 images, and one cloth-consistent subset containing the remaining 61 identities with 2,336 images without outfit changes. On average, there are 5 different clothes for each cloth-changing person, with the number of outfit changes ranging from 2 to 14.",https://naiq.github.io/LTCC_Perosn_ReID.html,EditUnknown,Image,,,,,783 images,,,"Unsupervised Person Re-Identification, Person Re-Identification","unsupervised-person-re-identification-on-ltcc, person-re-identification-on-ltcc",,See all 1951 tasks,Unsupervised Person Re-Identif,Unsupervised Person Re-Identif
PRCC,PRCC Dataset,"This dataset consists of 33698 images from 221 identities. Each person in Cameras A and B is wearing the same clothes, but the images are captured in different rooms. For Camera C, the person wears different clothes, and the images are captured in a different day.",https://www.isee-ai.cn/~yangqize/clothing.html,EditUnknown,Image,,,,,33698 images,,,"Unsupervised Person Re-Identification, Person Re-Identification","person-re-identification-on-prcc, unsupervised-person-re-identification-on-prcc",,See all 1951 tasks,Unsupervised Person Re-Identif,Unsupervised Person Re-Identif
VC-Clothes,VC-Clothes Dataset,"Person re-identification (Reid) is now an active research topic for AI-based video surveillance applications such as specific person search, but the practical issue that the target person(s) may change clothes (clothes inconsistency problem) has been overlooked for long. For the first time, this paper systematically studies this problem. We first overcome the difficulty of lack of suitable dataset, by collecting a small yet representative real dataset for testing whilst building a large realistic synthetic dataset for training and deeper studies. Facilitated by our new datasets, we are able to conduct various interesting new experiments for studying the influence of clothes inconsistency. We find that changing clothes makes Reid a much harder problem in the sense of bringing difficulties to learning effective representations and also challenges the generalization ability of previous Reid models to identify persons with unseen (new) clothes. Representative existing Reid models are adopted to show informative results on such a challenging setting, and we also provide some preliminary efforts on improving the robustness of existing models on handling the clothes inconsistency issue in the data. We believe that this study can be inspiring and helpful for encouraging more researches in this direction. The dataset is avaliable on the project website: https://wanfb.github.io/dataset.html.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://wanfb.github.io/dataset.html,Image,,,,,,,,"Unsupervised Person Re-Identification, Person Re-Identification","unsupervised-person-re-identification-on-vc, person-re-identification-on-vc-clothes",,See all 1951 tasks,Unsupervised Person Re-Identif,Unsupervised Person Re-Identif
Unsupervised_Video_Clustering_Dataset,Unsupervised Video Clustering Dataset,,https://paperswithcode.com/dataset/unsupervised-video-clustering,,,,,,,,,,,,,See all 1951 tasks,Unsupervised Video Clustering,Unsupervised Video Clustering
Unusual_Activity_Localization1_papers_with_code_Da,Unusual Activity Localization1 papers with code Dataset,,https://paperswithcode.com/dataset/unusual-activity-localization,,,,,,,,,,,,,See all 1951 tasks,Unusual Activity Localization1,Unusual Activity Localization1
Airport,Airport Dataset,"The Airport dataset is a dataset for person re-identification which consists of 39,902 images and 9,651 identities across six cameras.",https://arxiv.org/abs/1805.06086,EditUnknown,"Image, Video",,,,,902 images,,,"Person Re-Identification, Video-Based Person Re-Identification, Metric Learning",,,See all 1951 tasks,Video-Based Person Re-Identifi,Video-Based Person Re-Identifi
MARS-DL,MARS-DL Dataset,"MARS dataset processed with our re-Detect and Link (DL) module.

More information: https://github.com/jackie840129/CF-AAN",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,,,,Video-Based Person Re-Identification,,,See all 1951 tasks,Video-Based Person Re-Identifi,Video-Based Person Re-Identifi
P-DESTRE,P-DESTRE Dataset,"Provides consistent ID annotations across multiple days, making it suitable for the extremely challenging problem of person search, i.e., where no clothing information can be reliably used. Apart this feature, the P-DESTRE annotations enable the research on UAV-based pedestrian detection, tracking, re-identification and soft biometric solutions.",/paper/the-p-destre-a-fully-annotated-dataset-for,EditUnknown,"Image, Video",,,,,,,,"Person Search, Person Re-Identification, Video-Based Person Re-Identification",,,See all 1951 tasks,Video-Based Person Re-Identifi,Video-Based Person Re-Identifi
BAIR_Robot_Pushing,BAIR Robot Pushing Dataset,"Dataset of 64x64 images of a robot pushing objects on a table top. From Berkeley AI Research (BAIR).

Source: Self-Supervised Visual Planning with Temporal Skip Connections (https://arxiv.org/abs/1710.05268)

Video prediction : Conditioned on 2 frames, predict 14 frames.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Text, Time Series, Video",English,,,,64 images,,,"Video Prediction, Video Generation","video-prediction-on-bair-robot-pushing-1, video-generation-on-bair-robot-pushing",,See all 1951 tasks,Video Generation17 benchmarks4,Video Generation17 benchmarks4
CelebV-Text,CelebV-Text Dataset,"CelebV-Text comprises 70,000 in-the-wild face video clips with diverse visual content, each paired with 20 texts generated using the proposed semi-automatic text generation strategy. The provided texts describes both static and dynamic attributes precisely.",https://arxiv.org/pdf/2303.14717v1.pdf,EditUnknown,"Text, Video",English,,,,20 texts,,,"Text Generation, Video Generation, Text-to-Video Generation",,,See all 1951 tasks,Video Generation17 benchmarks4,Video Generation17 benchmarks4
How2Sign,How2Sign Dataset,"The How2Sign is a multimodal and multiview continuous American Sign Language (ASL) dataset consisting of a parallel corpus of more than 80 hours of sign language videos and a set of corresponding modalities including speech, English transcripts, and depth. A three-hour subset was further recorded in the Panoptic studio enabling detailed 3D pose estimation.",https://production-media.paperswithcode.com/datasets/how2sign.jpg,EditCreative Commons Attribution-NonCommercial 4.0 International License,"Image, Text, Video",English,,,,,,,"Sign Language Recognition, Topic Classification, Sign Language Translation, Video Generation, Gloss-free Sign Language Translation, Video Inpainting, Sign Language Production","video-inpainting-on-how2sign, video-generation-on-how2sign, sign-language-translation-on-how2sign, gloss-free-sign-language-translation-on-2",,See all 1951 tasks,Video Generation17 benchmarks4,Video Generation17 benchmarks4
InternVid,InternVid Dataset,"InternVid is a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodAL understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache-2.0 license,"Text, Video",English,,,,,,,"Video Understanding, Video Generation",,,See all 1951 tasks,Video Generation17 benchmarks4,Video Generation17 benchmarks4
Kinetics-600,Kinetics-600 Dataset,"The Kinetics-600 is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset.",https://arxiv.org/abs/2008.13705,EditCC BY 4.0,"Image, Text, Time Series, Video",English,,,,,,,"Video Prediction, Action Classification, Video Generation, Action Recognition In Videos, Self-Supervised Action Recognition","action-recognition-in-videos-on-kinetics-600, video-generation-on-kinetics-600-12-frames-1, action-classification-on-kinetics-600, video-generation-on-kinetics-600-48-frames, video-prediction-on-kinetics-600-12-frames, self-supervised-action-recognition-on, video-generation-on-kinetics-600-12-frames",,See all 1951 tasks,Video Generation17 benchmarks4,Video Generation17 benchmarks4
Kinetics,Kinetics Dataset,"The Kinetics dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube.",https://arxiv.org/abs/1902.06162,EditCC BY 4.0,"Image, Text, Time Series, Video",English,,,,,,,"Semantic Object Interaction Classification, Action Classification, Event Segmentation, Video Retrieval, Boundary Captioning, Spatio-Temporal Action Localization, Image Clustering, Self-Supervised Action Recognition Linear, Temporal Action Localization, Few Shot Action Recognition, Generic Event Boundary Detection, Boundary Detection, Boundary Grounding, Zero-Shot Action Recognition, Video Understanding, Video Classification, Video Grounding, Text-to-Video Generation, Text to Video Retrieval, Video Captioning, Action Recognition, Self-Supervised Action Recognition, Skeleton Based Action Recognition, Video Prediction, imbalanced classification, Video Recognition, Video Generation, Visual Tracking, Action Recognition In Videos, Long-tail Learning","zero-shot-action-recognition-on-kinetics, few-shot-action-recognition-on-kinetics-100, self-supervised-action-recognition-linear-on-3, video-generation-on-kinetics-600-12-frames-1, semantic-object-interaction-classification-on-2, boundary-grounding-on-kinetic-geb, video-generation-on-kinetics-600-48-frames, action-classification-on-kinetics-sounds, event-segmentation-on-kinetics-400, image-clustering-on-kinetics-700, skeleton-based-action-recognition-on-kinetics, action-classification-on-minikinetics, video-classification-on-kinetics, video-prediction-on-kinetics-600-12-frames, self-supervised-action-recognition-on-1, action-recognition-in-videos-on-kinetics-600, action-classification-on-kinetics-400, visual-tracking-on-kinetics, action-classification-on-kinetics-600, skeleton-based-action-recognition-on-kinetics-2, self-supervised-action-recognition-on, spatio-temporal-action-localization-on-ava, generic-event-boundary-detection-on-kinetics, text-to-video-retrieval-on-kinetic-geb, boundary-detection-on-kinetics-400, action-recognition-in-videos-on-kinetics-400-1, action-classification-on-kinetics-700, boundary-captioning-on-kinetic-geb, video-generation-on-kinetics-600-12-frames, text-to-video-generation-on-kinetics",,See all 1951 tasks,Video Generation17 benchmarks4,Video Generation17 benchmarks4
LAION-400M,LAION-400M Dataset,"LAION-400M is a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.

⚠️  Disclaimer & Content Warning (from the authors)
Our filtering protocol only removed NSFW images detected as illegal, but the dataset still has NSFW content accordingly marked in the metadata. When freely navigating through the dataset, keep in mind that it is a large-scale, non-curated set crawled from the internet for research purposes, such that collected links may lead to discomforting and disturbing content. Therefore, please use the demo links with caution. You can extract a “safe” subset by filtering out samples drawn with NSFW or via stricter CLIP filtering.

There is a certain degree of duplication because we used URL+text as deduplication criteria. The same image with the same caption may sit at different URLs, causing duplicates. The same image with other captions is not, however, considered duplicated.

Using KNN clustering should make it easy to further deduplicate by image content.",https://production-media.paperswithcode.com/datasets/969267f7-ca38-49cb-b843-08c1490b88e5.png,EditCreative Common CC-BY 4.0,"3D, Image, Text, Video",English,,,,,,,"Latent Diffusion Model for 3D-4C, Image-to-Text Retrieval, Latent Diffusion Model for 3D - Pano, Video Generation, Semantic Image-Text Similarity","latent-diffusion-model-for-3d-4c-on-laion, video-generation-on-laion-400m, latent-diffusion-model-for-3d-pano-on-laion",,See all 1951 tasks,Video Generation17 benchmarks4,Video Generation17 benchmarks4
MSR-VTT,MSR-VTT Dataset,"MSR-VTT (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.",https://arxiv.org/abs/2007.09049,EditUnknown,"Audio, Image, Text, Video",English,,,,,,20,"Zero-Shot Video Retrieval, Zero-Shot Video-Audio Retrieval, Video Question Answering, Text-to-Video Generation, Text to Video Retrieval, Video Generation, Video Captioning, Video Retrieval","text-to-video-retrieval-on-msr-vtt, video-retrieval-on-msr-vtt, video-retrieval-on-msr-vtt-1ka, video-generation-on-msr-vtt, zero-shot-video-retrieval-on-msr-vtt, video-question-answering-on-msr-vtt, video-captioning-on-msr-vtt-1, zero-shot-video-audio-retrieval-on-msr-vtt, text-to-video-generation-on-msr-vtt",,See all 1951 tasks,Video Generation17 benchmarks4,Video Generation17 benchmarks4
WebVid,WebVid Dataset,"WebVid contains 10 million video clips with captions, sourced from the web. The videos are diverse and rich in their content.

Both the full 10M set and a 2.5M subset is available for download:
https://github.com/m-bain/webvid-dataset",https://production-media.paperswithcode.com/datasets/Screenshot_2021-06-19_at_18.01.32.png,EditCustom,"Image, Text, Video",English,,,,,,,"Video-Text Retrieval, Text-to-Video Generation, Video Generation, Video Captioning, Video Retrieval",text-to-video-generation-on-webvid,,See all 1951 tasks,Video Generation17 benchmarks4,Video Generation17 benchmarks4
AVSBench,AVSBench Dataset,"AVSBench is a pixel-level audio-visual segmentation benchmark that provides ground truth labels for sounding objects. The dataset is divided into three subsets: AVSBench-object (Single-source subset, Multi-sources subset) and AVSBench-semantic (Semantic-labels subset). Accordingly, three settings are studied: 

1) semi-supervised audio-visual segmentation with a single sound source

2) fully-supervised audio-visual segmentation with multiple sound sources

3) fully-supervised audio-visual semantic segmentation",https://arxiv.org/pdf/2301.13190v1.pdf,EditApache-2.0 license,"Image, Video",,,,,,,,"Video Object Segmentation, Semantic Segmentation",,,See all 1951 tasks,Video Object Segmentation52 be,Video Object Segmentation52 be
DAVIS_2016,DAVIS 2016 Dataset,DAVIS16 is a dataset for video object segmentation which consists of 50 videos in total (30 videos for training and 20 for testing). Per-frame pixel-wise annotations are offered.,https://arxiv.org/abs/2008.01270,EditAnnotations under CC BY-SA 4.0,"Image, Video",,,,,,,,"Video Object Segmentation, Unsupervised Video Object Segmentation, Unsupervised Object Segmentation, Semi-Supervised Video Object Segmentation","video-object-segmentation-on-davis-2016, unsupervised-object-segmentation-on-davis, visual-object-tracking-on-davis-2016, unsupervised-video-object-segmentation-on-10",,See all 1951 tasks,Video Object Segmentation52 be,Video Object Segmentation52 be
DAVIS_2017,DAVIS 2017 Dataset,"DAVIS17 is a dataset for video object segmentation.  It contains a total of 150 videos - 60 for training, 30 for validation, 60 for testing",https://arxiv.org/abs/1911.12836,EditAnnotations under CC BY-SA 4.0,"Image, Time Series, Video",,,,,,,,"Unsupervised Video Object Segmentation, Interactive Video Object Segmentation, Video Prediction, Referring Expression Segmentation, Semi-Supervised Video Object Segmentation, Semantic Segmentation, Video Object Segmentation","interactive-video-object-segmentation-on, video-object-segmentation-on-davis-2017-val, video-prediction-on-davis-2017, visual-object-tracking-on-davis-2017, semi-supervised-video-object-segmentation-on-2, unsupervised-video-object-segmentation-on-5, video-object-segmentation-on-davis-2017, semi-supervised-video-object-segmentation-on-1, referring-expression-segmentation-on-davis, video-object-segmentation-on-davis-2017-test-1, unsupervised-video-object-segmentation-on-4",,See all 1951 tasks,Video Object Segmentation52 be,Video Object Segmentation52 be
FBMS-59,FBMS-59 Dataset,"The Freiburg-Berkeley Motion Segmentation Dataset (FBMS-59) is a dataset for motion segmentation, which extends the BMS-26 dataset with 33 additional video sequences. A total of 720 frames is annotated. FBMS-59 comes with a split into a training set and a test set. Typical challenges appear in both sets.",https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html,EditUnknown,"Image, Video",,,,,,,,"Video Object Segmentation, Video Salient Object Detection, Unsupervised Video Object Segmentation, Unsupervised Object Segmentation","video-object-segmentation-on-fbms-59, video-salient-object-detection-on-fbms-59, unsupervised-object-segmentation-on-fbms-59",,See all 1951 tasks,Video Object Segmentation52 be,Video Object Segmentation52 be
FBMS,FBMS Dataset,The Freiburg-Berkeley Motion Segmentation Dataset (FBMS-59) is an extension of the BMS dataset with 33 additional video sequences. A total of 720 frames is annotated. It has pixel-accurate segmentation annotations of moving objects. FBMS-59 comes with a split into a training set and a test set.,https://lmb.informatik.uni-freiburg.de/resources/datasets/,"EditCustom (research-only, non-commercial)","Image, Video",,,,,,,,"Video Object Segmentation, Video Salient Object Detection, Unsupervised Video Object Segmentation, Unsupervised Object Segmentation","video-object-segmentation-on-fbms-59, video-salient-object-detection-on-fbms-59, video-object-segmentation-on-fbms, unsupervised-object-segmentation-on-fbms-59",,See all 1951 tasks,Video Object Segmentation52 be,Video Object Segmentation52 be
MOSE,MOSE Dataset,"CoMplex video Object SEgmentation (MOSE) is a dataset to study the tracking and segmenting objects in complex environments. MOSE contains 2,149 video clips and 5,200 objects from 36 categories, with 431,725 high-quality object segmentation masks. The most notable feature of MOSE dataset is complex scenes with crowded and occluded objects.",https://arxiv.org/pdf/2302.01872.pdf,EditUnknown,"Image, Video",,,,,,,36,"Unsupervised Video Object Segmentation, Interactive Video Object Segmentation, Video Semantic Segmentation, Semi-Supervised Video Object Segmentation, Video Object Segmentation","semi-supervised-video-object-segmentation-on-21, video-object-segmentation-on-mose",,See all 1951 tasks,Video Object Segmentation52 be,Video Object Segmentation52 be
Referring_Expressions_for_DAVIS_2016___2017,Referring Expressions for DAVIS 2016 & 2017 Dataset,"Our task is to localize and provide a pixel-level mask of an object on all video frames given a language referring expression obtained either by looking at the first frame only or the full video. To validate our approach we employ two popular video object segmentation datasets, DAVIS16 [38] and DAVIS17 [42]. These two datasets introduce various challenges, containing videos with single or multiple salient objects, crowded scenes, similar looking instances, occlusions, camera view changes, fast motion, etc.

DAVIS16 [38] consists of 30 training and 20 test videos of diverse object categories with all frames annotated with pixel-level accuracy. Note that in this dataset only a single object is annotated per video. For the multiple object video segmentation task we consider DAVIS17. Compared to DAVIS16, this is a more challenging dataset, with multiple objects annotated per video and more complex scenes with more distractors, occlusions, smaller objects, and fine structures. Overall, DAVIS17 consists of a training set with 60 videos, and a validation/test-dev/test-challenge set with 30 sequences each. 

As our goal is to segment objects in videos using language specifications, we augment all objects annotated with mask labels in DAVIS16 and DAVIS17 with non-ambiguous referring expressions. We follow the work of [34] and ask the annotator to provide a language description of the object, which has a mask annotation, by looking only at the
first frame of the video. Then another annotator is given the first frame and the corresponding description, and asked to identify the referred object. If the annotator is unable to correctly identify the object, the description is corrected to remove ambiguity and to specify the object uniquely. We have collected two referring expressions per target
object annotated by non-computer vision experts (Annotator 1, 2).

However, by looking only at the 1st frame, the obtained referring expressions may potentially be invalid for an entire video. (We actually quantified that only∼ 15% of the
collected descriptions become invalid over time and it does not affect strongly segmentation results as temporal consistency step helps to disambiguate some of such cases, see the supp. material for details.) Besides, in many applications, such as video editing or video-based advertisement, the user has access to a full video. Providing a language
query which is valid for all frames might decrease the editing time and result in more coherent predictions. Thus, on DAVIS17 we asked the workers to provide a description of the object by looking at the full video. We have collected one expression of the full video type per target object. Future work may choose to use either setting.

The average length for the first frame/full video expressions is 5.5/6.3 words. For DAVIS17 first frame annotations we notice that descriptions given by Annotator 1 are longer than the ones by Annotator 2 (6.4 vs. 4.6 words). We evaluate the effect of description length on the grounding performance in §5. Besides, the expressions relevant to a full video mention verbs more often than the first frame descriptions (44% vs. 25%). This is intuitive, as referring to an object which changes its appearance and position over time may require mentioning its actions. Adjectives are present in over 50% for all annotations. Most of them refer to colors (over 70%), shapes and sizes (7%) and spatial/ordering words (6% first frame vs. 13% full video expressions). The full video expressions also have a higher number of adverbs and prepositions, and overall are more complex than the ones provided for the first frame.

Overall augmented DAVIS16/17 contains ∼ 1.2k referring expressions for more than 400 objects on 150 videos with ∼ 10k frames. We believe the collected data will be
of interest to segmentation as well as vision and language communities, providing an opportunity to explore language as alternative input for video object segmentation.",https://production-media.paperswithcode.com/datasets/referring_expression_annotations.png,EditUnknown,"Image, Video",,,,,,,,"Video Object Segmentation, Unsupervised Video Object Segmentation, Referring Expression Segmentation, Semi-Supervised Video Object Segmentation","video-object-segmentation-on-davis-2017-val, visual-object-tracking-on-davis-2017, referring-expression-segmentation-on-1, referring-expression-segmentation-on-davis, unsupervised-video-object-segmentation-on-4",,See all 1951 tasks,Video Object Segmentation52 be,Video Object Segmentation52 be
SegTrack-v2,SegTrack-v2 Dataset,SegTrack v2 is a video segmentation dataset with full pixel-level annotations on multiple objects at each frame within each video.,https://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html,EditUnknown,"Image, Video",,,,,,,,"Video Segmentation, Unsupervised Video Object Segmentation, Video Salient Object Detection, Video Semantic Segmentation, Semantic Segmentation, Unsupervised Object Segmentation, Video Object Segmentation","video-segmentation-on-segtrack-v2, video-object-segmentation-on-segtrack-v2-1, video-salient-object-detection-on-segtrack-v2, unsupervised-object-segmentation-on-segtrack, unsupervised-video-object-segmentation-on-3",,See all 1951 tasks,Video Object Segmentation52 be,Video Object Segmentation52 be
YouTube-VOS_2018,YouTube-VOS 2018 Dataset,"Youtube-VOS is a Video Object Segmentation dataset that contains 4,453 videos - 3,471 for training, 474 for validation, and 508 for testing. The training and validation videos have pixel-level ground truth annotations for every 5th frame (6 fps). It also contains Instance Segmentation annotations. It has more than 7,800 unique objects, 190k high-quality manual annotations and more than 340 minutes in duration.",https://arxiv.org/abs/1910.00132,EditCC BY 4.0,"Image, Video",,,,,,,,"Visual Object Tracking, One-shot visual object segmentation, Semi-Supervised Video Object Segmentation, Video Inpainting, Video Object Segmentation","video-object-segmentation-on-youtube-vos, semi-supervised-video-object-segmentation-on-18, one-shot-visual-object-segmentation-on-1, video-inpainting-on-youtube-vos-1, video-object-segmentation-on-youtube-vos-2019-2, video-inpainting-on-youtube-vos, visual-object-tracking-on-youtube-vos, one-shot-visual-object-segmentation-on, visual-object-tracking-on-youtube-vos-1, video-object-segmentation-on-youtube-vos-1",,See all 1951 tasks,Video Object Segmentation52 be,Video Object Segmentation52 be
BL30K,BL30K Dataset,"BL30K is a synthetic dataset rendered using Blender with ShapeNet's data. We break the dataset into six segments, each with approximately 5K videos. The videos are organized in a similar format as DAVIS and YouTubeVOS, so dataloaders for those datasets can be used directly. Each video is 160 frames long, and each frame has a resolution of 768*512. There are 3-5 objects per video, and each object has a random smooth trajectory -- we tried to optimize the trajectories in a greedy fashion to minimize object intersection (not guaranteed), with occlusions still possible (happen a lot in reality). See MiVOS for details.",https://production-media.paperswithcode.com/datasets/00004.jpg,EditUnknown,"Image, Video",,,,,,,,"Unsupervised Video Object Segmentation, Interactive Video Object Segmentation, Video Object Tracking, Semi-Supervised Video Object Segmentation, Video Object Segmentation",,,See all 1951 tasks,Video Object Tracking4 benchma,Video Object Tracking4 benchma
NT-VOT211,NT-VOT211 Dataset,"NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. To the best of our knowledge, it is the largest night-time tracking benchmark to-date that is specifically designed to address unique challenges such as adverse visibility, image blur, and distractors inherent to night-time tracking scenarios. 

Kindly click the 'Homepage' button below to be directed to our official website. There, you can easily access the independent leaderboard through the 'Evaluation on Server' section on our homepage. We offer comprehensive, step-by-step guidance on utilizing our dataset, and we also provide the annotation tools that were instrumental in the creation of this dataset.",https://production-media.paperswithcode.com/datasets/a289b312-86bc-47f9-8c07-e327099d8506.png,EditMIT,"Image, Video",,,,,,,,Video Object Tracking,video-object-tracking-on-nv-vot211,,See all 1951 tasks,Video Object Tracking4 benchma,Video Object Tracking4 benchma
RF100,RF100 Dataset,"The evaluation of object detection models is usually performed by optimizing a single metric, e.g. mAP, on a fixed set of datasets, e.g. Microsoft COCO and Pascal VOC. Due to image retrieval and annotation costs, these datasets consist largely of images found on the web and do not represent many real-life domains that are being modelled in practice, e.g. satellite, microscopic and gaming, making it difficult to assert the degree of generalization learned by the model.

We introduce the Roboflow-100 (RF100) consisting of 100 datasets, 7 imagery domains, 224,714 images, and 805 class labels with over 11,170 labelling hours. We derived RF100 from over 90,000 public datasets, 60 million public images that are actively being assembled and labelled by computer vision practitioners in the open on the web application Roboflow Universe. By releasing RF100, we aim to provide a semantically diverse, multi-domain benchmark of datasets to help researchers test their model's generalizability with real-life data. RF100 download and benchmark replication are available on GitHub.",https://production-media.paperswithcode.com/datasets/8921b708-f775-4e75-b7df-6726c0767ec4.jpg,EditMIT,"Image, Video",,,,,714 images,"valuation of object detection models is usually performed by optimizing a single metric, e.g. mAP, on a fixed set of datasets, e.g. Microsoft COCO and Pascal VOC. Due to image retrieval and annotation costs, these datasets consist largely of images",,"Robust Object Detection, Object Detection, Zero-Shot Object Detection, Object Localization, Visual Object Tracking, Video Object Tracking, Small Object Detection, Thermal Infrared Object Tracking, 2D Object Detection, Object Tracking, Moving Object Detection, Medical Object Detection, Image Classification, object-detection, Object Counting, Multi-object discovery",2d-object-detection-on-rf100,,See all 1951 tasks,Video Object Tracking4 benchma,Video Object Tracking4 benchma
TREK-150,TREK-150 Dataset,TREK-150 is a benchmark dataset for object tracking in First Person Vision (FPV) videos composed of 150 densely annotated video sequences.,https://production-media.paperswithcode.com/datasets/Screenshot_2021-09-08_at_16.23.47.jpg,EditUnknown,"Image, Video",,,,,,,,"Visual Object Tracking, Video Object Tracking, Object Tracking",,,See all 1951 tasks,Video Object Tracking4 benchma,Video Object Tracking4 benchma
VideoCube,VideoCube Dataset,VideoCube is a high-quality and large-scale benchmark to create a challenging real-world experimental environment for Global Instance Tracking (GIT). MGIT is a high-quality and multi-modal benchmark based on VideoCube-Tiny to fully represent the complex spatio-temporal and causal relationships coupled in longer narrative content.,https://production-media.paperswithcode.com/datasets/a2f05203-7453-4c03-bda1-60d3b598b4cf.jpg,EditCC BY-NC-SA 4.0,"Image, Video",,,,,,,,"Visual Object Tracking, Video Object Tracking, Object Tracking",visual-object-tracking-on-videocube,,See all 1951 tasks,Video Object Tracking4 benchma,Video Object Tracking4 benchma
VOT2014,VOT2014 Dataset,"The dataset comprises 25 short sequences showing various objects in challenging backgrounds. Eight sequences are from the VOT2013 challenge (bolt, bicycle, david, diving, gymnastics, hand, sunshade, woman). The new sequences show complementary objects and backgrounds, for example a fish underwater or a surfer riding a big wave. The sequences were chosen from a large pool of sequences using a methodology based on clustering visual features of object and background so that those 25 sequences sample evenly well the existing pool.",https://www.votchallenge.net/vot2014/dataset.html,EditUnknown,"Image, Video",,,,,,,,"Visual Object Tracking, Video Object Tracking, Object Tracking, Visual Tracking",visual-object-tracking-on-vot2014,,See all 1951 tasks,Video Object Tracking4 benchma,Video Object Tracking4 benchma
VOTChallenge,VOTChallenge Dataset,The Visual Object Tracking (VOT) dataset is a collection of video sequences used for evaluating and benchmarking visual object tracking algorithms. It provides a standardized platform for researchers and practitioners to assess the performance of different tracking methods.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,,,,"Visual Object Tracking, Video Object Tracking, Semi-Supervised Video Object Segmentation, Object Tracking, Visual Tracking","visual-object-tracking-on-vot2016, visual-object-tracking-on-vot2014, semi-supervised-video-object-segmentation-on-15, visual-object-tracking-on-vot2018, visual-object-tracking-on-vot201718, visual-object-tracking-on-vot2019, visual-object-tracking-on-vot2017, visual-object-tracking-on-vot2022",,See all 1951 tasks,Video Object Tracking4 benchma,Video Object Tracking4 benchma
DHF1K,DHF1K Dataset,"DHF1K is a video saliency dataset which contains a ground-truth map of binary pixel-wise gaze fixation points and a continuous map of the fixation points after being blurred by a gaussian filter. DHF1K contains 1000 videos in total. 700 of the videos are annotated, 600 of which are used for training and 100 for validation. The remaining 300 are the testing set which are to be evaluated on a public server.",https://arxiv.org/abs/1910.02793,EditAttribution 4.0 International,"Image, Time Series, Video",,,,,,,,"Video Saliency Detection, Video Saliency Prediction",video-saliency-detection-on-dhf1k,,See all 1951 tasks,Video Saliency Detection5 benc,Video Saliency Detection5 benc
LaRS,LaRS Dataset,"LaRS is the largest and most diverse panoptic maritime obstacle detection dataset.

Highlights:


Diverse scenes from manual capture, public online videos and existing datasets  
USV-centric point of view  
4000+ manually per-pixel labelled frames:  
3 stuff categories and 8 thing (dynamic obstacles) categories  
20 scene-level attributes (e.g. illumination, reflections, conditions)  


Temporal context for each annotated frame (9 preceding frames, total: 40k frames)",https://production-media.paperswithcode.com/datasets/cd2a87aa-734b-48e1-bfa8-4d3571491f18.jpg,EditCC-BY-NC 4.0,"Image, Video",,,,,,,,"Panoptic Segmentation, Semantic Segmentation, Video Semantic Segmentation, Video Panoptic Segmentation","video-semantic-segmentation-on-lars, panoptic-segmentation-on-lars, semantic-segmentation-on-lars",,See all 1951 tasks,Video Semantic Segmentation6 b,Video Semantic Segmentation6 b
ODMS,ODMS Dataset,"ODMS is a dataset for learning Object Depth via Motion and Segmentation. ODMS training data are configurable and extensible, with each training example consisting of a series of object segmentation masks, camera movement distances, and ground truth object depth. As a benchmark evaluation, the dataset provides four ODMS validation and test sets with 15,650 examples in multiple domains, including robotics and driving.",https://github.com/griffbr/ODMS,EditUnknown,"Image, Video",,,,,650 examples,"training data are configurable and extensible, with each training example consisting of a series of object segmentation masks, camera movement distances, and ground truth object depth. As a benchmark evaluation, the dataset provides four ODMS validation and test sets with 15,650 examples",,"Video Object Segmentation, Semantic Segmentation, Video Semantic Segmentation",,,See all 1951 tasks,Video Semantic Segmentation6 b,Video Semantic Segmentation6 b
VLOG_Dataset,VLOG Dataset Dataset,A large collection of interaction-rich video data which are annotated and analyzed.,/paper/from-lifestyle-vlogs-to-everyday-interactions,EditUnknown,"Image, Video",,,,,,,,"Optical Flow Estimation, Semantic Segmentation, Video Semantic Segmentation",,,See all 1951 tasks,Video Semantic Segmentation6 b,Video Semantic Segmentation6 b
VSPW,VSPW Dataset,A Large-scale Dataset for Video Scene Parsing in the Wild,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Video",,,,,,,,Video Semantic Segmentation,video-semantic-segmentation-on-vspw,,See all 1951 tasks,Video Semantic Segmentation6 b,Video Semantic Segmentation6 b
Video_Stabilization25_papers_with_code_Dataset,Video Stabilization25 papers with code Dataset,,https://paperswithcode.com/dataset/video-stabilization,,,,,,,,,,,,,See all 1951 tasks,Video Stabilization25 papers w,Video Stabilization25 papers w
Inter4K,Inter4K Dataset,"A video dataset for benchmarking upsampling methods. Inter4K contains 1,000 ultra-high resolution videos with 60 frames per second (fps) from online resources. The dataset provides standardized video resolutions at ultra-high definition (UHD/4K), quad-high definition (QHD/2K), full-high definition (FHD/1080p), (standard) high definition (HD/720p), one quarter of full HD (qHD/520p) and one ninth of a full HD (nHD/360p). We use frame rates of 60, 50, 30, 24 and 15 fps for each resolution. Based on this standardization, both super-resolution and frame interpolation tests can be performed for different scaling sizes ($\times 2$, $\times 3$ and $\times 4$). In this paper, we use Inter4K to address frame upsampling and interpolation. Inter4K provides both standardized UHD resolution and 60 fps for all of videos by also containing a diverse set of 1,000 5-second videos. Differences between scenes originate from the equipment (e.g., professional 4K cameras or phones), lighting conditions, variations in movements, actions or objects. The dataset is divided into 800 videos for training, 100 videos for validation and 100 videos for testing.",https://production-media.paperswithcode.com/datasets/4d0f258c-7982-4f6f-8b4b-cfc4a8ff54ab.png,EditAttribution-NonCommercial-ShareAlike 4.0 International,Video,,,,,,,,"Video Frame Interpolation, Multi-Frame Super-Resolution, Video Super-Resolution",,,See all 1951 tasks,Video Super-Resolution18 bench,Video Super-Resolution18 bench
MSU_Super-Resolution_for_Video_Compression,MSU Super-Resolution for Video Compression Dataset,"This is a dataset for a super-resolution task. The dataset contains 480x270 videos that were decoded with 6 different bitrates (100 - 4000 kbps) using 5 different codecs (H.264, H.265, H.266, AV1, and AVS3 standards). The dataset contains indoor and outdoor videos as well as animation. All videos have low SI/TI values and simple textures. It was made to minimize compression artifacts that may occur to make restoration of details possible.",https://production-media.paperswithcode.com/datasets/75a764e5-239c-4fd0-abc1-3f4be69bc986.png,EditUnknown,Video,,,,,,,,Video Super-Resolution,video-super-resolution-on-msu-super-1,,See all 1951 tasks,Video Super-Resolution18 bench,Video Super-Resolution18 bench
MSU_Video_Super_Resolution_Benchmark__Detail_Resto,MSU Video Super Resolution Benchmark: Detail Restoration Dataset,"This is a dataset for a video super-resolution task. The dataset contains the most complex content for the restoration task: faces, text, QR-codes, car numbers, unpatterned textures, small details. Videos include different types of motion and different types of degradation: bicubic interpolation (BI) and Gaussian blurring and downsampling (BD). The resolution of all input video sequences is 480x320.",https://videoprocessing.ai/benchmarks/video-super-resolution.html,EditUnknown,Video,,,,,,,,Video Super-Resolution,video-super-resolution-on-msu-vsr-benchmark,,See all 1951 tasks,Video Super-Resolution18 bench,Video Super-Resolution18 bench
MSU_Video_Upscalers__Quality_Enhancement,MSU Video Upscalers: Quality Enhancement Dataset,The dataset aims to find the algorithms that produce the most visually pleasant image possible and generalize well to a broad range of content. It consists of 30 clips and contains 15 2D-animated segments losslessly recorded from various video games and 15 camera-shot segments from high-bitrate YUV444 sources. The complexity of clips varies significantly in terms of spatial and temporal indexes. Multiple bicubic downscaling mixed with sharpening is used to simulate complex real-world camera degradation. The authors used slight compression and YUV420 conversion to simulate a practical use case. 1920×1080 sources were downscaled to 480×270 input.,https://production-media.paperswithcode.com/datasets/79d763f3-b9a0-46b1-abae-b588b674e17d.jpg,EditUnknown,Video,,1920,,,,,,Video Super-Resolution,video-super-resolution-on-msu-video-upscalers,,See all 1951 tasks,Video Super-Resolution18 bench,Video Super-Resolution18 bench
TbD-3D,TbD-3D Dataset,,https://production-media.paperswithcode.com/datasets/tbd3d.png,EditUnknown,Video,,,,,,,,Video Super-Resolution,video-super-resolution-on-tbd-3d,,See all 1951 tasks,Video Super-Resolution18 bench,Video Super-Resolution18 bench
TbD,TbD Dataset,,https://production-media.paperswithcode.com/datasets/tbd.png,EditUnknown,Video,,,,,,,,Video Super-Resolution,video-super-resolution-on-tbd,,See all 1951 tasks,Video Super-Resolution18 bench,Video Super-Resolution18 bench
Vid4,Vid4 Dataset,"The Vid4 dataset is generally used for testing video super-resolution. It consists of four sequences: walk (740x480, 47 frames), foliage (740x480, 49 frames), city (704x576, 34 frames), and calendar (720x576, 41 frames).",https://production-media.paperswithcode.com/datasets/a645db14-6d61-4189-bce0-6cc6e6571c27.png,EditUnknown,Video,,,,,,,,"Space-time Video Super-resolution, Federated Learning (Video Super-Resolution), Key-Frame-based Video Super-Resolution (K = 15), Video Super-Resolution, Video Frame Interpolation","federated-learning-video-super-resolution-on-1, video-super-resolution-on-vid4-4x-upscaling, video-frame-interpolation-on-vid4-4x, key-frame-based-video-super-resolution-k-15",,See all 1951 tasks,Video Super-Resolution18 bench,Video Super-Resolution18 bench
VideoLQ,VideoLQ Dataset,"VideoLQ consists of videos downloaded from various video hosting sites such as Flickr and YouTube, with a Creative Common license.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Video,,,,,,,,"Video Denoising, Video Super-Resolution",video-denoising-on-videolq,,See all 1951 tasks,Video Super-Resolution18 bench,Video Super-Resolution18 bench
Vimeo90K,Vimeo90K Dataset,"The Vimeo-90K is a large-scale high-quality video dataset for lower-level video processing. It proposes three different video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.",https://arxiv.org/pdf/1711.09078.pdf,EditUnknown,"Image, Time Series, Video",,,,,,,,"Image Super-Resolution, Optical Flow Estimation, Video Prediction, Video Super-Resolution, Video Frame Interpolation","video-super-resolution-on-vimeo90k, video-prediction-on-vimeo90k, video-frame-interpolation-on-vimeo90k",,See all 1951 tasks,Video Super-Resolution18 bench,Video Super-Resolution18 bench
Charades-STA,Charades-STA Dataset,Charades-STA is a new dataset built on top of Charades by adding sentence temporal annotations.,/paper/tall-temporal-activity-localization-via,EditCustom,"Image, Time Series, Video",,,,,,,,"Moment Retrieval, Partially Relevant Video Retrieval, Video Understanding, Temporal Sentence Grounding, Video Retrieval, Decision Making, Temporal Localization","partially-relevant-video-retrieval-on-1, video-retrieval-on-charades-sta, moment-retrieval-on-charades-sta, temporal-sentence-grounding-on-charades-sta",,See all 1951 tasks,Video Understanding21 benchmar,Video Understanding21 benchmar
EPIC-KITCHENS-55,EPIC-KITCHENS-55 Dataset,"The EPIC-KITCHENS-55 dataset comprises a set of 432 egocentric videos recorded by 32 participants in their kitchens at 60fps with a head mounted camera. There is no guiding script for the participants who freely perform activities in kitchens related to cooking, food preparation or washing up among others. Each video is split into short action segments (mean duration is 3.7s) with specific start and end times and a verb and noun annotation describing the action (e.g. ‘open fridge‘). The verb classes are 125 and the noun classes 331. The dataset is divided into one train and two test splits.",https://arxiv.org/abs/1905.00742,EditUnknown,"Image, Video",,,,,,,,"Video Object Detection, Action Recognition, Video Understanding, Egocentric Activity Recognition","action-recognition-in-videos-on-epic-kitchens, video-object-detection-on-epic-kitchens-55, egocentric-activity-recognition-on-epic-1",,See all 1951 tasks,Video Understanding21 benchmar,Video Understanding21 benchmar
MovieNet,MovieNet Dataset,"MovieNet is a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92 K tags of cinematic style.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_2.17.59_PM.png,EditUnknown,"Audio, Image, Text, Video",English,,,,,,,"Audio Generation, Video Understanding, Person Search, Scene Segmentation",scene-segmentation-on-movienet,,See all 1951 tasks,Video Understanding21 benchmar,Video Understanding21 benchmar
SEED-Bench,SEED-Bench Dataset,"SEED-Bench consists of 19K multiple choice questions with accurate human annotations (~6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality.",https://production-media.paperswithcode.com/datasets/0ea69657-2452-4e69-a8ff-07803c2ba813.png,EditApache License Version 2.0,Video,,,,,,,,Video Understanding,,,See all 1951 tasks,Video Understanding21 benchmar,Video Understanding21 benchmar
SoccerNet-v2,SoccerNet-v2 Dataset,"A novel large-scale corpus of manual annotations for the SoccerNet video dataset, along with open challenges to encourage more research in soccer understanding and broadcast production.",/paper/soccernet-v2-a-dataset-and-benchmarks-for,EditUnknown,"Image, Video",,,,,,,,"Boundary Detection, Replay Grounding, Camera shot segmentation, Action Spotting, Video Understanding, Person Re-Identification, Video Object Tracking, Action Classification, Camera shot boundary detection","person-re-identification-on-soccernet-v2, action-spotting-on-soccernet-v2, camera-shot-segmentation-on-soccernet-v2, camera-shot-boundary-detection-on-soccernet, replay-grounding-on-soccernet-v2, video-object-tracking-on-soccernet-v2, action-spotting-on-soccernet",,See all 1951 tasks,Video Understanding21 benchmar,Video Understanding21 benchmar
LLaVA-Bench,LLaVA-Bench Dataset,"LLaVA-Bench is a dataset created to evaluate the capability of large multimodal models (LMM) in more challenging tasks and generalizability to novel domains. It consists of a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc., and each image with a highly-detailed and manually-curated description and a proper selection of questions. The dataset is part of the LLaVA project, which aims to develop multimodal chatbots that follow human intents to complete various daily-life visual tasks in the wild.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,24 images,valuate the capability of large multimodal models (LMM) in more challenging tasks and generalizability to novel domains. It consists of a diverse set of 24 images,,visual instruction following,visual-instruction-following-on-llava-bench,,See all 1951 tasks,visual instruction following1 ,visual instruction following1 
VSTaR-1M,VSTaR-1M Dataset,"VSTaR-1M is a 1M instruction tuning dataset, created using Video-STaR, with the source datasets: 
* Kinetics700
* STAR-benchmark
* FineDiving

The videos for VSTaR-1M can be found in the links above. 

VSTaR-1M is built off of diverse task with the goal of enhancing video-language alignment in Large Video-Language Models (LVLMs).


kinetics700_tune_.json - Instruction tuning QA pairs for the Kinetics700 source dataset. Good for increasing diversity and for more fine-grained activity recognition.  
starb_tune_.json - Instruction tuning QA pairs for the STAR-benchmark source dataset. Good for temporal reasoning.
finediving_tune_.json - Instruction tuning QA pairs for the FineDiving source dataset. Example of adapting LVLMs for novel tasks (Olympic diving judge).",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,visual instruction following,,,See all 1951 tasks,visual instruction following1 ,visual instruction following1 
OTB-2013,OTB-2013 Dataset,"OTB2013 is the previous version of the current OTB2015 Visual Tracker Benchmark. It contains only 50 tracking sequences, as opposed to the 100 sequences in the current version of the benchmark.",https://arxiv.org/abs/2004.01382,EditUnknown,"Image, Video",,,,,,,,"Visual Object Tracking, Visual Tracking","visual-tracking-on-otb-2013, visual-object-tracking-on-otb-2013",,See all 1951 tasks,Visual Object Tracking26 bench,Visual Object Tracking26 bench
OTB-2015,OTB-2015 Dataset,"OTB-2015, also referred as Visual Tracker Benchmark, is a visual tracking dataset. It contains 100 commonly used video sequences for evaluating visual tracking.",https://production-media.paperswithcode.com/datasets/OTB-2015-0000000303-97cc742a_FnvT7cU.jpg,EditUnknown,"Image, Video",,2015,,,,,,"Visual Object Tracking, Visual Tracking",visual-object-tracking-on-otb-2015,,See all 1951 tasks,Visual Object Tracking26 bench,Visual Object Tracking26 bench
OTB,OTB Dataset,Object Tracking Benchmark (OTB) is a visual tracking benchmark that is widely used to evaluate the performance of a visual tracking algorithm. The dataset contains a total of 100 sequences and each is annotated frame-by-frame with bounding boxes and 11 challenge attributes. OTB-2013 dataset contains 51 sequences and the OTB-2015 dataset contains all 100 sequences of the OTB dataset.,https://arxiv.org/abs/1712.09153,EditUnknown,"Image, Video",,2013,,,,,,"Visual Object Tracking, Visual Tracking","visual-object-tracking-on-otb-2015, visual-object-tracking-on-otb-2013, visual-tracking-on-otb-2013, visual-object-tracking-on-otb-50",,See all 1951 tasks,Visual Object Tracking26 bench,Visual Object Tracking26 bench
TrackingNet,TrackingNet Dataset,"TrackingNet is a large-scale tracking dataset consisting of videos in the wild. It has a total of 30,643 videos split into 30,132 training videos and 511 testing videos, with an average of 470,9 frames.",https://arxiv.org/abs/1908.00855,EditApache 2.0,"Image, Video",,,,,,,,"Visual Object Tracking, Visual Tracking","visual-object-tracking-on-trackingnet, visual-tracking-on-trackingnet",,See all 1951 tasks,Visual Object Tracking26 bench,Visual Object Tracking26 bench
VOT2016,VOT2016 Dataset,"VOT2016 is a video dataset for visual object tracking. It contains 60 video clips and 21,646 corresponding ground truth maps with pixel-wise annotation of salient objects.",https://arxiv.org/abs/1807.04514,EditUnknown,"Image, Video",,,,,,,,Visual Object Tracking,visual-object-tracking-on-vot2016,,See all 1951 tasks,Visual Object Tracking26 bench,Visual Object Tracking26 bench
DDD17,DDD17 Dataset,"DDD17 has over 12 h of a 346x260 pixel DAVIS sensor recording highway and city driving in daytime, evening, night, dry and wet weather conditions, along with vehicle speed, GPS position, driver steering, throttle, and brake captured from the car's on-board diagnostics interface.",https://arxiv.org/pdf/1711.01458v1.pdf,EditCC BY-SA 4.0,"Image, Video",,,,,,,,"Visual Place Recognition, Autonomous Driving, Semantic Segmentation, Motion Estimation",semantic-segmentation-on-ddd17,,See all 1951 tasks,Visual Place Recognition38 ben,Visual Place Recognition38 ben
GSV-Cities,GSV-Cities Dataset,"GSV-Cities is a large-scale dataset for training deep neural network for the task of Visual Place Recognition.

The dataset contains more than 530k images:


There are more than 62k different places, spread across multiple cities around the globe.
Each place is depited by at least 4 images (up to 20 images).
All places are physically distant (at least 100 meters between any pair of places).",https://production-media.paperswithcode.com/datasets/e5c1d9bd-1779-4a0d-a22a-93403931527f.jpeg,EditUnknown,Image,,,,,530k images,,,Visual Place Recognition,,,See all 1951 tasks,Visual Place Recognition38 ben,Visual Place Recognition38 ben
Mapillary_Vistas_Dataset,Mapillary Vistas Dataset Dataset,Mapillary Vistas Dataset is a diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world.,https://www.mapillary.com/dataset/vistas?lat=20&lng=0&z=1.5&pKey=pBBmjuJ8yU1r2ROYRzWmFg,EditCustom (non-commercial),Image,,,,,,,,"Visual Place Recognition, Panoptic Segmentation, Semantic Segmentation","panoptic-segmentation-on-mapillary-val, visual-place-recognition-on-mapillary-val, semantic-segmentation-on-mapillary-val",,See all 1951 tasks,Visual Place Recognition38 ben,Visual Place Recognition38 ben
MSLS,MSLS Dataset,The largest and most diverse dataset for lifelong place recognition from image sequences in urban and suburban settings.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA) license,Image,,,,,,,,Visual Place Recognition,visual-place-recognition-on-msls,,See all 1951 tasks,Visual Place Recognition38 ben,Visual Place Recognition38 ben
NCLT,NCLT Dataset,"The NCLT dataset is a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan’s North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is comprised of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g., pedestrians, bicyclists, and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g., falling leaves and snow), and long-term structural changes caused by construction projects.",http://robots.engin.umich.edu/nclt/nclt.pdf,EditODbL,"3D, Image",,,,,,,,"Visual Place Recognition, Pose Estimation, Visual Localization",,,See all 1951 tasks,Visual Place Recognition38 ben,Visual Place Recognition38 ben
Nordland,Nordland Dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",https://production-media.paperswithcode.com/datasets/88ac82dd-6067-4f7f-8859-37d0d811907c.jpg,EditUnknown,Image,,,,,,,,Visual Place Recognition,visual-place-recognition-on-nordland,,See all 1951 tasks,Visual Place Recognition38 ben,Visual Place Recognition38 ben
Oxford_RobotCar_Dataset,Oxford RobotCar Dataset Dataset,"The Oxford RobotCar Dataset contains over 100 repetitions of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different combinations of weather, traffic and pedestrians, along with longer term changes such as construction and roadworks.",/paper/real-time-kinematic-ground-truth-for-the,EditUnknown,"3D, Image",,,,,,,,"3D Place Recognition, Visual Place Recognition, Unsupervised Domain Adaptation","unsupervised-domain-adaptation-on-cityscapes-2, visual-place-recognition-on-oxford-robotcar-4, 3d-place-recognition-on-oxford-robotcar",,See all 1951 tasks,Visual Place Recognition38 ben,Visual Place Recognition38 ben
MathVista,MathVista Dataset,"MathVista is a consolidated Mathematical reasoning benchmark within Visual contexts. It consists of three newly created datasets, IQTest, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logical reasoning on puzzle test figures, algebraic reasoning over functional plots, and scientific reasoning with academic paper figures, respectively. It also incorporates 9 MathQA datasets and 19 VQA datasets from the literature, which significantly enrich the diversity and complexity of visual perception and mathematical reasoning challenges within our benchmark. In total, MathVista includes 6,141 examples collected from 31 different datasets.


Project: https://mathvista.github.io/
Visualization: https://mathvista.github.io/#visualization
Leaderboard: https://mathvista.github.io/#leaderboard
Paper: https://arxiv.org/abs/2310.02255
Data: https://huggingface.co/datasets/AI4Math/MathVista
Code: https://github.com/lupantech/MathVista",https://production-media.paperswithcode.com/datasets/5f2b02d1-a1bf-4759-98b5-01f3890eeec9.png,EditCC BY-SA 4.0,"Image, Text",English,,,,141 examples,"Test, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logical reasoning on puzzle test figures, algebraic reasoning over functional plots, and scientific reasoning with academic paper figures, respectively. It also incorporates 9 MathQA datasets and 19 VQA datasets from the literature, which significantly enrich the diversity and complexity of visual perception and mathematical reasoning challenges within our benchmark. In total, MathVista includes 6,141 examples",,"Math Word Problem Solving, Visual Reasoning, Visual Question Answering, Multiple-choice, Mathematical Reasoning, Visual Question Answering (VQA), Question Answering",,,See all 1951 tasks,Visual Question Answering31 be,Visual Question Answering31 be
MM-Vet,MM-Vet Dataset,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,https://production-media.paperswithcode.com/datasets/3357518a-4594-44bf-8a58-dc611de92210.jpeg,EditCC BY-NC 4.0,"Image, Text",English,,,,,,,"Visual Question Answering, Visual Question Answering (VQA)","visual-question-answering-on-mm-vet, visual-question-answering-vqa-on-mm-vet",,See all 1951 tasks,Visual Question Answering31 be,Visual Question Answering31 be
MMBench,MMBench Dataset,"MMBench is a multi-modality benchmark. It methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluation questions and abilities. The second element introduces a novel CircularEval strategy and incorporates the use of ChatGPT. This implementation is designed to convert free-form predictions into pre-defined choices, thereby facilitating a more robust evaluation of the model's predictions.",https://production-media.paperswithcode.com/datasets/5fee3fd8-23d7-4741-9c2a-6d9e5963e5a8.jpeg,EditApache-2.0 license,"Image, Text",English,,,,,,,Visual Question Answering,visual-question-answering-on-mmbench,,See all 1951 tasks,Visual Question Answering31 be,Visual Question Answering31 be
MSRVTT-QA,MSRVTT-QA Dataset,"The MSR-VTT-QA dataset is a benchmark for the task of Visual Question Answering (VQA) on the MSR-VTT (Microsoft Research Video to Text) dataset. The MSR-VTT-QA benchmark is used to evaluate models on their ability to answer questions based on these videos. It's part of the tasks that this dataset is used for, along with Video Retrieval, Video Captioning, Zero-Shot Video Question Answering, Zero-Shot Video Retrieval, and Text-to-Video Generation.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text, Video",English,,,,,,,"Video Question Answering, Visual Question Answering, Zero-Shot Learning, Visual Question Answering (VQA), Zero-Shot Video Question Answer, Zeroshot Video Question Answer","visual-question-answering-on-msrvtt-qa-2, visual-question-answering-on-msrvtt-qa-1, video-question-answering-on-msrvtt-qa, zeroshot-video-question-answer-on-msrvtt-qa-1, zeroshot-video-question-answer-on-msrvtt-qa, zero-shot-learning-on-msrvtt-qa",,See all 1951 tasks,Visual Question Answering31 be,Visual Question Answering31 be
VizWiz,VizWiz Dataset,"The VizWiz-VQA dataset originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. The proposed challenge addresses the following two tasks for this dataset: predict the answer to a visual question and (2) predict whether a visual question cannot be answered.",https://vizwiz.org/tasks-and-datasets/vqa/,EditCC BY 4.0,"Image, Text",English,,,,,,,"Visual Question Answering, Visual Question Answering (VQA), Image Captioning","visual-question-answering-on-vizwiz-2020-vqa, visual-question-answering-on-vizwiz-1, visual-question-answering-on-vizwiz-2018, image-captioning-on-vizwiz-2020-test-dev, visual-question-answering-on-vizwiz-2020, visual-question-answering-on-vizwiz-2018-1, image-captioning-on-vizwiz-2020-test",,See all 1951 tasks,Visual Question Answering31 be,Visual Question Answering31 be
ActivityNet,ActivityNet Dataset,"The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.",https://arxiv.org/abs/1808.02536,EditUnknown,"Image, Text, Time Series, Video",English,,,,,,,"Zero-Shot Video Retrieval, Zero-Shot Action Recognition, Temporal Action Localization, ZSL Video Classification, Action Detection, Semi-Supervised Action Detection, Action Classification, Temporal Action Proposal Generation, Zero-Shot Action Detection, Visual Question Answering (VQA), Action Recognition In Videos, Video Retrieval, Action Recognition, GZSL Video Classification, Few Shot Temporal Action Localization, Weakly Supervised Action Localization","semi-supervised-action-detection-on, action-recognition-in-videos-on-activitynet-1, zsl-video-classification-on-activitynet-gzsl, action-classification-on-activitynet, action-classification-on-activitynet-12, gzsl-video-classification-on-activitynet-gzsl-1, few-shot-temporal-action-localization-on, zero-shot-action-detection-on-activitynet-1-3, temporal-action-localization-on-activitynet-1, temporal-action-proposal-generation-on, temporal-action-localization-on-activitynet, zero-shot-action-recognition-on-activitynet, weakly-supervised-action-localization-on-2, action-recognition-in-videos-on-activitynet, weakly-supervised-action-localization-on-1, zero-shot-video-retrieval-on-activitynet, visual-question-answering-vqa-on-activitynet-1, gzsl-video-classification-on-activitynet-gzsl, video-retrieval-on-activitynet",,See all 1951 tasks,Visual Question Answering  VQA,Visual Question Answering  VQA
CLEVR,CLEVR Dataset,"CLEVR (Compositional Language and Elementary Visual Reasoning) is a synthetic Visual Question Answering dataset. It contains images of 3D-rendered objects; each image comes with a number of highly compositional questions that fall into different categories. Those categories fall into 5 classes of tasks: Exist, Count, Compare Integer, Query Attribute and Compare Attribute. The CLEVR dataset consists of: a training set of 70k images and 700k questions, a validation set of 15k images and 150k questions, a test set of 15k images and 150k questions about objects, answers, scene graphs and functional programs for all train and validation images and questions. Each object present in the scene, aside of position, is characterized by a set of four attributes: 2 sizes: large, small, 3 shapes: square, cylinder, sphere, 2 material types: rubber, metal, 8 color types: gray, blue, brown, yellow, red, green, purple, cyan, resulting in 96 unique combinations.",https://arxiv.org/abs/1811.06529,EditCC BY 4.0,"Image, Text",English,,,,70k images,training set of 70k images,5,"Visual Question Answering, Visual Question Answering (VQA) Split B, Image Generation, Visual Question Answering (VQA), Visual Question Answering (VQA) Split A","visual-question-answering-vqa-split-b-on, visual-question-answering-vqa-split-a-on, image-generation-on-clevr, visual-question-answering-on-clevr-1, visual-question-answering-on-clevr",,See all 1951 tasks,Visual Question Answering  VQA,Visual Question Answering  VQA
TextVQA,TextVQA Dataset,"TextVQA is a dataset to benchmark visual reasoning based on text in images.
TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions.

Statistics
* 28,408 images from OpenImages
* 45,336 questions
* 453,360 ground truth answers",https://production-media.paperswithcode.com/datasets/f5989fab-b0bc-414f-bd10-feafda85c61b.png,EditCC BY 4.0,"Image, Text",English,,,,408 images,,,"Visual Question Answering, Visual Question Answering (VQA), Question Answering","visual-question-answering-on-textvqa-test-1, visual-question-answering-on-textvqa-test-2, visual-question-answering-vqa-on-textvqa",,See all 1951 tasks,Visual Question Answering  VQA,Visual Question Answering  VQA
Visual_Question_Answering_v2.0,Visual Question Answering v2.0 Dataset,"Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. It is the second version of the VQA dataset.


265,016 images (COCO and abstract scenes)
At least 3 questions (5.4 questions on average) per image
10 ground truth answers per question
3 plausible (but likely incorrect) answers per question
Automatic evaluation metric

The first version of the dataset was released in October 2015.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_1.57.28_PM.png,EditUnknown,"Image, Text",English,2015,,,016 images,,,"Visual Question Answering, Visual Question Answering (VQA)","visual-question-answering-on-vqa-v2-val-1, visual-question-answering-on-vqa-v2-val, visual-question-answering-on-vqa-v2-test-std-1, visual-question-answering-on-vqa-v2-1, visual-question-answering-on-vqa-v2-test-dev, visual-question-answering-on-vqa-v2-test-std, visual-question-answering-on-vqa-v2-test-dev-1",,See all 1951 tasks,Visual Question Answering  VQA,Visual Question Answering  VQA
TexRel,TexRel Dataset,"Green family of datasets for emergent communications on relations.

By comparison with other relations datasets, TexRel provides rapid training and experimentation, whilst being sufficiently large to avoid overfitting in the context of emergent communications.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-05-28_at_7.52.18_PM.png,EditMIT,"Graph, Image",,,,,,,,"Relational Pattern Learning, Relation Classification, Visual Relationship Detection, Few-Shot Relation Classification, Relation Extraction, Spatial Relation Recognition, Relational Reasoning",,,See all 1951 tasks,Visual Relationship Detection7,Visual Relationship Detection7
Visual_Relationship_Detection_Dataset,Visual Relationship Detection Dataset Dataset,"A dataset containing 5000 images with 37,993 thousand relationships. The dataset contains 100 object categories and 70 predicate categories connecting those objects together.",/paper/visual-relationship-detection-with-language-1,EditUnknown,Image,,,,,5000 images,,,Visual Relationship Detection,,,See all 1951 tasks,Visual Relationship Detection7,Visual Relationship Detection7
VRD,VRD Dataset,"The Visual Relationship Dataset (VRD) contains 4000 images for training and 1000 for testing annotated with visual relationships. Bounding boxes are annotated with a label containing 100 unary predicates. These labels refer to animals, vehicles, clothes and generic objects. Pairs of bounding boxes are annotated with a label containing 70 binary predicates. These labels refer to actions, prepositions, spatial relations, comparatives or preposition phrases. The dataset has 37993 instances of visual relationships and 6672 types of relationships. 1877 instances of relationships occur only in the test set and they are used to evaluate the zero-shot learning scenario.",https://arxiv.org/abs/1910.00462,EditUnknown,"Graph, Image, Text",English,,,,4000 images,,,"Relationship Detection, Scene Graph Detection, Scene Graph Generation, Visual Relationship Detection","scene-graph-generation-on-vrd, visual-relationship-detection-on-vrd-1, scene-graph-detection-on-vrd, visual-relationship-detection-on-vrd-phrase, visual-relationship-detection-on-vrd-2, visual-relationship-detection-on-vrd, relationship-detection-on-vrd",,See all 1951 tasks,Visual Relationship Detection7,Visual Relationship Detection7
Kubric,Kubric Dataset,"Kubric is a data generation pipeline for creating semi-realistic synthetic multi-object videos with rich annotations such as instance segmentation masks, depth maps, and optical flow.

It also presents a series of 13 different generated datasets for tasks ranging from studying 3D NeRF models to optical flow estimation.

Kubric is mainly built on-top of pybullet (for physics simulation) and Blender (for rendering); however, the code is kept modular to potentially support different rendering backends.",https://production-media.paperswithcode.com/datasets/a2fb933d-10eb-4934-88f2-9cd7744bf85e.png,EditApache License 2.0,"Image, Video",,,,,,,,Visual Tracking,visual-tracking-on-kubric,,See all 1951 tasks,Visual Tracking21 benchmarks19,Visual Tracking21 benchmarks19
TAP-Vid,TAP-Vid Dataset,"TAP-Vid is a benchmark which contains both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. This is designed for a new task called tracking any point.",https://production-media.paperswithcode.com/datasets/f7aa896a-cee1-4708-96f9-060d4524f258.png,EditCreative Commons Attribution 4.0 International License,"Image, Video",,,,,,,,"Visual Tracking, Point Tracking",point-tracking-on-tap-vid,,See all 1951 tasks,Visual Tracking21 benchmarks19,Visual Tracking21 benchmarks19
TNL2K,TNL2K Dataset,"Tracking by Natural Language (TNL2K) is constructed for the evaluation of tracking by natural language specification. TNL2K features:



Large-scale: 2,000 sequences, contains 1,244,340 frames, 663 words, 1300 / 700 for the train / testing respectively 



High-quality: Manual annotation with careful inspection in each frame 



Multi-modal: Providing visual and language annotation for each sequence 



Adversarial-samples: Randomly adding adversarial samples for research on adversarial attack and defence 



Significant-appearance-variation:  Containing videos with cloth/face change for pedestrian 



Heterogeneous: Containing RGB, thermal, Cartoon,  Synthetic data 



Multiple-baseline: Tracking-by-BBox, Tracking-by-Language, Tracking-by-Joint-BBox-Language",https://sites.google.com/view/langtrackbenchmark/,EditCustom,"Image, Video",,,,,,,,"Visual Object Tracking, Visual Tracking","visual-object-tracking-on-tnl2k, visual-tracking-on-tnl2k",,See all 1951 tasks,Visual Tracking21 benchmarks19,Visual Tracking21 benchmarks19
ESD,ESD Dataset,"ESD is an Emotional Speech Database for voice conversion research. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 hours of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies.",https://production-media.paperswithcode.com/datasets/esd.png,EditCustom (research-only),Audio,,,,,,,,Voice Conversion,,,See all 1951 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
GneutralSpeech_Female,GneutralSpeech Female Dataset,"A Brazilian Portuguese TTS dataset featuring a female voice
recorded with high quality in a controlled environment, with
neutral emotion and more than 20 hours of recordings.
 with
neutral emotion and more than 20 hours of recordings. Our
dataset aims to facilitate transfer learning for researchers and
developers working on TTS applications: a highly professional
neutral female voice can serve as a good warm-up stage for
learning language-specific structures, pronunciation and other
non-individual characteristics of speech, leaving to further
training procedures only to learn the specific adaptations
needed (e.g. timbre, emotion and prosody). This can surely
help enabling the accommodation of a more diverse range
of female voices in Brazilian Portuguese. By doing so, we
also hope to contribute to the development of accessible and
high-quality TTS systems for several use cases such as virtual
assistants, audiobooks, language learning tools and accessibility solutions.

Possible  use cases: 
TTS;
Voice Conversion;
ASR;
Speech Enhancement",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCustom,"Audio, Image, Text",English,,,,,,,"Text-To-Speech Synthesis, Automatic Speech Recognition (ASR), Speech Enhancement, Voice Cloning, Voice Conversion",,,See all 1951 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
GneutralSpeech_Male,GneutralSpeech Male Dataset,"A database containing high sampling rate recordings of a single speaker reading sentences in Brazilian
Portuguese with neutral voice, along with the corresponding text corpus.
Intended for speech synthesis and automatic speech recognition applications, the dataset contains text extracted from a popular Brazilian news
TV program, totalling roughly 20 h of audio spoken by a trained individual in a controlled environment. The text was normalized in the
recording process and special textual occurrences (e.g. acronyms, numbers, foreign names etc.) were replaced by their phonetic translation to
a readable text in Portuguese. There are no noticeable accidental sounds
and background noise has been kept to a minimum in all audio samples.

Intended for:
TTS
ASR
Speech Enhancement
Voice Conversion",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCustom,"Audio, Image, Text",English,,,,,,,"Text-To-Speech Synthesis, Automatic Speech Recognition (ASR), Speech Enhancement, Speech Synthesis, Voice Cloning, Voice Conversion",,,See all 1951 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
LibriSpeech,LibriSpeech Dataset,"The LibriSpeech corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ’clean’ and ’other’ categories, respectively, depending upon how well or challenging Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.",https://arxiv.org/abs/1910.00716,EditCC BY 4.0,"Audio, Image, Text",English,,,,,,,"Automatic Speech Recognition, Resynthesis, Voice Conversion, Speech Recognition","automatic-speech-recognition-on-librispeech-9, voice-conversion-on-librispeech-test-clean, automatic-speech-recognition-on-librispeech-8, automatic-speech-recognition-on-librispeech-10, speech-recognition-on-librispeech-test-clean, automatic-speech-recognition-on-librispeech-11, automatic-speech-recognition-on-librispeech-7, resynthesis-on-librispeech-1, speech-recognition-on-librispeech-test-other",,See all 1951 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
VCTK,VCTK Dataset,"This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald & Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, ""The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,"" https://doi.org/10.1109/ICSDA.2013.6709856. The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf. All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed.",https://datashare.is.ed.ac.uk/handle/10283/3443,EditCreative Commons License: Attribution 4.0 International,Audio,,2013,,,400 sentences,,,"Directional Hearing, Real-time Directional Hearing, Bandwidth Extension, Audio Super-Resolution, Voice Conversion","audio-super-resolution-on-voice-bank-corpus-1, voice-conversion-on-vctk, real-time-directional-hearing-on-vctk, bandwidth-extension-on-vctk, directional-hearing-on-vctk, audio-super-resolution-on-vctk-multi-speaker-1",,See all 1951 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
VESUS,VESUS Dataset,"The Varied Emotion in Syntactically Uniform Speech (VESUS) repository is a lexically controlled database collected by the NSA lab. Here, actors read a semantically neutral script of words, phrases, and sentences with different emotional inflections. VESUS contains 252 distinct phrases, each read by 10 actors in 5 emotional states (neutral, angry, happy, sad, fearful).",https://engineering.jhu.edu/nsa/vesus/,EditUnknown,"Audio, Image",,,,,,,,"Emotion Recognition, Voice Conversion",,,See all 1951 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
ACDC_Scribbles,ACDC Scribbles Dataset,"We release expert-made scribble annotations for the medical ACDC dataset [1]. The released data must be considered as extending the original ACDC dataset.
The ACDC dataset contains cardiac MRI images, paired with hand-made segmentation masks. It is possible to use the segmentation masks provided in the ACDC dataset to evaluate the performance of methods trained using only scribble supervision. 

References: 
[1] Bernard, Olivier, et al. ""Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: is the problem solved?."" IEEE transactions on medical imaging 37.11 (2018): 2514-2525.",https://production-media.paperswithcode.com/datasets/icon_19bWNjS.png,EditUnknown,Image,,2018,,,,,,"Medical Image Segmentation, Heart Segmentation, Weakly-Supervised Semantic Segmentation, Weakly supervised segmentation, Cardiac Segmentation, Semantic Segmentation",semantic-segmentation-on-acdc-scribbles,,See all 1951 tasks,Weakly-Supervised Semantic Seg,Weakly-Supervised Semantic Seg
CheXlocalize,CheXlocalize Dataset,"CheXlocalize is a radiologist-annotated segmentation dataset on chest X-rays. The dataset consists of two types of radiologist annotations for the localization of 10 pathologies: pixel-level segmentations and most-representative points. Annotations were drawn on images from the CheXpert validation and test sets. The dataset also consists of two separate sets of radiologist annotations: (1) ground-truth pixel-level segmentations on the validation and test sets, drawn by two board-certified radiologists, and (2) benchmark pixel-level segmentations and most-representative points on the test set, drawn by a separate group of three board-certified radiologists.

The validation and test sets consist of 234 chest X-rays from 200 patients and 668 chest X-rays from 500 patients, respectively. The 10 pathologies of interest are Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Lung Lesion, Lung Opacity, Pleural Effusion, Pneumothorax, and Support Devices.

For more details, please see our paper, Benchmarking saliency methods for chest X-ray interpretation.",https://production-media.paperswithcode.com/datasets/a3cf3492-c611-455c-a23c-1220a22d9988.png,EditMIT License,Image,,,,,,,,"2D Semantic Segmentation, Medical Image Segmentation, Weakly supervised Semantic Segmentation, Medical X-Ray Image Segmentation, Weakly-Supervised Semantic Segmentation, Weakly supervised segmentation, Semantic Segmentation, Weakly-Supervised Object Segmentation",,,See all 1951 tasks,Weakly-Supervised Semantic Seg,Weakly-Supervised Semantic Seg
PASCAL_VOC_2012_test,PASCAL VOC 2012 test Dataset,SCC Data Set,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Weakly Supervised Object Detection, Object Detection, Weakly-Supervised Semantic Segmentation, Semantic Segmentation","weakly-supervised-semantic-segmentation-on-1, weakly-supervised-object-detection-on-pascal, object-detection-on-pascal-voc-2012-test, semantic-segmentation-on-pascal-voc-2012",,See all 1951 tasks,Weakly-Supervised Semantic Seg,Weakly-Supervised Semantic Seg
ScribbleKITTI,ScribbleKITTI Dataset,ScribbleKITTI is a scribble-annotated dataset for LiDAR semantic segmentation.,https://production-media.paperswithcode.com/datasets/b50c946e-d8cc-40f5-b385-a5baea7b6379.png,EditUnknown,"3D, Image",English,,,,,,,"LIDAR Semantic Segmentation, Semi-Supervised Semantic Segmentation, Weakly Supervised 3D Point Cloud Segmentation, Weakly-Supervised Semantic Segmentation, 3D Semantic Segmentation","semi-supervised-semantic-segmentation-on-23, 3d-semantic-segmentation-on-scribblekitti",,See all 1951 tasks,Weakly-Supervised Semantic Seg,Weakly-Supervised Semantic Seg
ScribbleSup,ScribbleSup Dataset,"The PASCAL-Scribble Dataset is an extension of the PASCAL dataset with scribble annotations for semantic segmentation. The annotations follow two different protocols. In the first protocol, the PASCAL VOC 2012 set is annotated, with 20 object categories (aeroplane, bicycle, ...) and one background category. There are 12,031 images annotated, including 10,582 images in the training set and 1,449 images in the validation set.
In the second protocol, the 59 object/stuff categories and one background category involved in the PASCAL-CONTEXT dataset are used. Besides the 20 object categories in the first protocol, there are 39 extra categories (snow, tree, ...) included. This protocol is followed to annotate the PASCAL-CONTEXT dataset. 4,998 images in the training set have been annotated.",https://jifengdai.org/downloads/scribble_sup/,EditUnknown,Image,,2012,,,031 images,"training set and 1,449 images",,"Instance Segmentation, Weakly-Supervised Semantic Segmentation, Semantic Segmentation",,,See all 1951 tasks,Weakly-Supervised Semantic Seg,Weakly-Supervised Semantic Seg
SEN12MS,SEN12MS Dataset,"A dataset consisting of 180,662 triplets of dual-pol synthetic aperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches, and MODIS land cover maps.",/paper/sen12ms-a-curated-dataset-of-georeferenced,EditUnknown,Image,,,,,,,,"Weakly-Supervised Semantic Segmentation, Semantic Segmentation, Scene Classification",,,See all 1951 tasks,Weakly-Supervised Semantic Seg,Weakly-Supervised Semantic Seg
EgoSchema,EgoSchema Dataset,"EgoSchema is very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text, Video",English,,,,,,,"Visual Question Answering (VQA), Zero-Shot Video Question Answer","zero-shot-video-question-answer-on-egoschema, visual-question-answering-vqa-on-egoschema, zero-shot-video-question-answer-on-egoschema-1",,See all 1951 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
TVQA_,TVQA+ Dataset,"TVQA+ contains 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers.",https://arxiv.org/pdf/1904.11574v2.pdf,EditUnknown,"Image, Text, Video",English,,,,,,,"Video Question Answering, Zero-Shot Learning, Visual Question Answering (VQA), Zero-Shot Video Question Answer, Question Answering","video-question-answering-on-tvqa, zero-shot-video-question-answer-on-tvqa, zero-shot-learning-on-tvqa",,See all 1951 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
Video-MME,Video-MME Dataset,"Video-MME stands for Video Multi-Modal Evaluation. It is the first-ever comprehensive evaluation benchmark specifically designed for Multi-modal Large Language Models (MLLMs) in video analysis¹. This benchmark is significant because it addresses the need for a high-quality assessment of MLLMs' performance in processing sequential visual data, which has been less explored compared to their capabilities in static image understanding.

The Video-MME benchmark is characterized by its:
1. Diversity in video types, covering 6 primary visual domains with 30 subfields for broad scenario generalizability.
2. Duration in the temporal dimension, including short-, medium-, and long-term videos ranging from 11 seconds to 1 hour, to assess robust contextual dynamics.
3. Breadth in data modalities, integrating multi-modal inputs such as video frames, subtitles, and audios.
4. Quality in annotations, with rigorous manual labeling by expert annotators for precise and reliable model assessment¹.

The benchmark includes 900 videos totaling 256 hours, manually selected and annotated, resulting in 2,700 question-answer pairs. It has been used to evaluate various state-of-the-art MLLMs, including the GPT-4 series and Gemini 1.5 Pro, as well as open-source image and video models¹. The findings from Video-MME highlight the need for further improvements in handling longer sequences and multi-modal data, which is crucial for the advancement of MLLMs¹.

(1) [2405.21075] Video-MME: The First-Ever Comprehensive Evaluation .... https://arxiv.org/abs/2405.21075.
(2) Video-MME. https://video-mme.github.io/home_page.html.
(3) Video-MME: Welcome. https://video-mme.github.io/.
(4) undefined. https://doi.org/10.48550/arXiv.2405.21075.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Video,,,,,,,,Zero-Shot Video Question Answer,"zero-shot-video-question-answer-on-video-mme-1, zero-shot-video-question-answer-on-video-mme",,See all 1951 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
CaDIS,CaDIS Dataset,CaDIS: a Cataract Dataset for Image Segmentation is a dataset for semantic segmentation created by Digital Surgery Ltd. on top of the CATARACTS dataset. CaDIS consists of 4670 images sampled from the 25 videos on CATARACTS' training set. Each pixel in each image is labeled with its respective instrument or anatomical class from a set of 36 identified classes. More details about the dataset could be found in the paper (https://arxiv.org/pdf/1906.11586.pdf).,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,1906,,,4670 images,,,"2D Semantic Segmentation, 2D Semantic Segmentation task 1 (8 classes), 2D Semantic Segmentation task 2 (17 classes), 2D Semantic Segmentation task 3 (25 classes)","2d-semantic-segmentation-task-1-8-classes-on, 2d-semantic-segmentation-task-3-25-classes-on, 2d-semantic-segmentation-task-2-17-classes-on",,See all 844 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
CamVid,CamVid Dataset,"CamVid (Cambridge-driving Labeled Video Database) is a road/driving scene understanding database which was originally captured as five video sequences with a 960×720 resolution camera mounted on the dashboard of a car. Those sequences were sampled (four of them at 1 fps and one at 15 fps) adding up to 701 frames. Those stills were manually annotated with 32 classes: void, building, wall, tree, vegetation, fence, sidewalk, parking block, column/pole, traffic cone, bridge, sign, miscellaneous text, traffic light, sky, tunnel, archway, road, road shoulder, lane markings (driving), lane markings (non-driving), animal, pedestrian, child, cart luggage, bicyclist, motorcycle, car, SUV/pickup/truck, truck/bus, train, and other moving object",https://arxiv.org/abs/1704.06857,EditUnknown,"Image, Video",,,,,,,32,"2D Semantic Segmentation, Real-Time Semantic Segmentation, Semantic Segmentation, Video Semantic Segmentation","2d-semantic-segmentation-on-camvid, semantic-segmentation-on-camvid, real-time-semantic-segmentation-on-camvid, video-semantic-segmentation-on-camvid",,See all 844 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
ISTD_,ISTD+ Dataset,"ISTD+ consists of shadow images, shadow-free images, and shadow masks, with 1,330 training images and 540 testing images from 135 unique background scenes. ISTD suffers from color and luminosity inconsistencies between shadow and shadow-free images, which ISTD+ corrects with a color compensation mechanism to ensure uniform pixel colors across the ground-truth images.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,training images and 540 testing images,,"2D Semantic Segmentation, Shadow Removal",shadow-removal-on-istd-1,,See all 844 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
KITTI-360,KITTI-360 Dataset,"KITTI-360 is a large-scale dataset that contains rich sensory information and full annotations. It is the successor of the popular KITTI dataset,  providing more comprehensive semantic/instance labels in 2D and 3D, richer 360 degree sensory information (fisheye images and pushbroom laser scans), very accurate and geo-localized vehicle and camera poses, and a series of new challenging benchmarks.",https://production-media.paperswithcode.com/datasets/89bce839-f270-49f4-8f76-7d20eb99a1f2.jpg,"EditAll datasets and benchmarks on this page are copyright by us and published under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License. This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license. Per GDPR requirements, to download and to use the data you need to register and specify the i","3D, Image",English,,,,,,,"2D Semantic Segmentation, 3D Semantic Scene Completion, Semantic SLAM, Instance Segmentation, Weakly Supervised 3D Detection, Panoptic Segmentation, Novel View Synthesis, 3D Instance Segmentation, 3D Semantic Segmentation, 3D Semantic Scene Completion from a single RGB image, Semantic Segmentation, 3D Object Detection From Monocular Images","3d-semantic-scene-completion-on-kitti-360, weakly-supervised-3d-detection-on-kitti-360, panoptic-segmentation-on-kitti-360, 3d-semantic-scene-completion-from-a-single-2, 3d-object-detection-from-monocular-images-on-7, 3d-semantic-segmentation-on-kitti-360, semantic-segmentation-on-kitti-360",,See all 844 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
KITTI_MOTS,KITTI MOTS Dataset,"The Multi-Object and Segmentation (MOTS) benchmark 2 consists of 21 training sequences and 29 test sequences. It is based on the KITTI Tracking Evaluation 2012 and extends the annotations to the Multi-Object and Segmentation (MOTS) task. To this end, we added dense pixel-wise segmentation labels for every object. We evaluate submitted results using the metrics HOTA, CLEAR MOT, and MT/PT/ML. We rank methods by HOTA 1. Our development kit and GitHub evaluation code provide details about the data format as well as utility functions for reading and writing the label files. (adapted for the segmentation case). Evaluation is performed using the code from the TrackEval repository.

1 J. Luiten, A. Os̆ep, P. Dendorfer, P. Torr, A. Geiger, L. Leal-Taixé, B. Leibe: HOTA: A Higher Order Metric for Evaluating Multi-object Tracking. IJCV 2020.
2 P. Voigtlaender, M. Krause, A. Os̆ep, J. Luiten, B. Sekar, A. Geiger, B. Leibe: MOTS: Multi-Object Tracking and Segmentation. CVPR 2019.",https://production-media.paperswithcode.com/datasets/31c8042e-2eff-4210-8948-f06f76b41b54.jpg,EditCreative Commons Attribution-NonCommercial-ShareAlike 3.0,"Image, Video",English,2012,,,,,,"2D Semantic Segmentation, Multi-Object Tracking, Multi-Object Tracking and Segmentation, 2D Object Detection, Object Tracking, Multiple Object Tracking",multi-object-tracking-and-segmentation-on-1,,See all 844 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
Open_Images_V7,Open Images V7 Dataset,"Open Images is a computer vision dataset covering ~9 million images with labels spanning thousands of object categories. A subset of 1.9M includes diverse annotations types.


15,851,536 boxes on 600 classes
2,785,498 instance segmentations on 350 classes
3,284,280 relationship annotations on 1,466 relationships
675,155 localized narratives (synchronized voice, mouse trace, and text caption)
66,391,027 point-level annotations on 5,827 classes
61,404,966 image-level labels on 20,638 classes

Images are under a  CC BY 2.0 license, annotations under  CC BY 4.0 license.",https://production-media.paperswithcode.com/datasets/fcd9ec8c-20c9-4224-9dd6-5a11f97e9029.png,EditCC BY 4.0,"Audio, Image, Text",English,,,,,,600,"2D Semantic Segmentation, Object Detection, Visual Relationship Detection, Instance Segmentation, Image Captioning, 2D Object Detection, Semantic Segmentation, Speech Recognition, Image Classification",,,See all 844 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
SpaceNet_7,SpaceNet 7 Dataset,"Satellite imagery analytics have numerous human development and disaster response applications, particularly when time series methods are involved. For example, quantifying population statistics is fundamental to 67 of the 232 United Nations Sustainable Development Goals, but the World Bank estimates that more than 100 countries currently lack effective Civil Registration systems. The SpaceNet 7 Multi-Temporal Urban Development Challenge aims to help address this deficit and develop novel computer vision methods for non-video time series data. In this challenge, participants will identify and track buildings in satellite imagery time series collected over rapidly urbanizing areas. The competition centers around a new open source dataset of Planet satellite imagery mosaics, which includes 24 images (one per month) covering ~100 unique geographies. The dataset will comprise over 40,000 square kilometers of imagery and exhaustive polygon labels of building footprints in the imagery, totaling over 10 million individual annotations. Challenge participants will be asked to track building construction over time, thereby directly assessing urbanization.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-ShareAlike 4.0 International License,Image,,,,,24 images,,,2D Semantic Segmentation,,,See all 844 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
WaterScenes,WaterScenes Dataset,"A Multi-Task 4D Radar-Camera Fusion Dataset for Autonomous Driving on Water Surfaces description of the dataset 


WaterScenes, the first multi-task 4D radar-camera fusion dataset on water surfaces, which offers data from multiple sensors, including a 4D radar, monocular camera, GPS, and IMU. It can be applied in multiple tasks, such as object detection, instance segmentation, semantic segmentation, free-space segmentation, and waterline segmentation.
Our dataset covers diverse time conditions (daytime, nightfall, night), lighting conditions (normal, dim, strong), weather conditions (sunny, overcast, rainy, snowy) and waterway conditions (river, lake, canal, moat). An information list is also offered for retrieving specific data for experiments under different conditions.
We provide 2D box-level and pixel-level annotations for camera images, and 3D point-level annotations for radar point clouds. We also offer precise timestamps for the synchronization of different sensors, as well as intrinsic and extrinsic parameters.
We provide a toolkit for radar point clouds that includes: pre-processing, labeling, projection and visualization, assisting researchers in processing and analyzing our dataset.",https://production-media.paperswithcode.com/datasets/da2004b6-79c9-4bf1-8a8b-c196b656f822.png,EditUnknown,"3D, Image",,,,,,,,"2D Semantic Segmentation, Object Detection, Instance Segmentation, Panoptic Segmentation, Line Segment Detection, Semantic Segmentation, Point Cloud Segmentation","2d-semantic-segmentation-on-waterscenes, object-detection-on-waterscenes",,See all 844 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
xBD,xBD Dataset,"The xBD dataset contains over 45,000KM2 of polygon labeled pre and post disaster imagery. The dataset provides the post-disaster imagery with transposed polygons from pre over the buildings, with damage classification labels.",https://github.com/DIUx-xView/xview2-baseline,EditCC BY-NC-SA 4.0,Image,,,,,,,,"2D Semantic Segmentation, Disaster Response, Data Augmentation, Semantic Segmentation, Extracting Buildings In Remote Sensing Images","2d-semantic-segmentation-on-xbd, extracting-buildings-in-remote-sensing-images",,See all 844 tasks,2D Semantic Segmentation113 be,2D Semantic Segmentation113 be
DeepWeeds,DeepWeeds Dataset,"The DeepWeeds dataset consists of 17,509 images capturing eight different weed species native to Australia in situ with neighbouring flora.",https://github.com/AlexOlsen/DeepWeeds,EditUnknown,Image,,,,,509 images,,,"Active Learning, Semantic Segmentation, Robust classification",,,See all 844 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
DialoGLUE,DialoGLUE Dataset,"DialoGLUE is a natural language understanding benchmark for task-oriented dialogue designed to encourage dialogue research in representation-based transfer, domain adaptation, and sample-efficient task learning. It consisting of 7 task-oriented dialogue datasets covering 4 distinct natural language understanding tasks.",/paper/dialoglue-a-natural-language-understanding,EditUnknown,Text,English,,,,,,,"Active Learning, Dialogue Management, Natural Language Understanding","natural-language-understanding-on-dialoglue-1, natural-language-understanding-on-dialoglue",,See all 844 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
FDST,FDST Dataset,The Fudan-ShanghaiTech dataset (FDST) is a dataset for video crowd counting. It contains 15K frames with about 394K annotated heads captured from 13 different scenes,https://arxiv.org/abs/1907.07911,EditUnknown,,,,,,,,,"Active Learning, Binarization, Crowd Counting",,,See all 844 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
Groove,Groove Dataset,"The Groove MIDI Dataset (GMD) is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming. The dataset contains 1,150 MIDI files and over 22,000 measures of drumming.",https://www.tensorflow.org/datasets/catalog/groove,EditUnknown,"Audio, Image, Text, Video",English,,,,,,,"Music Generation, Beat Tracking, Quantization, Active Learning, Downbeat Tracking","beat-tracking-on-groove, downbeat-tracking-on-groove",,See all 844 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
MNIST-8M,MNIST-8M Dataset,MNIST8M is derived from the MNIST dataset by applying random deformations and translations to the dataset.,https://arxiv.org/abs/1602.08194,EditUnknown,,,,,,,,,"Stochastic Optimization, Distributed Computing, Active Learning",,,See all 844 tasks,Active Learning3 benchmarks884,Active Learning3 benchmarks884
BeNYfits,BeNYfits Dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

An agent benchmark for adaptive decision-making in dialog measuring agent accuracy and dialog turn efficiency in helping users determine eligibility for public, real-world opportunities.

This dataset containss the following:
* Natural language descriptions of 82 NYC public benefits programs (e.g., tax credits, childcare, subsidized AC)

Two simulated user datasets containing all features relevant to that household's eligibility:


Representative: 25 households with features drawn from real NYC demographic data
Diverse: 56 households which, collectively, satisfy nearly every possible acceptance/rejection criteria in all 82 public benefits opportunities",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Conversational Response Generation, Conversational Search, Task-Oriented Dialogue Systems, AI Agent, Decision Making, Sequential Decision Making",,,See all 844 tasks,AI Agent1 benchmark89 papers w,AI Agent1 benchmark89 papers w
DevAI,DevAI Dataset,"DEVAI is a benchmark of 55 realistic AI development tasks. It consists of plentiful manual annotations, including a total of 365 hierarchical user requirements. This dataset enables rich reinforcement signals for better automated AI software development.",https://production-media.paperswithcode.com/datasets/f3f7a914-71a1-445d-90f2-6030ef1494dd.png,EditUnknown,,,,,,,,,AI Agent,,,See all 844 tasks,AI Agent1 benchmark89 papers w,AI Agent1 benchmark89 papers w
Plancraft,Plancraft Dataset,An evaluation dataset for planning with LLM agents,https://production-media.paperswithcode.com/datasets/0983cedb-b605-4c01-afaa-0a8796406cd6.gif,EditMIT,,,,,,,,,"Task Planning, AI Agent",,,See all 844 tasks,AI Agent1 benchmark89 papers w,AI Agent1 benchmark89 papers w
ACOS,ACOS Dataset,"Most of the aspect based sentiment analysis research aims at identifying the sentiment polarities toward some explicit aspect terms while ignores implicit aspects in text. To capture both explicit and implicit aspects, we focus on aspect-category based sentiment analysis, which involves joint aspect category detection and category-oriented sentiment classification. However, currently only a few simple studies have focused on this problem. The shortcomings in the way they defined the task make their approaches difficult to effectively learn the inner-relations between categories and the inter-relations between categories and sentiments. In this work, we re-formalize the task as a category-sentiment hierarchy prediction problem, which contains a hierarchy output structure to first identify multiple aspect categories in a piece of text, and then predict the sentiment for each of the identified categories. Specifically, we propose a Hierarchical Graph Convolutional Network (Hier-GCN), where a lower-level GCN is to model the inner-relations among multiple categories, and the higher-level GCN is to capture the inter-relations between aspect categories and sentiments. Extensive evaluations demonstrate that our hierarchy output structure is superior over existing ones, and the Hier-GCN model can consistently achieve the best results on four benchmarks.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-absa-on-acos,,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
ASQP,ASQP Dataset,"Aspect-based sentiment analysis (ABSA) typically focuses on extracting aspects and predicting their sentiments on individual sentences such as customer reviews. Recently, another kind of opinion sharing platform, namely question answering (QA) forum, has received increasing popularity, which accumulates a large number of user opinions towards various aspects. This motivates us to investigate the task of ABSA on QA forums (ABSA-QA), aiming to jointly detect the discussed aspects and their sentiment polarities for a given QA pair. Unlike review sentences, a QA pair is composed of two parallel sentences, which requires interaction modeling to align the aspect mentioned in the question and the associated opinion clues in the answer. To this end, we propose a model with a specific design of cross-sentence aspect-opinion interaction modeling to address this task. The proposed method is evaluated on three real-world datasets and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-absa-on-asqp,,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
ASTE,ASTE Dataset,"Target-based sentiment analysis or aspect-based sentiment analysis (ABSA) refers to addressing various sentiment analysis tasks at a fine-grained level, which includes but is not limited to aspect extraction, aspect sentiment classification, and opinion extraction. There exist many solvers of the above individual subtasks or a combination of two subtasks, and they can work together to tell a complete story, i.e. the discussed aspect, the sentiment on it, and the cause of the sentiment. However, no previous ABSA research tried to provide a complete solution in one shot. In this paper, we introduce a new subtask under ABSA, named aspect sentiment triplet extraction (ASTE). Particularly, a solver of this task needs to extract triplets (What, How, Why) from the inputs, which show WHAT the targeted aspects are, HOW their sentiment polarities are and WHY they have such polarities (i.e. opinion reasons). For instance, one triplet from “Waiters are very friendly and the pasta is simply average” could be (‘Waiters’, positive, ‘friendly’). We propose a two-stage framework to address this task. The first stage predicts what, how and why in a unified model, and then the second stage pairs up the predicted what (how) and why from the first stage to output triplets. In the experiments, our framework has set a benchmark performance in this novel triplet extraction task. Meanwhile, it outperforms a few strong baselines adapted from state-of-the-art related methods.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-absa-on-aste,,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
CAIL2019-SCM,CAIL2019-SCM Dataset,"Chinese AI and Law 2019 Similar Case Matching dataset. CAIL2019-SCM contains 8,964 triplets of cases published by the Supreme People's Court of China. CAIL2019-SCM focuses on detecting similar cases, and the participants are required to check which two cases are more similar in the triplets.",/paper/cail2019-scm-a-dataset-of-similar-case,EditUnknown,Text,English,2019,,,,,,"Aspect-Based Sentiment Analysis (ABSA), Semantic Text Matching, Sentiment Analysis","semantic-text-matching-on-cail2019-scm-val, semantic-text-matching-on-cail2019-scm-test",,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
Laptop-ACOS,Laptop-ACOS Dataset,"Laptop-ACOS is a brand new Laptop dataset collected from the Amazon platform in the years 2017 and 2018 (covering ten types of laptops under six brands such as ASUS, Acer, Samsung, Lenovo, MBP, MSI, and so on). It contains 4,076 review sentences, much larger than the SemEval Laptop datasets.
For Laptop-ACOS, we annotate the four elements and their corresponding quadruples all by ourselves. We employ the aspect categories defined in the SemEval 2016 Laptop dataset. The Laptop-ACOS dataset contains 4076 sentences with 5758 quadruples. As we have mentioned, a large percentage of the quadruples contain implicit aspects or implicit opinions .  By comparing two datasets, it can be observed that Laptop-ACOS has a higher percentage of implicit opinions than Restaurant-ACOS . It is worth noting that the Laptop-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,2017,,,4076 sentences,,,"Aspect-Based Sentiment Analysis (ABSA), Aspect-Category-Opinion-Sentiment Quadruple Extraction, Sentiment Analysis",aspect-category-opinion-sentiment-quadruple-1,,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
MAMS,MAMS Dataset,"MAMS is a challenge dataset for aspect-based sentiment analysis (ABSA), in which each sentences contain at least two aspects with different sentiment polarities. MAMS dataset contains two versions: one for aspect-term sentiment analysis (ATSA) and one for aspect-category sentiment analysis (ACSA).",https://github.com/siat-nlp/MAMS-for-ABSA,EditUnknown,Text,English,,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-on-mams,,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
Restaurant-ACOS,Restaurant-ACOS Dataset,"The Restaurant-ACOS dataset is constructed based on the SemEval 2016 Restaurant dataset (Pontiki et al., 2016) and its expansion datasets (Fan et al., 2019; Xu et al., 2020).
The SemEval 2016 Restaurant dataset (Pontiki et al., 2016) was annotated with explicit and implicit aspects, categories, and sentiment. (Fan et al., 2019; Xu et al., 2020) further added the opinion annotations. We integrate their annotations to construct aspect-category-opinion-sentiment quadruples and further annotate the implicit opinions. The Restaurant-ACOS dataset contains 2286 sentences with 3658 quadruples.
It is worth noting that the Restaurant-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,2016,,,2286 sentences,,,"Aspect-Based Sentiment Analysis (ABSA), Aspect-Category-Opinion-Sentiment Quadruple Extraction, Sentiment Analysis",aspect-category-opinion-sentiment-quadruple,,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
SemEval-2014_Task-4,SemEval-2014 Task-4 Dataset,"Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided.

Subtask 2: Aspect term polarity

For a given set of aspect terms within a sentence, determine whether the polarity of each aspect term is positive, negative, neutral or conflict (i.e., both positive and negative).

For example:

“I loved their fajitas” → {fajitas: positive}
“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive}
“The fajitas are their first plate” → {fajitas: neutral}
“The fajitas were great to taste, but not to see” → {fajitas: conflict}",https://huggingface.co/datasets/Charitarth/SemEval2014-Task4,EditUnknown,"Image, Text",English,,,,,,,"Aspect-oriented  Opinion Extraction, Aspect-Based Sentiment Analysis, Aspect Category Detection, Aspect Extraction, Aspect-Based Sentiment Analysis (ABSA)","aspect-extraction-on-semeval-2014-task-4-sub-1, aspect-category-detection-on-semeval-2014-1, aspect-oriented-opinion-extraction-on-semeval, aspect-based-sentiment-analysis-on-semeval-10, aspect-based-sentiment-analysis-on-semeval",,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
TASD,TASD Dataset,"Aspect-based sentiment analysis (ABSA) aims to detect the targets (which are composed by continuous words), aspects and sentiment polarities in text. Published datasets from SemEval-2015 and SemEval-2016 reveal that a sentiment polarity depends on both the target and the aspect. However, most of the existing methods consider predicting sentiment polarities from either targets or aspects but not from both, thus they easily make wrong predictions on sentiment polarities. In particular, where the target is implicit, i.e., it does not appear in the given text, the methods predicting sentiment polarities from targets do not work. To tackle these limitations in ABSA, this paper proposes a novel method for target-aspect-sentiment joint detection. It relies on a pre-trained language model and can capture the dependence on both targets and aspects for sentiment prediction. Experimental results on the SemEval-2015 and SemEval-2016 restaurant datasets show that the proposed method achieves a high performance in detecting target-aspect-sentiment triples even for the implicit target cases; moreover, it even outperforms the state-of-the-art methods for those subtasks of target-aspect-sentiment detection that they are competent to.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,2015,,,,,,Aspect-Based Sentiment Analysis (ABSA),aspect-based-sentiment-analysis-absa-on-tasd,,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
Twitter_Sentiment_Analysis,Twitter Sentiment Analysis Dataset,"This is an entity-level Twitter Sentiment Analysis dataset. For each message, the task is to judge the sentiment of the entire sentence towards a given entity. For example, A outperforms B is positive for entity A but negative for entity B. The dataset contains ~70K labeled training messages and 1K labeled validation messages. It is available online for free on Kaggle.",https://production-media.paperswithcode.com/datasets/dataset-cover.jpeg,EditCC0: Public Domain,"Image, Text",English,,,,,,,"Aspect-Based Sentiment Analysis (ABSA), Text Classification, Twitter Sentiment Analysis",text-classification-on-twitter-sentiment-1,,See all 844 tasks,Aspect-Based Sentiment Analysi,Aspect-Based Sentiment Analysi
ASTE-Data-V2,ASTE-Data-V2 Dataset,"A benchmark dataset for the Aspect Sentiment Triplet Extraction, an updated version of ASTE-Data-V1.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Aspect Sentiment Triplet Extraction,aspect-sentiment-triplet-extraction-on-aste,,See all 844 tasks,Aspect Sentiment Triplet Extra,Aspect Sentiment Triplet Extra
MuseASTE,MuseASTE Dataset,"•A new benchmark dataset for Aspect Sentiment Triplet Extraction.
•First Aspect Sentiment Triplet Extraction (ASTE) Dataset in Automotive Domain.
•Largest ASTE Dataset to date with annotations for over 28,295 sentences.
•Dataset includes complex aspects not verbatim present in the sentence.
•Domain: Aspect-based sentiment analysis, ASTE, Opinion Mining, Recommender System.
•Four baseline SOTA models implemented on the dataset",https://production-media.paperswithcode.com/datasets/5a159f4f-ad7d-47e6-b795-69678b4d0f4e.jpeg,EditCreative Commons Licenses 4.0,Text,English,,,,295 sentences,,,"Aspect-Based Sentiment Analysis (ABSA), Aspect Sentiment Triplet Extraction",aspect-sentiment-triplet-extraction-on-1,,See all 844 tasks,Aspect Sentiment Triplet Extra,Aspect Sentiment Triplet Extra
AudioSet,AudioSet Dataset,"Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.",https://arxiv.org/abs/2001.09414,EditCC BY 4.0,"Audio, Image",,,,,,,,"Zero-shot Audio Classification, Audio Classification, Audio Tagging, Multi-modal Classification, Target Sound Extraction, Audio Source Separation","audio-classification-on-audioset, audio-source-separation-on-audioset, zero-shot-audio-classification-on-audioset, target-sound-extraction-on-audioset, multi-modal-classification-on-audioset, audio-tagging-on-audioset",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
Common_Voice,Common Voice Dataset,"Common Voice is an audio dataset that consists of a unique MP3 and corresponding text file. There are 9,283 recorded hours in the dataset. The dataset also includes demographic metadata like age, sex, and accent. The dataset consists of 7,335 validated hours in 60 languages.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-07_at_8.52.43_PM.png,EditCC0,"Audio, Image, Text",English,,,,,,,"Audio Classification, Language Identification, Few-Shot Audio Classification, Speech Recognition, Automatic Speech Recognition, Cross-Lingual ASR","speech-recognition-on-common-voice-8-0-29, speech-recognition-on-common-voice-dutch, automatic-speech-recognition-on-mozilla-71, automatic-speech-recognition-on-mozilla-63, audio-classification-on-common-voice-16-1, speech-recognition-on-common-voice-8-0-38, speech-recognition-on-common-voice-8-0-10, speech-recognition-on-common-voice-turkish, speech-recognition-on-common-voice-8-0-39, speech-recognition-on-common-voice-indonesian, speech-recognition-on-common-voice-8-0-uzbek, speech-recognition-on-common-voice-7-0-arabic, speech-recognition-on-common-voice-8-0-tatar, speech-recognition-on-common-voice-8-0-odia, speech-recognition-on-common-voice-8-0-erzya, speech-recognition-on-common-voice-8-0-kazakh, speech-recognition-on-common-voice-8-0-french, speech-recognition-on-common-voice-czech, speech-recognition-on-common-voice-8-0-41, speech-recognition-on-common-voice-frisian, speech-recognition-on-common-voice-arabic, speech-recognition-on-common-voice-7-0-29, automatic-speech-recognition-on-common-voice-18, speech-recognition-on-common-voice-7-0-german, speech-recognition-on-common-voice-8-0-german, speech-recognition-on-common-voice-8-0-9, speech-recognition-on-common-voice-7-0-abkhaz, speech-recognition-on-common-voice-8-0-33, automatic-speech-recognition-on-mozilla-84, speech-recognition-on-common-voice-welsh, automatic-speech-recognition-on-mozilla-66, speech-recognition-on-common-voice-2, automatic-speech-recognition-on-mozilla-108, few-shot-audio-classification-on-common-voice, automatic-speech-recognition-on-mozilla-64, speech-recognition-on-common-voice-8-0-20, speech-recognition-on-common-voice-8-0-11, speech-recognition-on-common-voice-8-0-hausa, automatic-speech-recognition-on-common-voice-17, speech-recognition-on-common-voice-maltese, speech-recognition-on-common-voice-swedish, speech-recognition-on-common-voice-8-0-breton, speech-recognition-on-common-voice-breton, speech-recognition-on-common-voice-persian, automatic-speech-recognition-on-mozilla-96, speech-recognition-on-common-voice-vi, speech-recognition-on-common-voice-8-0-basaa, speech-recognition-on-common-voice-russian, speech-recognition-on-common-voice-japanese, speech-recognition-on-common-voice-odia, speech-recognition-on-common-voice-vietnamese, speech-recognition-on-common-voice-7-0-1, speech-recognition-on-common-voice-8-0-dutch, speech-recognition-on-common-voice-8-0-24, speech-recognition-on-common-voice-7-0-hindi, automatic-speech-recognition-on-mozilla-114, speech-recognition-on-common-voice-spanish, speech-recognition-on-common-voice-8-0-3, speech-recognition-on-common-voice-8-0-37, automatic-speech-recognition-on-mozilla-126, speech-recognition-on-common-voice-8-0-30, speech-recognition-on-common-voice-8-0-2, speech-recognition-on-common-voice-lithuanian, speech-recognition-on-mozilla-common-voice-16, cross-lingual-asr-on-common-voice, speech-recognition-on-common-voice-polish, speech-recognition-on-common-voice-portuguese, speech-recognition-on-common-voice-georgian, automatic-speech-recognition-on-commonvoice-8, automatic-speech-recognition-on-mcv17, speech-recognition-on-common-voice-8-0-1, speech-recognition-on-common-voice-french, speech-recognition-on-mozilla-common-voice-9, automatic-speech-recognition-on-mozilla-127, speech-recognition-on-common-voice-8-0-kabyle, speech-recognition-on-common-voice-8-0-32, speech-recognition-on-common-voice-german, speech-recognition-on-common-voice-italian, speech-recognition-on-common-voice-8-0-votic, speech-recognition-on-common-voice-chinese-2, speech-recognition-on-common-voice-hindi, speech-recognition-on-common-voice-8-0-7, automatic-speech-recognition-on-commonvoice-4, speech-recognition-on-common-voice-7-0-votic, speech-recognition-on-common-voice-8-0-hindi, automatic-speech-recognition-on-mozilla-125, speech-recognition-on-common-voice-8-0-8, speech-recognition-on-common-voice-8-0-40, speech-recognition-on-common-voice-7-0-odia, speech-recognition-on-common-voice-english, speech-recognition-on-mozilla-common-voice-15, speech-recognition-on-common-voice-tamil, speech-recognition-on-common-voice-8-0-22",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
CREMA-D,CREMA-D Dataset,"CREMA-D is an emotional multimodal actor data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified).

Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).

Participants rated the emotion and emotion levels based on the combined audiovisual presentation, the video alone, and the audio alone. Due to the large number of ratings needed, this effort was crowd-sourced and a total of 2443 participants each rated 90 unique clips, 30 audio, 30 visual, and 30 audio-visual. 95% of the clips have more than 7 ratings.",https://www.tensorflow.org/datasets/catalog/crema_d,EditUnknown,"Audio, Image, Text, Video",English,,,,12 sentences,,,"Talking Face Generation, Audio Classification, Speech Emotion Recognition, Video Emotion Recognition, Facial Expression Recognition (FER), Few-Shot Audio Classification, Self-Supervised Learning","talking-face-generation-on-crema-d, video-emotion-recognition-on-crema-d, few-shot-audio-classification-on-crema-d, facial-expression-recognition-on-crema-d, self-supervised-learning-on-crema-d, speech-emotion-recognition-on-crema-d, audio-classification-on-crema-d",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
EPIC-KITCHENS-100,EPIC-KITCHENS-100 Dataset,"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the ""test of time"" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit ""two years on"".
The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.",https://production-media.paperswithcode.com/datasets/EPIC-Kitchens-0000000300-2485dab7_8WDhYkq_AkL1bgW.jpg,EditCC BY NC 4.0,"Audio, Image, Time Series, Video",,2018,,,,,,"Audio Classification, Open Vocabulary Action Recognition, Temporal Action Localization, Action Anticipation, Multi-Instance Retrieval, Action Recognition, Unsupervised Domain Adaptation","audio-classification-on-epic-kitchens-100, action-recognition-on-epic-kitchens-100, unsupervised-domain-adaptation-on-epic, action-anticipation-on-epic-kitchens-100, temporal-action-localization-on-epic-kitchens, multi-instance-retrieval-on-epic-kitchens-100, open-vocabulary-action-recognition-on-epic",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
ESC-50,ESC-50 Dataset,"The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org.",https://arxiv.org/abs/1902.08314,EditCC BY-NC 3.0,"Audio, Image",,2000,,,,,,"Zero-shot Audio Classification, Self-Supervised Audio Classification, Environment Sound Classification, Audio Classification, Data Augmentation, Environmental Sound Classification, Few-Shot Audio Classification, Image Classification, Zero-Shot Environment Sound Classification","audio-classification-on-esc-50, zero-shot-environment-sound-classification-on-1, zero-shot-audio-classification-on-esc-50, few-shot-audio-classification-on-esc-50, environmental-sound-classification-on-esc-50, environment-sound-classification-on-esc-50, self-supervised-audio-classification-on-esc, image-classification-on-esc-50",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
FSD50K,FSD50K Dataset,"Freesound Dataset 50k (or FSD50K for short) is an open dataset of human-labeled sound events containing 51,197 Freesound clips unequally distributed in 200 classes drawn from the AudioSet Ontology. FSD50K has been created at the Music Technology Group of Universitat Pompeu Fabra. It consists mainly of sound events produced by physical sound sources and production mechanisms, including human sounds, sounds of things, animals, natural sounds, musical instruments and more.",https://zenodo.org/record/4060432,EditOther (Attribution),"Audio, Image",,,,,,,200,"Contrastive Learning, Audio Classification, Environmental Sound Classification, Domain Adaptation","environmental-sound-classification-on-fsd50k, audio-classification-on-fsd50k",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
Speech_Commands,Speech Commands Dataset,Speech Commands is an audio dataset of spoken words designed to help train and evaluate keyword spotting systems .,https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.12.27_PM.png,EditCC BY,"Audio, Image, Text, Time Series",English,,,,,,,"Audio Classification, Keyword Spotting, Time Series Analysis, Speech Recognition, Federated Learning","time-series-on-speech-commands, audio-classification-on-speech-commands-1, keyword-spotting-on-google-speech-commands, speech-recognition-on-speech-commands-2",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
UCR_Time_Series_Classification_Archive,UCR Time Series Classification Archive Dataset,"The UCR Time Series Archive - introduced in 2002,
has become an important resource in the time series data mining
community, with at least one thousand published papers making
use of at least one data set from the archive. The original
incarnation of the archive had sixteen data sets but since that
time, it has gone through periodic expansions. The last expansion
took place in the summer of 2015 when the archive grew from
45 to 85 data sets. This paper introduces and will focus on the
new data expansion from 85 to 128 data sets. Beyond expanding
this valuable resource, this paper offers pragmatic advice to
anyone who may wish to evaluate a new algorithm on the archive.
Finally, this paper makes a novel and yet actionable claim: of the
hundreds of papers that show an improvement over the standard
baseline (1-nearest neighbor classification), a large fraction may
be misattributing the reasons for their improvement. Moreover,
they may have been able to achieve the same improvement with
a much simpler modification, requiring just a single line of code.",https://github.com/yuezhihan/ts2vec/blob/main/datasets/preprocess_electricity.py,EditUnknown,"Audio, Image, Time Series",,2002,,,,,,"Time Series Averaging, Audio Classification, Time Series Classification, Time Series Clustering, ECG Classification","ecg-classification-on-ucr-time-series, audio-classification-on-ucr-time-series",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
UrbanSound8K,UrbanSound8K Dataset,"Urban Sound 8K is an audio dataset that contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. All excerpts are taken from field recordings uploaded to www.freesound.org.",https://zenodo.org/record/401395,EditCC BY-NC 3.0,"Audio, Image",,,,,,,10,"Zero-shot Audio Classification, Environment Sound Classification, Audio Classification, Data Augmentation, Environmental Sound Classification","environmental-sound-classification-on, zero-shot-audio-classification-on, environment-sound-classification-on",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
VGG-Sound,VGG-Sound Dataset,Consists of more than 210k videos for 310 audio classes.,/paper/vggsound-a-large-scale-audio-visual-dataset,EditCC-BY 4.0,"Audio, Image, Text, Video",English,,,,,,,"Zero-shot Audio Classification, Video-to-Sound Generation, Audio Classification, Multi-modal Classification, Speaker Recognition, Image Classification, Speaker Verification","multi-modal-classification-on-vgg-sound, audio-classification-on-vggsound, zero-shot-audio-classification-on-vgg-sound, video-to-sound-generation-on-vgg-sound",,See all 844 tasks,Audio Classification28 benchma,Audio Classification28 benchma
Apron_Dataset,Apron Dataset Dataset,The Apron Dataset focuses on training and evaluating classification and detection models for airport-apron logistics. In addition to bounding boxes and object categories the dataset is enriched with meta parameters to quantify the models’ robustness against environmental influences.,https://production-media.paperswithcode.com/datasets/e875bf93-0135-41c7-8401-238077a6f2e8.png,EditCustom (non-commercial),Image,,,,,,,,"Robust Object Detection, Object Detection, Fine-Grained Image Classification, Small Object Detection, Benchmarking, Domain Adaptation, Autonomous Driving, Scene Understanding",,,See all 844 tasks,Benchmarking2 benchmarks2303 p,Benchmarking2 benchmarks2303 p
COCO-N_Medium,COCO-N Medium Dataset,"COCO-N Medium introduces a stochastic benchmark that simulates common real-world scenarios with noticeable label inaccuracies in the COCO dataset. This benchmark combines class and spatial noises to create a challenging yet realistic evaluation framework for instance segmentation models. It mimics datasets manually annotated by crowd workers, where a moderate level of label noise is expected. By incorporating both class and spatial inaccuracies, COCO-N Medium allows researchers to assess their models' basic robustness to label noise, providing insights into performance in typical real-world applications where perfect annotations are rare. This medium-level benchmark serves as a crucial middle ground, offering a more rigorous test than minimally noisy datasets while remaining within the bounds of commonly encountered data quality issues. COCO-N Medium enables a nuanced evaluation of model performance under realistic conditions, helping identify areas for improvement in handling noisy labels and guiding the development of more robust instance segmentation algorithms.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,English,,,,,,,"Learning with noisy labels, Instance Segmentation, Benchmarking",instance-segmentation-on-coco-n-medium,,See all 844 tasks,Benchmarking2 benchmarks2303 p,Benchmarking2 benchmarks2303 p
CropAndWeed,CropAndWeed Dataset,"The CropAndWeed dataset is focused on the fine-grained identification of 74 relevant crop and weed species with a strong emphasis on data variability. Annotations of labeled bounding boxes, semantic masks and stem positions are provided for about 112k instances in more than 8k high-resolution images of both real-world agricultural sites and specifically cultivated outdoor plots of rare weed types. Additionally, each sample is enriched with meta-annotations regarding environmental conditions.",https://production-media.paperswithcode.com/datasets/f7a87479-ba9a-49c7-8821-fa1d6012716a.png,EditCustom (non-commercial),"Image, Time Series",,,,,112k instances,,,"Object Detection, Crop Yield Prediction, Multi-Task Learning, Fine-Grained Image Classification, Instance Segmentation, Panoptic Segmentation, Benchmarking, Crop Classification, Domain Adaptation, Plant Phenotyping, Semantic Segmentation",,,See all 844 tasks,Benchmarking2 benchmarks2303 p,Benchmarking2 benchmarks2303 p
Europarl-ASR,Europarl-ASR Dataset,"Europarl-ASR (EN) is a 1300-hour English-language speech and text corpus of parliamentary debates for (streaming) Automatic Speech Recognition training and benchmarking, speech data filtering and speech data verbatimization, based on European Parliament speeches and their official transcripts (1996-2020). Includes dev-test sets for streaming ASR benchmarking, made up of 18 hours of manually revised speeches. The availability of manual non-verbatim and verbatim transcripts for dev-test speeches makes this corpus also useful for the assessment of automatic filtering and verbatimization techniques. The corpus is released under an open licence at https://www.mllp.upv.es/europarl-asr/

Europarl-ASR CONTENTS: [Speech data] 1300 hours of English-language annotated speech data, 3 full sets of timed transcriptions (official non-verbatim, automatically noise-filtered, automatically verbatimized), 18 hours of speech data with both manually revised verbatim transcriptions and official non-verbatim transcriptions, split in 2 independent validation- evaluation partitions for 2 realistic ASR tasks (with vs. without previous knowledge of the speaker); [Text data] 70 million tokens of English-language text data; [Pretrained language models] the Europarl-ASR English-language n-gram language model and vocabulary.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCustom,"Audio, Image, Text",English,1996,,,,,,"Benchmarking, Data Augmentation, Speech Recognition","speech-recognition-on-europarl-asr-en-guest, speech-recognition-on-europarl-asr-en-mep",,See all 844 tasks,Benchmarking2 benchmarks2303 p,Benchmarking2 benchmarks2303 p
FluidLab,FluidLab Dataset,FluidLab is a simulation environment with a diverse set of manipulation tasks involving complex fluid dynamics. These tasks address interactions between solid and fluid as well as among multiple fluids.,https://arxiv.org/pdf/2303.02346v1.pdf,EditUnknown,,,,,,,,,Benchmarking,,,See all 844 tasks,Benchmarking2 benchmarks2303 p,Benchmarking2 benchmarks2303 p
Wiki-40B,Wiki-40B Dataset,A new multilingual language model benchmark that is composed of 40+ languages spanning several scripts and linguistic families containing round 40 billion characters and aimed to accelerate the research of multilingual modeling.,https://www.shaip.com/blog/multimodal-large-language-models-mllms/,EditUnknown,Text,English,,,,,,,"Language Modelling, Quantization, Benchmarking","benchmarking-on-wiki-40b, quantization-on-wiki-40b, language-modelling-on-wiki-40b",,See all 844 tasks,Benchmarking2 benchmarks2303 p,Benchmarking2 benchmarks2303 p
Bilingual_Lexicon_Induction35_papers_with_code_Dat,Bilingual Lexicon Induction35 papers with code Dataset,,https://paperswithcode.com/dataset/bilingual-lexicon-induction,,,,,,,,,,,,,See all 844 tasks,Bilingual Lexicon Induction35 ,Bilingual Lexicon Induction35 
ChartQA,ChartQA Dataset,"Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Chart Question Answering,chart-question-answering-on-chartqa,,See all 844 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
DVQA,DVQA Dataset,DVQA is a synthetic question-answering dataset on images of bar-charts.,https://production-media.paperswithcode.com/datasets/dvqa.jpg,EditAttribution-NonCommercial 4.0 International,"Image, Text",English,,,,,,,"Object Detection, Visual Question Answering (VQA), Chart Question Answering, Question Answering",visual-question-answering-vqa-on-dvqa-test,,See all 844 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
FigureQA,FigureQA Dataset,"FigureQA is a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts.",/paper/figureqa-an-annotated-figure-dataset-for,EditCustom,"Image, Text",English,,,,000 images,,,"Visual Question Answering (VQA), Chart Question Answering, Visual Reasoning, Question Answering",visual-question-answering-on-figureqa-test-1,,See all 844 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
LEAF-QA,LEAF-QA Dataset,"LEAF-QA, a comprehensive dataset of 250,000 densely annotated figures/charts, constructed from real-world open data sources, along with ~2 million question-answer (QA) pairs querying the structure and semantics of these charts. LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community. Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz. FigureQA and DVQA, which present only limited variations in chart data. LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering.",https://arxiv.org/pdf/1907.12861,EditUnknown,"Image, Text",English,,,,,,,"Visual Question Answering (VQA), Chart Question Answering, Question Answering",,,See all 844 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
MMVP,MMVP Dataset,"The MMVP (Multimodal Visual Patterns) Benchmark focuses on identifying ""CLIP-blind pairs"" – images that appear similar to the CLIP model despite having clear visual differences. These patterns highlight the challenges these systems face in answering straightforward questions, often leading to incorrect responses and hallucinated explanations.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Text",English,,,,,,,"Music Question Answering, Chart Question Answering",,,See all 844 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
PlotQA,PlotQA Dataset,"PlotQA is a VQA dataset with 28.9 million question-answer pairs grounded over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates.
Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not contain variability in data labels, real-valued data, or complex reasoning questions. Consequently, proposed models for these datasets do not fully address the challenge of reasoning over plots. In particular, they assume that the answer comes either from a small fixed size vocabulary or from a bounding box within the image. However, in practice this is an unrealistic assumption because many questions require reasoning and thus have real valued answers which appear neither in a small fixed size vocabulary nor in the image. In this work, we aim to bridge this gap between existing datasets and real world plots by introducing PlotQA. Further, 80.76% of the out-of-vocabulary (OOV) questions in PlotQA have answers that are not in a fixed vocabulary.",https://production-media.paperswithcode.com/datasets/0c7a7156-5529-470c-87cd-09ee847f0f03.png,Editpublic,"Image, Text",English,,,,,,,"Visual Question Answering, Visual Question Answering (VQA), Chart Question Answering, Question Answering","visual-question-answering-on-plotqa-d2, chart-question-answering-on-plotqa, visual-question-answering-on-plotqa-d2-1, visual-question-answering-on-plotqa-d1, visual-question-answering-on-plotqa-d1-1",,See all 844 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
RealCQA,RealCQA Dataset,"RealCQA
Scientific Chart Question Answering as a Test-bed for First-Order Logic

check on huggingface : https://huggingface.co/datasets/sal4ahm/RealCQA",https://production-media.paperswithcode.com/datasets/a726c05f-49ef-4eea-bdf1-20147163aa1b.png,"EditGNU GENERAL PUBLIC LICENSE                        Version 3, 29 June 2007",Text,English,,,,,,,Chart Question Answering,chart-question-answering-on-realcqa,,See all 844 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
SBS_Figures,SBS Figures Dataset,"Building a large-scale figure QA dataset requires a considerable amount of work, from gathering and selecting figures to extracting attributes like text, numbers, and colors, and generating QAs. Although recent developments in LLMs have led to efforts to synthesize figures, most of these focus primarily on QA generation. Additionally, creating figures directly using LLMs often encounters issues such as code errors, similar-looking figures, and repetitive content in figures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic Figures), a dataset for pre-training figure QA. Our proposed pipeline enables the creation of chart figures with complete annotations of the visualized data and dense QA annotations without any manual annotation process. Our stage-by-stage pipeline makes it possible to create diverse topic and appearance figures efficiently while minimizing code errors. Our SBSFigures demonstrate a strong pre-training effect, making it possible to achieve efficient training with a limited amount of real-world chart data starting from our pre-trained weights.",https://production-media.paperswithcode.com/datasets/e21ad6a4-a241-4b1d-ae3f-b9f8fa1a7969.png,EditUnknown,Text,English,,,,,,,"Chart Understanding, Chart Question Answering",,,See all 844 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
Vega-Lite_Chart_Collection,Vega-Lite Chart Collection Dataset,"We present a new collection of 1,981 Vega-Lite specifications, which is used to demonstrate the generalizability and viability of our NL generation framework. This collection is the largest set of human-generated charts obtained from GitHub to date. It covers varying levels of complexity from a simple line chart without any interaction to a chart with four plots where data points are linked with selection interactions. Compared to the benchmarks, our dataset shows the highest average pairwise edit distance between specifications, which proves that the charts are highly diverse from one another. Moreover, it contains the largest number of charts with composite views, interactions (e.g., tooltips, panning & zooming, and linking), and diverse chart types (e.g., map, grid & matrix, diagram, etc.).",https://production-media.paperswithcode.com/datasets/67fb2170-2976-4c57-80d7-922d7402e038.png,EditMIT,Text,English,,,,,,,Chart Question Answering,,,See all 844 tasks,Chart Question Answering3 benc,Chart Question Answering3 benc
CLUENER2020,CLUENER2020 Dataset,CLUENER2020 is a well-defined fine-grained dataset for named entity recognition in Chinese. CLUENER2020 contains 10 categories.,/paper/cluener2020-fine-grained-name-entity,EditUnknown,"Image, Text",English,,,,,,10,"Chinese Named Entity Recognition, Named Entity Recognition (NER)",,,See all 844 tasks,Chinese Named Entity Recogniti,Chinese Named Entity Recogniti
OntoNotes_4.0,OntoNotes 4.0 Dataset,"OntoNotes Release 4.0 contains the content of earlier releases -- OntoNotes Release 1.0 LDC2007T21, OntoNotes Release 2.0 LDC2008T04 and OntoNotes Release 3.0 LDC2009T24 -- and adds newswire, broadcast news, broadcast conversation and web data in English and Chinese and newswire data in Arabic. This cumulative publication consists of 2.4 million words as follows: 300k words of Arabic newswire 250k words of Chinese newswire, 250k words of Chinese broadcast news, 150k words of Chinese broadcast conversation and 150k words of Chinese web text and 600k words of English newswire, 200k word of English broadcast news, 200k words of English broadcast conversation and 300k words of English web text.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditLDC User Agreement for Non-Members,"Image, Text",English,,,,,,,Chinese Named Entity Recognition,chinese-named-entity-recognition-on-ontonotes,,See all 844 tasks,Chinese Named Entity Recogniti,Chinese Named Entity Recogniti
Resume_NER,Resume NER Dataset,Resume contains eight fine-grained entity categories -score from 74.5% to 86.88%.,https://arxiv.org/abs/1908.09138,EditUnknown,"Image, Text",English,,,,,,,Chinese Named Entity Recognition,chinese-named-entity-recognition-on-resume,,See all 844 tasks,Chinese Named Entity Recogniti,Chinese Named Entity Recogniti
Weibo_NER,Weibo NER Dataset,The Weibo NER dataset is a Chinese Named Entity Recognition dataset drawn from the social media website Sina Weibo.,https://arxiv.org/abs/1805.02023,EditUnknown,"Image, Text",English,,,,,,,"Chinese Named Entity Recognition, Fake News Detection","fake-news-detection-on-weibo-ner, chinese-named-entity-recognition-on-weibo-ner",,See all 844 tasks,Chinese Named Entity Recogniti,Chinese Named Entity Recogniti
CIFAR-10,CIFAR-10 Dataset,"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.",https://production-media.paperswithcode.com/datasets/4fdf2b82-2bc3-4f97-ba51-400322b228b1.png,EditUnknown,"Audio, Graph, Image, Text",English,,,,6000 images,training images and 10000 test images,10,"Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly, Image Classification with Label Noise, Density Estimation, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, ROLSSL-Reversed, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Personalized Federated Learning, Classification, Conditional Image Generation, Anomaly Detection, Dataset Distillation - 1IPC, Continual Learning, Active Learning, Open-World Semi-Supervised Learning, Network Pruning, Graph Classification, Image Clustering, Zero-Shot Learning, Binarization, Novel Class Discovery, Out-of-Distribution Detection, Image Compression, Small Data Image Classification, Image Classification, Nature-Inspired Optimization Algorithm, ROLSSL-Consistent, Sparse Learning and binarization, Semi-Supervised Image Classification (Cold Start), Learning with noisy labels, Stochastic Optimization, Contrastive Learning, Classification with Binary Neural Network, Image Retrieval, Clean-label Backdoor Attack (0.05%), Online Clustering, Supervised Image Retrieval, Adversarial Robustness, ROLSSL-Uniform, Semi-Supervised Image Classification, Unsupervised Image Classification, Object Recognition, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Neural Network Compression, Partial Label Learning, Adversarial Defense, Classification with Binary Weight Network, Model Poisoning, Hard-label Attack, Image Classification with Human Noise, Quantization, Data Augmentation, Robust classification, Sequential Image Classification, Image Generation, Long-tail Learning on CIFAR-10-LT (ρ=100), Neural Architecture Search, Adversarial Attack, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, Self-Supervised Learning, Long-tail Learning, Provable Adversarial Defense, Transductive Zero-Shot Classification, Domain-IL Continual Learning","architecture-search-on-cifar-10-image, image-classification-with-label-noise-on-5, image-clustering-on-cifar-10, semi-supervised-image-classification-cold, neural-architecture-search-on-nats-bench-size-1, anomaly-detection-on-cifar-10, conditional-image-generation-on-cifar-10, robust-classification-on-cifar-10, density-estimation-on-cifar-10-conditional, small-data-image-classification-on-cifar-10-3, image-classification-with-human-noise-on, image-classification-with-label-noise-on-7, data-augmentation-on-cifar-10, nature-inspired-optimization-algorithm-on-1, partial-label-learning-on-cifar-10-partial-2, image-generation-on-cifar-10, long-tail-learning-on-cifar-10-lt-r-100, semi-supervised-image-classification-on-cifar-12, small-data-image-classification-on-cifar10-10, semi-supervised-image-classification-on-cifar-27, image-generation-on-cifar-10-20-data, classification-with-binary-weight-network-on, out-of-distribution-detection-on-cifar-10, continual-learning-on-split-cifar-10-5-tasks, semi-supervised-image-classification-on-cifar-17, binarization-on-cifar-10, unsupervised-anomaly-detection-with-specified-21, anomaly-detection-on-one-class-cifar-10, image-classification-on-cifar-10-image, neural-architecture-search-on-cifar-10, image-classification-with-label-noise-on-8, image-classification-on-cifar-104000, provable-adversarial-defense-on-cifar-10, image-classification-on-cifar10-1, open-world-semi-supervised-learning-on-cifar, personalized-federated-learning-on-cifar-10, image-classification-on-cifar-10-60-symmetric, dataset-distillation-1ipc-on-cifar-10, density-estimation-on-cifar-10, adversarial-robustness-on-cifar-10, partial-label-learning-on-cifar-10-partial-1, self-supervised-learning-on-cifar10, small-data-image-classification-on-cifar-10-1, zero-shot-learning-on-cifar-10, out-of-distribution-detection-on-cifar10-1, semi-supervised-image-classification-on-cifar-6, active-learning-on-cifar10-10000, image-compression-on-cifar-10, long-tail-learning-on-cifar-10-lt-r-10, stochastic-optimization-on-cifar-10-resnet-18, sparse-learning-and-binarization-on-cifar-10, small-data-image-classification-on-cifar-10-2, semi-supervised-image-classification-cold-2, semi-supervised-image-classification-on-cifar-11, model-poisoning-on-cifar-10, stochastic-optimization-on-cifar-10, classification-with-binary-neural-network-on, unsupervised-image-classification-on-cifar-10, rolssl-uniform-on-cifar-10, image-retrieval-on-cifar-10, image-generation-on-cifar-10-10-data, anomaly-detection-on-leave-one-class-out, sequential-image-classification-on-noise, rolssl-consistent-on-cifar-10, transductive-zero-shot-classification-on-7, quantization-on-cifar-10, supervised-image-retrieval-on-cifar-10, image-classification-with-label-noise-on-3, semi-supervised-image-classification-on-cifar-28, unsupervised-anomaly-detection-with-specified-9, adversarial-attack-on-cifar-10, learning-with-noisy-labels-on-cifar-10, self-supervised-learning-on-cifar-10, online-clustering-on-cifar10, clean-label-backdoor-attack-0-05-on-cifar-10, small-data-image-classification-on-cifar-10, image-classification-with-label-noise-on-14, novel-class-discovery-on-cifar10, semi-supervised-image-classification-on-cifar-10, image-classification-with-label-noise-on-4, semi-supervised-image-classification-on-cifar-16, graph-classification-on-cifar-10, unsupervised-anomaly-detection-with-specified-6, semi-supervised-image-classification-on-cifar-15, semi-supervised-image-classification-cold-8, network-pruning-on-cifar-10, graph-classification-on-cifar10-100k, domain-il-continual-learning-on-cifar10-5, semi-supervised-image-classification-on-cifar-7, neural-network-compression-on-cifar-10, contrastive-learning-on-cifar-10, image-classification-on-cifar-10, semi-supervised-image-classification-on-cifar, rolssl-reversed-on-cifar-10, image-classification-with-label-noise-on-6, stochastic-optimization-on-cifar-10-wrn-28-10, hard-label-attack-on-cifar-10, long-tail-learning-on-cifar-10-lt-r-100-on, image-classification-with-label-noise-on-9, adversarial-defense-on-cifar-10, image-classification-on-cifar-10-40-symmetric, semi-supervised-image-classification-on-3, out-of-distribution-detection-on-cifar-10-vs, image-classification-on-cifar-10-with-noisy, partial-label-learning-on-cifar-10-partial, classification-on-cifar10-1, unsupervised-anomaly-detection-with-specified-16, unsupervised-anomaly-detection-with-specified-7",,See all 844 tasks,Classification370 benchmarks37,Classification370 benchmarks37
CIFAR-100,CIFAR-100 Dataset,"The CIFAR-100 dataset (Canadian Institute for Advanced Research, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 images per class. Each image comes with a ""fine"" label (the class to which it belongs) and a ""coarse"" label (the superclass to which it belongs). There are 500 training images and 100 testing images per class.

The criteria for deciding whether an image belongs to a class were as follows:


The class name should be high on the list of likely answers to the question “What is in this picture?”
The image should be photo-realistic. Labelers were instructed to reject line drawings.
The image should contain only one prominent instance of the object to which the class refers.
The object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.",https://www.cs.toronto.edu/~kriz/cifar.html,EditUnknown,"Audio, Graph, Image, Text",English,,,,600 images,training images and 100 testing images,100,"Image Classification with Label Noise, Personalized Federated Learning, Classification, Conditional Image Generation, Knowledge Distillation, Anomaly Detection, Dataset Distillation - 1IPC, Continual Learning, Class Incremental Learning, Open-World Semi-Supervised Learning, Few-Shot Class-Incremental Learning, Network Pruning, Non-exemplar-based Class Incremental Learning, Image Clustering, Variational Inference, Zero-Shot Learning, Binarization, Learning with coarse labels, Novel Class Discovery, Out-of-Distribution Detection, Small Data Image Classification, Image Classification, Classifier calibration, Sparse Learning and binarization, Data Free Quantization, Learning with noisy labels, Stochastic Optimization, Classification with Binary Neural Network, Bayesian Inference, Incremental Learning, class-incremental learning, Adversarial Robustness, Semi-Supervised Image Classification, Adversarial Defense, Classification with Binary Weight Network, Few-Shot Image Classification, Image Generation, Neural Architecture Search, Adversarial Attack, Self-Supervised Learning, Long-tail Learning, Provable Adversarial Defense, Transductive Zero-Shot Classification","semi-supervised-image-classification-on-cifar-2, provable-adversarial-defense-on-cifar-100, few-shot-image-classification-on-cifar100-5, out-of-distribution-detection-on-cifar-100, image-classification-on-cifar100, incremental-learning-on-cifar-100-50-classes-3, adversarial-defense-on-cifar-100, few-shot-class-incremental-learning-on-cifar, class-incremental-learning-on-cifar100, novel-class-discovery-on-cifar100, image-generation-on-cifar-100, network-pruning-on-cifar-100, class-incremental-learning-on-cifar-100-50-1, semi-supervised-image-classification-on-cifar-8, learning-with-noisy-labels-on-cifar-100, anomaly-detection-on-unlabeled-cifar-10-vs, data-free-quantization-on-cifar-100, classification-with-binary-neural-network-on-2, incremental-learning-on-cifar-100-50-classes, classifier-calibration-on-cifar-100, sparse-learning-and-binarization-on-cifar-100, image-classification-with-label-noise-on-15, stochastic-optimization-on-cifar-100, learning-with-coarse-labels-on-cifar100, personalized-federated-learning-on-cifar-100, self-supervised-learning-on-cifar-100, classification-on-cifar-100, semi-supervised-image-classification-on-cifar-4, out-of-distribution-detection-on-cifar100, open-world-semi-supervised-learning-on-cifar-1, long-tail-learning-on-cifar-100-lt-r-10, incremental-learning-on-cifar-100-50-classes-2, semi-supervised-image-classification-on-cifar-9, classification-with-binary-weight-network-on-2, dataset-distillation-1ipc-on-cifar-100, conditional-image-generation-on-cifar-100, binarization-on-cifar-100, non-exemplar-based-class-incremental-learning, anomaly-detection-on-one-class-cifar-100, continual-learning-on-cifar100-20-tasks, semi-supervised-image-classification-on-cifar-3, classification-on-cifar100, small-data-on-cifar-100-1000-labels-1, class-incremental-learning-on-cifar-100-50-2, adversarial-robustness-on-cifar-100, long-tail-learning-on-cifar-100-lt-r-100, class-incremental-learning-on-cifar100-1, incremental-learning-on-cifar-100-50-classes-1, knowledge-distillation-on-cifar-100, variational-inference-on-cifar100, image-classification-on-cifar-100, image-clustering-on-cifar-100, self-supervised-learning-on-cifar100, bayesian-inference-on-cifar100, transductive-zero-shot-classification-on-8, zero-shot-learning-on-cifar-100, neural-architecture-search-on-cifar-100-1, adversarial-attack-on-cifar-100",,See all 844 tasks,Classification370 benchmarks37,Classification370 benchmarks37
DTD,DTD Dataset,The Describable Textures Dataset (DTD) contains 5640 texture images in the wild. They are annotated with human-centric attributes inspired by the perceptual properties of textures.,https://arxiv.org/abs/1911.02274,EditCustom (research-only),Image,,,,,,,,"Image Clustering, Prompt Engineering, Few-Shot Learning, Zero-Shot Learning, Classification, Neural Architecture Search, Image Classification, Transductive Zero-Shot Classification","prompt-engineering-on-dtd, neural-architecture-search-on-dtd, classification-on-dtd, zero-shot-learning-on-dtd, image-clustering-on-dtd, few-shot-learning-on-dtd, image-classification-on-dtd, transductive-zero-shot-classification-on-dtd",,See all 844 tasks,Classification370 benchmarks37,Classification370 benchmarks37
EuroSAT,EuroSAT Dataset,"Eurosat is a dataset and deep learning benchmark for land use and land cover classification. The dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images.",https://github.com/phelber/eurosat,EditUnknown,Image,,,,,,,10,"Image Clustering, Prompt Engineering, Few-Shot Learning, Zero-Shot Learning, Classification, Cross-Domain Few-Shot, Semantic Segmentation, Image Classification, Transductive Zero-Shot Classification","image-classification-on-eurosat, prompt-engineering-on-eurosat, image-clustering-on-eurosat, classification-on-eurosat, transductive-zero-shot-classification-on-1, few-shot-learning-on-eurosat, zero-shot-learning-on-eurosat, cross-domain-few-shot-on-eurosat",,See all 844 tasks,Classification370 benchmarks37,Classification370 benchmarks37
Food-101,Food-101 Dataset,"The  Food-101 dataset consists of 101 food categories with 750 training and 250 test images per category, making a total of 101k images. The labels for the test images have been manually cleaned, while the training set contains some noise.",https://arxiv.org/abs/1712.08730,EditUnknown,"Image, Text",English,,,,101k images,training and 250 test images,,"Image Clustering, Fine-Grained Image Classification, Learning with noisy labels, Zero-Shot Transfer Image Classification, Prompt Engineering, Multimodal Text and Image Classification, Few-Shot Learning, Document Text Classification, Zero-Shot Learning, Classification, Multi-Modal Document Classification, Neural Architecture Search, Image Compression, Image Classification, Transductive Zero-Shot Classification","multi-modal-document-classification-on-food, zero-shot-transfer-image-classification-on-17, transductive-zero-shot-classification-on-food, image-compression-on-food-101, fine-grained-image-classification-on-food-101, neural-architecture-search-on-food-101, few-shot-learning-on-food101, multimodal-text-and-image-classification-on-1, zero-shot-learning-on-food-101, learning-with-noisy-labels-on-food-101, classification-on-food101, prompt-engineering-on-food-101, image-clustering-on-food-101, image-classification-on-food-101-1, document-text-classification-on-food-101",,See all 844 tasks,Classification370 benchmarks37,Classification370 benchmarks37
GLUE,GLUE Dataset,"General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.",https://arxiv.org/abs/1908.06725,EditCustom (various),"Image, Text",English,,,,,,,"Linguistic Acceptability, Semantic Textual Similarity within Bi-Encoder, Stochastic Optimization, Model Compression, Few-Shot Learning, Semantic Textual Similarity, Natural Language Understanding, Data-free Knowledge Distillation, Natural Language Inference, Classification, QQP, Text Classification","few-shot-learning-on-glue-qqp, text-classification-on-glue-rte, data-free-knowledge-distillation-on-qnli, natural-language-inference-on-mrpc, natural-language-inference-on-qnli, text-classification-on-glue-mrpc, text-classification-on-sst-2, text-classification-on-glue-sst2, natural-language-inference-on-wnli, semantic-textual-similarity-on-mrpc, stochastic-optimization-on-cola, semantic-textual-similarity-within-bi-encoder, text-classification-on-glue-cola, few-shot-learning-on-mrpc, text-classification-on-glue-qqp, natural-language-understanding-on-glue, classification-on-sst-2, qqp-on-qqp, text-classification-on-glue, model-compression-on-qnli, natural-language-inference-on-glue, text-classification-on-glue-stsb, linguistic-acceptability-on-cola, classification-on-rte, natural-language-inference-on-rte",,See all 844 tasks,Classification370 benchmarks37,Classification370 benchmarks37
ImageNet,ImageNet Dataset,"The ImageNet dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection.
The publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld.
ILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”.
The ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.


Total number of non-empty WordNet synsets: 21841
Total number of images: 14197122
Number of images with bounding box annotations: 1,034,908
Number of synsets with SIFT features: 1000
Number of images with SIFT features: 1.2 million",https://arxiv.org/abs/1409.0575,"EditCustom (research, non-commercial)","3D, Graph, Image, Text",English,2010,,,,,,"Sparse Learning, Weakly Supervised Object Detection, Classification, Knowledge Distillation, Network Pruning, Image Super-Resolution, Image Compressed Sensing, Image Clustering, Feature Upsampling, Prompt Engineering, Zero-Shot Composed Image Retrieval (ZS-CIR), Medical Image Classification, Zero-Shot Learning, Binarization, Visual Question Answering (VQA), Image Classification, Image Reconstruction, Zero-Shot Transfer Image Classification, Weakly-Supervised Object Localization, Contrastive Learning, Model Compression, Few-Shot Learning, Classification with Binary Neural Network, Image Classification with Differential Privacy, Adversarial Robustness, Semi-Supervised Image Classification, Unsupervised Image Classification, Image Deblurring, Object Recognition, Image Colorization, Adversarial Defense, Image Inpainting, Few-Shot Image Classification, Quantization, Data Augmentation, Self-Supervised Image Classification, Zero-Shot Transfer Image Classification (CN), Color Image Denoising, Image Generation, Neural Architecture Search, Transductive Zero-Shot Classification, JPEG Decompression","weakly-supervised-object-localization-on-2, zero-shot-learning-on-imagenet, image-classification-with-dp-on-imagenet, quantization-on-imagenet, visual-question-answering-vqa-on-imagenet, transductive-zero-shot-classification-on, image-colorization-on-imagenet, zero-shot-composed-image-retrieval-zs-cir-on-5, few-shot-image-classification-on-imagenet-5, zero-shot-transfer-image-classification-on-1, semi-supervised-image-classification-on-2, self-supervised-image-classification-on-1, binarization-on-imagenet, sparse-learning-on-imagenet, color-image-denoising-on-imagenet-sigma100, color-image-denoising-on-imagenet-sigma200, classification-on-imagenet-1k, zero-shot-transfer-image-classification-on-3, semi-supervised-image-classification-on-1, prompt-engineering-on-imagenet-v2, medical-image-classification-on-imagenet, neural-architecture-search-on-imagenet, image-classification-on-imagenet-v2, unsupervised-image-classification-on-imagenet, contrastive-learning-on-imagenet-1k, semi-supervised-image-classification-on-16, data-augmentation-on-imagenet, image-super-resolution-on-imagenet, network-pruning-on-imagenet, jpeg-decompression-on-imagenet, adversarial-defense-on-imagenet, color-image-denoising-on-imagenet-sigma250, weakly-supervised-object-detection-on, few-shot-image-classification-on-imagenet-10, knowledge-distillation-on-imagenet, zero-shot-transfer-image-classification-cn-on, classification-with-binary-neural-network-on-1, image-clustering-on-imagenet, color-image-denoising-on-imagenet-sigma50, color-image-denoising-on-imagenet-sigma150, few-shot-image-classification-on-imagenet-1-1, image-deblurring-on-imagenet, image-inpainting-on-imagenet, model-compression-on-imagenet, prompt-engineering-on-imagenet, image-classification-on-imagenet-1k, self-supervised-image-classification-on, image-classification-on-imagenet, feature-upsampling-on-imagenet, adversarial-robustness-on-imagenet, image-compressed-sensing-on-imagenet, image-reconstruction-on-imagenet, image-clustering-on-imagenet-1k, few-shot-image-classification-on-imagenet-0",,See all 844 tasks,Classification370 benchmarks37,Classification370 benchmarks37
SST-2,SST-2 Dataset,"The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.

Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,2005,,,,,,"Few-Shot Learning, Explanation Fidelity Evaluation, Classification, Text Classification, Sentiment Analysis","text-classification-on-sst2, few-shot-learning-on-sst-2-binary, text-classification-on-sst-2, sentiment-analysis-on-sst-2-binary, explanation-fidelity-evaluation-on-sst2, classification-on-sst-2",,See all 844 tasks,Classification370 benchmarks37,Classification370 benchmarks37
SST,SST Dataset,"The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser and includes a total of 215,154 unique phrases
from those parse trees, each annotated by 3 human judges.

Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive.
The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.",https://production-media.paperswithcode.com/datasets/sst.jpg,EditUnknown,"Image, Text",English,2005,,,,,5,"Few-Shot Text Classification, Few-Shot Learning, Explanation Fidelity Evaluation, Classification, Out-of-Distribution Detection, Text Classification, Sentiment Analysis","out-of-distribution-detection-on-sst, sentiment-analysis-on-sst-5-fine-grained, explanation-fidelity-evaluation-on-sst-5, text-classification-on-sst2, few-shot-text-classification-on-sst-5, few-shot-learning-on-sst-2-binary, text-classification-on-sst-2, sentiment-analysis-on-sst-2-binary, explanation-fidelity-evaluation-on-sst2, classification-on-sst-2",,See all 844 tasks,Classification370 benchmarks37,Classification370 benchmarks37
Conformal_Prediction242_papers_with_code_Dataset,Conformal Prediction242 papers with code Dataset,,https://paperswithcode.com/dataset/conformal-prediction,,,,,,,,,,,,,See all 844 tasks,Conformal Prediction242 papers,Conformal Prediction242 papers
Earth_on_Canvas,Earth on Canvas Dataset,"A Zero-Shot Sketch-based Inter-Modal Object Retrieval Scheme for Remote Sensing Images

WITH the advancement in sensor technology, huge amounts of data are being collected from various satellites. Hence, the task of target-based data retrieval and acquisition has become exceedingly challenging. Existing satellites essentially scan a vast overlapping region of the Earth using various sensing techniques, like multi-spectral, hyperspectral, Synthetic Aperture Radar (SAR), video, and compressed sensing, to name a few. With increasing complexity and different sensing techniques at our disposal, it has become our primary interest to design efficient algorithms to retrieve data from multiple data modalities, given the complementary information that is captured by different sensors. This type of problem is referred to as inter-modal data retrieval. In remote sensing (RS), there are primarily two important types of problems, i.e., land-cover classification and object detection. In this work, we focus on the target-based object retrieval part, which falls under the realm of object detection in RS. Object retrieval essentially requires high-resolution imagery for objects to be distinctly visible in the image. The main challenge with the conventional retrieval approach using large-scale databases is that, quite often, we do not have any query image sample of the target class at our disposal. The target of interest solely exists as a perception to the user in the form of an imprecise sketch. In such situations where a photo query is absent, it can be immensely useful if we can promptly make a quick hand-made sketch of the target. Sketches are a highly symbolic and hieroglyphic representation of data. One can exploit the notion of this minimalistic representative of sketch queries for sketch-based image retrieval (SBIR) framework. While dealing with satellite images, it is imperative to collect as many samples of images as possible for each object class for object recognition with a high success rate. However, in general, there exists a considerable number of classes for which we seldom have any training data samples. Therefore, for such classes, we can use the zero-shot learning (ZSL) strategy. The ZSL approach aims to solve a task without receiving any example of that task during the training phase. This makes the network capable of handling an unseen class (new class) sample obtained during the inference phase upon deployment of the network. Hence, we propose the aerial sketch-image dataset, namely Earth on Canvas dataset.

Classes in this dataset:
Airplane, Baseball Diamond, Buildings, Freeway, Golf Course, Harbor, Intersection, Mobile home park, Overpass, Parking lot,  River, Runway, Storage tank, Tennis court.",https://production-media.paperswithcode.com/datasets/9352c765-33b5-44f4-b377-3165dfb4a0dd.png,Editcreative commons,,,,,,,"val and acquisition has become exceedingly challenging. Existing satellites essentially scan a vast overlapping region of the Earth using various sensing techniques, like multi-spectral, hyperspectral, Synthetic Aperture Radar (SAR), video, and compressed sensing, to name a few. With increasing complexity and different sensing techniques at our disposal, it has become our primary interest to design efficient algorithms to retrieve data from multiple data modalities, given the complementary information that is captured by different sensors. This type of problem is referred to as inter-modal data retrieval. In remote sensing (RS), there are primarily two important types of problems, i.e., land-cover classification and object detection. In this work, we focus on the target-based object retrieval part, which falls under the realm of object detection in RS. Object retrieval essentially requires high-resolution imagery for objects to be distinctly visible in the image. The main challenge with the conventional retrieval approach using large-scale databases is that, quite often, we do not have any query image sample of the target class at our disposal. The target of interest solely exists as a perception to the user in the form of an imprecise sketch. In such situations where a photo query is absent, it can be immensely useful if we can promptly make a quick hand-made sketch of the target. Sketches are a highly symbolic and hieroglyphic representation of data. One can exploit the notion of this minimalistic representative of sketch queries for sketch-based image retrieval (SBIR) framework. While dealing with satellite images",,"Cross-Modal Retrieval, Cross-Domain Few-Shot, Zero-Shot Cross-Modal Retrieval",,,See all 844 tasks,Cross-Domain Few-Shot10 benchm,Cross-Domain Few-Shot10 benchm
Places205,Places205 Dataset,"The Places205 dataset is a large-scale scene-centric dataset with 205 common scene categories. The training dataset contains around 2,500,000 images from these categories. In the training set, each scene category has the minimum 5,000 and maximum 15,000 images. The validation set contains 100 images per category (a total of 20,500 images), and the testing set includes 200 images per category (a total of 41,000 images).",https://arxiv.org/abs/1610.01119,EditCC BY,Image,,,,,000 images,"training dataset contains around 2,500,000 images",,"Scene Recognition, Image Classification, Cross-Domain Few-Shot","image-classification-on-places205, cross-domain-few-shot-on-places",,See all 844 tasks,Cross-Domain Few-Shot10 benchm,Cross-Domain Few-Shot10 benchm
DART,DART Dataset,"DART is a large dataset for open-domain structured data record to text generation. DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set.",/paper/dart-open-domain-structured-data-record-to,EditUnknown,"Tabular, Text",English,,,,191 examples,,,"Table-to-Text Generation, Text Generation, Language Modelling, Data-to-Text Generation, Question Answering","text-generation-on-dart, data-to-text-generation-on-dart, table-to-text-generation-on-dart",,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
E2E,E2E Dataset,"End-to-End NLG Challenge (E2E) aims to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena.",/paper/evaluating-the-state-of-the-art-of-end-to-end,EditCC BY-SA 4.0,"Tabular, Text",English,,,,,,,"Text Generation, Table-to-Text Generation, Language Modelling, Data-to-Text Generation","data-to-text-generation-on-e2e, data-to-text-generation-on-e2e-nlg-challenge, table-to-text-generation-on-e2e, data-to-text-generation-on-cleaned-e2e-nlg-1",,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
GenWiki,GenWiki Dataset,"GenWiki is a large-scale dataset for knowledge graph-to-text (G2T) and text-to-knowledge graph (T2G) conversion. It is introduced in the paper ""GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation"" by Zhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng Zhang at COLING 2020.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,2020,,,,,,"KG-to-Text Generation, Unsupervised KG-to-Text Generation, Data-to-Text Generation","unsupervised-kg-to-text-generation-on-genwiki, data-to-text-generation-on-genwiki, unsupervised-kg-to-text-generation-on-genwiki-1",,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
KELM,KELM Dataset,KELM is a large-scale synthetic corpus of Wikidata KG as natural text.,https://github.com/google-research-datasets/KELM-corpus,EditUnknown,Text,English,,,,,,,"Text Generation, Language Modelling, Data-to-Text Generation",,,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
MultiWOZ,MultiWOZ Dataset,"The Multi-domain Wizard-of-Oz (MultiWOZ) dataset is a large-scale human-human conversational corpus spanning over seven domains, containing 8438 multi-turn dialogues, with each dialogue averaging 14 turns. Different from existing standard datasets like WOZ and DSTC2, which contain less than 10 slots and only a few hundred values, MultiWOZ has 30 (domain, slot) pairs and over 4,500 possible values. The dialogues span seven domains: restaurant, hotel, attraction, taxi, train, hospital and police.",https://arxiv.org/abs/1905.07687,EditMIT,"Image, Text, Video",English,,,,,,,"Slot Filling, Dialogue State Tracking, Task-Oriented Dialogue Systems, Intent Detection, Data-to-Text Generation, Multi-domain Dialogue State Tracking, domain classification, End-To-End Dialogue Modelling","intent-detection-on-multiwoz-2-2, data-to-text-generation-on-multiwoz-2-1, multi-domain-dialogue-state-tracking-on-1, multi-domain-dialogue-state-tracking-on-4, multi-domain-dialogue-state-tracking-on-3, domain-classification-on-multiwoz-2-2, end-to-end-dialogue-modelling-on-multiwoz-2-1, multi-domain-dialogue-state-tracking-on, multi-domain-dialogue-state-tracking-on-2, end-to-end-dialogue-modelling-on-multiwoz-2-0, dialogue-state-tracking-on-multiwoz-2-1, dialogue-state-tracking-on-multiwoz-2-2, slot-filling-on-multiwoz-2-2, task-oriented-dialogue-systems-on-multiwoz-2",,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
RoboCup,RoboCup Dataset,"RoboCup is an initiative in which research groups compete by enabling their robots to play football matches. Playing football requires solving several challenging tasks, such as vision, motion, and team coordination. Framing the research efforts onto football attracts public interest (and potential research funding) in robotics, which may otherwise be less entertaining to non-experts.",https://arxiv.org/abs/1910.12294,EditUnknown,Text,English,,,,,,,"Text Generation, Knowledge Graphs, Data-to-Text Generation",,,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
RotoWire,RotoWire Dataset,"This dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box- and line-scores. Summaries taken from rotowire.com are referred to as the ""rotowire"" data.  There are 4853 distinct rotowire summaries, covering NBA games played between 1/1/2014 and 3/29/2017; some games have multiple summaries. The summaries have been randomly split into training, validation, and test sets consisting of 3398, 727, and 728 summaries, respectively.",https://arxiv.org/abs/1707.08052,EditUnknown,"Tabular, Text",English,2014,,,,,,"Table-to-Text Generation, Data-to-Text Generation","data-to-text-generation-on-rotowire-content, data-to-text-generation-on-rotowire-relation, data-to-text-generation-on-rotowire, data-to-text-generation-on-rotowire-content-1, table-to-text-generation-on-rotowire",,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
ToTTo,ToTTo Dataset,"ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description.

During the dataset creation process, tables from English Wikipedia are matched with (noisy) descriptions. Each table cell mentioned in the description is highlighted and the descriptions are iteratively cleaned and corrected to faithfully reflect the content of the highlighted cells.",https://github.com/google-research-datasets/totto,EditUnknown,Text,English,,,,,,,Data-to-Text Generation,data-to-text-generation-on-totto,,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
ViGGO,ViGGO Dataset,"The ViGGO corpus is a set of 6,900 meaning representation to natural language utterance pairs in the video game domain. The meaning representations are of 9 different dialogue acts.",https://nlds.soe.ucsc.edu/viggo,EditUnknown,Text,English,,,,,,,Data-to-Text Generation,data-to-text-generation-on-viggo-1,,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
WebNLG,WebNLG Dataset,"The WebNLG corpus comprises of sets of triplets describing facts (entities and relations between them) and the corresponding facts in form of natural language text. The corpus contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories.

Initially, the dataset was used for the WebNLG natural language generation challenge which consists of mapping the sets of triplets to text, including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation.
The corpus is also used for a reverse task of triplets extraction.

Versioning history of the dataset can be found here.

It's also available here: https://huggingface.co/datasets/web_nlg
Note: ""The v3 release (release_v3.0_en, release_v3.0_ru) for the WebNLG2020 challenge also supports a semantic parsing task.""",https://arxiv.org/abs/1904.03396,EditCC BY-NC-SA 4.0,"Graph, Tabular, Text, Time Series",English,,,,,,,"Table-to-Text Generation, Graph-to-Sequence, Relation Extraction, KG-to-Text Generation, Data-to-Text Generation, Unsupervised semantic parsing, Joint Entity and Relation Extraction, Unsupervised KG-to-Text Generation","joint-entity-and-relation-extraction-on-1, joint-entity-and-relation-extraction-on-8, kg-to-text-generation-on-webnlg-all, unsupervised-kg-to-text-generation-on-webnlg, data-to-text-generation-on-webnlg-full-1, kg-to-text-generation-on-webnlg-2-0, table-to-text-generation-on-webnlg-unseen, kg-to-text-generation-on-webnlg-2-0-1, relation-extraction-on-webnlg, table-to-text-generation-on-webnlg-seen, data-to-text-generation-on-webnlg-en, graph-to-sequence-on-webnlg, data-to-text-generation-on-webnlg, table-to-text-generation-on-webnlg-all, unsupervised-semantic-parsing-on-webnlg-v2-1, kg-to-text-generation-on-webnlg-unseen, kg-to-text-generation-on-webnlg-seen",,See all 844 tasks,Data-to-Text Generation44 benc,Data-to-Text Generation44 benc
Clotho,Clotho Dataset,"Clotho is an audio captioning dataset, consisting of 4981 audio samples, and each audio sample has five captions (a total of 24 905 captions). Audio samples are of 15 to 30 s duration and captions are eight to 20 words long.",https://zenodo.org/record/3490684,EditOther (Attribution),"Audio, Image, Text",English,,,,,,,"Multi-Task Learning, Zero-shot Text to Audio Retrieval, Zero-shot Audio Captioning, Data Augmentation, Language Modelling, Audio captioning, Audio to Text Retrieval, Text to Audio Retrieval","zero-shot-text-to-audio-retrieval-on-clotho, audio-captioning-on-clotho, zero-shot-audio-captioning-on-clotho, audio-to-text-retrieval-on-clotho, text-to-audio-retrieval-on-clotho",,See all 844 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
IAM,IAM Dataset,"The IAM database contains 13,353 images of handwritten lines of text created by 657 writers. The texts those writers transcribed are from the Lancaster-Oslo/Bergen Corpus of British English. It includes contributions from 657 writers making a total of 1,539 handwritten pages comprising of 115,320 words and is categorized as part of modern collection. The database is labeled at the sentence, line, and word levels.",https://arxiv.org/abs/1904.03734,"EditCustom (research-only, non-commercial, attribution)","Image, Text",English,,,,353 images,,,"HTR, Data Augmentation, Handwriting Recognition, Handwritten Text Recognition, Optical Character Recognition (OCR)","handwritten-text-recognition-on-iam, htr-on-iam",,See all 844 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
ImageNet-Sketch,ImageNet-Sketch Dataset,"ImageNet-Sketch data set consists of 50,889 images,  approximately 50 images for each of the 1000 ImageNet classes. The data set is constructed with Google Image queries ""sketch of "", where  is the standard class name. Only within the ""black and white"" color scheme is searched. 100 images are initially queried for every class, and the pulled images are cleaned by deleting the irrelevant images and images that are for similar but different classes. For some classes, there are less than 50 images after manually cleaning, and then the data set is augmented by flipping and rotating the images.",https://github.com/HaohanWang/ImageNet-Sketch,EditUnknown,Image,English,,,,889 images,,,"Zero-Shot Transfer Image Classification, Data Augmentation, Domain Adaptation, Domain Generalization, Image Classification","image-classification-on-imagenet-sketch, domain-generalization-on-imagenet-sketch, zero-shot-transfer-image-classification-on-8",,See all 844 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
MathQA,MathQA Dataset,MathQA significantly enhances the AQuA dataset with fully-specified operational programs.,/paper/mathqa-towards-interpretable-math-word,EditCustom,Text,English,,,,,,,"Math Word Problem Solving, Data Augmentation, Question Answering",math-word-problem-solving-on-mathqa,,See all 844 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
MuST-C,MuST-C Dataset,"MuST-C currently represents the largest publicly available multilingual corpus (one-to-many) for speech translation. It covers eight language directions, from English to German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian. The corpus consists of audio, transcriptions and translations of English TED talks, and it comes with a predefined training, validation and test split.",https://arxiv.org/abs/1910.03320,EditCC BY-NC-ND 4.0,"Audio, Image, Text",English,,,,,,,"Data Augmentation, Speech-to-Text Translation, Speech Recognition","speech-to-text-translation-on-must-c-1, speech-to-text-translation-on-must-c-en-de",,See all 844 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
PAWS,PAWS Dataset,"Paraphrase Adversaries from Word Scrambling (PAWS) is a dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. The dataset has two subsets, one based on Wikipedia and the other one based on the Quora Question Pairs (QQP) dataset.",https://github.com/google-research-datasets/paws,EditCustom,Text,English,,,,,,,"Data Augmentation, Natural Language Inference, Paraphrase Identification",,,See all 844 tasks,Data Augmentation4 benchmarks3,Data Augmentation4 benchmarks3
Deblur-NeRF,Deblur-NeRF Dataset,"This dataset focus on two blur types: camera motion blur and defocus blur. For each type of blur we synthesize $5$ scenes using Blender. We manually place multi-view cameras to mimic real data capture. To render images with camera motion blur, we randomly perturb the camera pose, and then linearly interpolate poses between the original and perturbed poses for each view. We render images from interpolated poses and blend them in linear RGB space to generate the final blurry images. For defocus blur, we use the built-in functionality to render depth-of-field images. We fix the aperture and randomly choose a focus plane between the nearest and furthest depth.

We also captured $20$ real world scenes with $10$ scenes for each blur type for a qualitative study. The camera used was a Canon EOS RP with manual exposure mode. We captured the camera motion blur images by manually shaking the camera during exposure, while the reference images are taken using a tripod. To capture defocus images, we choose a large aperture. We compute the camera poses of blurry and reference images in the real world scenes using the COLMAP.",https://production-media.paperswithcode.com/datasets/31403458-a405-48e1-94ff-0b0fe9ee13e6.png,EditMIT,,,,,,,,,"Deblurring, Novel View Synthesis",,,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
Dialogue_State_Tracking_Challenge,Dialogue State Tracking Challenge Dataset,"The Dialog State Tracking Challenges 2 & 3 (DSTC2&3) were research challenge focused on improving the state of the art in tracking the state of spoken dialog systems. State tracking, sometimes called belief tracking, refers to accurately estimating the user's goal as a dialog progresses. Accurate state tracking is desirable because it provides robustness to errors in speech recognition, and helps reduce ambiguity inherent in language within a temporal process like dialog.
In these challenges, participants were given labelled corpora of dialogs to develop state tracking algorithms. The trackers were then evaluated on a common set of held-out dialogs, which were released, un-labelled, during a one week period.

The corpus was collected using Amazon Mechanical Turk, and consists of dialogs in two domains: restaurant information, and tourist information. Tourist information subsumes restaurant information, and includes bars, cafés etc. as well as multiple new slots. There were two rounds of evaluation using this data:

DSTC 2 released a large number of training dialogs related to restaurant search. Compared to DSTC (which was in the bus timetables domain), DSTC 2 introduces changing user goals, tracking 'requested slots' as well as the new restaurants domain. Results from DSTC 2 were presented at SIGDIAL 2014.
DSTC 3 addressed the problem of adaption to a new domain - tourist information. DSTC 3 releases a small amount of labelled data in the tourist information domain; participants will use this data plus the restaurant data from DSTC 2 for training.
Dialogs used for training are fully labelled; user transcriptions, user dialog-act semantics and dialog state are all annotated. (This corpus therefore is also suitable for studies in Spoken Language Understanding.)",https://github.com/matthen/dstc,EditUnknown,"Image, Text, Video",English,2014,,,,,,"Spoken Dialogue Systems, Slot Filling, Dialogue State Tracking, Spoken Language Understanding, Deblurring, Intent Detection, domain classification","domain-classification-on-dialogue-state, dialogue-state-tracking-on-second-dialogue, slot-filling-on-dialogue-state-tracking, intent-detection-on-dialogue-state-tracking, deblurring-on-second-dialogue-state-tracking",,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
GoPro,GoPro Dataset,"The GoPro dataset for deblurring consists of 3,214 blurred images with the size of 1,280×720 that are divided into 2,103 training images and 1,111 test images. The dataset consists of pairs of a realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera.",https://arxiv.org/abs/1903.10157,EditUnknown,"Image, Video",,,,,,"training images and 1,111 test images",,"Video Frame Interpolation, Unified Image Restoration, Deblurring, Image Deblurring","image-deblurring-on-gopro, unified-image-restoration-on-gopro, video-frame-interpolation-on-gopro, deblurring-on-gopro",,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
HIDE,HIDE Dataset,"Consists of 8,422 blurry and sharp image pairs with 65,784 densely annotated FG human bounding boxes.",/paper/human-aware-motion-deblurring-1,EditUnknown,Image,,,,,,,,"Image Restoration, Image Super-Resolution, Deblurring, Image Deblurring","deblurring-on-hide, image-deblurring-on-hide, image-deblurring-on-hide-trained-on-gopro, deblurring-on-hide-trained-on-gopro",,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
MSU_BASED,MSU BASED Dataset,"Qualitative dataset with real blurred videos, created by using beam-splitter setup in lab environment",https://production-media.paperswithcode.com/datasets/1e4ff292-3a0d-454e-9408-73265eaff1f8.png,EditUnknown,,,,,,,,,Deblurring,deblurring-on-based,,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
QMUL-SurvFace,QMUL-SurvFace Dataset,"QMUL-SurvFace is a surveillance face recognition benchmark that contains 463,507 face images of 15,573 distinct identities captured in real-world uncooperative surveillance scenes over wide space and time.",/paper/surveillance-face-recognition-challenge,EditUnknown,Image,,,,,,,,"Super-Resolution, Face Recognition, Deblurring, Face Verification",face-verification-on-qmul-survface,,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
Real_Blur_Dataset,Real Blur Dataset Dataset,"The dataset consists of 4,738 pairs of images of 232 different scenes including reference pairs. All images were captured both in the camera raw and JPEG formats, hence generating two datasets: RealBlur-R from the raw images, and RealBlur-J from the JPEG images. Each training set consists of 3,758 image pairs, while each test set consists of 980 image pairs.

The deblurring result is first aligned to its ground truth sharp image using a homography estimated by the enhanced correlation coefficients method, and PSNR or SSIM is computed in sRGB color space.",https://production-media.paperswithcode.com/datasets/qualatitive_result_web.png,EditUnknown,Image,,,,,,,,"Deblurring, Image Deblurring","deblurring-on-realblur-j-trained-on-gopro, deblurring-on-realblur-j-1, image-deblurring-on-realblur-r, deblurring-on-realblur-r-trained-on-gopro, image-deblurring-on-realblur-j, deblurring-on-realblur-r",,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
REDS,REDS Dataset,"The realistic and dynamic scenes (REDS) dataset was proposed in the NTIRE19 Challenge. The dataset is composed of 300 video sequences with resolution of 720×1,280, and each video has 100 frames, where the training set, the validation set and the testing set have 240, 30 and 30 videos, respectively",https://arxiv.org/abs/2007.12928,EditUnknown,,,,,,,,,"Joint Demosaicing and Denoising, Deblurring","deblurring-on-reds, joint-demosaicing-and-denoising-on-reds",,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
Rendered_WB_dataset,Rendered WB dataset Dataset,"A dataset of over 65,000 pairs of incorrectly white-balanced images and their corresponding correctly white-balanced images.",/paper/when-color-constancy-goes-wrong-correcting,EditUnknown,Image,,,,,,,,"Image Dehazing, Deblurring, Color Constancy",,,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
RSBlur,RSBlur Dataset,"The RSBlur dataset provides pairs of real and synthetic blurred images with ground truth sharp images. The dataset enables the evaluation of deblurring methods and blur synthesis methods on real-world blurred images.  Training, validation, and test sets consist of 8,878, 1,120, and 3,360 blurred images, respectively.",https://production-media.paperswithcode.com/datasets/bd573da2-a4c8-4c19-8eeb-9ef1dfcbfd47.png,EditUnknown,,,,,,,"valuation of deblurring methods and blur synthesis methods on real-world blurred images.  Training, validation, and test sets consist of 8,878, 1,120, and 3,360 blurred images",,Deblurring,"deblurring-on-rsblur, deblurring-on-rsblur-trained-on-synthetic",,See all 844 tasks,Deblurring32 benchmarks395 pap,Deblurring32 benchmarks395 pap
Decoder4201_papers_with_code_Dataset,Decoder4201 papers with code Dataset,,https://paperswithcode.com/dataset/decoder,,,,,,,,,,,,,See all 844 tasks,Decoder4201 papers with code,Decoder4201 papers with code
Deep_Hashing57_papers_with_code_Dataset,Deep Hashing57 papers with code Dataset,,https://paperswithcode.com/dataset/deep-hashing,,,,,,,,,,,,,See all 844 tasks,Deep Hashing57 papers with cod,Deep Hashing57 papers with cod
Deep_Learning2603_papers_with_code_Dataset,Deep Learning2603 papers with code Dataset,,https://paperswithcode.com/dataset/deep-learning,,,,,,,,,,,,,See all 844 tasks,Deep Learning2603 papers with ,Deep Learning2603 papers with 
Deep_Reinforcement_Learning1691_papers_with_code_D,Deep Reinforcement Learning1691 papers with code Dataset,,https://paperswithcode.com/dataset/deep-reinforcement-learning,,,,,,,,,,,,,See all 844 tasks,Deep Reinforcement Learning169,Deep Reinforcement Learning169
DialogSum,DialogSum Dataset,"DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 dialogues with corresponding manually labeled summaries and topics.

This work is accepted by ACL findings 2021. You may find the paper here: https://arxiv.org/pdf/2105.06762.pdf.

If you want to use our dataset, please cite our paper. 

Dialogue Data
We collect dialogue data for DialogSum from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2019), as well as an English speaking practice website. 
These datasets contain face-to-face spoken dialogues that cover a wide range of daily-life topics, including schooling, work, medication, shopping, leisure, travel.
Most conversations take place between friends, colleagues, and between service providers and customers.

Compared with previous datasets, dialogues from DialogSum have distinct characteristics: 
* Under rich real-life scenarios, including more diverse task-oriented scenarios;
* Have clear communication patterns and intents, which is valuable to serve as summarization sources;
* Have a reasonable length, which comforts the purpose of automatic summarization.

Summaries
We ask annotators to summarize each dialogue based on the following criteria:
* Convey the most salient information;
* Be brief;
* Preserve important named entities within the conversation;
* Be written from an observer perspective;
* Be written in formal language.

Topics
In addition to summaries, we also ask annotators to write a short topic for each dialogue, which can be potentially useful for future work, e.g. generating summaries by leveraging topic information.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-06-02_at_17.29.45.png,EditMIT,Text,English,2021,,,,,,"Dialogue Generation, Text Summarization, Abstractive Text Summarization",text-summarization-on-dialogsum,,See all 844 tasks,Dialogue Generation14 benchmar,Dialogue Generation14 benchmar
Doc2Dial,Doc2Dial Dataset,"For goal-oriented document-grounded dialogs, it often involves complex contexts for identifying the most relevant information, which requires better understanding of the inter-relations between conversations and documents. Meanwhile, many online user-oriented documents use both semi-structured and unstructured contents for guiding users to access information of different contexts. Thus, we create a new goal-oriented document-grounded dialogue dataset that captures more diverse scenarios derived from various document contents from multiple domains such ssa.gov and studentaid.gov. For data collection, we propose a novel pipeline approach for dialogue data construction, which has been adapted and evaluated for several domains.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache-2.0 License,"Image, Text",English,,,,,,,"Dialogue Generation, Conversational Question Answering, Dialogue Understanding, Dialogue Act Classification, Goal-Oriented Dialog, Question Answering",,,See all 844 tasks,Dialogue Generation14 benchmar,Dialogue Generation14 benchmar
FaithDial,FaithDial Dataset,"FaithDial is a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark.

FaithDial contains around 50K turns across 5.5K conversations. If trained on FaithDial, state-of-the-art dialogue models are significantly more faithful while also enhancing other dialogue aspects like cooperativeness, creativity and engagement.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Dialogue Generation, Dialogue Evaluation",,,See all 844 tasks,Dialogue Generation14 benchmar,Dialogue Generation14 benchmar
MultiDoc2Dial,MultiDoc2Dial Dataset,"MultiDoc2Dial is a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as a machine reading comprehension task based on a single given document or passage. We aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents.",https://huggingface.co/datasets/IBM/multidoc2dial,EditApache-2.0 License,Text,English,,,,,,,"Dialogue Generation, Open-Domain Dialog, Goal-Oriented Dialogue Systems, Conversational Question Answering, Question Answering",,,See all 844 tasks,Dialogue Generation14 benchmar,Dialogue Generation14 benchmar
PG-19,PG-19 Dataset,A new open-vocabulary language modelling benchmark derived from books.,/paper/compressive-transformers-for-long-range-1,EditUnknown,Text,English,,,,,,,"Dialogue Generation, Reading Comprehension, Language Modelling",dialogue-generation-on-pg-19,,See all 844 tasks,Dialogue Generation14 benchmar,Dialogue Generation14 benchmar
SODA,SODA Dataset,"SODA is a high-quality social dialogue dataset. In contrast to most existing crowdsourced, small-scale dialogue corpora, Soda distills 1.5M socially-grounded dialogues from a pre-trained language model (InstructGPT; Ouyang et al., ). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x).",https://arxiv.org/pdf/2212.10465v1.pdf,EditCC-BY-4.0,Text,English,,,,,,,Dialogue Generation,,,See all 844 tasks,Dialogue Generation14 benchmar,Dialogue Generation14 benchmar
UDC,UDC Dataset,"Ubuntu Dialogue Corpus (UDC) is a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter.",https://arxiv.org/pdf/1506.08909v3.pdf,EditUnknown,Text,English,,,,,,,"Dialogue Generation, Conversational Response Selection, Answer Selection","answer-selection-on-ubuntu-dialogue-v2, dialogue-generation-on-ubuntu-dialogue, dialogue-generation-on-ubuntu-dialogue-entity, answer-selection-on-ubuntu-dialogue-v1, dialogue-generation-on-ubuntu-dialogue-tense, conversational-response-selection-on-ubuntu-1, dialogue-generation-on-ubuntu-dialogue-cmd, conversational-response-selection-on-ubuntu-2",,See all 844 tasks,Dialogue Generation14 benchmar,Dialogue Generation14 benchmar
3DIdent,3DIdent Dataset,"Novel benchmark which features aspects of natural scenes, e.g. a complex 3D object and different lighting conditions, while still providing access to the continuous ground-truth factors.

We use the Blender rendering engine to create visually complex 3D images. Each image in the dataset shows a colored 3D object which is located and rotated above a colored ground in a 3D space. Additionally, each scene contains a colored spotlight which is focused on the object and located on a half-circle around the scene. The observations are encoded with an RGB color space, and the spatial resolution is 224x224 pixels.

The images are rendered based on a 10-dimensional latent, where: (1) three dimensions describe the XYZ position, (2) three dimensions describe the rotation of the object in Euler angles, (3) two dimensions describe the color of the object and the ground of the scene, respectively, and (4) two dimensions describe the position and color of the spotlight. We use the HSV color space to describe the color of the object and the ground with only one latent each by having the latent factor control the hue value.

The training set and test set contain 250,000 and 25,000 observation-latent pairs, respectively, whereby the latents are uniformly sampled from the unit hyperrectangle.",https://production-media.paperswithcode.com/datasets/1aca66a6-66f2-443a-b9b9-7d49cc810182.png,EditCreative Commons Attribution 4.0 International,,,,,,,,,Disentanglement,disentanglement-on-3dident,,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
3D_Cars,3D Cars Dataset,"Car CAD models from ""3d object detection and viewpoint estimation with a deformable
3d cuboid model"" were used to generate the dataset. For each of the 199 car models, the authors generated $64\times64$ color renderings from 24 rotation angles each offset by 15 degrees, as well as from 4 different camera elevations.",https://production-media.paperswithcode.com/datasets/ffa55f37-436f-4f60-8815-f620cc24261f.gif,EditUnknown,,,,,,,,,Disentanglement,,,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
3D_Shapes_Dataset,3D Shapes Dataset Dataset,"3dshapes is a dataset of 3D shapes procedurally generated from 6 ground truth independent latent factors. These factors are floor colour, wall colour, object colour, scale, shape and orientation.",https://github.com/deepmind/3d-shapes,EditUnknown,,,,,,,,,Disentanglement,,,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
Causal3DIdent,Causal3DIdent Dataset,"Update on 3DIdent, where we introduce six additional object classes (Hare, Dragon, Cow, Armadillo, Horse, and Head), and impose a causal graph over the latent variables. For further details, see Appendix B in the associated paper (https://arxiv.org/abs/2106.04619).",https://production-media.paperswithcode.com/datasets/1e720be9-fc37-4d1d-9260-d0543b2b6179.png,EditCreative Commons Attribution 4.0 International,Image,,,,,,,,"Image Classification, Disentanglement",image-classification-on-causal3dident,,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
dSprites,dSprites Dataset,"dSprites is a dataset of 2D shapes procedurally generated from 6 ground truth independent latent factors. These factors are color, shape, scale, rotation, x and y positions of a sprite.

All possible combinations of these latents are present exactly once, generating N = 737280 total images.",https://github.com/deepmind/dsprites-dataset,EditUnknown,,,,,,,,,Disentanglement,,,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
Fitness-AQA,Fitness-AQA Dataset,"Largest, first-of-its-kind, in-the-wild, fine-grained workout/exercise posture analysis dataset, covering three different exercises: BackSquat, Barbell Row, and Overhead Press. Seven different types of exercise errors are covered. Unlabeled data is also provided to facilitate self-supervised learning.",https://production-media.paperswithcode.com/datasets/b05790c3-c044-477e-a88d-a415732aaf5d.png,EditUnknown,"3D, Image, Video",,,,,,,,"Pose Estimation, Action Quality Assessment, Pose Contrastive Learning, 3D Pose Estimation, 2D Human Pose Estimation, Video Understanding, Action Understanding, 3D Action Recognition, Disentanglement, Motion Disentanglement, Action Recognition",,,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
KITTI-Masks,KITTI-Masks Dataset,"This Dataset consists of 2120 sequences of binary masks of pedestrians. The sequence length varies between 2-710. For details, we refer to our paper. It is based on the original KITTI Segmentation challenge which can be found at https://www.vision.rwth-aachen.de/page/mots 

A detailed description can be found at: https://openreview.net/pdf?id=EbIDjBynYJ8

An example dataloader can be found at: 
https://github.com/bethgelab/slow_disentanglement/",https://production-media.paperswithcode.com/datasets/2bbcb796-5e70-4cf4-9bab-df0d8c8f69d3.png,EditCreative Commons Attribution 4.0 International,"Image, 3D",English,,,,,,,Disentanglement,disentanglement-on-kitti-masks,,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
MPI3D_Disentanglement,MPI3D Disentanglement Dataset,"A data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position.",/paper/on-the-transfer-of-inductive-bias-from,EditUnknown,,,,,,,,,Disentanglement,,,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
smallNORB,smallNORB Dataset,"The smallNORB dataset is a datset for 3D object recognition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees).
The training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).",https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/,"EditCustom (research-only, non-commercial, attribution)",Image,,,,,5 instances,,,"Image Classification, Disentanglement",image-classification-on-smallnorb,,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
Sprites,Sprites Dataset,"The Sprites dataset contains 60 pixel color images of animated characters (sprites). There are 672 sprites, 500 for training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images, so the full dataset has 120K images in total. There are many changes in the appearance of the sprites, they differ in their body shape, gender, hair, armor, arm type, greaves, and weapon.",https://arxiv.org/abs/1711.02245,EditUnknown,"Time Series, Video",,,,,178 images,"training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images",,"Video Prediction, Imputation, Disentanglement","imputation-on-sprites, video-prediction-on-sprites, video-prediction-on-colored-dsprites",,See all 844 tasks,Disentanglement3 benchmarks712,Disentanglement3 benchmarks712
HOC,HOC Dataset,The Hallmarks of Cancer (*HOC) corpus consists of 1852 PubMed publication abstracts manually annotated by experts according to the Hallmarks of Cancer taxonomy. The taxonomy consists of 37 classes in a hierarchy. Zero or more class labels are assigned to each sentence in the corpus.,https://s-baker.net/resource/hoc/,EditUnknown,"Image, Text",English,,,,,,37,Document Classification,document-classification-on-hoc,,See all 844 tasks,Document Classification21 benc,Document Classification21 benc
IMDB-MULTI,IMDB-MULTI Dataset,"IMDB-MULTI is a relational dataset that consists of a network of 1000 actors or actresses who played roles in movies in IMDB. A node represents an actor or actress, and an edge connects two nodes when they appear in the same movie. In IMDB-MULTI, the edges are collected from three different genres: Comedy, Romance and Sci-Fi.",https://arxiv.org/abs/1904.12189,EditUnknown,"Graph, Image, Text",English,,,,,,,"Graph Similarity, Graph Classification, Document Classification","document-classification-on-imdb-m, graph-classification-on-imdb-m",,See all 844 tasks,Document Classification21 benc,Document Classification21 benc
LUN,LUN Dataset,"LUN is used for unreliable news source classification, this dataset includes 17,250 articles from satire, propaganda, and hoaxe.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,Document Classification,document-classification-on-lun,,See all 844 tasks,Document Classification21 benc,Document Classification21 benc
MPQA_Opinion_Corpus,MPQA Opinion Corpus Dataset,"The MPQA Opinion Corpus contains 535 news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).",http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf,EditCustom (research-only),"Image, Text",English,,,,,,,"Sentiment Analysis, Fine-Grained Opinion Analysis, Opinion Mining, Keyword Extraction, Document Classification","fine-grained-opinion-analysis-on-mpqa, sentiment-analysis-on-mpqa, document-classification-on-mpqa",,See all 844 tasks,Document Classification21 benc,Document Classification21 benc
MultiEURLEX,MultiEURLEX Dataset,"MultiEURLEX is a multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union (EU) laws, officially translated in 23 languages, annotated with multiple labels from the EUROVOC taxonomy. The dataset covers 23 official EU languages from 7 language families.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Multilingual text classification, Document Classification",,,See all 844 tasks,Document Classification21 benc,Document Classification21 benc
WOS,WOS Dataset,"Web of Science (WOS) is a document classification dataset that contains 46,985 documents with 134 categories which include 7 parents categories.",/paper/hdltex-hierarchical-deep-learning-for-text,EditUnknown,"Image, Text",English,,,,985 documents,,134,"Image Classification, Text Classification, Hierarchical Multi-label Classification, Document Classification","hierarchical-multi-label-classification-on-16, document-classification-on-wos-11967, document-classification-on-wos-46985, document-classification-on-wos-5736",,See all 844 tasks,Document Classification21 benc,Document Classification21 benc
CMU-MOSEI,CMU-MOSEI Dataset,CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) is the largest dataset of sentence-level sentiment analysis and emotion recognition in online videos. CMU-MOSEI contains over 12 hours of annotated video from over 1000 speakers and 250 topics.,https://production-media.paperswithcode.com/datasets/CMU-MOSEI-0000001483-1f1c1e18_qDoVx1a.jpg,EditCustom,"Image, Text, Video",English,,,,,,,"Video Emotion Detection, Emotion Classification, Multimodal Emotion Recognition, Facial Expression Recognition, Multimodal Sentiment Analysis","facial-expression-recognition-on-cmu-mosei, multimodal-sentiment-analysis-on-cmu-mosei-1, emotion-classification-on-cmu-mosei",,See all 844 tasks,Emotion Classification9 benchm,Emotion Classification9 benchm
DEAP,DEAP Dataset,"The DEAP dataset consists of two parts:


The ratings from an online self-assessment where 120 one-minute extracts of music videos were each rated by 14-16 volunteers based on arousal, valence and dominance.
The participant ratings, physiological recordings and face video of an experiment where 32 volunteers watched a subset of 40 of the above music videos. EEG and physiological signals were recorded and each participant also rated the videos as above. For 22 participants frontal face video was also recorded.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Emotion Recognition, Emotion Classification, Multimodal Emotion Recognition, EEG Emotion Recognition",eeg-emotion-recognition-on-deap,,See all 844 tasks,Emotion Classification9 benchm,Emotion Classification9 benchm
EmoBank,EmoBank Dataset,"EmoBank is a corpus of 10k English sentences balancing multiple genres, annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design.",https://www.aclweb.org/anthology/E17-2092,EditCC-BY-SA 4.0,"Image, Text",English,,,,,,,"Emotion Recognition, Emotion Classification, Natural Language Understanding",,,See all 844 tasks,Emotion Classification9 benchm,Emotion Classification9 benchm
EmotionLines,EmotionLines Dataset,"EmotionLines contains a total of 29245 labeled utterances from 2000 dialogues. Each utterance in dialogues is labeled with one of seven emotions, six Ekman’s basic emotions plus the neutral emotion. Each labeling was accomplished by 5 workers, and for each utterance in a label, the emotion category with the highest votes was set as the label of the utterance. Those utterances voted as more than two different emotions were put into the non-neutral category. Therefore the dataset has a total of 8 types of emotion labels, anger, disgust, fear, happiness, sadness, surprise, neutral, and non-neutral.",https://arxiv.org/abs/1905.11240,EditCC BY-NC-ND,"Image, Text",English,2000,,,,,,"Language Modelling, Emotion Recognition in Conversation, Emotion Recognition, Emotion Classification",emotion-recognition-in-conversation-on-5,,See all 844 tasks,Emotion Classification9 benchm,Emotion Classification9 benchm
ISEAR,ISEAR Dataset,"Over a period of many years during the 1990s, a large group of psychologists all over the world collected data in the ISEAR project, directed by Klaus R. Scherer and Harald Wallbott. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of 7 major emotions (joy, fear, anger, sadness, disgust, shame, and guilt). In each case, the questions covered the way they had appraised the situation and how they reacted. The final data set thus contained reports on seven emotions each by close to 3000 respondents in 37 countries on all 5 continents.",https://www.unige.ch/cisa/research/materials-and-online-research/research-material/,EditCC BY-NC-SA 3.0,Image,,,,,,,,"Emotion Recognition, Emotion Classification, Word Embeddings",,,See all 844 tasks,Emotion Classification9 benchm,Emotion Classification9 benchm
MSP-IMPROV,MSP-IMPROV Dataset,"We present the MSP-IMPROV corpus, a multimodal emotional database, where the goal is to have control over lexical content and emotion while also promoting naturalness in the recordings. Studies on emotion perception often require stimuli with fixed lexical content, but that convey different emotions. These stimuli can also serve as an instrument to understand how emotion modulates speech at the phoneme level, in a manner that controls for coarticulation. Such audiovisual data are not easily available from natural recordings. A common solution is to record actors reading sentences that portray different emotions, which may not produce natural behaviors. We propose an alternative approach in which we define hypothetical scenarios for each sentence that are carefully designed to elicit a particular emotion. Two actors improvise these emotion-specific situations, leading them to utter contextualized, non-read renditions of sentences that have fixed lexical content and convey different emotions. We describe the context in which this corpus was recorded, the key features of the corpus, the areas in which this corpus can be useful, and the emotional content of the recordings. The paper also provides the performance for speech and facial emotion classifiers. The analysis brings novel classification evaluations where we study the performance in terms of inter-evaluator agreement and naturalness perception, leveraging the large size of the audiovisual database.",https://production-media.paperswithcode.com/datasets/d5144405-2dc4-4522-bd92-0bd0e5c3bd52.png,EditAcademic License,"Audio, Image",,,,,,,,"Speech Emotion Recognition, Valence Estimation, Emotion Recognition, Dominance Estimation, Arousal Estimation, Emotion Classification","speech-emotion-recognition-on-msp-improv, arousal-estimation-on-msp-improv",,See all 844 tasks,Emotion Classification9 benchm,Emotion Classification9 benchm
RAVDESS,RAVDESS Dataset,"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7,356 files (total size: 24.8 GB). The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.

Paper: The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",https://zenodo.org/record/1188976#.YFZuJ0j7SL8,EditAttribution-NonCommercial-ShareAlike 4.0 International,"Audio, Image, Video",,,,,,,,"Audio Classification, Speech Emotion Recognition, Video Emotion Recognition, Emotion Recognition, Facial Expression Recognition (FER), Facial Emotion Recognition, Music Emotion Recognition, Emotion Classification","emotion-recognition-on-ravdess, emotion-classification-on-ravdess, speech-emotion-recognition-on-ravdess, audio-classification-on-ravdess, facial-expression-recognition-on-ravdess, facial-emotion-recognition-on-ravdess",,See all 844 tasks,Emotion Classification9 benchm,Emotion Classification9 benchm
ROCStories,ROCStories Dataset,"ROCStories is a collection of commonsense short stories. The corpus consists of 100,000 five-sentence stories. Each story logically follows everyday topics created by Amazon Mechanical Turk workers. These stories contain a variety of commonsense causal and temporal relations between everyday events. Writers also develop an additional 3,742 Story Cloze Test stories which contain a four-sentence-long body and two candidate endings. The endings were collected by asking Mechanical Turk workers to write both a right ending and a wrong ending after eliminating original endings of given short stories. Both endings were required to make logical sense and include at least one character from the main story line. The published ROCStories dataset is constructed with ROCStories as a training set that includes 98,162 stories that exclude candidate wrong endings, an evaluation set, and a test set, which have the same structure (1 body + 2 candidate endings) and a size of 1,871.",https://arxiv.org/abs/1811.00625,EditUnknown,"Image, Text",English,,,,,,,"Text Generation, Emotion Classification","text-generation-on-rocstories, emotion-classification-on-rocstories",,See all 844 tasks,Emotion Classification9 benchm,Emotion Classification9 benchm
Story_Commonsense,Story Commonsense Dataset,"Story Commonsense is a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.",/paper/modeling-naive-psychology-of-characters-in,EditUnknown,"Image, Text",English,,,,,,,"Natural Language Inference, Emotion Classification",,,See all 844 tasks,Emotion Classification9 benchm,Emotion Classification9 benchm
AFW,AFW Dataset,"AFW (Annotated Faces in the Wild) is a face detection dataset that contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box.",https://ieeexplore.ieee.org/document/6248014,EditUnknown,Image,,,,,205 images,,,"Face Alignment, Face Detection, Facial Landmark Detection",face-detection-on-annotated-faces-in-the-wild,,See all 844 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
COCO-WholeBody,COCO-WholeBody Dataset,"COCO-WholeBody is an extension of COCO dataset with whole-body annotations. There are 4 types of bounding boxes (person box, face box, left-hand box, and right-hand box) and 133 keypoints (17 for body, 6 for feet, 68 for face and 42 for hands) annotations for each person in the image.",/paper/whole-body-human-pose-estimation-in-the-wild,EditCC-BY-NC 4.0  ( not for commercial purpose),"3D, Image",English,,,,,,,"Face Detection, Pose Estimation, Multi-Person Pose Estimation, 2D Human Pose Estimation, Foot keypoint detection, Hand Pose Estimation, Facial Landmark Detection","multi-person-pose-estimation-on-coco-1, hand-pose-estimation-on-coco-wholebody, facial-landmark-detection-on-coco-wholebody, face-detection-on-coco-wholebody, foot-keypoint-detection-on-coco-wholebody, 2d-human-pose-estimation-on-coco-wholebody-1",,See all 844 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
FDDB,FDDB Dataset,"The Face Detection Dataset and Benchmark (FDDB) dataset is a collection of labeled faces from Faces in the Wild dataset. It contains a total of 5171 face annotations, where images are also of various resolution, e.g. 363x450 and 229x410. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Both greyscale and color images are included.",https://arxiv.org/abs/1809.03336,EditUnknown,Image,,,,,,,,Face Detection,face-detection-on-fddb,,See all 844 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
MALF,MALF Dataset,"The MALF dataset is a large dataset with 5,250 images annotated with multiple facial attributes and it is specifically constructed for fine grained evaluation.",https://arxiv.org/abs/1804.10275,EditUnknown,Image,,,,,250 images,,,"Robust Face Recognition, Object Detection, Face Detection",,,See all 844 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
MaskedFace-Net,MaskedFace-Net Dataset,"Proposes three types of masked face detection dataset; namely, the Correctly Masked Face Dataset (CMFD), the Incorrectly Masked Face Dataset (IMFD) and their combination for the global masked face detection (MaskedFace-Net).",/paper/maskedface-net-a-dataset-of-correctly,EditUnknown,Image,,,,,,,,Face Detection,,,See all 844 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
PASCAL_Face,PASCAL Face Dataset,"The PASCAL FACE dataset is a dataset for face detection and face recognition. It has a total of 851 images which are a subset of the PASCAL VOC and has a total of 1,341 annotations. These datasets contain only a few hundreds of images and have limited variations in face appearance.",https://arxiv.org/abs/1804.10275,EditUnknown,Image,,,,,851 images,,,Face Detection,face-detection-on-pascal-face,,See all 844 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
UMDFaces,UMDFaces Dataset,"UMDFaces is a face dataset divided into two parts:


Still Images - 367,888 face annotations for 8,277 subjects.
Video Frames - Over 3.7 million annotated video frames from over 22,000 videos of 3100 subjects.

Part 1 - Still Images

The dataset contains 367,888 face annotations for 8,277 subjects divided into 3 batches. The annotations contain human curated bounding boxes for faces and estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.

Part 2 - Video Frames

The second part contains 3,735,476 annotated video frames extracted from a total of 22,075 for 3,107 subjects. The annotations contain the estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_5.51.14_PM.png,EditUnknown,Image,,,,,,,,"Face Recognition, Face Verification, Face Detection",,,See all 844 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
WIDER,WIDER Dataset,"WIDER is a dataset for complex event recognition from static images. As of v0.1, it contains 61 event categories and around 50574 images annotated with event class labels.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_4.05.24_PM.png,EditCustom (research-only),"Image, Text",English,,,,50574 images,,,"Face Detection, Multi-Task Learning, Blind Face Restoration, Image Captioning",blind-face-restoration-on-wider,,See all 844 tasks,Face Detection17 benchmarks143,Face Detection17 benchmarks143
mini-Imagenet,mini-Imagenet Dataset,"mini-Imagenet is proposed by  Matching Networks for One Shot Learning
. In NeurIPS, 2016. This dataset consists of 50000 training images and 10000 testing images, evenly
distributed across 100 classes.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2016,,,,training images and 10000 testing images,100,"Few-Shot Image Classification, Few-Shot Learning, Unsupervised Few-Shot Image Classification, Cross-Domain Few-Shot, Continual Learning, Few-Shot Class-Incremental Learning","unsupervised-few-shot-image-classification-on, few-shot-image-classification-on-mini-9, few-shot-learning-on-mini-imagenet-1-shot-2, few-shot-learning-on-mini-imagenet-5-shot, few-shot-class-incremental-learning-on-mini, few-shot-image-classification-on-mini-4, continual-learning-on-miniimagenet, few-shot-image-classification-on-mini-6, cross-domain-few-shot-on-miniimagenet, few-shot-image-classification-on-mini-12, few-shot-image-classification-on-mini-13, few-shot-image-classification-on-mini-3, few-shot-image-classification-on-mini-8, few-shot-learning-on-mini-imagenet-5-way-1, few-shot-image-classification-on-mini-7, few-shot-image-classification-on-mini-1, few-shot-image-classification-on-mini-10, few-shot-image-classification-on-mini-2, few-shot-image-classification-on-mini-5, unsupervised-few-shot-image-classification-on-1",,See all 844 tasks,Few-Shot Learning66 benchmarks,Few-Shot Learning66 benchmarks
MRPC,MRPC Dataset,"Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases).",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313059/,EditUnknown,Text,English,,,,,,,"Natural Language Inference, Semantic Textual Similarity within Bi-Encoder, Few-Shot Learning, Semantic Textual Similarity","semantic-textual-similarity-within-bi-encoder, natural-language-inference-on-mrpc, semantic-textual-similarity-on-mrpc, few-shot-learning-on-mrpc, semantic-textual-similarity-on-mrpc-dev",,See all 844 tasks,Few-Shot Learning66 benchmarks,Few-Shot Learning66 benchmarks
UCF101,UCF101 Dataset,"UCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 × 240.",https://arxiv.org/abs/1711.03273,EditMIT,"Image, Text, Time Series, Video",English,,,,,,101,"Human Activity Recognition, Open Set Action Recognition, Self-supervised Video Retrieval, Action Classification, Image Clustering, Self-Supervised Action Recognition Linear, Temporal Action Localization, Prompt Engineering, Early Action Prediction, Zero-Shot Learning, Few Shot Action Recognition, Zero-Shot Action Recognition, Few-Shot Learning, Text-to-Video Generation, Action Recognition, Self-Supervised Action Recognition, Skeleton Based Action Recognition, Video Generation, Action Recognition In Videos, Video Frame Interpolation, Transductive Zero-Shot Classification","self-supervised-action-recognition-on-ucf101-1, self-supervised-action-recognition-on-ucf101, action-recognition-in-videos-on-ucf-101, action-recognition-in-videos-on-ucf101-2, text-to-video-generation-on-ucf-101, video-generation-on-ucf-101-16-frames-128x128, self-supervised-action-recognition-linear-on, human-activity-recognition-on-ucf-101, video-generation-on-ucf-101, zero-shot-action-recognition-on-ucf101, transductive-zero-shot-classification-on-9, self-supervised-video-retrieval-on-ucf101, prompt-engineering-on-ucf101, video-generation-on-ucf-101-16-frames, early-action-prediction-on-ucf101, video-generation-on-ucf-101-16-frames-64x64, action-classification-on-ucf101, image-clustering-on-ucf101, skeleton-based-action-recognition-on-ucf101, few-shot-learning-on-ucf101, action-recognition-on-ucf-101, zero-shot-learning-on-ucf101, action-recognition-in-videos-on-ucf101, few-shot-action-recognition-on-ucf101, open-set-action-recognition-on-ucf101-mitv2, video-frame-interpolation-on-ucf101-1",,See all 844 tasks,Few-Shot Learning66 benchmarks,Few-Shot Learning66 benchmarks
Chem-FINESE,Chem-FINESE Dataset,"The dataset contains two few-shot chemical fine-grained entity extraction datasets, based on human-annotated ChemNER+ and CHEMET.
For each dataset, we randomly sample a subset based on the frequency of each type class. Specifically, given a dataset, we first set the number of maximum entity mentions $k$ for the most frequent entity type in the dataset. We then randomly sample other types and ensure that the distribution of each type remains the same as in the original dataset. We choose the values $6, 9, 12, 15, 18$ as the potential maximum entity mentions for $k$. The ChemNER+ and CHEMET few-shot datasets contain 52 and 28 types respectively.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Chemical Entity Recognition, Named Entity Recognition (NER), Few-shot NER",,,See all 844 tasks,Few-shot NER4 benchmarks41 pap,Few-shot NER4 benchmarks41 pap
Few-NERD,Few-NERD Dataset,"Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens. Three benchmark tasks are built, one is supervised (Few-NERD (SUP)) and the other two are few-shot (Few-NERD (INTRA) and Few-NERD (INTER)).",https://production-media.paperswithcode.com/datasets/few-nerd.png,EditCC BY-SA 4.0,"Image, Text",English,,,,200 sentences,,,"Multi-Grained Named Entity Recognition, Named Entity Recognition (NER), Few-shot NER, Entity Typing, Low Resource Named Entity Recognition, Named Entity Recognition","named-entity-recognition-on-few-nerd-sup, named-entity-recognition-on-finegrained, few-shot-ner-on-few-nerd-intra, few-shot-ner-on-few-nerd-inter",,See all 844 tasks,Few-shot NER4 benchmarks41 pap,Few-shot NER4 benchmarks41 pap
XGLUE,XGLUE Dataset,"XGLUE is an evaluation benchmark XGLUE,which is composed of 11 tasks that span 19 languages. For each task, the training data is only available in English. This means that to succeed at XGLUE, a model must have a strong zero-shot cross-lingual transfer capability to learn from the English data of a specific task and transfer what it learned to other languages. Comparing to its concurrent work XTREME, XGLUE has two characteristics: First, it includes cross-lingual NLU and cross-lingual NLG tasks at the same time; Second, besides including 5 existing cross-lingual tasks (i.e. NER, POS, MLQA, PAWS-X and XNLI), XGLUE selects 6 new tasks from Bing scenarios as well, including News Classification (NC), Query-Ad Matching (QADSM), Web Page Ranking (WPR), QA Matching (QAM), Question Generation (QG) and News Title Generation (NTG). Such diversities of languages, tasks and task origin provide a comprehensive benchmark for quantifying the quality of a pre-trained model on cross-lingual natural language understanding and generation.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCustom (non-commercial),"Audio, Text",English,,,,,,,"Cross-Lingual Natural Language Inference, Cross-Lingual POS Tagging, Cross-Lingual NER, Natural Language Understanding, Part-Of-Speech Tagging, Few-shot NER, Natural Language Inference","part-of-speech-tagging-on-xglue, few-shot-ner-on-xglue",,See all 844 tasks,Few-shot NER4 benchmarks41 pap,Few-shot NER4 benchmarks41 pap
FewSOL,FewSOL Dataset,"The Few-Shot Object Learning (FewSOL) dataset can be used for object recognition with a few images per object. It contains 336 real-world objects with 9 RGB-D images per object from different views. Object segmentation masks, object poses and object attributes are provided. In addition, synthetic images generated using 330 3D object models are used to augment the dataset.  FewSOL dataset can be used to study a set of few-shot object recognition problems such as classification, detection and segmentation, shape reconstruction, pose estimation, keypoint correspondences and attribute recognition. 

Motivation: If robots can recognize objects from a few exemplar images, it is possible to scale up the number of objects a robot can recognize because collecting a few images per object is a much easier process compared to building a 3D model of an object. In addition, models trained in the meta-learning setting can generalize to new objects without re-training.",https://production-media.paperswithcode.com/datasets/f99cb513-7f27-4592-b02e-35141f22afb2.png,EditMIT,"3D, Image, Text",English,,,,,,,"Object Recognition, Pose Estimation, Few-Shot Image Classification, Few-Shot Learning, Few-Shot Semantic Segmentation, Image-to-Text Retrieval, 3D Shape Reconstruction, Zero-shot Text-to-Image Retrieval, Key Point Matching, Image-text Classification",,,See all 844 tasks,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation
FSS-1000,FSS-1000 Dataset,"FSS-1000 is a 1000 class dataset for few-shot segmentation. The dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc.",https://github.com/HKUSTCV/FSS-1000,EditUnknown,Image,,,,,,,,Few-Shot Semantic Segmentation,few-shot-semantic-segmentation-on-fss-1000,,See all 844 tasks,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation
GAS,GAS Dataset,"GAS (Grasp Area Segmentation) dataset consists of 10089 RGB images of cluttered scenes grouped into 1121 grasp-area segmentation tasks. For each RGB image we provide a binary segmentation map with the graspable and non-graspable regions for every object in the scene. The dataset can be used for meta-training part-based grasp area estimation networks.

For creating the GAS dataset we use the RGB images and corresponding ground truth segmentation masks from the GraspNet 1-Billion dataset.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Time Series",,,,,,,,"Grasp Contact Prediction, Few-Shot Semantic Segmentation",,,See all 844 tasks,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation
PASCAL-5i,PASCAL-5i Dataset,PASCAL-5i is a dataset used to evaluate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples from 5 classes that are used for evaluating the few-shot learning method. The rest 15 classes are used for training.,https://arxiv.org/abs/1902.11123,EditUnknown,Image,,,,,,valuate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples,5,"Few-Shot Learning, Semantic Segmentation, Few-Shot Semantic Segmentation",few-shot-semantic-segmentation-on-pascal5i-1,,See all 844 tasks,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation
Citeseer,Citeseer Dataset,The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words.,https://linqs.soe.ucsc.edu/data,EditUnknown,"Graph, Image, Time Series",,,,,,,,"Node Clustering, Link Prediction, Graph Classification, Graph Clustering, Node Classification, Community Detection","node-classification-on-citeseer-1, community-detection-on-citeseer, link-prediction-on-citeseer-biased-evaluation, graph-classification-on-citeseer, node-clustering-on-citeseer, node-classification-on-citeseer-with-public-1, graph-clustering-on-citeseer, node-classification-on-citeseer-random, link-prediction-on-citeseer, node-classification-on-citeseer-05, node-classification-on-citeseer-with-public, node-classification-on-citeseer, node-classification-on-citeseer-full, link-prediction-on-citeseer-nonstandard",,See all 844 tasks,Graph Classification76 benchma,Graph Classification76 benchma
Cora,Cora Dataset,The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.,https://relational.fit.cvut.cz/dataset/CORA,EditUnknown,"Graph, Image, Text, Time Series",English,,,,,,,"Node Clustering, Link Prediction, Graph Classification, Graph structure learning, Graph Clustering, Node Classification, Community Detection, Document Classification","30-trainning-unsupervised-with-linear, link-prediction-on-cora-nonstandard-variant, node-classification-on-cora-random-partition, node-classification-on-cora-fixed-5-node-per, link-prediction-on-cora, node-classification-on-cora, graph-classification-on-cora, graph-structure-learning-on-cora, link-prediction-on-cora-biased-evaluation, node-clustering-on-cora, node-classification-on-cora-1, node-classification-on-cora-full-supervised, node-classification-on-cora-fixed-10-node-per, node-classification-on-cora-05, graph-clustering-on-cora, node-classification-on-cora-3, node-classification-on-cora-with-public-split, document-classification-on-cora, node-classification-on-cora-fixed-20-node-per, community-detection-on-cora",,See all 844 tasks,Graph Classification76 benchma,Graph Classification76 benchma
MNIST,MNIST Dataset,"The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.",http://yann.lecun.com/exdb/mnist/,EditUnknown,"Graph, Image, Text, Time Series, Video",English,,,,000 examples,"training set of 60,000 examples",,"Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly, Density Estimation, Handwritten Digit Recognition, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Domain Adaptation, Personalized Federated Learning, Anomaly Detection, Continuously Indexed Domain Adaptation, Continual Learning, Network Pruning, Malicious Detection, Graph Classification, Image Clustering, One-Shot Learning, Fine-Grained Image Classification, Unsupervised MNIST, Superpixel Image Classification, Rotated MNIST, Unsupervised Image-To-Image Translation, Image Classification, Core set discovery, Nature-Inspired Optimization Algorithm, Sparse Learning and binarization, Multiview Clustering, Stochastic Optimization, Unsupervised Anomaly Detection, Structured Prediction, Adversarial Defense against FGSM Attack, Unsupervised Image Classification, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Adversarial Defense, Classification with Binary Weight Network, Model Poisoning, Hard-label Attack, Video Prediction, Clustering Algorithms Evaluation, Sequential Image Classification, Image Generation, Deep Clustering, Neural Architecture Search, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, General Classification","image-clustering-on-mnist-test, image-classification-on-mnist, domain-adaptation-on-svnh-to-mnist, density-estimation-on-mnist, neural-architecture-search-on-mnist, image-clustering-on-mnist-full, continuously-indexed-domain-adaptation-on-3, network-pruning-on-mnist, unsupervised-image-classification-on-mnist, model-poisoning-on-mnist, unsupervised-anomaly-detection-with-specified-17, graph-classification-on-mnist, adversarial-defense-on-mnist, anomaly-detection-on-mnist-test, continual-learning-on-rotated-mnist, image-generation-on-mnist, clustering-algorithms-evaluation-on-mnist, image-classification-on-noisy-mnist-awgn, sparse-learning-and-binarization-on-mnist, core-set-discovery-on-mnist, stochastic-optimization-on-mnist, anomaly-detection-on-mnist, hard-label-attack-on-mnist, classification-with-binary-weight-network-on-3, superpixel-image-classification-on-75, image-classification-on-noisy-mnist-motion, domain-adaptation-on-usps-to-mnist, deep-clustering-on-mnist, unsupervised-mnist-on-mnist, handwritten-digit-recognition-on-mnist, unsupervised-anomaly-detection-with-specified-22, malicious-detection-on-mnist, domain-adaptation-on-rotating-mnist, fine-grained-image-classification-on-mnist, video-prediction-on-moving-mnist, unsupervised-anomaly-detection-with-specified-10, unsupervised-anomaly-detection-with-specified-23, image-clustering-on-mnist, one-shot-learning-on-mnist, unsupervised-anomaly-detection-on-mnist-1, domain-adaptation-on-mnist-to-usps, sequential-image-classification-on-sequential, structured-prediction-on-mnist, multiview-clustering-on-mnist, adversarial-defense-against-fgsm-attack-on, unsupervised-anomaly-detection-with-specified-13, image-classification-on-noisy-mnist-contrast, unsupervised-image-to-image-translation-on, personalized-federated-learning-on-mnist-1, general-classification-on-mnist, nature-inspired-optimization-algorithm-on, rotated-mnist-on-rotated-mnist-1",,See all 844 tasks,Graph Classification76 benchma,Graph Classification76 benchma
MUTAG,MUTAG Dataset,"In particular, MUTAG is a collection of nitroaromatic compounds and the goal is to predict their mutagenicity on Salmonella typhimurium. Input graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms. It includes 188 samples of chemical compounds with 7 discrete node labels.",https://arxiv.org/abs/1911.08941,EditUnknown,"Graph, Image",,,,,188 samples,,,"Explanation Fidelity Evaluation, Graph Classification, Node Classification","graph-classification-on-mutag, node-classification-on-mutag, explanation-fidelity-evaluation-on-mutag",,See all 844 tasks,Graph Classification76 benchma,Graph Classification76 benchma
OGB,OGB Dataset,"The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. OGB datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can be evaluated using the OGB Evaluator in a unified manner.
OGB is a community-driven initiative in active development.",https://ogb.stanford.edu/,EditUnknown,"Graph, Image, Time Series",,,,,,,,"Link Prediction, Graph Classification, Graph Property Prediction, Link Property Prediction, Node Property Prediction, Node Classification","graph-property-prediction-on-ogbg-molhiv, node-property-prediction-on-ogbn-arxiv, link-property-prediction-on-ogbl-wikikg2, node-property-prediction-on-ogbn-papers100m, node-property-prediction-on-ogbn-proteins, link-property-prediction-on-ogbl-collab, node-property-prediction-on-ogbn-products, link-property-prediction-on-ogbl-ppa, graph-property-prediction-on-ogbg-molpcba, link-property-prediction-on-ogbl-biokg, link-property-prediction-on-ogbl-citation2, link-property-prediction-on-ogbl-ddi, graph-property-prediction-on-ogbg-ppa, node-property-prediction-on-ogbn-mag, graph-property-prediction-on-ogbg-code2, link-prediction-on-ogbl-collab",,See all 844 tasks,Graph Classification76 benchma,Graph Classification76 benchma
PROTEINS,PROTEINS Dataset,PROTEINS is a dataset of proteins that are classified as enzymes or non-enzymes. Nodes represent the amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart.,https://arxiv.org/abs/1911.08941,EditVarious,"Graph, Image",,,,,,,,Graph Classification,graph-classification-on-proteins,,See all 844 tasks,Graph Classification76 benchma,Graph Classification76 benchma
COMA,COMA Dataset,"CoMA contains 17,794 meshes of the human face in various expressions",https://arxiv.org/abs/1905.10290,EditCustom (non-commercial),Graph,,,,,,,,Graph Representation Learning,graph-representation-learning-on-coma,,See all 844 tasks,Graph Representation Learning2,Graph Representation Learning2
IMDB-BINARY,IMDB-BINARY Dataset,"IMDB-BINARY is a movie collaboration dataset that consists of the ego-networks of 1,000 actors/actresses who played roles in movies in IMDB. In each graph, nodes represent actors/actress, and there is an edge between them if they appear in the same movie. These graphs are derived from the Action and Romance genres.",https://arxiv.org/abs/1811.03508,EditUnknown,"Graph, Image",,,,,,,,"Graph Representation Learning, Graph Classification","graph-classification-on-imdb-b, graph-classification-on-imdb-binary",,See all 844 tasks,Graph Representation Learning2,Graph Representation Learning2
Myket_Android_Application_Install,Myket Android Application Install Dataset,"This dataset contains information on application install interactions of users in the Myket android application market. The dataset was created for the purpose of evaluating interaction prediction models, requiring user and item identifiers along with timestamps of the interactions. Hence, the dataset can be used for interaction prediction and building a recommendation system. Furthermore, the data forms a dynamic network of interactions, and we can also perform network representation learning on the nodes in the network, which are users and applications.

Data Creation
The dataset was initially generated by the Myket data team, and later cleaned and subsampled by Erfan Loghmani a master student at Sharif University of Technology at the time. The data team focused on a two-week period and randomly sampled 1/3 of the users with interactions during that period. They then selected install and update interactions for three months before and after the two-week period, resulting in interactions spanning about 6 months and two weeks.

We further subsampled and cleaned the data to focus on application download interactions. We identified the top 8000 most installed applications and selected interactions related to them. We retained users with more than 32 interactions, resulting in 280,391 users. From this group, we randomly selected 10,000 users, and the data was filtered to include only interactions for these users. The detailed procedure can be found in here.

Data Structure
The dataset has two main files.


myket.csv: This file contains the interaction information and follows the same format as the datasets used in the ""JODIE: Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks"" (ACM SIGKDD 2019) project. However, this data does not contain state labels and interaction features, resulting in associated columns being all zero.
app_info_sample.csv: This file comprises features associated with applications present in the sample. For each individual application, information such as the approximate number of installs, average rating, count of ratings, and category are included. These features provide insights into the applications present in the dataset.

Dataset Details

Total Instances: 694,121 install interaction instances
Instances Format: Triplets of user_id, app_name, timestamp
10,000 users and 7,988 android applications
Item features for 7,606 applications

For a detailed summary of the data's statistics, including information on users, applications, and interactions, please refer to the Python notebook available at summary-stats.ipynb. The notebook provides an overview of the dataset's characteristics and can be helpful for understanding the data's structure before using it for research or analysis.

Top 20 Most Installed Applications
| Package Name                       | Count of Interactions |
| ---------------------------------- | --------------------- |
| com.instagram.android              | 15292                 |
| ir.resaneh1.iptv                   | 12143                 |
| com.tencent.ig                     | 7919                  |
| com.ForgeGames.SpecialForcesGroup2 | 7797                  |
| ir.nomogame.ClutchGame             | 6193                  |
| com.dts.freefireth                 | 6041                  |
| com.whatsapp                       | 5876                  |
| com.supercell.clashofclans         | 5817                  |
| com.mojang.minecraftpe             | 5649                  |
| com.lenovo.anyshare.gps            | 5076                  |
| ir.medu.shad                       | 4673                  |
| com.firsttouchgames.dls3           | 4641                  |
| com.activision.callofduty.shooter  | 4357                  |
| com.tencent.iglite                 | 4126                  |
| com.aparat                         | 3598                  |
| com.kiloo.subwaysurf               | 3135                  |
| com.supercell.clashroyale          | 2793                  |
| co.palang.QuizOfKings              | 2589                  |
| com.nazdika.app                    | 2436                  |
| com.digikala                       | 2413                  |

Comparison with SNAP Datasets
The Myket dataset introduced in this repository exhibits distinct characteristics compared to the real-world datasets used by the project. The table below provides a comparative overview of the key dataset characteristics:

| Dataset         | #Users           | #Items          | #Interactions | Average Interactions per User | Average Unique Items per User |
| --------------- | ---------------- | --------------- | ------------- | ----------------------------- | ----------------------------- |
| Myket | 10,000 | 7,988 | 694,121       | 69.4                          | 54.6                          |
| LastFM          | 980              | 1,000           | 1,293,103     | 1,319.5                       | 158.2                         |
| Reddit          | 10,000 | 984             | 672,447       | 67.2                          | 7.9                           |
| Wikipedia       | 8,227            | 1,000           | 157,474       | 19.1                          | 2.2                           |
| MOOC            | 7,047            | 97              | 411,749       | 58.4                          | 25.3                          |

The Myket dataset stands out by having an ample number of both users and items, highlighting its relevance for real-world, large-scale applications. Unlike LastFM, Reddit, and Wikipedia datasets, where users exhibit repetitive item interactions, the Myket dataset contains a comparatively lower amount of repetitive interactions. This unique characteristic reflects the diverse nature of user behaviors in the Android application market environment.

Citation
If you use this dataset in your research, please cite the following preprint:

@misc{loghmani2023effect,
      title={Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks}, 
      author={Erfan Loghmani and MohammadAmin Fazli},
      year={2023},
      eprint={2308.06862},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}",https://production-media.paperswithcode.com/datasets/7d27d890-9462-4628-b0c3-8b0a2ae42d4e.png,EditMIT License,"Graph, Time Series",,2019,,,,,,"Graph Representation Learning, Link Prediction",,,See all 844 tasks,Graph Representation Learning2,Graph Representation Learning2
REDDIT-BINARY,REDDIT-BINARY Dataset,"REDDIT-BINARY consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them respond to the other’s comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answer-based community or a discussion-based community.",https://arxiv.org/abs/1811.03508,EditUnknown,"Graph, Image",,,,,,,,"Graph Representation Learning, Graph Classification","graph-classification-on-reddit-b, graph-classification-on-reddit-binary",,See all 844 tasks,Graph Representation Learning2,Graph Representation Learning2
Reddit,Reddit Dataset,"The Reddit dataset is a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or “subreddit”, that a post belongs to. 50 large communities have been sampled to build a post-to-post graph, connecting posts if the same user comments on both. In total this dataset contains 232,965 posts with an average degree of 492. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). For features, off-the-shelf 300-dimensional GloVe CommonCrawl word vectors are used.",https://arxiv.org/pdf/1706.02216.pdf,EditUnknown,"Graph, Image, Text, Time Series",English,2014,,,,,,"Dialogue Generation, Topic Models, Open-Domain Dialog, Classification, Text Summarization, Graph Classification, Topological Data Analysis, Dynamic Link Prediction, Graph Representation Learning, Text Classification, News Generation, Node Classification, Question Answering, News Recommendation, Dialogue Evaluation, Sarcasm Detection, Quantization, Generative Question Answering, Conversational Response Selection, Abstractive Text Summarization","graph-classification-on-reddit-multi-12k, sarcasm-detection-on-figlang-2020-reddit, graph-classification-on-reddit-multi-5k, dialogue-generation-on-reddit-multi-ref, conversational-response-selection-on-polyai, dynamic-link-prediction-on-reddit, classification-on-reddit-ideology-database, graph-classification-on-reddit-12k, graph-classification-on-reddit-binary, question-answering-on-squadshifts-reddit, node-classification-on-reddit, graph-classification-on-reddit-b, text-summarization-on-reddit-tifu",,See all 844 tasks,Graph Representation Learning2,Graph Representation Learning2
WikiGraphs,WikiGraphs Dataset,"WikiGraphs is a dataset of Wikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning. Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data. 

WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark with a subgraph from the Freebase knowledge graph. This makes it easy to benchmark against other state-of-the-art text generative models that are capable of generating long paragraphs of coherent text. Both the graphs and the text data are of significantly larger scale compared to prior graph-text paired datasets.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-07-21_at_13.26.42.png,EditApache License 2.0,"Graph, Text",English,,,,,,,"Graph Representation Learning, KG-to-Text Generation, Conditional Text Generation, Graph Generation",kg-to-text-generation-on-wikigraphs,,See all 844 tasks,Graph Representation Learning2,Graph Representation Learning2
ADE20K,ADE20K Dataset,"The ADE20K semantic segmentation dataset contains more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels. There are totally 150 semantic categories, which include stuffs like sky, road, grass, and discrete objects like person, car, bed.",https://arxiv.org/abs/1911.00679,"EditCustom (research-only, non-commercial)","3D, Audio, Image, Text",English,,,,,,,"Scene Recognition, Instance Segmentation, Overlapped 100-10, Reconstruction, Face Detection, Open Vocabulary Semantic Segmentation, Semantic Segmentation, Speech Prompted Semantic Segmentation, Sound Prompted Semantic Segmentation, Image-to-Image Translation, Semi-Supervised Semantic Segmentation, Unsupervised Semantic Segmentation with Language-image Pre-training, Zero-Shot Semantic Segmentation, Scene Understanding, Overlapped 25-25, Overlapped 100-5, Continual Semantic Segmentation, Pose Transfer, Open Vocabulary Panoptic Segmentation, Panoptic Segmentation, Weakly-Supervised Semantic Segmentation, Overlapped 50-50, Overlapped 100-50","zero-shot-semantic-segmentation-on-ade20k-847, overlapped-100-5-on-ade20k, speech-prompted-semantic-segmentation-on, unsupervised-semantic-segmentation-with-4, face-detection-on-ade20k, reconstruction-on-ade20k, open-vocabulary-panoptic-segmentation-on, scene-recognition-on-ade20k, weakly-supervised-semantic-segmentation-on-20, panoptic-segmentation-on-ade20k, open-vocabulary-semantic-segmentation-on-3, open-vocabulary-semantic-segmentation-on-2, continual-semantic-segmentation-on-ade20k, semantic-segmentation-on-ade20k, overlapped-100-50-on-ade20k, semi-supervised-semantic-segmentation-on-41, panoptic-segmentation-on-ade20k-val, overlapped-25-25-on-ade20k, pose-transfer-on-ade20k, scene-understanding-on-ade20k-val-1, overlapped-100-10-on-ade20k, image-to-image-translation-on-ade20k-labels, semantic-segmentation-on-ade20k-val, instance-segmentation-on-ade20k-val, image-to-image-translation-on-ade20k-outdoor, overlapped-50-50-on-ade20k, semi-supervised-semantic-segmentation-on-42, sound-prompted-semantic-segmentation-on",,See all 844 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
AFHQ,AFHQ Dataset,"Animal FacesHQ (AFHQ) is a dataset of animal faces consisting of 15,000 high-quality images at 512 × 512 resolution. The dataset includes three domains of cat, dog, and wildlife, each providing 5000 images. By having multiple (three) domains and diverse images of various
breeds (≥ eight) per each domain, AFHQ sets a more challenging image-to-image translation problem. 
All images are vertically and horizontally aligned to have the eyes at the center. The low-quality images were discarded by human effort.",https://arxiv.org/abs/1912.01865,EditCC BY-NC 4.0,"Image, Text",English,,,,5000 images,,,"Multimodal Unsupervised Image-To-Image Translation, Image Generation, Image-to-Image Translation","image-to-image-translation-on-afhq, image-generation-on-afhq-cat, image-generation-on-afhq-dog, image-generation-on-afhq-wild, image-generation-on-afhqv2, multimodal-unsupervised-image-to-image-5",,See all 844 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
CelebA-HQ,CelebA-HQ Dataset,"The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.",https://arxiv.org/abs/1807.06358,EditCC BY-NC 4,"Image, Text",English,,,,000 images,,,"Image Super-Resolution, Density Estimation, Image-to-Image Translation, Unconditional Image Generation, Image Inpainting, Blind Face Restoration, Multimodal Unsupervised Image-To-Image Translation, Image Generation","image-generation-on-celeba-hq, image-generation-on-celeba-hq-256x256, blind-face-restoration-on-celeba-hq, image-to-image-translation-on-celeba-hq, image-super-resolution-on-celeba-hq-128x128, image-inpainting-on-celeba-hq, unconditional-image-generation-on-celeba-hq, density-estimation-on-celeba-hq-256x256, image-generation-on-celeba-hq-128x128, image-super-resolution-on-celeb-hq-4x, image-generation-on-celeba-hq-64x64, image-generation-on-celeba-hq-512x512, multimodal-unsupervised-image-to-image-4, image-generation-on-celeba-hq-1024x1024",,See all 844 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
COCO-Stuff,COCO-Stuff Dataset,"The Common Objects in COntext-stuff (COCO-stuff) dataset is a dataset for scene understanding tasks like semantic segmentation, object detection and image captioning. It is constructed by annotating the original COCO dataset, which originally annotated things while neglecting stuff annotations. There are 164k images in COCO-stuff dataset that span over 172 categories including 80 things, 91 stuff, and 1 unlabeled class.",https://arxiv.org/abs/2008.10774,EditVarious,"Image, Text",English,,,,164k images,,172,"Layout-to-Image Generation, Unsupervised Semantic Segmentation, Image-to-Image Translation, Open Vocabulary Semantic Segmentation, Sketch-to-Image Translation, Unsupervised Semantic Segmentation with Language-image Pre-training, Zero-Shot Semantic Segmentation, Unsupervised Image Segmentation, Real-Time Semantic Segmentation, Semantic Segmentation","semantic-segmentation-on-coco-stuff-27, open-vocabulary-semantic-segmentation-on-coco, layout-to-image-generation-on-coco-stuff-2, unsupervised-semantic-segmentation-on-coco-7, unsupervised-image-segmentation-on-coco-stuff, image-to-image-translation-on-coco-stuff, semantic-segmentation-on-coco-stuff-test, unsupervised-semantic-segmentation-on-coco-8, real-time-semantic-segmentation-on-coco-stuff-1, unsupervised-semantic-segmentation-on-coco-1, unsupervised-semantic-segmentation-on-coco, semantic-segmentation-on-coco-stuff, semantic-segmentation-on-coco-stuff-full, unsupervised-semantic-segmentation-with-1, unsupervised-semantic-segmentation-with-9, layout-to-image-generation-on-coco-stuff-3, layout-to-image-generation-on-coco-stuff-4, sketch-to-image-translation-on-coco-stuff, zero-shot-semantic-segmentation-on-coco-stuff, unsupervised-semantic-segmentation-on-coco-6",,See all 844 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
DeepFashion,DeepFashion Dataset,"DeepFashion is a dataset containing around 800K diverse fashion images with their rich annotations (46 categories, 1,000 descriptive attributes, bounding boxes and landmark information) ranging from well-posed product images to real-world-like consumer photos.",https://arxiv.org/abs/2007.05080,"EditCustom (research-only, non-commercial, attribution)","3D, Image, Text",English,,,,,,46,"Unsupervised Human Pose Estimation, Image-to-Image Translation, Pose Transfer, Image Retrieval, Virtual Try-on, Text-to-3D-Human Generation","pose-transfer-on-deep-fashion, image-to-image-translation-on-deep-fashion-1, image-retrieval-on-deepfashion, unsupervised-human-pose-estimation-on, text-to-3d-human-generation-on-deepfashion, virtual-try-on-on-deep-fashion",,See all 844 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
GTA5,GTA5 Dataset,The GTA5 dataset contains 24966 synthetic images with pixel level semantic annotation. The images have been rendered using the open-world video game Grand Theft Auto 5 and are all from the car perspective in the streets of American-style virtual cities. There are 19 semantic classes which are compatible with the ones of Cityscapes dataset.,https://arxiv.org/abs/1909.00781,EditResearch and educational use only,"Image, Text",English,,,,,,,"Image-to-Image Translation, One-shot Unsupervised Domain Adaptation, Source-Free Domain Adaptation, Domain Adaptation, Synthetic-to-Real Translation, Semantic Segmentation, Unsupervised Domain Adaptation","source-free-domain-adaptation-on-gta5-to, semantic-segmentation-on-gtav-to-cityscapes-1, one-shot-unsupervised-domain-adaptation-on, image-to-image-translation-on-gtav-to, synthetic-to-real-translation-on-gtav-to, unsupervised-domain-adaptation-on-gtav-to, domain-adaptation-on-gta5-synscapes-to, domain-adaptation-on-gta5-to-cityscapes",,See all 844 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
Perceptual_Similarity,Perceptual Similarity Dataset,Perceptual Similarity is a dataset of human perceptual similarity judgments.,https://arxiv.org/pdf/1801.03924.pdf,EditUnknown,"Image, Text",English,,,,,,,"Image Super-Resolution, Image Generation, Image-to-Image Translation",,,See all 844 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
SYNTHIA,SYNTHIA Dataset,The SYNTHIA dataset is a synthetic dataset that consists of 9400 multi-viewpoint photo-realistic frames rendered from a virtual city and comes with pixel-level semantic annotations for 13 classes. Each frame has resolution of 1280 × 960.,https://arxiv.org/abs/1907.12849,EditCC BY-NC-SA 3.0,"Image, Text",English,,,,,,13,"Image-to-Image Translation, One-shot Unsupervised Domain Adaptation, Source-Free Domain Adaptation, Novel View Synthesis, Domain Adaptation, Synthetic-to-Real Translation, Semantic Segmentation, Unsupervised Domain Adaptation","semantic-segmentation-on-synthia-to, novel-view-synthesis-on-synthia-novel-view, semantic-segmentation-on-synthia, image-to-image-translation-on-synthia-fall-to, semantic-segmentation-on-synthia-cvpr16, image-to-image-translation-on-synthia-to, one-shot-unsupervised-domain-adaptation-on-1, synthetic-to-real-translation-on-synthia-to-1, source-free-domain-adaptation-on-synthia-to, domain-adaptation-on-synthia-to-cityscapes, unsupervised-domain-adaptation-on-synthia-to",,See all 844 tasks,Image-to-Image Translation62 b,Image-to-Image Translation62 b
Intel_Image_Classification,Intel Image Classification Dataset,"Context
This is image data of Natural Scenes around the world.

Content
This Data contains around 25k images of size 150x150 distributed under 6 categories.
{'buildings' -> 0,
'forest' -> 1,
'glacier' -> 2,
'mountain' -> 3,
'sea' -> 4,
'street' -> 5 }

The Train, Test and Prediction data is separated in each zip files. There are around 14k images in Train, 3k in Test and 7k in Prediction.
This data was initially published on https://datahack.analyticsvidhya.com by Intel to host a Image classification Challenge.

Acknowledgements
Thanks to https://datahack.analyticsvidhya.com for the challenge and Intel for the Data

Photo by Jan Böttinger on Unsplash

Inspiration
Want to build powerful Neural network that can classify these images with more accuracy.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,25k images,"Train, Test and Prediction data is separated in each zip files. There are around 14k images",6,"Image Classification, Image Augmentation","image-augmentation-on-intel-image, image-classification-on-intel-image",,See all 844 tasks,Image Augmentation1 benchmark1,Image Augmentation1 benchmark1
Leishmania_parasite_dataset,Leishmania parasite dataset Dataset,"This dataset includes sharp-blur pairs of Leishmania image, which is a protozoan parasite microscopy image dataset of Leishmania, obtained from the preserved slides stained with Giemsa. The paired blur-sharp images are acquired by employing a bright-field microscope (Olympus IX53) with 100× magnification oil immersion objectives.We first capture the sharp images as ground truth, then acquire its corresponding out-of-focus images. The extent and nature of defocusing are random along the optical axis, where the degree of out-of-focus is inconsistent from image-to-image. This dataset includes 764 in-focus and 764 corresponding out-of-focus images, where each image is composed of 2304 × 1728 pixels in 24-bit JPG format.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Image Deblurring, Medical Image Generation",,,See all 844 tasks,Image Deblurring10 benchmarks1,Image Deblurring10 benchmarks1
CelebA,CelebA Dataset,"CelebFaces Attributes dataset contains 202,599 face images of the size 178×218 from 10,177 celebrities, each annotated with 40 binary labels indicating facial attributes like hair color, gender and age.",https://arxiv.org/abs/1811.07483,EditUnknown,"Image, Text, Time Series",English,,,,,,,"HeavyMakeup/Bias-conflicting, Face Alignment, Facial Expression Translation, Physical Attribute Prediction, Image Super-Resolution, Image Compressed Sensing, Multi-Task Learning, HairColor/Bias-conflicting, Concept-based Classification, Image Attribution, Image Classification, Blind Face Restoration, Image Deblurring, HairColor/Unbiased, Image Colorization, Image Inpainting, Interpretability Techniques for Deep Learning, HeavyMakeup/Unbiased, Image Generation, Long-tail Learning","image-deblurring-on-celeba, image-generation-on-celeba-3, concept-based-classification-on-celeba, face-alignment-on-celeba-aligned, multi-task-learning-on-celeba, heavymakeup-unbiased-on-celeba, haircolor-bias-conflicting-on-celeba, image-classification-on-celeba-64x64, facial-expression-translation-on-celeba, image-generation-on-celeba-64x64, haircolor-unbiased-on-celeba, interpretability-techniques-for-deep-learning-1, image-super-resolution-on-celeba, image-generation-on-celeba-128x128, image-generation-on-celeba-256x256, image-inpainting-on-celeba, heavymakeup-bias-conflicting-on-celeba, face-alignment-on-celeba-aflw-unaligned, image-compressed-sensing-on-celeba, blind-face-restoration-on-celeba-test, image-attribution-on-celeba, long-tail-learning-on-celeba-5, image-colorization-on-celeba",,See all 844 tasks,Image Generation243 benchmarks,Image Generation243 benchmarks
CUB-200-2011,CUB-200-2011 Dataset,"The Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from Reed et al.. They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.",https://arxiv.org/abs/1709.00340,EditUnknown,"3D, Audio, Graph, Image, Text",English,2011,,,788 images,,,"Long-tail learning with class descriptors, Multimodal Deep Learning, Cross-Domain Few-Shot, Dataset Distillation - 1IPC, Multi-Modal Document Classification, Few-Shot Class-Incremental Learning, Image Clustering, Interpretable Machine Learning, Concept-based Classification, Fine-Grained Image Classification, Text-to-Image Generation, Error Understanding, Document Text Classification, Generalized Zero-Shot Learning, Zero-Shot Learning, Bird Species Classification With Audio-Visual Data, Image Attribution, Small Data Image Classification, Image Classification, Weakly-Supervised Object Localization, Image Retrieval, Few-Shot Learning, Graph Matching, Single-View 3D Reconstruction, Unsupervised Keypoint Estimation, Fine-Grained Image Recognition, Semantic correspondence, Generalized Few-Shot Learning, Point-interactive Image Colorization, Fine-Grained Visual Recognition, Multimodal Text and Image Classification, Few-Shot Image Classification, Metric Learning, Image Generation, Transductive Zero-Shot Classification","zero-shot-learning-on-cub-200-2011, few-shot-image-classification-on-cub-200-50, image-clustering-on-cub-birds, concept-based-classification-on-cub-200-2011, long-tail-learning-with-class-descriptors-on, few-shot-image-classification-on-cub-200-2011-1, unsupervised-keypoint-estimation-on-cub, image-clustering-on-cub-200-2011, transductive-zero-shot-classification-on-cub, few-shot-image-classification-on-cub-200-5, image-classification-on-cub, image-classification-on-cub-200-2011-3, cross-domain-few-shot-on-cub, fine-grained-image-classification-on, dataset-distillation-1ipc-on-cub-200-2011, few-shot-image-classification-on-cub-200-0, fine-grained-image-recognition-on-cub-birds, zero-shot-learning-on-cub-200-0-shot-learning-1, point-interactive-image-colorization-on-cub, fine-grained-visual-recognition-on-cub-200-1, fine-grained-image-classification-on-cub-200, multi-modal-document-classification-on-cub, small-data-on-cub-200-2011-5-samples-per-1, generalized-zero-shot-learning-on-cub-200, multimodal-deep-learning-on-cub-200-2011, interpretable-machine-learning-on-cub-200, image-classification-on-imbalanced-cub-200, image-attribution-on-cub-200-2011-1, weakly-supervised-object-localization-on-cub, error-understanding-on-cub-200-2011-1, metric-learning-on-cub-200-2011-4, fine-grained-image-recognition-on-cub-200, weakly-supervised-object-localization-on-cub-2, generalized-few-shot-learning-on-cub, graph-matching-on-cub, few-shot-image-classification-on-cub-200-2011-2, few-shot-image-classification-on-cub-200-2011, few-shot-class-incremental-learning-on-cub, image-generation-on-cub-128-x-128, image-retrieval-on-cub-200-2011, weakly-supervised-object-localization-on-cub-1, metric-learning-on-cub-200-2011, single-view-3d-reconstruction-on-cub-200-2011, small-data-on-cub-200-2011-30-samples-per-1, document-text-classification-on-cub-200-2011, multimodal-text-and-image-classification-on, text-to-image-generation-on-cub, semantic-correspondence-on-cub-200-2011, few-shot-image-classification-on-cub-200-5-1, fine-grained-image-classification-on-cub-200-1",,See all 844 tasks,Image Generation243 benchmarks,Image Generation243 benchmarks
Fashion-MNIST,Fashion-MNIST Dataset,"Fashion-MNIST is a dataset comprising of 28×28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST.",https://arxiv.org/abs/1807.02588,EditMIT,"Image, Text",English,,,,000 images,"training set has 60,000 images",10,"Outlier Detection, Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Image Clustering, Multiview Clustering, Model Poisoning, Clustering Algorithms Evaluation, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, Unsupervised Anomaly Detection, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Image Generation, Domain Generalization, Out-of-Distribution Detection, Anomaly Detection, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, Image Classification, General Classification","unsupervised-anomaly-detection-with-specified-25, unsupervised-anomaly-detection-with-specified-14, unsupervised-anomaly-detection-with-specified-11, image-classification-on-fashion-mnist, anomaly-detection-on-fashion-mnist, unsupervised-anomaly-detection-with-specified-18, out-of-distribution-detection-on-fashion, general-classification-on-fashion-mnist, clustering-algorithms-evaluation-on-fashion-2, image-generation-on-fashion-mnist, unsupervised-anomaly-detection-with-specified-27, outlier-detection-on-fashion-mnist, unsupervised-anomaly-detection-on-fashion-1, domain-generalization-on-rotated-fashion, image-clustering-on-fashion-mnist, model-poisoning-on-fashion-mnist, multiview-clustering-on-fashion-mnist",,See all 844 tasks,Image Generation243 benchmarks,Image Generation243 benchmarks
FFHQ,FFHQ Dataset,"Flickr-Faces-HQ (FFHQ) consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped using dlib. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Amazon Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.",https://github.com/NVlabs/ffhq-dataset,EditCC BY-NC-SA 4.0,"3D, Image, Text",English,,,,,,,"Image Super-Resolution, 3D-Aware Image Synthesis, Image Inpainting, Image Denoising, Image Generation, Face Hallucination, Facial Inpainting","image-generation-on-ffhq-64x64-4x-upscaling, 3d-aware-image-synthesis-on-ffhq-512-x-512-4x, image-denoising-on-ffhq, image-denoising-on-ffhq-64x64-4x-upscaling, facial-inpainting-on-ffhq, image-super-resolution-on-ffhq-512-x-512-4x, image-generation-on-ffhq-256-x-256, 3d-aware-image-synthesis-on-ffhq-256-x-256, image-super-resolution-on-ffhq-1024-x-1024-4x, image-inpainting-on-ffhq-1024-x-1024, image-generation-on-ffhq-1024-x-1024, face-hallucination-on-ffhq-512-x-512-16x, image-generation-on-ffhq-u, image-inpainting-on-ffhq-512-x-512, image-generation-on-ffhq-512-x-512, image-generation-on-ffhq, image-super-resolution-on-ffhq-256-x-256-4x",,See all 844 tasks,Image Generation243 benchmarks,Image Generation243 benchmarks
Oxford_102_Flower,Oxford 102 Flower Dataset,"Oxford 102 Flower is an image classification dataset consisting of 102 flower categories. The flowers chosen to be flower commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images.

The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories.",https://production-media.paperswithcode.com/datasets/flowers.jpg,EditUnknown,"Image, Text",English,,,,258 images,,,"Point-interactive Image Colorization, Image Clustering, Fine-Grained Image Classification, Prompt Engineering, Unsupervised Image Segmentation, Few-Shot Image Classification, Text-to-Image Generation, Few-Shot Learning, Generalized Zero-Shot Learning, Zero-Shot Learning, Continual Learning, Image Generation, Neural Architecture Search, Image Classification, Transductive Zero-Shot Classification","point-interactive-image-colorization-on-1, generalized-zero-shot-learning-on-oxford-102-1, prompt-engineering-on-oxford-102-flower, fine-grained-image-classification-on-oxford, few-shot-image-classification-on-oxford-102, image-classification-on-flowers-102, continual-learning-on-flowers-fine-grained-6, image-generation-on-oxford-102-flowers-256-x, few-shot-learning-on-flowers-102, zero-shot-learning-on-oxford-102-flower, image-clustering-on-flowers-102, text-to-image-generation-on-oxford-102, unsupervised-image-segmentation-on-flowers, neural-architecture-search-on-oxford-102, few-shot-image-classification-on-flowers-102-1, transductive-zero-shot-classification-on-2, zero-shot-learning-on-flowers-102",,See all 844 tasks,Image Generation243 benchmarks,Image Generation243 benchmarks
ApolloScape,ApolloScape Dataset,"ApolloScape is a large dataset consisting of over 140,000 video frames (73 street scene videos) from various locations in China under varying weather conditions. Pixel-wise semantic annotation of the recorded data is provided in 2D, with point-wise semantic annotation in 3D for 28 classes. In addition, the dataset contains lane marking annotations in 2D.",https://arxiv.org/abs/2004.06320,EditCustom (research-only),"Image, Time Series, Video",,,,,,,28,"Object Detection, Image Inpainting, Motion Segmentation, Autonomous Driving, Semantic Segmentation, Trajectory Prediction","semantic-segmentation-on-apolloscape, trajectory-prediction-on-apolloscape, image-inpainting-on-apolloscape, test-results-on-apolloscape, motion-segmentation-on-apolloscape",,See all 844 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
CASIA_V2,CASIA V2 Dataset,"CASIA V2 is a dataset for forgery classification. It contains 4795 images, 1701 authentic and 3274 forged.",https://arxiv.org/abs/1911.07932,EditUnknown,Image,,,,,4795 images,,,"Image Manipulation, Image Inpainting, Domain Adaptation",,,See all 844 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
FVI,FVI Dataset,"The Free-Form Video Inpainting dataset is a dataset used for training and evaluation video inpainting models. It consists of 1940 videos from the YouTube-VOS dataset and 12,600 videos from the YouTube-BoundingBoxes.",https://arxiv.org/abs/1904.10247,EditUnknown,"Image, Video",,1940,,,,,,"Image Inpainting, Video Inpainting",,,See all 844 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
Places,Places Dataset,"The Places dataset is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5,000 images per category.",https://arxiv.org/abs/1902.06162,EditCC BY,Image,,,,,000 images,,,"Uncropping, Image Inpainting, Cross-Domain Few-Shot","image-inpainting-on-places2-1, cross-domain-few-shot-on-places, uncropping-on-places2-val, image-inpainting-on-places2-val",,See all 844 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
Places365,Places365 Dataset,"The Places365 dataset is a scene recognition dataset. It is composed of 10 million images comprising 434 scene classes. There are two versions of the dataset: Places365-Standard with 1.8 million train and 36000 validation images from K=365 scene classes, and Places365-Challenge-2016, in which the size of the training set is increased up to 6.2 million extra images, including 69 new scene classes (leading to a total of 8 million train images from 434 scene classes).",https://arxiv.org/abs/1909.02410,EditUnknown,Image,,2016,,,,train and 36000 validation images,,"Scene Recognition, Image Inpainting, Semantic Segmentation, Scene Classification, Out-of-Distribution Detection, Image Classification, Image Outpainting","scene-classification-on-places365-standard, image-outpainting-on-places365-standard, image-classification-on-places365-standard, image-inpainting-on-places365, scene-recognition-on-places365, out-of-distribution-detection-on-imagenet-1k-12, image-classification-on-places365, out-of-distribution-detection-on-imagenet-1k-9",,See all 844 tasks,Image Inpainting17 benchmarks3,Image Inpainting17 benchmarks3
CASIA__OSN-transmitted_-_Facebook_,CASIA (OSN-transmitted - Facebook) Dataset,"This dataset is an OSN-transmitted (OSN = Online Social Network) version of the CASIA dataset. 
The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN  - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing 
and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against
transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Image Manipulation, Image Forensics, Image Manipulation Detection",image-manipulation-detection-on-casia-osn,,See all 844 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
CASIA__OSN-transmitted_-_Weibo_,CASIA (OSN-transmitted - Weibo) Dataset,"This dataset is an OSN-transmitted (OSN = Online Social Network) version of the CASIA dataset. 
The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN  - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing 
and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against
transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Detecting Image Manipulation, Image Manipulation, Image Manipulation Localization, Image Manipulation Detection, Image Forgery Detection",image-manipulation-detection-on-casia-osn-3,,See all 844 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
CASIA__OSN-transmitted_-_Whatsapp_,CASIA (OSN-transmitted - Whatsapp) Dataset,"This dataset is an OSN-transmitted (OSN = Online Social Network) version of the CASIA dataset. 
The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN  - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing 
and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against
transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Image Manipulation, Image Manipulation Localization, Image Forensics, Image Manipulation Detection",image-manipulation-detection-on-casia-osn-2,,See all 844 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
CelebAMask-HQ,CelebAMask-HQ Dataset,"CelebAMask-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has segmentation mask of facial attributes corresponding to CelebA.",https://github.com/switchablenorms/CelebAMask-HQ,EditCustom (non-commercial),"3D, Image, Text",English,,,,,,,"Reconstruction, Image-to-Image Translation, 3D-Aware Image Synthesis, Image Manipulation, Pose Transfer, Conditional Image Generation, Image Generation, Face Parsing","conditional-image-generation-on-celebamask-hq, reconstruction-on-celebamask-hq, face-parsing-on-celebamask-hq, 3d-aware-image-synthesis-on-celebamask-hq, pose-transfer-on-celebamask-hq",,See all 844 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
Columbia__OSN-transmitted_-_Facebook_,Columbia (OSN-transmitted - Facebook) Dataset,"This dataset is an OSN-transmitted (Online Social Network) version of the Columbia dataset. Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Image Manipulation, Detecting Image Manipulation, Image Manipulation Detection, Image Forensics",image-manipulation-detection-on-columbia-osn,,See all 844 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
Digital_Forensics_2023_dataset_-_DF2023,Digital Forensics 2023 dataset - DF2023 Dataset,"The deliberate manipulation of public opinion, especially through altered images, poses a significant danger to society. To fight this issue on a technical level we support the research community by releasing the Digital Forensics 2023 (DF2023) training and validation dataset.

The DF2023 training dataset comprises one million images from four major forgery categories:


splicing (400K)
copy-move (300K)
enhancement (200K)
removal (100K)

This dataset enables an objective comparison of network architectures and can significantly reduce the time and effort of researchers preparing datasets.

For a detailed description of the DF2023 dataset, please refer to:

@inproceedings{Fischinger2023DFNet,
title={DF2023: The Digital Forensics 2023 Dataset for Image Forgery Detection},
author={David Fischinger and Martin Boyer},
journal={The 25th Irish Machine Vision and Image Processing conference. (IMVIP)},
year={2023}
}
available from: Zenodo

Naming convention
The naming convention of DF2023 encodes information about the applied manipulations. Each image name has the following form:

COCO_DF_0123456789_NNNNNNNN.{EXT} (e.g. COCO_DF_E000G40117_00200620.jpg)

After the identifier of the image data source (""COCO"") and the self-reference to the Digital Forensics (""DF"") dataset, there are 10 digits as placeholders for the manipulation. Position 0 defines the manipulation types copy-move, splicing, removal, enhancement ([C,S,R,E]). The following digits 1-9 represent donor patch manipulations. For positions [1,2,7,8] (resample, flip, noise and brightness), a binary value indicates if this manipulation was applied to the donor image patch. Position 3 (rotate) indicates by the values 0-3 if the rotation was executed by 0, 90, 180 or 270 degrees. Position 4 defines if BoxBlur (B) or GaussianBlur (G) was used. Position 5 specifies the blurring radius. A value of 0 indicates that no blurring was executed. Position 6 indicates which of the Python-PIL contrast filters EDGE ENHANCE, EDGE ENHANCE MORE, SHARPEN, UnsharpMask or ImageEnhance (values 1-5) was applied. If none of them was applied, this value is set to 0. Finally, position 9 is set to the JPEG compression factor modulo 10, a value of 0 indicates that no JPEG compression was applied. The 8 characters NNNNNNNN in the image name template stand for a running number of the images.",https://production-media.paperswithcode.com/datasets/197859aa-633b-4b18-8fdd-73be98ee9226.jpg,"EditThe DF2023 dataset is based on the MS COCO dataset. Therefore, rules for using the images form MS COCO apply also for DF2023:      Images      The COCO Consortium does not own the copyright of the images. Use of the images must abide by the Flickr Terms of Use. The users of the images accept full responsibility for the use of the dataset, including but not limited to the use of any copies of copyrighted images that they may create from the dataset.",Image,,2023,,,,"val, enhancement ([C,S,R,E]). The following digits 1-9 represent donor patch manipulations. For positions [1,2,7,8] (resample, flip, noise and brightness), a binary value indicates if this manipulation was applied to the donor image patch. Position 3 (rotate) indicates by the values 0-3 if the rotation was executed by 0, 90, 180 or 270 degrees. Position 4 defines if BoxBlur (B) or GaussianBlur (G) was used. Position 5 specifies the blurring radius. A value of 0 indicates that no blurring was executed. Position 6 indicates which of the Python-PIL contrast filters EDGE ENHANCE, EDGE ENHANCE MORE, SHARPEN, UnsharpMask or ImageEnhance (values 1-5) was applied. If none of them was applied, this value is set to 0. Finally, position 9 is set to the JPEG compression factor modulo 10, a value of 0 indicates that no JPEG compression was applied. The 8 characters NNNNNNNN in the image name template stand for a running number of the images",,"Image Manipulation, Image Manipulation Localization, Image Forgery Detection, Image Manipulation Detection",,,See all 844 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
DSO__OSN-transmitted_-_Facebook_,DSO (OSN-transmitted - Facebook) Dataset,"This dataset is an OSN-transmitted (Online Social Network) version of the DSO dataset. Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2022,,,,,,"Image Manipulation, Image Manipulation Localization, Image Forensics, Image Manipulation Detection",image-manipulation-detection-on-dso-osn,,See all 844 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
LRS2,LRS2 Dataset,"The Oxford-BBC Lip Reading Sentences 2 (LRS2) dataset is one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists of mainly news and talk shows from BBC programs. Each sentence is up to 100 characters in length. The training, validation and test sets are divided according to broadcast date. It is a challenging set since it contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances and the test set contains 1,242 utterances.",https://arxiv.org/abs/2001.01656,EditCustom (non-commercial),"Audio, Image, Text",English,,,,,,,"Audio-Visual Speech Recognition, Speech Recognition, Automatic Speech Recognition (ASR), Image Manipulation, Visual Keyword Spotting, Unconstrained Lip-synchronization, Speech Separation, Visual Speech Recognition, Landmark-based Lipreading, Lipreading","lipreading-on-lrs2, landmark-based-lipreading-on-lrs2, visual-speech-recognition-on-lrs2, speech-recognition-on-lrs2, visual-keyword-spotting-on-lrs2, speech-separation-on-lrs2, lip-sync-on-lrs2, automatic-speech-recognition-on-lrs2, audio-visual-speech-recognition-on-lrs2, image-manipulation-on-lrs2",,See all 844 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
NIST__OSN-transmitted_-_Facebook_,NIST (OSN-transmitted - Facebook) Dataset,"This dataset is an OSN-transmitted (Online Social Network) version of the NIST dataset (https://www.nist.gov/itl/iad/mig/nimble-challenge-2017-evaluation). Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,2017,,,,,,"Image Manipulation, Image Manipulation Localization, Image Manipulation Detection, Image Forgery Detection, Image Forensics",image-manipulation-detection-on-nist-osn,,See all 844 tasks,Image Manipulation1 benchmark1,Image Manipulation1 benchmark1
2DeteCT,2DeteCT Dataset,"Maximilian B. Kiss, Sophia B. Coban, K. Joost Batenburg, Tristan van Leeuwen, and Felix Lucka ""2DeteCT - A large 2D expandable, trainable, experimental Computed Tomography dataset for machine learning"",  Sci Data 10, 576 (2023) or arXiv:2306.05907 (2023)

Abstract:
""Recent research in computational imaging largely focuses on developing machine learning (ML) techniques for image reconstruction, which requires large-scale training datasets consisting of measurement data and ground-truth images. However, suitable experimental datasets for X-ray Computed Tomography (CT) are scarce, and methods are often developed and evaluated only on simulated data. We fill this gap by providing the community with a versatile, open 2D fan-beam CT dataset suitable for developing ML techniques for a range of image reconstruction tasks. To acquire it, we designed a sophisticated, semi-automatic scan procedure that utilizes a highly-flexible laboratory X-ray CT setup. A diverse mix of samples with high natural variability in shape and density was scanned slice-by-slice (5000 slices in total) with high angular and spatial resolution and three different beam characteristics: A high-fidelity, a low-dose and a beam-hardening-inflicted mode. In addition, 750 out-of-distribution slices were scanned with sample and beam variations to accommodate robustness and segmentation tasks. We provide raw projection data, reference reconstructions and segmentations based on an open-source data processing pipeline.""

The data collection has been acquired using a highly flexible, programmable and custom-built X-ray CT scanner, the FleX-ray scanner, developed by TESCAN-XRE NV, located in the FleX-ray Lab at the Centrum Wiskunde & Informatica (CWI) in Amsterdam, Netherlands. It consists of a cone-beam microfocus X-ray point source (limited to 90 kV and 90 W) that projects polychromatic X-rays onto a 14-bit CMOS (complementary metal-oxide semiconductor) flat panel detector with CsI(Tl) scintillator (Dexella 1512NDT) and 1536-by-1944 pixels,  each. To create a 2D dataset, a fan-beam geometry was mimicked by only reading out the central row of the detector. Between source and detector there is a rotation stage, upon which samples can be mounted. The machine components (i.e., the source, the detector panel, and the rotation stage) are mounted on translation belts that allow the moving of the components independently from one another.

Please refer to the paper for all further technical details.

The complete data collection can be found via the following links: 1-1,000, 1,001-2,000, 2,001-3,000, 3,001-4,000, 4,001-5,000, 5,521-6,370.

Each slice folder ‘slice00001 - slice05000’ and ‘slice05521 - slice06370’ contains three folders for each mode: ‘mode1’, ‘mode2’, ‘mode3’. In each of these folders there are the sinogram, the dark-field, and the two flat-fields for the raw data archives, or just the reconstructions and for mode2 the additional reference segmentation.

The corresponding reference reconstructions and segmentations can be found via the following links: 1-1,000, 1,001-2,000, 2,001-3,000, 3,001-4,000, 4,001-5,000, 5,521-6,370.

The corresponding Python scripts for loading, pre-processing, reconstructing and segmenting the projection data in the way described in the paper can be found on github. A machine-readable file with the used scanning parameters and instrument data for each acquisition mode as well as a script loading it can be found on the GitHub repository as well.

Note: It is advisable to use the graphical user interface when decompressing the .zip archives. If you experience a zipbomb error when unzipping the file on a Linux system rerun the command with the UNZIP_DISABLE_ZIPBOMB_DETECTION=TRUE environment variable by setting in your .bashrc “export UNZIP_DISABLE_ZIPBOMB_DETECTION=TRUE”.

For more information or guidance in using the data collection, please get in touch with

Maximilian.Kiss [at] cwi.nl

Felix.Lucka [at] cwi.nl",https://production-media.paperswithcode.com/datasets/bbe47a7b-9ba1-4b7b-aaaf-c0316e74ad75.png,EditCC Attribution 4.0 International,"3D, Image",,2023,,,,"training datasets consisting of measurement data and ground-truth images. However, suitable experimental datasets for X-ray Computed Tomography (CT) are scarce, and methods are often developed and evaluated only on simulated data. We fill this gap by providing the community with a versatile, open 2D fan-beam CT dataset suitable for developing ML techniques for a range of image reconstruction tasks. To acquire it, we designed a sophisticated, semi-automatic scan procedure that utilizes a highly-flexible laboratory X-ray CT setup. A diverse mix of samples",,"Low-Dose X-Ray Ct Reconstruction, Image Reconstruction, Denoising, CT Reconstruction, Image Denoising, Image Segmentation, Tomographic Reconstructions, Computed Tomography (CT)",,,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
CheXmask,CheXmask Dataset,"The CheXmask Database presents a comprehensive, uniformly annotated collection of chest radiographs, constructed from five public databases: ChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest and VinDr-CXR. The database aggregates 657,566 anatomical segmentation masks derived from images which have been processed using the HybridGNet model to ensure consistent, high-quality segmentation. To confirm the quality of the segmentations, we include in this database individual Reverse Classification Accuracy (RCA) scores for each of the segmentation masks. This dataset is intended to catalyze further innovation and refinement in the field of semantic chest X-ray analysis, offering a significant resource for researchers in the medical imaging domain.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution 4.0 International Public License,Image,,,,,,,,"Medical Image Segmentation, Heart Segmentation, Image Segmentation, Medical X-Ray Image Segmentation",,,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
EntitySeg,EntitySeg Dataset,"The EntitySeg dataset contains 33,227 images with high-quality mask annotations. Compared with existing dataets, there are three distinct properties in EntitySeg. First, 71.25% and 86.23% of the images are of high resolution with at least 2000px×2000px and 1000px×1000px which is more consistent with current digital imaging trends. Second, the dataset is open-world and is not limited to predefined classes. Third, the mask annotation along the boundaries are more accurate than existing datasets.",https://arxiv.org/pdf/2211.05776v1.pdf,EditCreative Commons Attribution-NonCommercial 4.0 International License,Image,,,,,227 images,,,"Image Segmentation, Semantic Segmentation",,,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
MARIDA,MARIDA Dataset,"MARIDA (Marine Debris Archive) is the first dataset based on the multispectral Sentinel-2 (S2) satellite data, which distinguishes Marine Debris from various marine features that co-exist, including Sargassum macroalgae, Ships, Natural Organic Material, Waves, Wakes, Foam, dissimilar water types (i.e., Clear, Turbid Water, Sediment-Laden Water, Shallow Water), and Clouds. MARIDA is an open-access dataset which enables the research community to explore the spectral behaviour of certain floating materials, sea state features and water types, to develop and evaluate Marine Debris detection solutions based on artificial intelligence and deep learning architectures, as well as satellite pre-processing pipelines.  Although it is designed to be beneficial for several machine learning tasks, it primarily aims to benchmark weakly supervised pixel-level semantic segmentation learning methods. 

MARIDA can be downloaded from the repository Zenodo (https://doi.org/10.5281/zenodo.5151941). A quick start guide for all ML benchmarks and the detailed overview of the dataset are available at https://marine-debris.github.io/.",https://production-media.paperswithcode.com/datasets/1b137f41-d688-438b-9daa-9d3b5d5c3d55.jpg,EditCreative Commons Attribution 4.0 International,Image,,,,,,,,"Weakly supervised segmentation, Image Segmentation",image-segmentation-on-marida,,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
MCubeS,MCubeS Dataset,"Multimodal material segmentation (MCubeS) dataset contains 500 sets of images from 42 street scenes. Each scene has images for four modalities: RGB, angle of linear polarization (AoLP), degree of linear polarization (DoLP), and near-infrared (NIR). The dataset provides annotated ground truth labels for both material and semantic segmentation for every pixel. The dataset is divided training set with 302 image sets, validation set with 96 image sets, and test set with 102 image sets. Each image has  1224 x 1024 pixels and a total of 20 class labels per pixel.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMiT,Image,,,,,,,,"Image Segmentation, Material Recognition, Semantic Segmentation",semantic-segmentation-on-mcubes,,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
MSD__Mirror_Segmentation_Dataset_,MSD (Mirror Segmentation Dataset) Dataset,"We construct the first large-scale mirror dataset, named MSD. It includes 4, 018 pairs of images containing mirrors and their corresponding manually annotated masks.",https://production-media.paperswithcode.com/datasets/8c3386db-8473-4a03-aa98-314e72e96e7e.jpg,EditUnknown,Image,,,,,,,,"Mirror Detection, Image Segmentation",image-segmentation-on-msd-mirror-segmentation,,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
Pascal_Panoptic_Parts,Pascal Panoptic Parts Dataset,The Pascal Panoptic Parts dataset consists of annotations for the part-aware panoptic segmentation task on the PASCAL VOC 2010 dataset. It is created by merging scene-level labels from PASCAL-Context with part-level labels from PASCAL-Part,https://arxiv.org/abs/2106.06351s,EditUnknown,Image,,2010,,,,,,"Human Part Segmentation, Part-aware Panoptic Segmentation, Panoptic Segmentation, Image Segmentation, Scene Understanding","part-aware-panoptic-segmentation-on-pascal, image-segmentation-on-pascal-panoptic-parts",,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
PASCAL_VOC,PASCAL VOC Dataset,"The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set.",https://arxiv.org/abs/1902.06162,EditCustom,"3D, Graph, Image, Text",English,2012,,,464 images,"train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images",,"Object Detection, Single-object colocalization, 3D Face Animation, Talking Face Generation, Open Vocabulary Semantic Segmentation, Single-object discovery, Interactive Segmentation, Unsupervised Semantic Segmentation with Language-image Pre-training, Multi-object colocalization, Zero-Shot Semantic Segmentation, Graph Matching, Semantic Segmentation, Knowledge Distillation, Image Segmentation, Node Classification, Object Counting, Multi-object discovery","single-object-discovery-on-voc-all, single-object-discovery-on-voc-6x2, 3d-face-animation-on-vocaset, open-vocabulary-semantic-segmentation-on-9, multi-object-colocalization-on-voc-all, node-classification-on-pascalvoc-sp-1, image-segmentation-on-pascal-voc, knowledge-distillation-on-pascal-voc, semantic-segmentation-on-pascal-voc, interactive-segmentation-on-pascal-voc, single-object-colocalization-on-voc-all, multi-object-discovery-on-voc-all, unsupervised-semantic-segmentation-with-7, object-detection-on-pascal-voc-10, object-counting-on-pascal-voc, object-detection-on-pascal-voc, object-counting-on-pascal-voc-2007-count-test, unsupervised-semantic-segmentation-with-11, graph-matching-on-pascal-voc, zero-shot-semantic-segmentation-on-pascal-voc, single-object-colocalization-on-voc-6x2, open-vocabulary-semantic-segmentation-on-5",,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
PMD,PMD Dataset,"We propose a large-scale benchmark here, which contains a total of 6,461 mirror images with ground truth annotations.",https://production-media.paperswithcode.com/datasets/fe60cb9c-afc3-4d35-af4f-b7d2e57bb7f9.jpg,EditUnknown,Image,,,,,,,,"Mirror Detection, Image Segmentation",image-segmentation-on-pmd,,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
SA-1B,SA-1B Dataset,"SA-1B consists of 11M diverse, high resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks.",https://arxiv.org/pdf/2304.02643v1.pdf,EditUnknown,Image,,,,,,,,"Segmentation, Image Segmentation",segmentation-on-sa-1b,,See all 844 tasks,Image Segmentation12 benchmark,Image Segmentation12 benchmark
Financial_Dynamic_Knowledge_Graph,Financial Dynamic Knowledge Graph Dataset,"FinDKG: The Global Financial Dynamic Knowledge Graph Dataset
FinDKG is an open-source dataset focused on creating a temporally-resolved Financial Dynamic Knowledge Graph. Designed to bridge the gap in industry-specific knowledge graphs, particularly in the financial sector, FinDKG provides a high-touch, temporally-aware representation of global economic and market dynamics. This repository includes comprehensive details about the dataset, methodology, and schema, aiming to facilitate academic research and actionable insights in global financial markets.

Background
While general-purpose knowledge graphs are abundant, industry-specific ones are comparatively rare, especially in the financial sector. FinDKG aims to fill this void by offering a resource for researchers and professionals looking to leverage knowledge graph technology in finance.

FinDKG Dataset
The dataset's foundation lies in an extensive news corpus curated to capture both qualitative and quantitative indicators in the financial landscape. We utilized the Wayback Machine to amass a dataset comprising global financial news. 

Dataset Structure

Temporal Knowledge Graph (TKG) with daily-resolved event triplets
Event triplets are tagged with specific timestamps corresponding to their release dates
Training, validation, and test splits organized chronologically
Weekly aggregation of event triplets as the basic unit of time

Data Format
/FinDKG is the default study dataset folder including the graph dataset and the corresponding data splits. The graph dataset is organized in the following structure:



'train.txt', 'valid.txt', and 'test.txt': The first four columns correspond to subject, relation, object, and time. The fifth column is ignored.



'stat.txt': The first two columns correspond to the number of entities and relations, respectively.



Test set is held-out for evaluating the model performance. This should match the results of the original paper regarding the Temporal Link Prediction evaluation.

/FinDKG-full: The full dataset including a larger size of the event triplets. This graph dataset adopts the same format as /FinDKG while is left for future extended research.


'time2id.txt': This time mapping table further provided the mapping from time ID to realistic date for real-world application.

Usage
The dataset is designed for graph-based AI methods aiming to generate actionable insights in the financial domain. It is freely available for academic and research purposes. Refer details to our designated FinDKG website.

(Description from Dataset Repo)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditGNU General Public License v3.0 (GPL-3.0),"Graph, Image, Text, Time Series",English,,,,,,,"Link Prediction, Financial Relation Extraction, Relation Extraction, Inductive knowledge graph completion, Node Classification, Named Entity Recognition, Graph Embedding",,,See all 844 tasks,Inductive knowledge graph comp,Inductive knowledge graph comp
Inductive_Relation_Prediction11_papers_with_code_D,Inductive Relation Prediction11 papers with code Dataset,,https://paperswithcode.com/dataset/inductive-relation-prediction,,,,,,,,,,,,,See all 844 tasks,Inductive Relation Prediction1,Inductive Relation Prediction1
ComplexWebQuestions,ComplexWebQuestions Dataset,"ComplexWebQuestions is a dataset for answering complex questions that require reasoning over multiple web snippets. It contains a large set of complex questions in natural language, and can be used in multiple ways:


By interacting with a search engine;
As a reading comprehension task: the authors release 12,725,989 web snippets that are relevant for the questions, and were collected during the development of their model;
As a semantic parsing task: each question is paired with a SPARQL query that can be executed against Freebase to retrieve the answer.",https://allenai.org/data/complexwebquestions,EditUnknown,Text,English,,,,,,,"Knowledge Graphs, Semantic Parsing, Knowledge Base Question Answering, Question Answering","knowledge-base-question-answering-on, question-answering-on-complexwebquestions",,See all 844 tasks,Knowledge Base Question Answer,Knowledge Base Question Answer
FrenchMedMCQA,FrenchMedMCQA Dataset,"This paper introduces FrenchMedMCQA, the first publicly available Multiple-Choice Question Answering (MCQA) dataset in French for medical domain. It is composed of 3,105 questions taken from real exams of the French medical specialization diploma in pharmacy, mixing single and multiple answers. Each instance of the dataset contains an identifier, a question, five possible answers and their manual correction(s). We also propose first baseline models to automatically process this MCQA task in order to report on the current performances and to highlight the difficulty of the task. A detailed analysis of the results showed that it is necessary to have representations adapted to the medical domain or to the MCQA task: in our case, English specialized models yielded better results than generic French ones, even though FrenchMedMCQA is in French. Corpus, models and tools are available online.",https://production-media.paperswithcode.com/datasets/1296a2a5-ab1e-4313-846e-8edb37d6d4eb.png,EditApache 2.0,Text,English,,,,,,,"Multiple Choice Question Answering (MCQA), Science Question Answering, Generative Question Answering, Knowledge Base Question Answering, Question-Answer-Generation, Question Answering",multiple-choice-question-answering-mcqa-on-22,,See all 844 tasks,Knowledge Base Question Answer,Knowledge Base Question Answer
GrailQA,GrailQA Dataset,"GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: i.i.d., compositional, and zero-shot.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Knowledge Base Question Answering,knowledge-base-question-answering-on-grailqa,,See all 844 tasks,Knowledge Base Question Answer,Knowledge Base Question Answer
LC-QuAD,LC-QuAD Dataset,"LC-QuAD is a Large Question Answering dataset with 30,000 pairs of questions and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-29_at_13.39.42.png,EditUnknown,"Graph, Text",English,2018,,,,,,"Relation Linking, Knowledge Base Question Answering","relation-linking-on-lc-quad-1-0, knowledge-base-question-answering-on-lc-quad, relation-linking-on-lc-quad-2-0",,See all 844 tasks,Knowledge Base Question Answer,Knowledge Base Question Answer
RuBQ,RuBQ Dataset,"The first Russian knowledge base question answering (KBQA) dataset. The high-quality dataset consists of 1,500 Russian questions of varying complexity, their English machine translations, SPARQL queries to Wikidata, reference answers, as well as a Wikidata sample of triples containing entities with Russian labels. The dataset creation started with a large collection of question-answer pairs from online quizzes. The data underwent automatic filtering, crowd-assisted entity linking, automatic generation of SPARQL queries, and their subsequent in-house verification.",/paper/rubq-a-russian-dataset-for-question-answering,EditUnknown,Text,English,,,,,,,"Knowledge Base Question Answering, Entity Linking, Question Answering",,,See all 844 tasks,Knowledge Base Question Answer,Knowledge Base Question Answer
SimpleQuestions,SimpleQuestions Dataset,"SimpleQuestions is a large-scale factoid question answering dataset. It consists of 108,442 natural language questions, each paired with a corresponding fact from Freebase knowledge base. Each fact is a triple (subject, relation, object) and the answer to the question is always the object. The dataset is divided into training, validation, and test  sets with 75,910, 10,845 and 21,687 questions respectively.",https://arxiv.org/abs/1605.07427,EditBSD-3-Clause License,Text,English,,,,,,,"Knowledge Base Question Answering, Question Answering","knowledge-base-question-answering-on-2, question-answering-on-simplequestions",,See all 844 tasks,Knowledge Base Question Answer,Knowledge Base Question Answer
SimpleQuestionsWikiData,SimpleQuestionsWikiData Dataset,"SimpleQuestionsWikidata maps SimpleQuestions to Wikidata.

It was proposed in the paper Question Answering Benchmarks for Wikidata by Diefenbach et al.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC License,Text,English,,,,,,,"Knowledge Base Question Answering, Semantic Parsing",knowledge-base-question-answering-on-4,,See all 844 tasks,Knowledge Base Question Answer,Knowledge Base Question Answer
WebQSP,WebQSP Dataset,,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Knowledge Base Question Answering, Entity Linking","knowledge-base-question-answering-on-webqsp, entity-linking-on-webqsp-wd, knowledge-base-question-answering-on-webqsp-1",,See all 844 tasks,Knowledge Base Question Answer,Knowledge Base Question Answer
WebQuestionsSP,WebQuestionsSP Dataset,"The WebQuestionsSP dataset is released as part of our ACL-2016 paper “The Value of Semantic Parse Labeling for Knowledge Base Question Answering” [Yih, Richardson, Meek, Chang & Suh, 2016], in which we evaluated the value of gathering semantic parses, vs. answers, for a set of questions that originally comes from WebQuestions [Berant et al., 2013]. The WebQuestionsSP dataset contains full semantic parses in SPARQL queries for 4,737 questions, and “partial” annotations for the remaining 1,073 questions for which a valid parse could not be formulated or where the question itself is bad or needs a descriptive answer. This release also includes an evaluation script and the output of the STAGG semantic parsing system when trained using the full semantic parses. More detail can be found in the document and labeling instructions included in this release, as well as the paper.",https://www.microsoft.com/en-us/download/details.aspx?id=52763,EditUnknown,Text,English,2016,,,,,,"Knowledge Base Question Answering, Semantic Parsing, Entity Linking, Question Answering","entity-linking-on-webqsp-wd, semantic-parsing-on-webquestionssp, question-answering-on-webquestionssp, knowledge-base-question-answering-on-1, knowledge-base-question-answering-on-webqsp",,See all 844 tasks,Knowledge Base Question Answer,Knowledge Base Question Answer
ATOMIC,ATOMIC Dataset,"ATOMIC is an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., ""if X pays Y a compliment, then Y will likely return the compliment"").",/paper/atomic-an-atlas-of-machine-commonsense-for-if,EditUnknown,Text,English,,,,,,,"Language Modelling, Knowledge Graphs, Question Answering",,,See all 844 tasks,Knowledge Graphs61 benchmarks1,Knowledge Graphs61 benchmarks1
CORD-19,CORD-19 Dataset,"CORD-19 is a free resource of tens of thousands of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses for use by the global research community.",https://allenai.org/data/cord-19,EditSemantic Scholar Dataset License,Text,English,,,,,,,"Text Summarization, Unsupervised Text Summarization, Knowledge Graphs, Information Retrieval, Question Answering","text-summarization-on-cord-19, unsupervised-text-summarization-on-cord-19",,See all 844 tasks,Knowledge Graphs61 benchmarks1,Knowledge Graphs61 benchmarks1
DBP15K,DBP15K Dataset,"DBP15k contains four language-specific KGs that are respectively extracted from English (En), Chinese (Zh), French (Fr) and Japanese (Ja) DBpedia, each of which contains around 65k-106k entities. Three sets of 15k alignment labels are constructed to align entities between each of the other three languages and En.",https://arxiv.org/abs/2005.00171,EditUnknown,,,,,,,,,"Entity Alignment, Knowledge Graphs, Multi-modal Entity Alignment","entity-alignment-on-dbp15k-zh-en, entity-alignment-on-dbp15k-ja-en, entity-alignment-on-dbp15k-fr-en",,See all 844 tasks,Knowledge Graphs61 benchmarks1,Knowledge Graphs61 benchmarks1
FB15k,FB15k Dataset,"The FB15k dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. It has a total of  592,213 triplets with 14,951 entities and 1,345 relationships. FB15K-237 is a variant of the original dataset where inverse relations are removed, since it was found that a large number of test triplets could be obtained by inverting triplets in the training set.",https://www.microsoft.com/en-us/download/details.aspx?id=52312,EditCC BY 2.5,"Graph, Time Series",,,,,,,,"Link Prediction, Knowledge Graph Completion, Knowledge Graph Embedding, Complex Query Answering, Knowledge Graphs","link-prediction-on-fb15k, knowledge-graph-embedding-on-fb15k-1, link-prediction-on-fb15k-1, knowledge-graph-completion-on-fb15k-237, link-prediction-on-fb15k-237, knowledge-graphs-on-fb15k, complex-query-answering-on-fb15k-237, link-prediction-on-fb15k-filtered, complex-query-answering-on-fb15k",,See all 844 tasks,Knowledge Graphs61 benchmarks1,Knowledge Graphs61 benchmarks1
MetaQA,MetaQA Dataset,"The MetaQA dataset consists of a movie ontology derived from the WikiMovies Dataset and three sets of question-answer pairs written in natural language: 1-hop, 2-hop, and 3-hop queries.",https://arxiv.org/abs/1907.08176,EditCustom,Text,English,,,,,,,"Language Modelling, Knowledge Graphs, Question Answering",question-answering-on-metaqa,,See all 844 tasks,Knowledge Graphs61 benchmarks1,Knowledge Graphs61 benchmarks1
OpenDialKG,OpenDialKG Dataset,OpenDialKG contains utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts.,/paper/opendialkg-explainable-conversational,EditCC BY-NC 4.0,Text,English,,,,,,,"Text Generation, Knowledge Graphs, Dialogue Generation",,,See all 844 tasks,Knowledge Graphs61 benchmarks1,Knowledge Graphs61 benchmarks1
ReDial,ReDial Dataset,"ReDial (Recommendation Dialogues) is an annotated dataset of dialogues, where users recommend movies to each other. The dataset consists of over 10,000 conversations centered around the theme of providing movie recommendations.",/paper/towards-deep-conversational-recommendations,EditCC BY 4.0,Text,English,,,,,,,"Text Generation, Knowledge Graphs, Recommendation Systems, Sentiment Analysis","text-generation-on-redial, recommendation-systems-on-redial",,See all 844 tasks,Knowledge Graphs61 benchmarks1,Knowledge Graphs61 benchmarks1
Semantic_Scholar,Semantic Scholar Dataset,"The Semantic Scholar corpus (S2) is composed of titles from scientific papers published in machine learning conferences and journals from 1985 to 2017, split by year (33 timesteps).

Image Source: [http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/] (http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/)",https://arxiv.org/abs/1909.04985,EditODC-BY,Text,English,1985,,,,,,"Language Modelling, Knowledge Graphs, Question Answering",,,See all 844 tasks,Knowledge Graphs61 benchmarks1,Knowledge Graphs61 benchmarks1
Language_Modeling5151_papers_with_code_Dataset,Language Modeling5151 papers with code Dataset,,https://paperswithcode.com/dataset/language-modeling,,,,,,,,,,,,,See all 844 tasks,Language Modeling5151 papers w,Language Modeling5151 papers w
BIG-bench,BIG-bench Dataset,The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. Big-bench include more than 200 tasks.,https://production-media.paperswithcode.com/datasets/1f6230f4-8e97-4c9f-9609-67967be56663.png,EditApache License 2.0,"Audio, Image, Text, Time Series",English,,,,,,,", Movie Dialog Same Or Different, High School Government and Politics, High School Computer Science, Dark Humor Detection, Abstract Algebra, Medical Genetics, Public Relations, Intent Recognition, BIG-bench Machine Learning, Security Studies, General Knowledge, Question Selection, Marketing, FEVER (2-way), Irony Identification, College Mathematics, High School Macroeconomics, Multiple Choice Question Answering (MCQA), Global Facts, Crass AI, Analytic Entailment, Sociology, Professional Medicine, Entailed Polarity, Understanding Fables, Memorization, LAMBADA, Implicatures, Moral Permissibility, Logical Args, College Chemistry, Presuppositions As NLI, Nutrition, Philosophy, Virology, Professional Law, Similarities Abstraction, High School Microeconomics, Logical Fallacies, Moral Scenarios, International Law, Metaphor Boolean, High School World History, Timedial, High School Geography, GRE Reading Comprehension, Riddle Sense, TriviaQA, Astronomy, Physical Intuition, Sentence Ambiguity, Elementary Mathematics, Sarcasm Detection, College Medicine, Natural Questions, Analogical Similarity, Business Ethics, Word Sense Disambiguation, High School Psychology, Prehistory, Jurisprudence, Multi-task Language Understanding, Electrical Engineering, FEVER (3-way), Implicit Relations, Empirical Judgments, Econometrics, High School Physics, Conceptual Physics, Human Aging, Evaluating Information Essentiality, Nonsense Words Grammar, High School Biology, Discourse Marker Prediction, Physics MC, RACE-h, Common Sense Reasoning, Figure Of Speech Detection, English Proverbs, US Foreign Policy, Misconceptions, College Computer Science, World Religions, Identify Odd Metapor, Miscellaneous, High School Chemistry, Moral Disputes, College Biology, Computer Security, Language Modelling, Management, Clinical Knowledge, Human Organs Senses Multiple Choice, Phrase Relatedness, Epistemic Reasoning, Professional Accounting, Mathematical Induction, Crash Blossom, High School US History, Logical Reasoning, Formal Logic, Auto Debugging, RACE-m, Anatomy, High School Statistics, High School Mathematics, College Physics, High School European History, Fantasy Reasoning, Odd One Out, Professional Psychology, Human Sexuality","college-medicine-on-big-bench, crass-ai-on-big-bench, formal-logic-on-big-bench, high-school-geography-on-big-bench, nonsense-words-grammar-on-big-bench, abstract-algebra-on-big-bench, general-knowledge-on-big-bench, similarities-abstraction-on-big-bench, race-m-on-big-bench, auto-debugging-on-big-bench-lite, fever-2-way-on-big-bench, english-proverbs-on-big-bench, common-sense-reasoning-on-big-bench-logical, college-physics-on-big-bench, analogical-similarity-on-big-bench, on-big-bench-navigate, natural-questions-on-big-bench, presuppositions-as-nli-on-big-bench, human-aging-on-big-bench, understanding-fables-on-big-bench, logical-reasoning-on-big-bench-strategyqa, high-school-macroeconomics-on-big-bench, multiple-choice-question-answering-mcqa-on-30, irony-identification-on-big-bench, discourse-marker-prediction-on-big-bench, intent-recognition-on-big-bench, anatomy-on-big-bench, common-sense-reasoning-on-big-bench-causal, dark-humor-detection-on-big-bench, entailed-polarity-on-big-bench, odd-one-out-on-big-bench, elementary-mathematics-on-big-bench, empirical-judgments-on-big-bench, moral-scenarios-on-big-bench, human-organs-senses-multiple-choice-on-big, professional-psychology-on-big-bench, implicit-relations-on-big-bench, on-big-bench-snarks, miscellaneous-on-big-bench, econometrics-on-big-bench, management-on-big-bench, logical-reasoning-on-big-bench-reasoning, high-school-psychology-on-big-bench, moral-disputes-on-big-bench, lambada-on-big-bench, marketing-on-big-bench, college-computer-science-on-big-bench, on-big-bench-ruin-names, logical-reasoning-on-big-bench-logical, timedial-on-big-bench, multiple-choice-question-answering-mcqa-on-27, identify-odd-metapor-on-big-bench, logical-reasoning-on-big-bench-temporal, computer-security-on-big-bench, fever-3-way-on-big-bench, sarcasm-detection-on-big-bench-snarks, high-school-world-history-on-big-bench, international-law-on-big-bench, high-school-statistics-on-big-bench, logical-reasoning-on-big-bench-formal, high-school-government-and-politics-on-big, common-sense-reasoning-on-big-bench, prehistory-on-big-bench, college-biology-on-big-bench, movie-dialog-same-or-different-on-big-bench, analytic-entailment-on-big-bench, machine-learning-on-big-bench, moral-permissibility-on-big-bench, question-selection-on-big-bench, logical-reasoning-on-big-bench-penguins-in-a, high-school-mathematics-on-big-bench, conceptual-physics-on-big-bench, gre-reading-comprehension-on-big-bench, figure-of-speech-detection-on-big-bench, clinical-knowledge-on-big-bench, high-school-european-history-on-big-bench, logical-reasoning-on-big-bench-logic-grid, implicatures-on-big-bench, multi-task-language-understanding-on-bbh-nlp, common-sense-reasoning-on-big-bench-winowhy, mathematical-induction-on-big-bench, language-modelling-on-big-bench-lite, misconceptions-on-big-bench, professional-accounting-on-big-bench, metaphor-boolean-on-big-bench, logical-fallacies-on-big-bench, high-school-biology-on-big-bench, business-ethics-on-big-bench, security-studies-on-big-bench, crash-blossom-on-big-bench, college-mathematics-on-big-bench, medical-genetics-on-big-bench, multiple-choice-question-answering-mcqa-on-28, sociology-on-big-bench, evaluating-information-essentiality-on-big, physical-intuition-on-big-bench, astronomy-on-big-bench, common-sense-reasoning-on-big-bench-date, race-h-on-big-bench, public-relations-on-big-bench, global-facts-on-big-bench, philosophy-on-big-bench, high-school-microeconomics-on-big-bench, physics-mc-on-big-bench, memorization-on-big-bench-hindu-knowledge, multiple-choice-question-answering-mcqa-on-29, logical-args-on-big-bench, jurisprudence-on-big-bench, college-chemistry-on-big-bench, fantasy-reasoning-on-big-bench, riddle-sense-on-big-bench, nutrition-on-big-bench, triviaqa-on-big-bench, common-sense-reasoning-on-big-bench-sports, professional-law-on-big-bench, on-big-bench-hyperbaton, world-religions-on-big-bench, word-sense-disambiguation-on-big-bench, phrase-relatedness-on-big-bench, us-foreign-policy-on-big-bench, high-school-chemistry-on-big-bench, multi-task-language-understanding-on-bbh-alg, professional-medicine-on-big-bench, common-sense-reasoning-on-big-bench-known, virology-on-big-bench, electrical-engineering-on-big-bench, high-school-physics-on-big-bench, multiple-choice-question-answering-mcqa-on-31, on-big-bench-hard, high-school-computer-science-on-big-bench, high-school-us-history-on-big-bench, epistemic-reasoning-on-big-bench, sentence-ambiguity-on-big-bench, human-sexuality-on-big-bench",,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
BookCorpus,BookCorpus Dataset,"BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).",https://arxiv.org/abs/1805.10956,EditUnknown,Text,English,,,,74M sentences,,,"Text Generation, Language Modelling, Word Embeddings",language-modelling-on-bookcorpus2,,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
C4,C4 Dataset,"C4 is a colossal, cleaned version of Common Crawl's web crawl corpus. It was based on Common Crawl dataset: https://commoncrawl.org. It was used to train the T5 text-to-text Transformer models.

The dataset can be downloaded in a pre-processed form from allennlp.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_5.00.06_PM.png,EditUnknown,Text,English,,,,,,,Language Modelling,language-modelling-on-c4,,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
IMDb_Movie_Reviews,IMDb Movie Reviews Dataset,"The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.",http://nlpprogress.com/english/sentiment_analysis.html,EditUnknown,"Graph, Image, Text, Time Series",English,,,,,,,"Node Clustering, Link Prediction, Opinion Mining, SQL Parsing, Language Modelling, Graph Similarity, Text Classification, Paraphrase Identification, Sentiment Analysis","paraphrase-identification-on-imdb, graph-similarity-on-imdb, text-classification-on-imdb-movie-reviews-1, opinion-mining-on-imdb-movie-reviews, sql-parsing-on-imdb, node-clustering-on-imdb, sentiment-analysis-on-imdb, sentiment-analysis-on-user-and-product, sentiment-analysis-on-imdb-movie-reviews-1, text-classification-on-imdb, link-prediction-on-imdb",,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
LAMBADA,LAMBADA Dataset,"The LAMBADA (LAnguage Modeling Broadened to Account for Discourse Aspects) benchmark is an open-ended cloze task which consists of about 10,000 passages from BooksCorpus where a missing target word is predicted in the last sentence of each passage. The missing word is constrained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples were further filtered to ensure that missing words could not be guessed without the context, ensuring that models attempting the dataset would need to reason over the entire paragraph to answer questions.",https://arxiv.org/abs/1904.01172,EditCC BY 4.0,Text,English,,,,,"trained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples",,Language Modelling,language-modelling-on-lambada,,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
Penn_Treebank,Penn Treebank Dataset,"The English Penn Treebank (PTB) corpus, and in particular the section of the corpus corresponding to the articles of Wall Street Journal (WSJ), is one of the most known and used corpus for the evaluation of models for sequence labelling. The task consists of annotating each word with its Part-of-Speech tag. In the most common split of this corpus,  sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).
The corpus is also commonly used for character-level and word-level Language Modelling.",https://arxiv.org/abs/1904.04733,EditCustom,"Audio, Text",English,,,,219 sentences,,,"Chunking, Stochastic Optimization, Constituency Parsing, Part-Of-Speech Tagging, Language Modelling, Missing Elements, Unsupervised Dependency Parsing, Open Information Extraction, Dependency Parsing","chunking-on-penn-treebank, constituency-parsing-on-penn-treebank, unsupervised-dependency-parsing-on-penn, stochastic-optimization-on-penn-treebank, missing-elements-on-penn-treebank, part-of-speech-tagging-on-penn-treebank, dependency-parsing-on-penn-treebank, open-information-extraction-on-penn-treebank, language-modelling-on-penn-treebank-word, language-modelling-on-penn-treebank-character",,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
Pubmed,Pubmed Dataset,The PubMed dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words.,https://linqs.soe.ucsc.edu/data,EditUnknown,"Graph, Image, Text, Time Series",English,,,,,,,"Node Clustering, Link Prediction, Text Summarization, Graph Classification, Node Classification on Non-Homophilic (Heterophilic) Graphs, Sentence Classification, Extended Summarization, Language Modelling, Graph Clustering, Unsupervised Extractive Summarization, Node Classification, Community Detection","extended-summarization-on-pubmed-long-val, node-classification-on-pubmed-random, graph-clustering-on-pubmed, graph-classification-on-pubmed, node-classification-on-pubmed-fixed-20-node, link-prediction-on-pubmed, node-clustering-on-pubmed, unsupervised-extractive-summarization-on-1, node-classification-on-pubmed-full-supervised, community-detection-on-pubmed, node-classification-on-pubmed-with-public, node-classification-on-pubmed-005, link-prediction-on-pubmed-nonstandard-variant, node-classification-on-pubmed-60-20-20-random, extended-summarization-on-pubmed-long-test, text-summarization-on-pubmed-1, node-classification-on-non-homophilic-16, node-classification-on-pubmed-48-32-20-fixed, node-classification-on-pubmed-01, link-prediction-on-pubmed-biased-evaluation, language-modelling-on-pubmed-central, node-classification-on-pubmed-003, node-classification-on-pubmed, sentence-classification-on-pubmed-20k-rct",,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
The_Pile,The Pile Dataset,"The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together.

Datasheet: Datasheet for the Pile",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-07_at_8.09.05_PM.png,EditUnknown,Text,English,,,,,,,Language Modelling,language-modelling-on-the-pile,,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
WikiText-103,WikiText-103 Dataset,"The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.

Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.",https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/,EditCC BY-SA 3.0,Text,English,,,,,,,"Text Generation, Language Modelling","text-generation-on-wikitext-103, language-modelling-on-wikitext-103",,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
WikiText-2,WikiText-2 Dataset,"The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.

Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.",https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/,EditCC BY-SA 3.0,Text,English,,,,,,,"Text Generation, Language Modelling","text-generation-on-wikitext-103, language-modelling-on-wikitext-103, language-modelling-on-wikitext-2",,See all 844 tasks,Language Modelling85 benchmark,Language Modelling85 benchmark
GlotSparse,GlotSparse Dataset,Collection of news websites in low-resource languages.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC0,Text,English,,,,,,,"Language Modelling, Language Identification, Large Language Model",,,See all 844 tasks,Large Language Model31 benchma,Large Language Model31 benchma
HiXSTest,HiXSTest Dataset,"For testing refusal behavior in a language-specific setting, we introduce HiXSTest — a set of manually curated prompts in the Hindi language designed to measure exaggerated safety. It comprises 25 safe-unsafe pairs of prompts, carefully phrased to challenge the LLMs’ safety boundaries.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache 2.0,Text,English,,,,,,,"Text Generation, Language Modelling, AI and Safety, Large Language Model",,,See all 844 tasks,Large Language Model31 benchma,Large Language Model31 benchma
Human_Simulacra,Human Simulacra Dataset,"Human Simulacra is a virtual character dataset that contains 129k texts across 11 virtual characters, with each character having unique attributes, biographies, and stories.",https://production-media.paperswithcode.com/datasets/356a9585-a51b-4eb6-9189-d08ddd98eb44.png,EditCC BY-NC-SA 4.0,Text,English,,,,129k texts,,,Large Language Model,,,See all 844 tasks,Large Language Model31 benchma,Large Language Model31 benchma
IFEval,IFEval Dataset,"This dataset evaluates instruction following ability of large language models. There are 500+ prompts with instructions such as ""write an article with more than 800 words"", ""wrap your response with double quotation marks"", etc.",https://production-media.paperswithcode.com/datasets/afa3db9b-c9d9-432e-a741-534d5a3022ee.png,EditUnknown,Text,English,,,,,,,"Instruction Following, Large Language Model",instruction-following-on-ifeval,,See all 844 tasks,Large Language Model31 benchma,Large Language Model31 benchma
MedConceptsQA,MedConceptsQA Dataset,"MedConceptsQA - Open Source Medical Concepts QA Benchmark

The benchmark can be found here: 
https://huggingface.co/datasets/ofir408/MedConceptsQA",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Medical Procedure, Large Language Model, Few-Shot Learning, Zero-Shot Learning, Question Answering, Medical Diagnosis","few-shot-learning-on-medconceptsqa, zero-shot-learning-on-medconceptsqa",,See all 844 tasks,Large Language Model31 benchma,Large Language Model31 benchma
SGXSTest,SGXSTest Dataset,"For testing refusal behavior in a cultural setting, we introduce SGXSTest — a set of manually curated prompts designed to measure exaggerated safety within the context of Singaporean culture. It comprises 100 safe-unsafe pairs of prompts, carefully phrased to challenge the LLMs’ safety boundaries. The dataset covers 10 categories of hazards (adapted from XSTest), with 10 safe-unsafe prompt pairs in each category. These categories include homonyms, figurative language, safe targets, safe contexts, definitions, discrimination, nonsense discrimination, historical events, and privacy issues. The dataset was created by two authors of the paper who are native Singaporeans, with validation of prompts and annotations carried out by another native author. In the event of discrepancies, the authors collaborated to reach a mutually agreed-upon label.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache 2.0,Text,English,,,,,,10,"Text Generation, Language Modelling, AI and Safety, Large Language Model",,,See all 844 tasks,Large Language Model31 benchma,Large Language Model31 benchma
TriviaHG,TriviaHG Dataset,"Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human’s cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 humans with answering questions using the provided hints. The effectiveness of hints varied, with success rates of 96%, 78%, and 36% for questions with easy, medium, and hard answers, respectively. Moreover, the proposed automatic evaluation methods showed a robust correlation with annotators’ results. Conclusively, the findings highlight three key insights: the facilitative role of hints in resolving unknown questions, the dependence of hint quality on answer difficulty, and the feasibility of employing automatic evaluation methods for hint assessment.",https://production-media.paperswithcode.com/datasets/75f7ac9b-206d-4b6d-95ee-2ae25b6764ae.png,EditMIT License,Text,English,,,,,,,"Large Language Model, Hint Generation, Information Retrieval, Question Answering",,,See all 844 tasks,Large Language Model31 benchma,Large Language Model31 benchma
VMD,VMD Dataset,"This dataset contains synthetically generated discussions and annotations using exclusively LLM agents. Discussions are performed between randomly selected users, with a LLM moderator featuring various moderation strategies.

Each LLM user and annotator use a distinct Sociodemographic Background. User-agents also have different intents when joining the discussion. Each discussion consists of 28 comments - 14 participant comments and 14 moderator interventions. Each comment is annotated by 10 different LLM annotators for toxicity and argument quality. 

Designed to analyze moderation strategies and synthetic online discussion simulation but can also be used for LLM moderator finetuning. Available in both CSV (61,147 × 33) and JSON formats.",https://production-media.paperswithcode.com/datasets/21307120-5e54-4329-8d9c-8afce3515777.jpg,EditCC BY-SA,Text,English,,,,,,,"text annotation, Short-Text Conversation, User Simulation, Large Language Model",,,See all 844 tasks,Large Language Model31 benchma,Large Language Model31 benchma
LRA,LRA Dataset,"Long-range arena (LRA) is an effort toward systematic evaluation of efficient transformer models. The project aims at establishing benchmark tasks/datasets using which we can evaluate transformer-based models in a systematic way, by assessing their generalization power, computational efficiency, memory foot-print, etc. Long-Range Arena is specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning.

Description from: Long Range Arena : A Benchmark for Efficient Transformers",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,"valuation of efficient transformer models. The project aims at establishing benchmark tasks/datasets using which we can evaluate transformer-based models in a systematic way, by assessing their generalization power, computational efficiency, memory foot-print, etc. Long-Range Arena is specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images",,"Long-range modeling, Language Modelling",long-range-modeling-on-lra,,See all 844 tasks,Long-range modeling2 benchmark,Long-range modeling2 benchmark
MuLD,MuLD Dataset,"MuLD (Multitask Long Document Benchmark) is a set of 6 NLP tasks where the inputs consist of at least 10,000 words. The benchmark covers a wide variety of task types including translation, summarization, question answering, and classification. Additionally there is a range of output lengths from a single word classification label all the way up to an output longer than the input text.",https://production-media.paperswithcode.com/datasets/af9d0659-695b-423b-a151-29d9f5ab84d0.png,EditCustom,"Image, Text",English,,,,,,,"Long-range modeling, Style change detection, Summarization, Natural Language Understanding, Text Classification, Translation, Question Answering","translation-on-muld-opensubtitles, question-answering-on-muld-hotpotqa, question-answering-on-muld-narrativeqa, summarization-on-muld-vlsp, text-classification-on-muld-character-type, style-change-detection-on-muld-style-change",,See all 844 tasks,Long-range modeling2 benchmark,Long-range modeling2 benchmark
Pathfinder-X2,Pathfinder-X2 Dataset,"Pathfinder and Pathfinder-X have proven to be instrumental in training and testing Large Language Models with long-range dependencies.
Recently, Meta's Moving Average Equipped Gated Attention model scored a 97% on the Pathfinder-X dataset, indicating a need for a larger,
more challenging dataset.  Whereas Pathfinder-X only went up to 256 x 256 pixel images (or a sequence length of 65,536 tokens), Pathfinder-X2 introduces images of 512 x 512 pixels, or 262,144 tokens.  

Each image is meant to be read as a sequence of pixels.  A LLM's task is to segment out the one snake in each image with a circle at its tip.  The dataset includes 200,000 images and 200,000 segmentation masks, one for each image.",https://production-media.paperswithcode.com/datasets/be32f7d6-c944-4640-87af-5510a75ef8c0.png,EditC.C. B.Y. 4.0,,,,,,000 images,,,Long-range modeling,,,See all 844 tasks,Long-range modeling2 benchmark,Long-range modeling2 benchmark
SCROLLS,SCROLLS Dataset,"SCROLLS (Standardized CompaRison Over Long Language Sequences) is an NLP benchmark consisting of a suite of tasks that require reasoning over long texts. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. The dataset is made available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.

The SCROLLS benchmark contains the datasets GovReport, SummScreenFD, QMSum, QASPER, NarrativeQA, QuALITY and ContractNLI.",https://production-media.paperswithcode.com/datasets/ce649cee-c50c-477b-b30e-49abc075b480.jpg,EditMIT,Text,English,,,,,,,"Natural Language Inference, Long-range modeling, Text Summarization, Question Answering",long-range-modeling-on-scrolls,,See all 844 tasks,Long-range modeling2 benchmark,Long-range modeling2 benchmark
C3,C3 Dataset,C3 is a free-form multiple-Choice Chinese machine reading Comprehension dataset.,https://huggingface.co/datasets/dataset-org/c3,EditUnknown,Text,English,,,,,,,"Machine Reading Comprehension, Common Sense Reasoning (Few-Shot), Common Sense Reasoning (One-Shot), Language Modelling, Common Sense Reasoning (Zero-Shot), Reading Comprehension","common-sense-reasoning-one-shot-on-c3, common-sense-reasoning-zero-shot-on-c3, common-sense-reasoning-few-shot-on-c3",,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
CBT,CBT Dataset,Children’s Book Test (CBT) is designed to measure directly how well language models can exploit wider linguistic context. The CBT is built from books that are freely available thanks to Project Gutenberg.,https://research.fb.com/downloads/babi/,EditGNU Free Documentation License,"Text, Time Series",English,,,,,,,"Reading Comprehension, Machine Reading Comprehension, Click-Through Rate Prediction, Question Answering","click-through-rate-prediction-on-childrens, question-answering-on-childrens-book-test",,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
CMRC,CMRC Dataset,"CMRC is a dataset is annotated by human experts with near 20,000 questions as well as a challenging set which is composed of the questions that need reasoning over multiple clues.",https://www.aclweb.org/anthology/D19-1600.pdf,EditCC-BY-SA-4.0,Text,English,,,,,,,"Reading Comprehension (Zero-Shot), Reading Comprehension (Few-Shot), Machine Reading Comprehension, Cloze (multi-choices) (One-Shot), Cloze (multi-choices) (Few-Shot), Language Modelling, Reading Comprehension (One-Shot), Cloze (multi-choices) (Zero-Shot), Reading Comprehension, Chinese Reading Comprehension","reading-comprehension-few-shot-on-cmrc-2018, cloze-multi-choices-zero-shot-on-cmrc-2019, cloze-multi-choices-one-shot-on-cmrc-2019, cloze-multi-choices-few-shot-on-cmrc-2019, chinese-reading-comprehension-on-cmrc-2019, reading-comprehension-one-shot-on-cmrc-2018, cloze-multi-choices-one-shot-on-cmrc-2017, reading-comprehension-zero-shot-on-cmrc-2018, cloze-multi-choices-few-shot-on-cmrc-2017, cloze-multi-choices-zero-shot-on-cmrc-2017, chinese-reading-comprehension-on-cmrc-2018-3",,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
DRCD,DRCD Dataset,"Delta Reading Comprehension Dataset (DRCD) is an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators.",https://github.com/DRCKnowledgeTeam/DRCD,EditCC-BY-SA 3.0,Text,English,,,,014 paragraphs,,,"Reading Comprehension (Zero-Shot), Reading Comprehension (Few-Shot), Machine Reading Comprehension, Reading Comprehension (One-Shot), Reading Comprehension, Chinese Reading Comprehension, Question Answering","reading-comprehension-one-shot-on-drcd, chinese-reading-comprehension-on-drcd-1, chinese-reading-comprehension-on-drcd, reading-comprehension-zero-shot-on-drcd, reading-comprehension-few-shot-on-drcd",,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
DREAM,DREAM Dataset,"DREAM is a multiple-choice Dialogue-based REAding comprehension exaMination dataset. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding.

DREAM contains 10,197 multiple choice questions for 6,444 dialogues, collected from English-as-a-foreign-language examinations designed by human experts. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge.",https://dataset.org/dream/,"EditCustom (research-only, non-commercial)","Image, Text",English,,,,,,,"Reading Comprehension, Sleep spindles detection, Machine Reading Comprehension, Question Answering","sleep-spindles-detection-on-dreams-sleep, machine-reading-comprehension-on-dream",,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
LogiQA,LogiQA Dataset,"LogiQA consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. The dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting.",/paper/logiqa-a-challenge-dataset-for-machine,EditUnknown,Text,English,,,,,,,"Decision Making, Machine Reading Comprehension, Natural Language Understanding",,,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
MCTest,MCTest Dataset,"MCTest is a freely available set of stories and associated questions intended for research on the machine comprehension of text. 

MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.",https://www.aclweb.org/anthology/D13-1020.pdf,EditCustom (see LICENSE.pdf),Text,English,,,,,,,"Reading Comprehension, Machine Reading Comprehension, Question Answering","question-answering-on-mctest-160, question-answering-on-mctest-500",,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
MRQA,MRQA Dataset,The MRQA (Machine Reading for Question Answering) dataset is a dataset for evaluating the generalization capabilities of reading comprehension systems.,/paper/mrqa-2019-shared-task-evaluating,EditUnknown,Text,English,,,,,,,"Reading Comprehension, Machine Reading Comprehension, Question Answering",question-answering-on-mrqa-2019,,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
MuTual,MuTual Dataset,"MuTual is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified from Chinese high school English listening comprehension test data. It tests dialogue reasoning via next utterance prediction.",https://github.com/Nealcly/MuTual,EditUnknown,Text,English,,,,,,,"Text Generation, Task-Oriented Dialogue Systems, Machine Reading Comprehension",,,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
ReClor,ReClor Dataset,"Logical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary language as the definition from Law School Admission Council. ReClor is a dataset extracted from logical reasoning questions of standardized graduate admission examinations.",https://whyu.me/reclor/,EditUnknown,Text,English,,,,,,,"Logical Reasoning Question Answering, Reading Comprehension, Machine Reading Comprehension, Question Answering","reading-comprehension-on-reclor, question-answering-on-reclor, logical-reasoning-question-ansering-on-reclor, machine-reading-comprehension-on-reclor",,See all 844 tasks,Machine Reading Comprehension4,Machine Reading Comprehension4
ASPEC,ASPEC Dataset,"ASPEC, Asian Scientific Paper Excerpt Corpus, is constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT). It consists of a Japanese-English paper abstract corpus of 3M parallel sentences (ASPEC-JE) and a Japanese-Chinese paper excerpt corpus of 680K parallel sentences (ASPEC-JC). This corpus is one of the achievements of the Japanese-Chinese machine translation project which was run in Japan from 2006 to 2010.",http://lotus.kuee.kyoto-u.ac.jp/ASPEC/,EditCustom (non-commercial),Text,English,2006,,,,,,"Low-Resource Neural Machine Translation, Machine Translation, Domain Adaptation",,,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
Europarl-ST,Europarl-ST Dataset,"Europarl-ST is a multilingual Spoken Language Translation corpus containing paired audio-text samples for SLT from and into 9 European languages, for a total of 72 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012.",/paper/europarl-st-a-multilingual-corpus-for-speech,EditCustom,"Audio, Image, Text",English,2008,,,,,,"Data Augmentation, Machine Translation, Speech Recognition",,,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
Europarl,Europarl Dataset,"A corpus of parallel text in 21 European languages from the proceedings of the European Parliament.

The Europarl parallel corpus is extracted from the proceedings of the European Parliament (1996-2011). It includes versions in 21 European languages: Romanic (French, Italian, Spanish, Portuguese, Romanian), Germanic (English, Dutch, German, Danish, Swedish), Slavik (Bulgarian, Czech, Polish, Slovak, Slovene), Finni-Ugric (Finnish, Hungarian, Estonian), Baltic (Latvian, Lithuanian), and Greek. Parallel sentence counts are in the range 400K-2M, depending on the language combination.

The goal of the extraction and processing was to generate sentence aligned text for statistical machine translation systems. For this purpose we extracted matching items and labeled them with corresponding document IDs. Using a preprocessor we identified sentence boundaries. We sentence aligned the data using a tool based on the Church and Gale algorithm.

The Europarl corpus was collected mainly to aid research in statistical machine translation (training, evaluation), but it has been used for many other natural language problems: word sense disambiguation, anaphora resolution, information extraction, etc.

Monolingual datasets are also available for 9 languages. These are supersets of the parallel versions. Monolingual word counts are in the range 7M-54M, depending on the language.

Test Sets: Several test sets have been released for the Europarl corpus. In general, the Q4/2000 portion of the data (2000-10 to 2000-12) should be reserved for testing. All released test sets have been selected from this quarter.",https://www.statmt.org/europarl/,EditCustom,"Audio, Image, Text",English,1996,,,,,,"Machine Translation, Speech Recognition",,,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
FLoRes-101,FLoRes-101 Dataset,"FLoRes-101 is an evaluation benchmark for low-resource and multilingual machine translation. It consists of 3001 sentences extracted from English Wikipedia, covering a variety of different topics and domains. These sentences have been translated into 101 languages by professional translators through a carefully controlled process.

The FLoRes-101 dataset was introduced to address the lack of good evaluation benchmarks for low-resource languages. It enables better assessment of model quality in these languages and allows for the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC-BY-SA,Text,English,,,,3001 sentences,,,"Translation eng-deu, Translation tur-eng, Translation nld-eng, Translation deu-afr, Translation eng-nld, Translation eng-tur, Translation afr-deu, Translation ara-eng, Translation eng-afr, Translation deu-eng, Translation eng-ara, Translation eng-spa, Machine Translation, Translation afr-eng","translation-eng-afr-on-flores101-devtest, translation-eng-tur-on-flores101-devtest, translation-eng-deu-on-flores101-devtest, translation-eng-nld-on-flores101-devtest, translation-deu-afr-on-flores101-devtest, translation-deu-eng-on-flores101-devtest, translation-tur-eng-on-flores101-devtest, translation-afr-eng-on-flores101-devtest, translation-eng-spa-on-flores101-devtest, translation-nld-eng-on-flores101-devtest, translation-afr-deu-on-flores101-devtest, translation-eng-ara-on-flores101-devtest, translation-ara-eng-on-flores101-devtest",,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
FLoRes-200,FLoRes-200 Dataset,"FLoRes-200 doubles the existing language coverage of FLoRes-101. Given the nature of the new languages, which have less standardization and require more specialized professional translations, the verification process became more complex. This required modifications to the translation workflow. FLoRes-200 has several languages which were not translated from English. Specifically, several languages were translated from Spanish, French, Russian, and Modern Standard Arabic.",https://production-media.paperswithcode.com/datasets/d5cb61ca-6b3d-4783-bc22-9794534fe704.png,EditUnknown,Text,English,,,,,,,Machine Translation,machine-translation-on-flores-200,,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
MLQA,MLQA Dataset,"MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance. MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between 4 different languages on average.",https://github.com/facebookresearch/mlqa,EditCC-BY-SA 3.0,Text,English,,,,,,,"Cross-Lingual Question Answering, Cross-Lingual Transfer, Machine Translation, Question Answering",cross-lingual-question-answering-on-mlqa,,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
OpenSubtitles,OpenSubtitles Dataset,OpenSubtitles is collection of multilingual parallel corpora. The dataset is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages.,https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_12.26.15_PM.png,EditUnknown,Text,English,,,,,,,"Dialogue Generation, Language Identification, Domain Adaptation, Language Modelling, Machine Translation","language-modelling-on-opensubtitles, language-identification-on-opensubtitles",,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
OPUS-100,OPUS-100 Dataset,OPUS-100 is an English-centric multilingual corpus covering 100 languages. It was randomly sampled from the OPUS collection.,https://huggingface.co/datasets/opus100,EditUnknown,Text,English,,,,,,,"Reading Comprehension, Machine Translation",,,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
WMT_2014,WMT 2014 Dataset,"WMT 2014 is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation. The workshop featured four tasks:


a news translation task,
a quality estimation task,
a metrics task,
a medical text translation task.",https://www.aclweb.org/anthology/W14-3302.pdf,EditUnknown,Text,English,2014,,,,,,"Translation eng-deu, Unsupervised Machine Translation, Machine Translation, Translation deu-eng","machine-translation-on-wmt2014-german-english, machine-translation-on-wmt2014-french-english, machine-translation-on-wmt2014-english-czech, unsupervised-machine-translation-on-wmt2014-1, unsupervised-machine-translation-on-wmt2014-2, translation-eng-deu-on-newstest2014-deen, translation-deu-eng-on-newstest2014-deen, machine-translation-on-wmt2014-english-french, unsupervised-machine-translation-on-wmt2014, machine-translation-on-wmt2014-english-german, unsupervised-machine-translation-on-wmt2014-3",,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
WMT_2016,WMT 2016 Dataset,"WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.

The conference featured ten shared tasks:


a news translation task,
an IT domain translation task,
a biomedical translation task,
an automatic post-editing task,
a metrics task (assess MT quality given reference translation).
a quality estimation task (assess MT quality without access to any reference),
a tuning task (optimize a given MT system),
a pronoun translation task,
a bilingual document alignment task,
a multimodal translation task.",http://www.statmt.org/wmt16/index.html,EditUnknown,"Text, Time Series",English,2016,,,,,,"Translation eng-deu, Unsupervised Machine Translation, Sequence-to-sequence Language Modeling, Translation deu-eng, Machine Translation","unsupervised-machine-translation-on-wmt2016-2, machine-translation-on-wmt2016-russian, machine-translation-on-wmt2016-english-1, machine-translation-on-wmt2016-finnish, machine-translation-on-wmt2016-english-german, translation-eng-deu-on-newstest2016-ende, unsupervised-machine-translation-on-wmt2016, unsupervised-machine-translation-on-wmt2016-3, machine-translation-on-wmt2016-english-french, sequence-to-sequence-language-modeling-on-1, machine-translation-on-wmt2016-romanian, machine-translation-on-wmt2016-german-english, unsupervised-machine-translation-on-wmt2016-1, unsupervised-machine-translation-on-wmt2016-5, machine-translation-on-wmt2016-english, machine-translation-on-wmt2016-czech-english, machine-translation-on-wmt2016-english-czech, translation-deu-eng-on-newstest2016-deen",,See all 844 tasks,Machine Translation100 benchma,Machine Translation100 benchma
ACNE04,ACNE04 Dataset,"The ACNE04 dataset includes 3756 Chinese face images with Acne.  The ACNE04 dataset includes the annotations of local lesion numbers and global acne
severity based on Hayashi Criterion.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Image,,,,,,,,"Acne Severity Grading, Medical Image Classification",acne-severity-grading-on-acne04,,See all 844 tasks,Medical Image Classification11,Medical Image Classification11
COVIDGR,COVIDGR Dataset,"Under a close collaboration with an expert radiologist team of the Hospital Universitario San Cecilio, the COVIDGR-1.0 dataset of patients' anonymized X-ray images has been built. 852 images have been collected following a strict labeling protocol. They are categorized into 426 positive cases and 426 negative cases. Positive images correspond to patients who have been tested positive for COVID-19 using RT-PCR within a time span of at most 24h between the X-ray image and the test. Every image has been taken using the same type of equipment and with the same format: only the posterior-anterior view is considered.",https://github.com/ari-dasci/covidgr,EditUnknown,Image,,,,,852 images,,,"COVID-19 Diagnosis, Medical Image Classification, Domain Adaptation","covid-19-diagnosis-on-covidgr, medical-image-classification-on-covidgr",,See all 844 tasks,Medical Image Classification11,Medical Image Classification11
IDRiD,IDRiD Dataset,Indian Diabetic Retinopathy Image Dataset (IDRiD) dataset consists of typical diabetic retinopathy lesions and normal retinal structures annotated at a pixel level. This dataset also provides information on the disease severity of diabetic retinopathy and diabetic macular edema for each image. This dataset is perfect for the development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.,https://production-media.paperswithcode.com/datasets/idrid_logo.png,EditUnknown,Image,,,,,,,,"Optic Disc Detection, Fovea Detection, Medical Image Classification","optic-disc-detection-on-idrid, medical-image-classification-on-idrid, fovea-detection-on-idrid",,See all 844 tasks,Medical Image Classification11,Medical Image Classification11
ISIC_2020_Challenge_Dataset,ISIC 2020 Challenge Dataset Dataset,"The dataset contains 33,126 dermoscopic training images of unique benign and malignant skin lesions from over 2,000 patients. Each image is associated with one of these individuals using a unique patient identifier. All malignant diagnoses have been confirmed via histopathology, and benign diagnoses have been confirmed using either expert agreement, longitudinal follow-up, or histopathology. A thorough publication describing all features of this dataset is available in the form of a pre-print that has not yet undergone peer review.

The dataset was generated by the International Skin Imaging Collaboration (ISIC) and images are from the following sources: Hospital Clínic de Barcelona, Medical University of Vienna, Memorial Sloan Kettering Cancer Center, Melanoma Institute Australia, University of Queensland, and the University of Athens Medical School.

The dataset was curated for the SIIM-ISIC Melanoma Classification Challenge hosted on Kaggle during the Summer of 2020.

DOI: https://doi.org/10.34970/2020-ds01",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCreative Commons Attribution-Non Commercial 4.0 International License.,Image,,2020,,,,,,"Skin Cancer Classification, Skin Lesion Classification, Skin Cancer Segmentation, Medical Image Classification",medical-image-classification-on-isic-2020,,See all 844 tasks,Medical Image Classification11,Medical Image Classification11
LIMUC,LIMUC Dataset,"The LIMUC dataset is the largest publicly available labeled ulcerative colitis dataset that compromises 11276 images from 564 patients and 1043 colonoscopy procedures. Three experienced gastroenterologists were involved in the annotation process, and all images are labeled according to the Mayo endoscopic score (MES).",https://production-media.paperswithcode.com/datasets/d4fa3957-7be3-4a6c-93ae-6cd2284384f9.png,EditCreative Commons Attribution 4.0 International,Image,,,,,11276 images,,,"Medical Image Classification, Self-Supervised Learning, Image Classification, Semi-Supervised Image Classification, Medical Diagnosis",image-classification-on-limuc,,See all 844 tasks,Medical Image Classification11,Medical Image Classification11
Malaria_Dataset,Malaria Dataset Dataset,"The dataset contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells.",https://lhncbc.nlm.nih.gov/publication/pub9932,EditUnknown,Image,,,,,,,,"Image Classification, Medical Image Classification","medical-image-classification-on-malaria, image-classification-on-malaria-dataset",,See all 844 tasks,Medical Image Classification11,Medical Image Classification11
NCT-CRC-HE-100K,NCT-CRC-HE-100K Dataset,"The NCT-CRC-HE-100K dataset is a set of 100,000 non-overlapping image patches extracted from 86 H$\&$E stained human cancer tissue slides and normal tissue from the NCT biobank (National Center for Tumor Diseases) and the UMM pathology archive (University Medical Center Mannheim). While the dataset Colorectal Cacner-Validation-Histology-7K (CRC-VAL-HE-7K) consist of 7180 images extracted from 50 patients with colorectal adenocarcinoma and were used to create a dataset that does not overlap with patients in the NCT-CRC-HE-100K dataset. It was created by pathologists by manually delineating tissue regions in whole slide images into the following nine tissue classes: Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR), colorectal adenocarcinoma epithelium (TUM).",https://production-media.paperswithcode.com/datasets/91495aef-1582-41c6-abc2-1268c00bbd66.png,EditUnknown,Image,,,,,7180 images,Validation-Histology-7K (CRC-VAL-HE-7K) consist of 7180 images,,"Image Classification, Medical Image Classification","image-classification-on-nct-crc-he-100k, medical-image-classification-on-nct-crc-he",,See all 844 tasks,Medical Image Classification11,Medical Image Classification11
NIH-CXR-LT,NIH-CXR-LT Dataset,"NIH-CXR-LT. NIH ChestXRay14 contains over 100,000 chest X-rays labeled with 14 pathologies, plus a “No Findings” class. We construct a single-label, long-tailed version of the NIH ChestXRay14 dataset by introducing five new disease findings described above. The resulting NIH-CXR-LT dataset has 20 classes, including 7 head classes, 10 medium classes, and 3 tail classes. NIH-CXR-LT contains 88,637 images labeled with one of 19 thorax diseases, with 68,058 training and 20,279 test images. The validation and balanced test sets contain 15 and 30 images per class, respectively.",https://production-media.paperswithcode.com/datasets/e5b719ab-afb1-417a-b520-ab19c665c218.png,EditUnknown,Image,,,,,637 images,"training and 20,279 test images",20,"Image Classification, Long-tail Learning, Medical Image Classification",long-tail-learning-on-nih-cxr-lt,,See all 844 tasks,Medical Image Classification11,Medical Image Classification11
OASIS,OASIS Dataset,"A dataset for single-image 3D in the wild consisting of annotations of detailed 3D geometry for 140,000 images.",/paper/oasis-a-large-scale-dataset-for-single-image-1,EditUnknown,"3D, Graph, Image",,,,,000 images,,,"Depth Estimation, Graph Classification, Medical Image Classification, Medical Image Registration","medical-image-classification-on-oasis-3, medical-image-registration-on-oasis, graph-classification-on-oasis",,See all 844 tasks,Medical Image Classification11,Medical Image Classification11
aquamuse,aquamuse Dataset,"5,519 query-based summaries, each associated with an average of 6 input documents selected from an index of 355M documents from Common Crawl.",/paper/aquamuse-automatically-generating-datasets,EditUnknown,Text,English,,,,355M documents,,,"Multi-Document Summarization, Document Summarization, Abstractive Text Summarization",,,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
DUC_2004,DUC 2004 Dataset,"The DUC2004 dataset is a dataset for document summarization. Is designed and used for testing only. It consists of 500 news articles, each paired with four human written summaries. Specifically it consists of 50 clusters of Text REtrieval Conference (TREC) documents, from the following collections: AP newswire, 1998-2000; New York Times newswire, 1998-2000; Xinhua News Agency (English version), 1996-2000. Each cluster contained on average 10 documents.",https://arxiv.org/abs/2005.01791,EditUnknown,Text,English,1998,,,10 documents,,,"Multi-Document Summarization, Text Summarization, Extractive Text Summarization","extractive-text-summarization-on-duc-2004, extractive-text-summarization-on-duc-2004-1, text-summarization-on-duc-2004-task-1, multi-document-summarization-on-duc-2004",,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
MATINF,MATINF Dataset,"Maternal and Infant (MATINF) Dataset is a large-scale dataset jointly labeled for classification, question answering and summarization in the domain of maternity and baby caring in Chinese. An entry in the dataset includes four fields: question (Q), description (D), class (C) and answer (A).

Nearly two million question-answer pairs are collected with fine-grained human-labeled classes from a large Chinese maternity and baby caring QA site. Authors conduct both automatic and manual data cleansing and remove: (1) classes with insufficient samples; (2) entries in which the length of the description filed is less than the length of the question field; (3) data with any field longer than 256 characters; (4) human-spotted ill-formed data. After the data cleansing, MATINF is constructed with the remaining 1.07 million entries",https://arxiv.org/pdf/2004.12302v2.pdf,EditUnknown,Text,English,,,,,,,"Multi-Document Summarization, Document Summarization, Question Answering",,,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
MS_2,MS^2 Dataset,"MS^2 (Multi-Document Summarization of Medical Studies) is a dataset of over 470k documents and 20k summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is one of the first large-scale, publicly available multi-document summarization dataset in the biomedical domain.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-04-15_at_17.47.58.png,EditUnknown,Text,English,,,,470k documents,,,"Multi-Document Summarization, Scientific Document Summarization",multi-document-summarization-on-ms-2,,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
Multi-News,Multi-News Dataset,"Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited.",https://arxiv.org/pdf/1906.01749.pdf,EditCustom,Text,English,,,,,,,"Cross-Document Language Modeling, Text Summarization, Information Threading, Summarization, Multi-Document Summarization, Document Summarization","information-threading-on-multi-news, summarization-on-multi-news, cross-document-language-modeling-on-multinews, multi-document-summarization-on-multi-news, cross-document-language-modeling-on-multinews-1",,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
Multi-XScience,Multi-XScience Dataset,"Multi-XScience is a large-scale dataset for multi-document summarization of scientific articles. It has 30,369, 5,066 and 5,093 samples for the train, validation and test split respectively. The average document length is 778.08 words and the average summary length is 116.44 words.",https://github.com/yaolu/Multi-XScience,EditUnknown,Text,English,,,,093 samples,,,"Multi-Document Summarization, Document Summarization",,,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
OpoSum,OpoSum Dataset,"OPOSUM is a dataset for the training and evaluation of Opinion Summarization models which contains Amazon reviews from six product domains: Laptop Bags, Bluetooth Headsets, Boots, Keyboards, Televisions, and Vacuums.
The six training collections were created by downsampling from the Amazon Product Dataset introduced in McAuley et al. (2015) and contain reviews and their respective ratings. 

A subset of the dataset has been manually annotated, specifically, for each domain, 10 different products were uniformly sampled (across ratings) with 10 reviews each, amounting to a total of 600 reviews, to be used only for development (300) and testing (300).",https://arxiv.org/abs/1808.08858,EditUnknown,Text,English,2015,,,,,,"Multi-Document Summarization, Document Summarization, Topic Models",,,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
WCEP,WCEP Dataset,"The WCEP dataset for multi-document summarization (MDS) consists of short, human-written summaries about news events, obtained from the Wikipedia Current Events Portal (WCEP), each paired with a cluster of news articles associated with an event. These articles consist of sources cited by editors on WCEP, and are extended with articles automatically obtained from the Common Crawl News dataset.",https://github.com/complementizer/wcep-mds-dataset,EditUnknown,Text,English,,,,,,,"Multi-Document Summarization, Document Summarization",multi-document-summarization-on-wcep,,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
Wikipedia_Generation,Wikipedia Generation Dataset,Wikipedia Generation is a dataset for article generation from Wikipedia from references at the end of Wikipedia page and the top 10 search results for the Wikipedia topic.,https://github.com/tensorflow/tensor2tensor,EditUnknown,Text,English,,,,,,,"Text Generation, Multi-Document Summarization, Document Summarization",,,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
WikiSum,WikiSum Dataset,"WikiSum is a dataset based on English Wikipedia and suitable for a task of multi-document abstractive summarization. In each instance, the input is comprised of a Wikipedia topic (title of article) and a collection of non-Wikipedia reference documents, and the target is the Wikipedia article text. The dataset is restricted to the articles with at least one crawlable citation. The official split divides the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples respectively.",https://arxiv.org/pdf/1801.10198.pdf,EditUnknown,Text,English,,,,232998 examples,"split divides the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples",,"Multi-Document Summarization, Document Summarization, Abstractive Text Summarization",,,See all 844 tasks,Multi-Document Summarization5 ,Multi-Document Summarization5 
HaVG,HaVG Dataset,"A dataset that contains the description of an image or a section within the image in Hausa and its equivalent in English.  Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It is estimated that about 100 to 150 million people speak the language, with more than 80 million indigenous speakers.  The dataset comprises 32,923 images and their descriptions that are divided into training, development, test, and challenge test set. The Hausa Visual Genome is the first dataset of its kind and can be used for Hausa-English machine translation, multi-modal research, and image description, among various other natural language processing and generation tasks.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,923 images,"valent in English.  Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It is estimated that about 100 to 150 million people speak the language, with more than 80 million indigenous speakers.  The dataset comprises 32,923 images",,"Multimodal Machine Translation, Translation, Machine Translation",,,See all 844 tasks,Multimodal Machine Translation,Multimodal Machine Translation
Hindi_Visual_Genome,Hindi Visual Genome Dataset,Hindi Visual Genome is a multimodal dataset consisting of text and images suitable for English-Hindi multimodal machine translation task and multimodal research.,/paper/hindi-visual-genome-a-dataset-for-multimodal,EditUnknown,Text,English,,,,,,,"Multimodal Machine Translation, Machine Translation",,,See all 844 tasks,Multimodal Machine Translation,Multimodal Machine Translation
Multi30K,Multi30K Dataset,"Multi30K is a large-scale multilingual multimodal dataset for interdisciplinary machine learning research. It extends the Flickr30K dataset with German translations created by professional translators over a subset of the English descriptions, and descriptions crowdsourced independently of the original English descriptions. The dataset was introduced to stimulate multilingual multimodal research.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Translation eng-deu, Multimodal Machine Translation, Real-time Instance Segmentation, Translation deu-eng","translation-deu-eng-on-multi30k-test-2017-1, translation-deu-eng-on-multi30k-test-2017, multimodal-machine-translation-on-multi30k, translation-deu-eng-on-multi30k-test-2018, real-time-instance-segmentation-on-multi30k, translation-eng-deu-on-multi30k-test-2018, translation-deu-eng-on-multi30k-test-2016, translation-eng-deu-on-multi30k-test-2016, translation-eng-deu-on-multi30k-test-2017, translation-eng-deu-on-multi30k-test-2017-1",,See all 844 tasks,Multimodal Machine Translation,Multimodal Machine Translation
WMT_2016_Biomedical,WMT 2016 Biomedical Dataset,"The Biomedical Translation Shared Task was first introduced at the First Conference of Machine Translation. The task aims to evaluate systems for the translation of biomedical titles and abstracts from scientific publications. The data includes three language pairs (English ↔ Portuguese, English  ↔ Spanish, English  ↔ French) and two sub-domains of biological sciences and health sciences.

The training data consists mainly of the Scielo corpus, a parallel collection of scientific publications composed of either titles, abstracts or title and abstracts which were retrieved from the Scielo database. For the Scielo corpus, a parallel documents are provided for all language pairs in the two sub-domains, except for the English  ↔ French, where only health was considered, as there were inadequate parallel documents available for biology in that pair. The training data was aligned using the GMA alignment tool. Additionally, a corpus of parallel titles from MEDLINEⓇ for all three language pairs were provided as well as monolingual documents for the four languages, retrieved from the Scielo database. These consist of documents in the Scielo database which have no corresponding document in another language.

The test set consisted of 500 documents (title and abstract) for each of the two directions of each language pair. None of the test documents was included in the training data and there is no overlap of documents between the test sets for any language pair, translation direction and sub-domain.",http://www.statmt.org/wmt16/index.html,EditUnknown,Text,English,,,,500 documents,,,"Automatic Post-Editing, Multimodal Machine Translation, Machine Translation",,,See all 844 tasks,Multimodal Machine Translation,Multimodal Machine Translation
WMT_2016_IT,WMT 2016 IT Dataset,"The IT Translation Task is a shared task introduced in the First Conference on Machine Translation. Compared to WMT 2016 News, this task brought several novelties to WMT:


4 out of the 7 langauges of the IT task are new in WMT,
adaptation to the IT domain with its specifics such as frequent named entities (mostly menu items, names of products and companies) and technical jargon,
adaptation to translation of answers in helpdesk service setting (many of the sentences are instructions with imperative verbs, which is very rare in the News translation task).

The test set consisted of 1000 answers from the Batch 3 of the QTLeap Corpus. The in-domain training data contained 2000 answers from the Batches 1 and 2 and also localization files from several open-source projects (LibreOffice, KDE, VLC) and bilingual dictionaries of IT-related terms extracted from Wikipedia. The out-of-domain training data contained all the corpora from the WMT 2016 News, plus PaCo2-EuEn Basque-English corpus and SETimes with Bulgarian-English parallel sentences. “Constrained” systems were restricted to use only these training data provided by the organizers.

The task was evaluated on the following language pairs:


English → Bulgarian
English → Czech
English → German
English → Spanish
English → Basque
English → Dutch
English → Portuguese",http://www.statmt.org/wmt16/index.html,EditUnknown,Text,English,2016,,,,,,"Automatic Post-Editing, Multimodal Machine Translation, Machine Translation",,,See all 844 tasks,Multimodal Machine Translation,Multimodal Machine Translation
B-T4SA,B-T4SA Dataset,,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Multimodal Sentiment Analysis,multimodal-sentiment-analysis-on-b-t4sa,,See all 844 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
CH-SIMS,CH-SIMS Dataset,"CH-SIMS is a Chinese single- and multimodal sentiment analysis dataset which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.",https://www.aclweb.org/anthology/2020.acl-main.343.pdf,EditUnknown,Text,English,,,,,,,"Multi-Task Learning, Multimodal Sentiment Analysis, Sentiment Analysis",multimodal-sentiment-analysis-on-ch-sims,,See all 844 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
CMU-MOSI,CMU-MOSI Dataset,"The Multimodal Corpus of Sentiment Intensity (CMU-MOSI) dataset is a collection of 2199 opinion video clips. Each opinion video is annotated with sentiment in the range [-3,3]. The dataset is rigorously annotated with labels for subjectivity, sentiment intensity, per-frame and per-opinion annotated visual features, and per-milliseconds annotated audio features.",https://production-media.paperswithcode.com/datasets/bccc5f4c-f9a9-4efa-addf-07e96f4357d8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Emotion Recognition in Conversation, Multimodal Sentiment Analysis","emotion-recognition-in-conversation-on-cmu, multimodal-sentiment-analysis-on-cmu-mosi",,See all 844 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
Multimodal_Opinionlevel_Sentiment_Intensity,Multimodal Opinionlevel Sentiment Intensity Dataset,"Multimodal Opinionlevel Sentiment Intensity (MOSI) contains: (1) multimodal observations including transcribed speech and visual gestures as well as automatic audio and visual features, (2) opinion-level subjectivity segmentation, (3) sentiment intensity annotations with high coder agreement, and (4) alignment between words, visual and acoustic features.",https://arxiv.org/pdf/1606.06259.pdf,EditUnknown,Text,English,,,,,,,Multimodal Sentiment Analysis,multimodal-sentiment-analysis-on-mosi,,See all 844 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
MuSe-CaR,MuSe-CaR Dataset,"The MuSe-CAR database is a large, multimodal (video, audio, and text) dataset which has been gathered in-the-wild with the intention of further understanding Multimodal Sentiment Analysis in-the-wild, e.g., the emotional engagement that takes place during product reviews (i.e., automobile reviews) where a sentiment is linked to a topic or entity. 

The estimated age range of the professional, semi-professional (influncers), and casual reviewers is between the middle of 20s until the late 50s. Most are native English speakers from the UK or the US, while a small minority are non-native, yet fluent English speakers.",https://production-media.paperswithcode.com/datasets/musecar.png,EditUnknown,Text,English,,,,,,,Multimodal Sentiment Analysis,,,See all 844 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
SemEval-2020_Task-8,SemEval-2020 Task-8 Dataset,A multimodal dataset for sentiment analysis on internet memes.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Multimodal Sentiment Analysis, Sentiment Analysis",,,See all 844 tasks,Multimodal Sentiment Analysis5,Multimodal Sentiment Analysis5
EMOPIA,EMOPIA Dataset,"EMOPIA (pronounced ‘yee-mò-pi-uh’) dataset is a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators.",https://production-media.paperswithcode.com/datasets/emopia.png,EditCC BY,"Audio, Image, Text",English,,,,,,,"Music Tagging, Music Classification, Music Generation, Emotion Recognition, Music Information Retrieval, Emotion Classification, Music Style Transfer",,,See all 844 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
JSB_Chorales,JSB Chorales Dataset,"The JSB chorales are a set of short, four-voice pieces of music well-noted for their stylistic homogeneity. The chorales were originally composed by Johann Sebastian Bach in the
18th century. He wrote them by first taking pre-existing melodies from contemporary Lutheran hymns and then harmonising them to create the parts for the remaining
three voices. The version of the dataset used canonically in representation learning contexts consists of 382 such chorales, with a train/validation/test split of 229, 76 and 77 samples respectively.",https://production-media.paperswithcode.com/datasets/Johann_Sebastian_Bach.jpeg,EditPublic Domain,"Audio, Text",English,,,,77 samples,"train/validation/test split of 229, 76 and 77 samples",,"Music Generation, Music Modeling",music-modeling-on-jsb-chorales,,See all 844 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
Lakh_MIDI_Dataset,Lakh MIDI Dataset Dataset,"The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files). Around 10% of all MIDI files include timestamped lyrics events with lyrics are often transcribed at the word, syllable or character level.

LMD-full denotes the whole dataset. LMD-matched is the subset of LMD-full that consists of MIDI files matched with the Million Song Dataset entries. LMD-aligned contains all the files of LMD-matched, aligned to preview MP3s from the Million Song Dataset.

A lakh is a unit of measure used in the Indian number system which signifies 100,000.",https://colinraffel.com/projects/lmd/,EditCC-BY 4.0,"Audio, Text",English,,,,,,,"Music Generation, Music Modeling",,,See all 844 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
MAESTRO,MAESTRO Dataset,"The MAESTRO dataset contains over 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. The MIDI data includes key strike velocities and sustain/sostenuto/una corda pedal positions. Audio and MIDI files are aligned with ∼3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. Uncompressed audio is of CD quality or higher (44.1–48 kHz 16-bit PCM stereo).",https://magenta.tensorflow.org/datasets/maestro,EditUnknown,"Audio, Text",English,,,,,,,"Audio Generation, Music Generation, Music Transcription",music-transcription-on-maestro,,See all 844 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
MuseData,MuseData Dataset,MuseData is an electronic library of orchestral and piano classical music from CCARH. It consists of around 3MB of 783 files.,https://arxiv.org/pdf/1206.6392v1.pdf,EditUnknown,"Audio, Text",English,,,,,,,"Music Generation, Language Modelling",,,See all 844 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
Music21,Music21 Dataset,Music21 is an untrimmed video dataset crawled by keyword query from Youtube. It contains music performances belonging to 21 categories. This dataset is relatively clean and collected for the purpose of training and evaluating visual sound source separation models.,https://arxiv.org/abs/2004.09476,EditBSD-3,"Audio, Text",English,,,,,,21,"Music Generation, Music Transcription, Music Modeling",,,See all 844 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
MusicCaps,MusicCaps Dataset,"MusicCaps is a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts. For each 10-second music clip, MusicCaps provides: 

1) A free-text caption consisting of four sentences on average, describing the music and 

2) A list of music aspects, describing genre, mood, tempo, singer voices, instrumentation, dissonances, rhythm, etc.",https://arxiv.org/pdf/2301.11325v1.pdf,EditCC BY-SA 4.0,"Audio, Image, Text",English,,,,,,,"Text-to-Music Generation, Music Generation, Music Captioning",text-to-music-generation-on-musiccaps,,See all 844 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
NSynth,NSynth Dataset,"NSynth is a dataset of one shot instrumental notes, containing 305,979 musical notes with unique pitch, timbre and envelope. The sounds were collected from 1006 instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. Four second monophonic 16kHz audio snippets were generated (notes) for the instruments.",https://arxiv.org/abs/1907.08520,EditCC BY 4.0,"Audio, Image, Text",English,,,,,,,"Audio Generation, Music Generation, Pitch Classification, Instrument Recognition, Few-Shot Audio Classification, Self-Supervised Learning","instrument-recognition-on-nsynth, pitch-classification-on-nsynth, few-shot-audio-classification-on-nsynth",,See all 844 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
URMP,URMP Dataset,"URMP (University of Rochester Multi-Modal Musical Performance) is a dataset for facilitating audio-visual analysis of musical performances. The dataset comprises 44 simple multi-instrument musical pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece the dataset provided the musical score in MIDI format, the high-quality individual instrument audio recordings and the videos of the assembled pieces.",http://www2.ece.rochester.edu/projects/air/projects/URMP.html,EditUnknown,"Audio, Text",English,,,,,,,"Multi-instrument Music Transcription, Music Transcription, Music Generation, Music Information Retrieval","music-transcription-on-urmp, multi-instrument-music-transcription-on-urmp",,See all 844 tasks,Music Generation1 benchmark173,Music Generation1 benchmark173
BC5CDR,BC5CDR Dataset,"BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions.",https://www.ncbi.nlm.nih.gov/research/bionlp/Data/,EditCustom,"Image, Text",English,,,,,,,"Weakly-Supervised Named Entity Recognition, UIE, Named Entity Recognition (NER)","weakly-supervised-named-entity-recognition-on-5, weakly-supervised-named-entity-recognition-on-8, weakly-supervised-named-entity-recognition-on-6, named-entity-recognition-ner-on-bc5cdr, named-entity-recognition-on-bc5cdr-disease, named-entity-recognition-on-bc5cdr-chemical, uie-on-bc5cdr",,See all 844 tasks,Named Entity Recognition  NER ,Named Entity Recognition  NER 
BLUE,BLUE Dataset,"The BLUE benchmark consists of five different biomedicine text-mining tasks with ten corpora. These tasks cover a diverse range of text genres (biomedical literature and clinical notes), dataset sizes, and degrees of difficulty and, more importantly, highlight common biomedicine text-mining challenges.",/paper/transfer-learning-in-biomedical-natural,EditPublic domain,"Graph, Image, Text",English,,,,,,,"Relation Extraction, Language Modelling, Named Entity Recognition (NER)",,,See all 844 tasks,Named Entity Recognition  NER ,Named Entity Recognition  NER 
CoNLL,CoNLL Dataset,The CoNLL dataset is a widely used resource in the field of natural language processing (NLP). The term “CoNLL” stands for Conference on Natural Language Learning. It originates from a series of shared tasks organized at the Conferences of Natural Language Learning.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Graph, Image, Text",English,,,,,,,"Semantic Role Labeling (predicted predicates), Entity Typing, Sentence segmentation, Named Entity Recognition (NER), Morphological Tagging, Text Segmentation, Entity Disambiguation, Semantic Role Labeling, Dependency Parsing, Cross-Domain Named Entity Recognition, FG-1-PG-1, Weakly-Supervised Named Entity Recognition, UCCA Parsing, Grammatical Error Correction, Token Classification, Cross-Lingual NER, Entity Linking, Part-Of-Speech Tagging, Low Resource Named Entity Recognition, Coreference Resolution, Grammatical Error Detection, Joint Entity and Relation Extraction, Predicate Detection, Chunking, Cross-Lingual Transfer, Relation Extraction, Sequential sentence segmentation, Named Entity Recognition, Chinese Semantic Role Labeling","chunking-on-conll-2003-english, cross-lingual-ner-on-conll-german, named-entity-recognition-on-conll-2000, semantic-role-labeling-on-conll05-brown, entity-linking-on-aida-conll, predicate-detection-on-conll-2005, fg-1-pg-1-on-conll2003, named-entity-recognition-on-conll-2003-german-1, ucca-parsing-on-conll-2019, named-entity-recognition-on-conll03, chunking-on-conll-2000, named-entity-recognition-on-conll-2002-dutch, named-entity-recognition-on-conll-2003-german, entity-typing-on-aida-conll, named-entity-recognition-on-conll, semantic-role-labeling-on-conll-2009, grammatical-error-correction-on-conll-2014, cross-lingual-ner-on-conll-dutch, entity-linking-on-conll-aida, dependency-parsing-on-conll-2009, cross-lingual-ner-on-conll-spanish, semantic-role-labeling-predicted-predicates-1, named-entity-recognition-ner-on-conll-2003, chinese-semantic-role-labeling-on-conll-2009, token-classification-on-conll2003, named-entity-recognition-on-conll-2012-1, coreference-resolution-on-conll12, cross-domain-named-entity-recognition-on, low-resource-named-entity-recognition-on-4, grammatical-error-detection-on-conll-2014-a2, named-entity-recognition-on-conll-2003-3, semantic-role-labeling-on-conll05-wsj, semantic-role-labeling-predicted-predicates, joint-entity-and-relation-extraction-on-2, semantic-role-labeling-on-conll-2012, weakly-supervised-named-entity-recognition-on, predicate-detection-on-conll-2012, semantic-role-labeling-on-conll-2005, relation-extraction-on-conll04, grammatical-error-correction-on-conll-2014-1, low-resource-named-entity-recognition-on-5, coreference-resolution-on-conll-2012, named-entity-recognition-on-conll-2002, chunking-on-conll-2003-german, low-resource-named-entity-recognition-on-6, grammatical-error-detection-on-conll-2014-a1, semantic-role-labeling-on-conll12, entity-disambiguation-on-aida-conll",,See all 844 tasks,Named Entity Recognition  NER ,Named Entity Recognition  NER 
FUNSD,FUNSD Dataset,"Form Understanding in Noisy Scanned Documents (FUNSD) comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking.",/paper/190513538,EditCustom,"Graph, Image, Tabular, Text",English,,,,,,,"Table Detection, Named Entity Recognition (NER), Semantic entity labeling, Entity Linking, Relation Extraction, Optical Character Recognition (OCR)","relation-extraction-on-funsd, semantic-entity-labeling-on-funsd, entity-linking-on-funsd",,See all 844 tasks,Named Entity Recognition  NER ,Named Entity Recognition  NER 
NCBI_Disease,NCBI Disease Dataset,"The NCBI Disease corpus consists of 793 PubMed abstracts, which are separated into training (593), development (100) and test (100) subsets. The NCBI Disease corpus is annotated with disease mentions, using concept identifiers from either MeSH or OMIM.",https://arxiv.org/abs/1812.06081,EditUnknown,"Image, Text",English,,,,,,,"UIE, Named Entity Recognition, Named Entity Recognition (NER)","named-entity-recognition-ner-on-ncbi-disease, uie-on-ncbi-disease, named-entity-recognition-on-ncbi-disease, named-entity-recognition-on-ncbi-disease-1",,See all 844 tasks,Named Entity Recognition  NER ,Named Entity Recognition  NER 
OntoNotes_5.0,OntoNotes 5.0 Dataset,"OntoNotes 5.0 is a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).

OntoNotes Release 5.0 contains the content of earlier releases - and adds source data from and/or additional annotations for, newswire, broadcast news, broadcast conversation, telephone conversation and web data in English and Chinese and newswire data in Arabic.",https://catalog.ldc.upenn.edu/LDC2013T19,EditUnknown,"Image, Text",English,,,,,,,"FG-1-PG-1, Weakly-Supervised Named Entity Recognition, Named Entity Recognition (NER), Generalized Zero-Shot Learning, Entity Typing, Coreference Resolution, Chinese Named Entity Recognition, UIE, Semantic Role Labeling","entity-typing-on-ontonotes-v5-english, coreference-resolution-on-ontonotes, semantic-role-labeling-on-ontonotes, uie-on-ontonotes-5-0, chinese-named-entity-recognition-on-ontonotes-2, generalized-zero-shot-learning-on-ontonotes, named-entity-recognition-on-ontonotes-5-0, fg-1-pg-1-on-ontonotes-5-0, named-entity-recognition-ner-on-ontonotes-v5, entity-typing-on-ontonotes, weakly-supervised-named-entity-recognition-on-2, named-entity-recognition-on-ontonotes",,See all 844 tasks,Named Entity Recognition  NER ,Named Entity Recognition  NER 
PubMedQA,PubMedQA Dataset,"The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts.

PubMedQA has 1k expert labeled, 61.2k unlabeled and 211.3k artificially generated QA instances.",https://pubmedqa.github.io/,EditCustom,"Image, Text",English,,,,,,,"Language Modelling, Few-Shot Learning, Named Entity Recognition (NER), Question Answering","question-answering-on-pubmedqa, few-shot-learning-on-pubmedqa",,See all 844 tasks,Named Entity Recognition  NER ,Named Entity Recognition  NER 
SciERC,SciERC Dataset,"SciERC dataset is a collection of 500 scientific abstract annotated with scientific entities, their relations, and coreference clusters. The abstracts are taken from 12 AI conference/workshop proceedings in four AI communities, from the Semantic Scholar Corpus. SciERC extends previous datasets in scientific articles SemEval 2017 Task 10 and SemEval 2018 Task 7 by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links.",http://nlp.cs.washington.edu/sciIE/,EditUnknown,"Graph, Image, Text",English,2017,,,,,,"Named Entity Recognition (NER), Continual Pretraining, Relation Extraction, Joint Entity and Relation Extraction, Few-Shot Relation Classification, UIE, Named Entity Recognition","named-entity-recognition-ner-on-scierc, named-entity-recognition-on-scierc, uie-on-scierc, few-shot-relation-classification-on-scierc, continual-pretraining-on-scierc, relation-extraction-on-scierc, joint-entity-and-relation-extraction-on, relation-extraction-on-scierc-sent",,See all 844 tasks,Named Entity Recognition  NER ,Named Entity Recognition  NER 
WNUT_2017,WNUT 2017 Dataset,"This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarisation), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet “so.. kktny in 30 mins?” - even human experts find entity kktny hard to detect and resolve. This task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text.

The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities.",https://huggingface.co/datasets/leondz/wnut_17,EditCC-BY 4.0,"Image, Text",English,,,,,,,"UIE, Named Entity Recognition (NER), Few-shot NER","named-entity-recognition-on-wnut-2017, uie-on-wnut-2017",,See all 844 tasks,Named Entity Recognition  NER ,Named Entity Recognition  NER 
Adverse_Drug_Events__ADE__Corpus,Adverse Drug Events (ADE) Corpus Dataset,"Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports.

A significant amount of information about drug-related safety issues such as adverse effects are published in medical case reports that can only be explored by human readers due to their unstructured nature. The work presented here aims at generating a systematically annotated corpus that can support the development and validation of methods for the automatic extraction of drug-related adverse effects from medical case reports. The documents are systematically double annotated in various rounds to ensure consistent annotations. The annotated documents are finally harmonized to generate representative consensus annotations. In order to demonstrate an example use case scenario, the corpus was employed to train and validate models for the classification of informative against the non-informative sentences. A Maximum Entropy classifier trained with simple features and evaluated by 10-fold cross-validation resulted in the F₁ score of 0.70 indicating a potential useful application of the corpus.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Graph, Image, Text",English,,,,,,,"Named Entity Recognition (NER), Relation Extraction, Clinical Concept Extraction, Text Classification, NER","text-classification-on-adverse-drug-events, named-entity-recognition-on-adverse-drug, relation-extraction-on-ade-corpus",,See all 844 tasks,NER4 benchmarks648 papers with,NER4 benchmarks648 papers with
BC4CHEMD,BC4CHEMD Dataset,"Introduced by Krallinger et al. in The CHEMDNER corpus of chemicals and drugs and its annotation principles

BC4CHEMD is a collection of 10,000 PubMed abstracts that contain a total of 84,355 chemical entity mentions labeled manually by expert chemistry literature curators.",https://huggingface.co/datasets/drAbreu/bc4chemd_ner,EditUnknown,"Image, Text",English,,,,,,,"Named Entity Recognition (NER), NER",named-entity-recognition-on-bc4chemd,,See all 844 tasks,NER4 benchmarks648 papers with,NER4 benchmarks648 papers with
FiNER-139,FiNER-139 Dataset,"FiNER-139 is comprised of 1.1M sentences annotated with eXtensive Business Reporting Language (XBRL) tags extracted from annual and quarterly reports of publicly-traded companies in the US. Unlike other entity extraction tasks, like named entity recognition (NER) or contract element extraction, which typically require identifying entities of a small set of common types (e.g., persons, organizations), FiNER-139 uses a much larger label set of 139 entity types. Another important difference from typical entity extraction is that FiNER focuses on numeric tokens, with the correct tag depending mostly on context, not the token itself.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,,,,1M sentences,,,NER,,,See all 844 tasks,NER4 benchmarks648 papers with,NER4 benchmarks648 papers with
HarveyNER,HarveyNER Dataset,fine-grained location names extraction from disaster-related tweets,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,"Image, Text",English,,,,,,,"Zero-shot Named Entity Recognition (NER), NER",zero-shot-named-entity-recognition-ner-on-2,,See all 844 tasks,NER4 benchmarks648 papers with,NER4 benchmarks648 papers with
InLegalNER,InLegalNER Dataset,InLegalNER is a corpus of 46545 annotated legal named entities mapped to 14 legal entity types. It is designed for named entity recognition in indian court judgement.,https://arxiv.org/pdf/2211.03442v1.pdf,EditMIT license,"Image, Text",English,,,,,,,"Named Entity Recognition (NER), NER",ner-on-inlegalner,,See all 844 tasks,NER4 benchmarks648 papers with,NER4 benchmarks648 papers with
MSRA_CN_NER,MSRA CN NER Dataset,"Simplified Chinese dataset for NER in The Third International Chinese Language Processing Bakeoff (2006), provided by Microsoft Research Asia (MSRA).",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,2006,,,,,,"Chinese Named Entity Recognition, Chinese Word Segmentation, Cross-Lingual NER, NER","cross-lingual-ner-on-msra, chinese-word-segmentation-on-msra, chinese-named-entity-recognition-on-msra",,See all 844 tasks,NER4 benchmarks648 papers with,NER4 benchmarks648 papers with
NuNER,NuNER Dataset,"The dataset used to pre-train NuNER from the NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data

Contains AI-extracted entities, their concepts, and descriptions from the given text",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Named Entity Recognition, named-entity-recognition, NER",,,See all 844 tasks,NER4 benchmarks648 papers with,NER4 benchmarks648 papers with
SuperMat,SuperMat Dataset,"A growing number of papers are published in the area of superconducting materials science. However, novel text and data mining (TDM) processes are still needed to efficiently access and exploit this accumulated knowledge, paving the way towards data-driven materials design. Herein, we present SuperMat (Superconductor Materials), an annotated corpus of linked data derived from scientific publications on superconductors, which comprises 142 articles, 16052 entities, and 1398 links that are characterised into six categories: the names, classes, and properties of materials; links to their respective superconducting critical temperature (Tc); and parametric conditions such as applied pressure or measurement methods. The construction of SuperMat resulted from a fruitful collaboration between computer scientists and material scientists, and its high quality is ensured through validation by domain experts. The quality of the annotation guidelines was ensured by satisfactory Inter Annotator Agreement (IAA) between the annotators and the domain experts.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMixed,,,,,,,,,NER,ner-on-supermat,,See all 844 tasks,NER4 benchmarks648 papers with,NER4 benchmarks648 papers with
TLUnified-NER,TLUnified-NER Dataset,"We present the development of a Named Entity Recognition (NER) dataset for Tagalog. This corpus helps fill the resource gap present in Philippine languages today, where NER resources are scarce. The texts were obtained from a pretraining corpora containing news reports, and were labeled by native speakers in an iterative fashion. The resulting dataset contains ~7.8k documents across three entity types: Person, Organization, and Location. The inter-annotator agreement, as measured by Cohen's κ, is 0.81. We also conducted extensive empirical evaluation of state-of-the-art methods across supervised and transfer learning settings. Finally, we released the data and processing code publicly to inspire future work on Tagalog NLP.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditGNU GPL v3,,,,,,8k documents,,,NER,,,See all 844 tasks,NER4 benchmarks648 papers with,NER4 benchmarks648 papers with
ACE_2004,ACE 2004 Dataset,"ACE 2004 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2004 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities and relations and was created by Linguistic Data Consortium with support from the ACE Program, with additional assistance from the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) Program.
The objective of the ACE program is to develop automatic content extraction technology to support automatic processing of human language in text form. In September 2004, sites were evaluated on system performance in six areas: Entity Detection and Recognition (EDR), Entity Mention Detection (EMD), EDR Co-reference, Relation Detection and Recognition (RDR), Relation Mention Detection (RMD), and RDR given reference entities. All tasks were evaluated in three languages: English, Chinese and Arabic.",https://catalog.ldc.upenn.edu/LDC2005T09,EditCustom,"Graph, Image, Text",English,2004,,,,,,"Nested Mention Recognition, Nested Named Entity Recognition, Named Entity Recognition (NER), Relation Extraction, Entity Disambiguation, UIE","relation-extraction-on-ace-2004, entity-disambiguation-on-ace2004, nested-named-entity-recognition-on-ace-2004, uie-on-ace-2004, nested-mention-recognition-on-ace-2004, named-entity-recognition-on-ace-2004",,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
ACE_2005,ACE 2005 Dataset,"ACE 2005 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2005 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities, relations and events by the Linguistic Data Consortium (LDC) with support from the ACE Program and additional assistance from LDC.",https://catalog.ldc.upenn.edu/LDC2006T06,EditUnknown,"Graph, Image, Text",English,2005,,,,,,"Event Argument Extraction, Nested Mention Recognition, Nested Named Entity Recognition, Named Entity Recognition (NER), Relation Extraction, Joint Entity and Relation Extraction, Event Extraction","nested-named-entity-recognition-on-ace-2005, relation-extraction-on-ace-2005, event-extraction-on-ace2005, event-argument-extraction-on-ace2005, joint-entity-and-relation-extraction-on-ace, named-entity-recognition-on-ace2005, joint-entity-and-relation-extraction-on-7, named-entity-recognition-on-ace-2005, nested-mention-recognition-on-ace-2005",,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
AMALGUM,AMALGUM Dataset,"AMALGUM is a machine annotated multilayer corpus following the same design and annotation layers as GUM, but substantially larger (around 4M tokens). The goal of this corpus is to close the gap between high quality, richly annotated, but small datasets, and the larger but shallowly annotated corpora that are often scraped from the Web.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC-BY-NC-SA,"Audio, Image, Text",English,,,,,,,"Lemmatization, Nested Mention Recognition, Nested Named Entity Recognition, Named Entity Recognition (NER), Discourse Parsing, Part-Of-Speech Tagging, Discourse Segmentation, Coreference Resolution, Dependency Parsing",,,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
Chilean_Waiting_List,Chilean Waiting List Dataset,"The Chilean Waiting List corpus comprises de-identified referrals from the waiting list in Chilean public hospitals. A subset of 10,000 referrals (including medical
and dental notes) was manually annotated with ten entity types with clinical relevance, keeping 1,000 annotations for a future shared task. A trained medical doctor or dentist annotated these referrals and then, together with three other researchers, consolidated each of the annotations. The annotated corpus has more than 48% of entities embedded in
other entities or containing another. This corpus can be a useful resource to build new models for Nested Named Entity Recognition (NER). This work constitutes the first
annotated corpus using clinical narratives from Chile and one of the few in Spanish.

Hugging Face datasets: https://huggingface.co/plncmm. After predicting over each entity type, merge the prediction to obtain your final micro f1-score. This is for a fair comparison with actual state-of-the-art models.",https://production-media.paperswithcode.com/datasets/ca8722ef-cdba-41de-8fbd-68878e7d796e.png,EditCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.,"Image, Text",English,,,,,,,Nested Named Entity Recognition,nested-named-entity-recognition-on-chilean,,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
DaN_,DaN+ Dataset,DaN+ is a new multi-domain corpus and annotation guidelines for Danish nested named entities (NEs) and lexical normalization to support research on cross-lingual cross-domain learning for a less-resourced language.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT License,"Image, Text",English,,,,,,,"Nested Named Entity Recognition, Named Entity Recognition (NER)",,,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
GENIA,GENIA Dataset,"The GENIA corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology.

The corpus contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms “human”, “blood cells”, and “transcription factors”. The corpus has been annotated with various levels of linguistic and semantic information.

The primary categories of annotation in the GENIA corpus and the corresponding subcorpora are:


Part-of-Speech annotation
Constituency (phrase structure) syntactic annotation
Term annotation
Event annotation
Relation annotation
Coreference annotation",http://www.geniaproject.org/genia-corpus,EditUnknown,"Image, Text",English,,,,,,,"Nested Named Entity Recognition, Named Entity Recognition (NER), UIE, Dependency Parsing, Event Extraction","event-extraction-on-genia, named-entity-recognition-on-genia, event-extraction-on-genia-2013, dependency-parsing-on-genia-uas, dependency-parsing-on-genia-las, nested-named-entity-recognition-on-genia, uie-on-genia",,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
GUM,GUM Dataset,"GUM is an open source multilayer English corpus of richly annotated texts from twelve text types. Annotations include:


Multiple POS tags, morphological features and lemmatization
Sentence segmentation and rough speech act
Document structure in TEI XML (paragraphs, headings, figures, etc.)
ISO date/time annotations
Speaker and addressee information (where relevant)
Constituent and dependency syntax
Information status (given, accessible, new, split antecedent)
Entity and coreference annotation, including bridging anaphora
Entity linking (Wikification)
Discourse parses in Rhetorical Structure Theory and discourse dependencies",https://production-media.paperswithcode.com/datasets/503c5d36-87b9-4803-8427-0fbbb1ed114a.png,EditCC-BY-NC-SA,"Audio, Graph, Image, Text",English,,,,,,,"Timex normalization, Lemmatization, Nested Mention Recognition, Relation Classification, Nested Named Entity Recognition, Named Entity Recognition (NER), Discourse Parsing, Entity Linking, Entity Typing, Part-Of-Speech Tagging, Bridging Anaphora Resolution, Discourse Segmentation, Coreference Resolution, Dependency Parsing, Named Entity Recognition","named-entity-recognition-on-gum, entity-linking-on-gum",,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
LegalNERo,LegalNERo Dataset,"LegalNERo is a manually annotated corpus for named entity recognition in the Romanian legal domain. 
It provides gold annotations for organizations, locations, persons, time and legal resources mentioned in legal documents.
Additionally it offers GEONAMES codes for the named entities annotated as location (where a link could be established). 

The LegalNERo corpus is available in different formats: span-based, token-based and RDF. 
The Linguistic Linked Open Data (LLOD) version is provided in RDF-Turtle format.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY-NC-ND 4.0,"Image, Text",English,,,,,,,"Nested Named Entity Recognition, Named Entity Recognition (NER)",named-entity-recognition-on-legalnero,,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
NNE,NNE Dataset,NNE is a dataset for Nested Named Entity Recognition in English Newswire,https://production-media.paperswithcode.com/datasets/nne.png,EditUnknown,"Image, Text",English,,,,,,,"Nested Named Entity Recognition, Named Entity Recognition (NER)",nested-named-entity-recognition-on-nne,,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
The_QUAERO_French_Medical_Corpus,The QUAERO French Medical Corpus Dataset,"A vast amount of information in the biomedical domain is available as natural language free text. An increasing number of documents in the field are written in languages other than English. Therefore, it is essential to develop resources, methods and tools that address Natural Language Processing in the variety of languages used by the biomedical community. In this paper, we report on the development of an extensive corpus of biomedical documents in French annotated at the entity and concept level. Three text genres are covered, comprising a total of 103,056 words. Ten entity categories corresponding to UMLS Semantic Groups were annotated, using automatic pre-annotations validated by trained human annotators. The pre-annotation method was found helful for entities and achieved above 0.83 precision for all text genres. Overall, a total of 26,409 entity annotations were mapped to 5,797 unique UMLS concepts.",https://production-media.paperswithcode.com/datasets/854a6bff-0a9a-4070-b45d-dcfe0d093bb3.jpg,EditGFDL 1.3,"Image, Text",English,,,,,,,"Medical Named Entity Recognition, Nested Named Entity Recognition, Named Entity Recognition (NER)",,,See all 844 tasks,Nested Named Entity Recognitio,Nested Named Entity Recognitio
arXiv_Astro-Ph,arXiv Astro-Ph Dataset,"Arxiv ASTRO-PH (Astro Physics) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to Astro Physics category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.",https://snap.stanford.edu/data/ca-AstroPh.html,EditUnknown,"Graph, Image, Time Series",,,,,,,,"Multi-Label Classification, Link Prediction, Clique Prediction, Network Embedding","clique-prediction-on-arxiv-astroph-4-clique, clique-prediction-on-arxiv-astroph-2-clique",,See all 844 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
BIRDeep,BIRDeep Dataset,"The BIRDeep Audio Annotations dataset is a collection of bird vocalizations from Doñana National Park, Spain. It was created as part of the BIRDeep project, which aims to optimize the detection and classification of bird species in audio recordings using deep learning techniques. The dataset is intended for use in training and evaluating models for bird vocalization detection and identification.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Graph, Image",,,,,,,,"Bird Audio Detection, Event Detection, Network Embedding, Bird Species Classification With Audio-Visual Data",,,See all 844 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
IS-A,IS-A Dataset,"The IS-A dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are related by the “is a” relation. For example, ‘acute leukemia’ is a ‘leukemia’. The dataset has 294,693 nodes with 356,541 edges between them.",https://arxiv.org/pdf/1906.05939.pdf,EditUnknown,"Graph, Time Series",,,,,,,,"Link Prediction, Network Embedding, Graph Embedding",,,See all 844 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
Netzschleuder,Netzschleuder Dataset,"This is a catalogue and repository of network datasets with the aim of aiding scientific research.

This website is meant to be browsed both by humans and machines alike, and can also be accessed via a convenient JSON API, or via the graph-tool library. The network datasets themselves are available in several machine-readable formats, in particular gt, GraphML, GML and CSV.

The upstream origin of each dataset is meant to be as transparent as possible. Each dataset contains its own publicly available extraction and parsing script, accessible via a git repository, which also includes the entire code for this website, released as Free Software under the AGPLv3.

Users are encouraged to inspect the entire pipeline from original upstream data publication, downloading, parsing and format conversion.

Users are also welcome to report problems or omissions with the datasets, as well as suggest new ones, either by opening an issue, or simply by forking the git repository and proposing a merge request.",https://production-media.paperswithcode.com/datasets/netzschleuder.png,EditAGPLv3,"3D, Graph",,,,,,,,"Graph Reconstruction, Graph Mining, Graphon Estimation, Network Embedding, Graph Clustering, Network Pruning, Network Community Partition",,,See all 844 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
PART-OF,PART-OF Dataset,"The PART-OF dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are parts of the human body. The dataset has 16,894 nodes with 19,436 edges between them.",https://arxiv.org/pdf/1906.05939.pdf,EditUnknown,"Graph, Time Series",,,,,,,,"Link Prediction, Network Embedding",,,See all 844 tasks,Network Embedding161 papers wi,Network Embedding161 papers wi
MatSim,MatSim Dataset,"MatSim is a synthetic dataset, and natural image benchmark for computer vision-based recognition of similarities and transitions between materials and textures, focusing on identifying any material under any conditions using one or a few examples (one-shot learning), including materials states and subclasses.",https://arxiv.org/pdf/2212.00648v1.pdf,EditMIT,Image,,,,,,,,"One-Shot Learning, Contrastive Learning, Material Recognition",,,See all 844 tasks,One-Shot Learning1 benchmark10,One-Shot Learning1 benchmark10
TACO,TACO Dataset,"TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches. These images are manually labelled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms. The annotations are provided in COCO format.",https://github.com/pedropro/TACO,EditUnknown,Image,,,,,,,,"Instance Segmentation, One-Shot Learning, Semantic Segmentation",,,See all 844 tasks,One-Shot Learning1 benchmark10,One-Shot Learning1 benchmark10
TopLogo-10,TopLogo-10 Dataset,Collected from top 10 most popular clothing/wearable brandname logos captured in rich visual context.,/paper/deep-learning-logo-detection-with-data,EditUnknown,Image,,,,,,,,"Metric Learning, Incremental Learning, Traffic Sign Recognition, One-Shot Learning",traffic-sign-recognition-on-toplogo-10,,See all 844 tasks,One-Shot Learning1 benchmark10,One-Shot Learning1 benchmark10
DuReader,DuReader Dataset,"DuReader is a large-scale open-domain Chinese machine reading comprehension dataset. The dataset consists of 200K questions, 420K answers and 1M documents. The questions and documents are based on Baidu Search and Baidu Zhidao. The answers are manually generated. The dataset additionally provides question type annotations – each question was manually annotated as either Entity, Description or YesNo and one of Fact or Opinion.",https://arxiv.org/pdf/1711.05073v4.pdf,EditUnknown,Text,English,,,,1M documents,,,"Reading Comprehension (Zero-Shot), Reading Comprehension (Few-Shot), Open-Domain Question Answering, Reading Comprehension (One-Shot), Reading Comprehension","open-domain-question-answering-on-dureader, reading-comprehension-zero-shot-on-dureader, reading-comprehension-one-shot-on-dureader, reading-comprehension-few-shot-on-dureader",,See all 844 tasks,Open-Domain Question Answering,Open-Domain Question Answering
ELI5,ELI5 Dataset,"ELI5 is a dataset for long-form question answering. It contains 270K complex, diverse questions that require explanatory multi-sentence answers. Web search results are used as evidence documents to answer each question.

ELI5 is also a task in Dodecadialogue.",https://facebookresearch.github.io/ELI5/,EditUnknown,Text,English,,,,,,,"Text Generation, Open-Domain Question Answering, Language Modelling, Long Form Question Answering, Question Answering",open-domain-question-answering-on-eli5,,See all 844 tasks,Open-Domain Question Answering,Open-Domain Question Answering
KILT,KILT Dataset,"KILT (Knowledge Intensive Language Tasks) is a benchmark consisting of 11 datasets representing 5 types of tasks:


Fact-checking (FEVER),
Entity linking (AIDA CoNLL-YAGO, WNED-WIKI, WNED-CWEB),
Slot filling (T-Rex, Zero Shot RE),
Open domain QA (Natural Questions, HotpotQA, TriviaQA, ELI5),
Dialog generation (Wizard of Wikipedia).

All these datasets have been grounded in a single pre-processed wikipedia snapshot, allowing for fairer and more consistent evaluation as well as enabling new task setups such as multitask and transfer learning.",https://ai.facebook.com/tools/kilt/,EditUnknown,Text,English,,,,,,,"Slot Filling, Open-Domain Dialog, Entity Linking, Open-Domain Question Answering, Question Answering, Fact Verification","open-domain-question-answering-on-kilt, open-domain-question-answering-on-kilt-eli5, slot-filling-on-kilt-t-rex, entity-linking-on-kilt-aida-yago2, open-domain-question-answering-on-kilt-1, open-domain-dialog-on-kilt-wizard-of, slot-filling-on-kilt-zero-shot-re, fact-verification-on-kilt-fever, open-domain-question-answering-on-kilt-2, entity-linking-on-kilt-wned-cweb, entity-linking-on-kilt-wned-wiki, question-answering-on-kilt-eli5",,See all 844 tasks,Open-Domain Question Answering,Open-Domain Question Answering
LAMA,LAMA Dataset,"LAnguage Model Analysis (LAMA) consists of a set of knowledge sources, each comprised of a set of facts. LAMA is a probe for analyzing the factual and commonsense knowledge contained in pretrained language models.",https://arxiv.org/pdf/1909.01066v2.pdf,EditCC-BY-NC 4.0,Text,English,,,,,,,"Open-Domain Question Answering, Language Modelling, Question Answering",,,See all 844 tasks,Open-Domain Question Answering,Open-Domain Question Answering
ScienceQA,ScienceQA Dataset,"Science Question Answering (ScienceQA) is a new benchmark that consists of 21,208 multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations. Out of the questions in ScienceQA, 10,332 (48.7%) have an image context, 10,220 (48.2%) have a text context, and 6,532 (30.8%) have both. Most questions are annotated with grounded lectures (83.9%) and detailed explanations (90.5%). The lecture and explanation provide general external knowledge and specific reasons, respectively, for arriving at the correct answer. To the best of our knowledge, ScienceQA is the first large-scale multimodal dataset that annotates lectures and explanations for the answers.

ScienceQA, in contrast to previous datasets, has richer domain diversity from three subjects: natural science, language science, and social science. Questions in each subject are categorized first by the topic (Biology, Physics, Chemistry, etc.), then by the category (Plants, Cells, Animals, etc.), and finally by the skill (Classify fruits and vegetables as plant parts, Identify countries of Africa, etc.). ScienceQA features 26 topics, 127 categories, and 379 skills that cover a wide range of domains.",https://production-media.paperswithcode.com/datasets/79d32b50-be5c-4d15-af34-c69c98ba7e61.png,EditCC BY-NC-SA,"Image, Text",English,,,,,,127,"Science Question Answering, Open-Domain Question Answering, Multimodal Deep Learning, Visual Question Answering (VQA), Visual Commonsense Reasoning, Explainable Models, Question Answering",science-question-answering-on-scienceqa,,See all 844 tasks,Open-Domain Question Answering,Open-Domain Question Answering
SearchQA,SearchQA Dataset,"SearchQA was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis.",https://arxiv.org/pdf/1704.05179.pdf,EditUnknown,Text,English,,,,,,,Open-Domain Question Answering,open-domain-question-answering-on-searchqa,,See all 844 tasks,Open-Domain Question Answering,Open-Domain Question Answering
WebQuestions,WebQuestions Dataset,"The WebQuestions dataset is a question answering dataset using Freebase as the knowledge base and contains 6,642 question-answer pairs. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. The original split uses 3,778 examples for training and 2,032 for testing. All answers are defined as Freebase entities.

Example questions (answers) in the dataset include “Where did Edgar Allan Poe died?” (baltimore) or “What degrees did Barack Obama get?” (bachelor_of_arts, juris_doctor).",https://arxiv.org/abs/1406.3676,EditUnknown,Text,English,,,,778 examples,"split uses 3,778 examples",,"Open-Domain Question Answering, KG-to-Text Generation, Knowledge Base Question Answering, Question Answering","open-domain-question-answering-on, knowledge-base-question-answering-on-3, question-answering-on-webquestions, kg-to-text-generation-on-webquestions",,See all 844 tasks,Open-Domain Question Answering,Open-Domain Question Answering
SFD,SFD Dataset,"The Short Film Dataset (SFD) is a long video question-answering benchmark. It consists of 1,078 movies and 4,885 questions, spanning 250 hours of video data (13 minutes per movie on average).",https://production-media.paperswithcode.com/datasets/e0d26845-7d7c-4655-aef4-3d49fa9cfc21.png,Editcc-by-nc-sa-4.0,"Text, Video",English,,,,,,,"zero-shot long video question answering, Open-Ended Question Answering",,,See all 844 tasks,Open-Ended Question Answering2,Open-Ended Question Answering2
WorldCuisines,WorldCuisines Dataset,"Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.

The dataset can be found in https://huggingface.co/datasets/worldcuisines/vqa",https://production-media.paperswithcode.com/datasets/3a7927e6-5a82-4e00-9a29-0221fa2aa2fe.png,EditCreative Commons Attribution-ShareAlike 4.0 International License,"Image, Text",English,,,,60k instances,"valuate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images",,"Visual Question Answering, Multiple-choice, Visual Question Answering (VQA), Open-Ended Question Answering",,,See all 844 tasks,Open-Ended Question Answering2,Open-Ended Question Answering2
DAVIS-DTA,DAVIS-DTA Dataset,"Dataset Description: The interaction of 72 kinase inhibitors with 442 kinases covering >80% of the human catalytic protein kinome.

Task Description: Regression. Given the target amino acid sequence/compound SMILES string, predict their binding affinity.

Dataset Statistics: 0.3.2 Update: 25,772 DTI pairs, 68 drugs, 379 proteins. Before: 27,621 DTI pairs, 68 drugs, 379 proteins.

[1] Davis, M., Hunt, J., Herrgard, S. et al. Comprehensive analysis of kinase inhibitor selectivity. Nat Biotechnol 29, 1046–1051 (2011).

[2] Huang, Kexin, et al. “DeepPurpose: a Deep Learning Library for Drug-Target Interaction Prediction” Bioinformatics.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,2011,,,,,,", Protein Language Model, Drug Discovery","drug-discovery-on-davis-dta, on-davis-dta, protein-language-model-on-davis-dta",,See all 844 tasks,Protein Language Model1 benchm,Protein Language Model1 benchm
ConceptNet,ConceptNet Dataset,"ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use.",https://arxiv.org/pdf/1612.03975v2.pdf,EditCC BY-SA 4.0,Text,English,,,,,,,"Knowledge Graphs, Word Embeddings, 16k, Question Answering",16k-on-conceptnet,,See all 844 tasks,Question Answering256 benchmar,Question Answering256 benchmar
HellaSwag,HellaSwag Dataset,"HellaSwag is a challenge dataset for evaluating commonsense NLI that is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy).",https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_16.30.44.png,EditMIT,Text,English,,,,,,,"Text Generation, parameter-efficient fine-tuning, Sentence Completion, Question Answering","text-generation-on-hellaswag-10-shot, text-generation-on-hellaswag-tr, question-answering-on-hellaswag, sentence-completion-on-hellaswag, parameter-efficient-fine-tuning-on-hellaswag",,See all 844 tasks,Question Answering256 benchmar,Question Answering256 benchmar
MMLU,MMLU Dataset,"MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a model’s blind spots.",https://production-media.paperswithcode.com/datasets/cfd67fe7-4a00-4aa5-b2a1-2e0c866fd617.png,EditCustom,Text,English,,,,,,,"Multiple Choice Question Answering (MCQA), Natural Language Understanding, Text Generation, Multi-task Language Understanding, Question Answering","multiple-choice-question-answering-mcqa-on-16, multiple-choice-question-answering-mcqa-on-6, multiple-choice-question-answering-mcqa-on-11, multiple-choice-question-answering-mcqa-on-25, multiple-choice-question-answering-mcqa-on-13, multiple-choice-question-answering-mcqa-on-26, text-generation-on-mmlu-tr, multiple-choice-question-answering-mcqa-on-2, multiple-choice-question-answering-mcqa-on-23, multiple-choice-question-answering-mcqa-on-9, multiple-choice-question-answering-mcqa-on-4, multiple-choice-question-answering-mcqa-on-14, question-answering-on-mmlu, multiple-choice-question-answering-mcqa-on-20, multiple-choice-question-answering-mcqa-on-7, multiple-choice-question-answering-mcqa-on-18, multiple-choice-question-answering-mcqa-on-3, multi-task-language-understanding-on-mmlu, multiple-choice-question-answering-mcqa-on-19, text-generation-on-mmlu-5-shot, multiple-choice-question-answering-mcqa-on-24, multiple-choice-question-answering-mcqa-on-12, multiple-choice-question-answering-mcqa-on-8, multiple-choice-question-answering-mcqa-on-5, multiple-choice-question-answering-mcqa-on-17, multiple-choice-question-answering-mcqa-on-10, multiple-choice-question-answering-mcqa-on-15, multi-task-language-understanding-on-mmlu-5-1",,See all 844 tasks,Question Answering256 benchmar,Question Answering256 benchmar
SQuAD,SQuAD Dataset,"The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.",https://arxiv.org/abs/2004.03705,EditCC BY-SA 4.0,Text,English,,,,,,,"Data-free Knowledge Distillation, Open-Domain Question Answering, Question Generation, Reading Comprehension, Question Answering","question-answering-on-squad-1, question-answering-on-squad20, question-answering-on-squad11, open-domain-question-answering-on-squad11, data-free-knowledge-distillation-on-squad, question-generation-on-squad11, question-answering-on-squad-v2, question-answering-on-squad20-dev, question-answering-on-squad-adversarial, question-generation-on-squad, question-answering-on-squad11-dev, open-domain-question-answering-on-squad1-1",,See all 844 tasks,Question Answering256 benchmar,Question Answering256 benchmar
TriviaQA,TriviaQA Dataset,"TriviaQA is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets.",https://arxiv.org/abs/1903.06164,EditUnknown,Text,English,,,,662K documents,,,"Open-Domain Question Answering, Reading Comprehension, Question Generation, Question Answering","open-domain-question-answering-on-kilt-2, question-generation-on-triviaqa, question-answering-on-triviaqa, open-domain-question-answering-on-triviaqa",,See all 844 tasks,Question Answering256 benchmar,Question Answering256 benchmar
CRSB,CRSB Dataset,The Official dataset proposed int the paper Context Awareness Gate For Retrieval Augmented Generation,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"RAG, Retrieval, Question Answering",,,See all 844 tasks,RAG515 papers with code,RAG515 papers with code
PeerQA,PeerQA Dataset,"We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health. PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens.",https://production-media.paperswithcode.com/datasets/0587bbe8-a4e7-41c8-bebe-a6b829831966.jpg,EditCC-BY-NC-SA 4.0,"Text, Time Series",English,,,,,,,"RAG, Sentence Retrieval, Passage Retrieval, answerability prediction, Text Retrieval, Question Answering","question-answering-on-peerqa, sentence-retrieval-on-peerqa, passage-retrieval-on-peerqa, answerability-prediction-on-peerqa",,See all 844 tasks,RAG515 papers with code,RAG515 papers with code
CDR,CDR Dataset,"The BioCreative V CDR task corpus is manually annotated for chemicals, diseases and chemical-induced disease (CID) relations. It contains the titles and abstracts of 1500 PubMed articles and is split into equally sized train, validation and test sets. It is common to first tune a model on the validation set and then train on the combination of the train and validation sets before evaluating on the test set. It is also common to filter negative relations with disease entities that are hypernyms of a corresponding true relations disease entity within the same abstract (see Appendix C of this paper for details).",https://production-media.paperswithcode.com/datasets/cdr.jpg,EditUnknown,Graph,,,,,,,,"Relation Extraction, Joint Entity and Relation Extraction, Reflection Removal","joint-entity-and-relation-extraction-on-cdr, relation-extraction-on-cdr",,See all 844 tasks,Reflection Removal5 benchmarks,Reflection Removal5 benchmarks
PolarRR,PolarRR Dataset,PolarRR is a new dataset with more than 100 types of glass in which obtained transmission images are perfectly aligned with input mixed images.,https://openaccess.thecvf.com/content_CVPR_2020/papers/Lei_Polarized_Reflection_Removal_With_Perfect_Alignment_in_the_Wild_CVPR_2020_paper.pdf,EditUnknown,Image,,,,,,,,"Image Enhancement, Reflection Removal",,,See all 844 tasks,Reflection Removal5 benchmarks,Reflection Removal5 benchmarks
SlowFlow,SlowFlow Dataset,SlowFlow is an optical flow dataset collected by applying Slow Flow technique on data from a high-speed camera and analyzing the performance of the state-of-the-art in optical flow under various levels of motion blur.,/paper/slow-flow-exploiting-high-speed-cameras-for,EditCustom,Video,,,,,,,,"Video Frame Interpolation, Optical Flow Estimation, Reflection Removal",,,See all 844 tasks,Reflection Removal5 benchmarks,Reflection Removal5 benchmarks
iris,iris Dataset,"The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula ""all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus"".",https://production-media.paperswithcode.com/datasets/Large53.jpeg,EditUnknown,"Image, Text",English,1936,,,,,,"Clustering Algorithms Evaluation, Quantum Machine Learning, Incremental Constrained Clustering, Denoising, Image/Document Clustering, Reinforcement Learning, Feature Importance, General Classification","clustering-algorithms-evaluation-on-iris, image-document-clustering-on-iris, denoising-on-iris, incremental-constrained-clustering-on-iris, general-classification-on-iris, reinforcement-learning-on-iris, feature-importance-on-iris, quantum-machine-learning-on-iris",,See all 844 tasks,Reinforcement Learning1 benchm,Reinforcement Learning1 benchm
TruthGen,TruthGen Dataset,"TruthGen is a dataset of generated true and false statements, intended for research on truthfulness in reward models and language models, specifically in contexts where political bias is undesirable. This dataset contains 1,987 statement pairs (3,974 statements in total), with each pair containing one objectively true statement and one false statement. It spans a variety of everyday and scientific facts, excluding politically charged topics to the greatest extent possible. The dataset is particularly useful for evaluating reward models trained for alignment with truth, as well as for research on mitigating political bias while improving model accuracy on truth-related tasks.",https://huggingface.co/datasets/wwbrannon/TruthGen,EditCC BY 4.0,"Image, Text",English,,,,,,,"Reinforcement Learning, Text Classification",,,See all 844 tasks,Reinforcement Learning1 benchm,Reinforcement Learning1 benchm
Animals-10,Animals-10 Dataset,"It contains about 28K medium quality animal images belonging to 10 categories: dog, cat, horse, spyder, butterfly, chicken, sheep, cow, squirrel, and elephant.

All the images have been collected from ""google images"" and have been checked by humans. There is some erroneous data to simulate real conditions (eg. images taken by users of your app).
The main directory is divided into folders, one for each category. The image count for each category varies from 2K to 5 K units.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://www.kaggle.com/datasets/alessiocorrado99/animals10,"Image, Video",,,,,,,10,"Image Classification, Multi-Animal Tracking with identification, Representation Learning",representation-learning-on-animals-10,,See all 844 tasks,Representation Learning16 benc,Representation Learning16 benc
Causal_Triplet,Causal Triplet Dataset,"Causal Triplet is a causal representation learning benchmark featuring not only visually more complex scenes, but also two crucial desiderata commonly overlooked in previous works: 

1) An actionable counterfactual setting, where only certain object-level variables allow forcounterfactual observations whereas others do not. 

2) An interventional downstream task with an emphasis on out-of-distribution robustness from the independent causal mechanisms principle.",https://arxiv.org/pdf/2301.05169v1.pdf,EditApache-2.0 license,,,,,,,,,Representation Learning,,,See all 844 tasks,Representation Learning16 benc,Representation Learning16 benc
EXTREME_CLASSIFICATION,EXTREME CLASSIFICATION Dataset,"The objective in extreme multi-label classification is to learn feature architectures and classifiers that can automatically tag a data point with the most relevant subset of labels from an extremely large label set. This repository provides resources that can be used for evaluating the performance of extreme multi-label algorithms including datasets, code, and metrics.

For more details please visit the link http://manikvarma.org/downloads/XC/XMLRepository.html",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY-NC-ND,"Image, Text",English,,,,,,,"Product Recommendation, Multi-Label Classification, Multi-Label Learning, Extreme Multi-Label Classification, Multi-Label Text Classification, Representation Learning",,,See all 844 tasks,Representation Learning16 benc,Representation Learning16 benc
OmniBenchmark,OmniBenchmark Dataset,"Omni-Realm Benchmark (OmniBenchmark) is a diverse (21 semantic realm-wise datasets) and concise (realm-wise datasets have no concepts overlapping) benchmark for evaluating pre-trained model generalization across semantic super-concepts/realms, e.g. across mammals to aircraft. 

[ECCV2022]",https://production-media.paperswithcode.com/datasets/59a61c4e-b055-422e-8723-c63284bd8c86.png,EditCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,Image,,,,,,,,"Prompt Engineering, Image Classification, Representation Learning, Fine-Grained Image Recognition",image-classification-on-omnibenchmark,,See all 844 tasks,Representation Learning16 benc,Representation Learning16 benc
SciDocs,SciDocs Dataset,SciDocs evaluation framework consists of a suite of evaluation tasks designed for document-level tasks.,https://github.com/allenai/scidocs,EditUnknown,"Image, Text",English,,,,,,,"Zero-shot Text Search, Re-Ranking, Language Modelling, Text Retrieval, Representation Learning, Document Classification","zero-shot-text-search-on-scidocs, re-ranking-on-scidocs, text-retrieval-on-scidocs, representation-learning-on-scidocs",,See all 844 tasks,Representation Learning16 benc,Representation Learning16 benc
Sports10,Sports10 Dataset,"Games dataset containing 100,000 Gameplay Images of 175 Video Games across 10 Sports Genres - AMERICAN FOOTBALL, BASKETBALL, BIKE RACING, CAR RACING, FIGHTING, HOCKEY, SOCCER, TABLE TENNIS, TENNIS. 



Hand-curated images to remove menu/transition frames and only include gameplay sequences.



Games are divided into three visual styling categories: 
        RETRO (arcade-style, 1990s and earlier)
        MODERN (roughly 2000s)
        PHOTOREAL (roughly late 2010s).",https://production-media.paperswithcode.com/datasets/Sports10_Banner_Image.png,EditApache License 2.0,Image,,,,,,,,"Image Classification, Representation Learning","representation-learning-on-sports10, image-classification-on-sports10",,See all 844 tasks,Representation Learning16 benc,Representation Learning16 benc
SYNTH-PEDES,SYNTH-PEDES Dataset,"SYNTH-PEDES is a large-scale person dataset with image-text pairs by far, which contains 312,321 identities, 4,791,711 images, and 12,138,157 textual descriptions.",https://github.com/zplusdragon/plip,EditMIT License,,,,,,711 images,,,Representation Learning,,,See all 844 tasks,Representation Learning16 benc,Representation Learning16 benc
TYC_Dataset,TYC Dataset Dataset,"We introduce the trapped yeast cell (TYC) dataset, a novel dataset for understanding instance-level semantics and motions of cells in microstructures. We release $105$ dense annotated high-resolution brightfield microscopy images, including about $19$k instance masks. We also release $261$ curated video clips composed of $1293$ high-resolution microscopy images to facilitate unsupervised understanding of cell motions and morphology.",https://production-media.paperswithcode.com/datasets/179dbc32-c6f0-45c6-98eb-93a3a8b45ca7.gif,EditCC BY 4.0,Image,,,,,,,,"Unsupervised Pre-training, Instance Segmentation, Unsupervised Image Segmentation, Panoptic Segmentation, Cell Segmentation, Representation Learning",,,See all 844 tasks,Representation Learning16 benc,Representation Learning16 benc
xView3-SAR,xView3-SAR Dataset,"Unsustainable fishing practices worldwide pose a major threat to marine resources and ecosystems. Identifying vessels that do not show up in conventional monitoring systems---known as ``dark vessels''---is key to managing and securing the health of marine environments. With the rise of satellite-based synthetic aperture radar (SAR) imaging and modern machine learning (ML), it is now possible to automate detection of dark vessels day or night, under all-weather conditions. SAR images, however, require a domain-specific treatment and are not widely accessible to the ML community. Maritime objects (vessels and offshore infrastructure) are relatively small and sparse, challenging traditional computer vision approaches. We present the largest labeled dataset for training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The images are annotated using a combination of automated and manual analysis. Co-located bathymetry and wind state rasters accompany every SAR image. We also provide an overview of the xView3 Computer Vision Challenge, an international competition using xView3-SAR for ship detection and characterization at large scale. We release the data  (\href{https://iuu.xview.us/}{https://iuu.xview.us/}) and code (\href{https://github.com/DIUx-xView}{https://github.com/DIUx-xView}) to support ongoing development and evaluation of ML approaches for this important application.",https://production-media.paperswithcode.com/datasets/40879ffc-551e-4dd1-a051-8c0f1797cbb8.jpg,EditCC BY-NC-SA 4.0,Image,,,,,,"training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images",,"Object Detection, Dense Object Detection, Holdout Set, regression, Decision Making Under Uncertainty, Representation Learning",holdout-set-on-xview3-sar,,See all 844 tasks,Representation Learning16 benc,Representation Learning16 benc
ALCE,ALCE Dataset,ALCE is a benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations.,https://arxiv.org/pdf/2305.14627v1.pdf,EditMIT License,,,,,,,,,Retrieval,,,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
ARO,ARO Dataset,"Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO-Order & Flickr30k-Order, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases.",https://production-media.paperswithcode.com/datasets/09c174ea-97ca-404d-97dd-80e5e0992c57.png,EditUnknown,,,,,,,,,Retrieval,,,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
HotpotQA,HotpotQA Dataset,"HotpotQA is a question answering dataset collected on the English Wikipedia, containing about 113K crowd-sourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. 

A diverse range of reasoning strategies are featured in HotpotQA, including questions involving missing entities in the question, intersection questions (What satisfies property A and property B?), and comparison questions, where two entities are compared by a common attribute, among others. In the few-document distractor setting, the QA models are given ten paragraphs in which the gold paragraphs are guaranteed to be found; in the open-domain fullwiki setting, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F1, and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F1). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously.",https://arxiv.org/abs/1910.07000,EditCC BY-SA 4.0,Text,English,,,,,,,"Reading Comprehension, Text Retrieval, Retrieval, Question Answering","retrieval-on-hotpotqa, text-retrieval-on-hotpotqa, question-answering-on-hotpotqa",,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
InfoSeek,InfoSeek Dataset,"In this project, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training.",https://production-media.paperswithcode.com/datasets/2f8d4199-ea7b-4d02-b117-7f404ac67520.jpeg,EditApache-2.0,"Image, Text",English,,,,,,,"Open-Domain Question Answering, Visual Question Answering (VQA), Retrieval","retrieval-on-infoseek, visual-question-answering-vqa-on-infoseek",,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
Natural_Questions,Natural Questions Dataset,"The Natural Questions corpus is a question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples. Each example is comprised of a google.com query and a corresponding Wikipedia page. Each Wikipedia page has a passage (or long answer) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer. The long and the short answer annotations can however be empty. If they are both empty, then there is no answer on the page at all. If the long answer annotation is non-empty, but the short answer annotation is empty, then the annotated passage answers the question but no explicit short answer could be found. Finally 1% of the documents have a passage annotated with a short answer that is “yes” or “no”, instead of a list of short spans.",https://arxiv.org/abs/1901.08634,EditCC BY-SA 3.0,Text,English,,,,,"training examples, 7,830 development examples",,"Zero-shot Text Search, Passage Retrieval, Retrieval, Open-Domain Question Answering, Question Generation, Text Retrieval, Question Answering","zero-shot-text-search-on-nq, open-domain-question-answering-on-natural, retrieval-on-natural-questions, open-domain-question-answering-on-natural-1, question-generation-on-natural-questions, text-retrieval-on-natural-questions, question-answering-on-nq-beir, question-answering-on-natural-questions, passage-retrieval-on-natural-questions, question-answering-on-natural-questions-long",,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
OK-VQA,OK-VQA Dataset,"Outside Knowledge Visual Question Answering (OK-VQA) includes more than 14,000 questions that require external knowledge to answer.",https://arxiv.org/pdf/1906.00067v2.pdf,EditUnknown,"Image, Text",English,,,,,,,"Question Generation, Visual Question Answering (VQA), Retrieval, Question Answering","retrieval-on-ok-vqa, visual-question-answering-on-ok-vqa",,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
Polyvore,Polyvore Dataset,"This dataset contains 21,889 outfits from polyvore.com, in which 17,316 are for training, 1,497 for validation and 3,076 for testing.",https://github.com/xthan/polyvore-dataset,EditUnknown,,,,,,,,,"Slot Filling, Recommendation Systems, Retrieval","retrieval-on-polyvore, slot-filling-on-polyvore, recommendation-systems-on-polyvore",,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
PopQA,PopQA Dataset,"PopQA is an open-domain QA dataset with 14k QA pairs with fine-grained Wikidata entity ID, Wikipedia page views, and relationship type information.",https://arxiv.org/pdf/2212.10511v1.pdf,EditUnknown,Text,English,,,,,,,"Knowledge Probing, Memorization, Retrieval, Question Answering",question-answering-on-popqa,,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
QAMPARI,QAMPARI Dataset,"QAMPARI is an ODQA benchmark, where question answers are lists of entities, spread across many paragraphs. It was created by (a) generating questions with multiple answers from Wikipedia's knowledge graph and tables, (b) automatically pairing answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing questions and validating each answer.",https://paperswithcode.com/paper/qampari-an-open-domain-question-answering,EditUnknown,Text,English,,,,,,,"Answer Generation, Passage Retrieval, Retrieval, Open-Domain Question Answering, Natural Questions, Question Answering",,,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
Quora_Question_Pairs,Quora Question Pairs Dataset,"Quora Question Pairs (QQP) dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other.",/paper/bilateral-multi-perspective-matching-for,EditCustom (non-commercial),Text,English,,,,,,,"Community Question Answering, Paraphrase Identification within Bi-Encoder, Retrieval, Paraphrase Generation, Natural Language Inference, QQP, Text Retrieval, Paraphrase Identification, Question Answering","qqp-on-qqp, community-question-answering-on-quora, natural-language-inference-on-quora-question, text-retrieval-on-quora-question-pairs, retrieval-on-quora-question-pairs, question-answering-on-quora-question-pairs, paraphrase-identification-on-quora-question-1, paraphrase-generation-on-quora-question-pairs-1, paraphrase-identification-within-bi-encoder, paraphrase-identification-on-quora-question",,See all 844 tasks,Retrieval25 benchmarks4912 pap,Retrieval25 benchmarks4912 pap
PGDP5K,PGDP5K Dataset,"PGDP5K is a dataset consisting of 5000 diagram samples composed of 16 shapes, covering 5 positional relations, 22 symbol types and 6 text types, labeled with more fine-grained annotations at primitive level, including primitive classes, locations and relationships, where 1,813 non-duplicated images are selected from the Geometry3K dataset and other 3,187 images are collected from three popular textbooks across grades 6-12 on mathematics curriculum websites by taking screenshots from PDF books.",https://production-media.paperswithcode.com/datasets/5752857f-98f8-4797-9a87-160a79aea19b.png,EditUnknown,"Image, Text",English,,,,187 images,,,"Multi-Task Learning, Visual Reasoning, Scene Parsing",scene-parsing-on-pgdp5k,,See all 844 tasks,Scene Parsing68 benchmarks80 p,Scene Parsing68 benchmarks80 p
Stanford_Background,Stanford Background Dataset,The Stanford Background dataset contains 715 RGB images and the corresponding label images. Images are approximately 240×320 pixels in size and pixels are classified into eight different categories,https://arxiv.org/abs/1605.01368,EditCustom,"Image, Text",English,,,,,,,"Scene Labeling, Semantic Segmentation, Scene Parsing",,,See all 844 tasks,Scene Parsing68 benchmarks80 p,Scene Parsing68 benchmarks80 p
UNDD,UNDD Dataset,"UNDD consists of 7125 unlabelled day and night images; additionally, it has 75 night images with pixel-level annotations having classes equivalent to Cityscapes dataset.",https://github.com/sauradip/night_image_semantic_segmentation,EditUnknown,"Image, Text",English,,,,,,,"Autonomous Driving, Semantic Segmentation, Scene Parsing",,,See all 844 tasks,Scene Parsing68 benchmarks80 p,Scene Parsing68 benchmarks80 p
ACL_ARC,ACL ARC Dataset,"ACL Anthology Reference Corpus (ACL ARC) is a collection of 10,920 academic papers from the ACL Anthology. ACL ARC is cleaned to remove:


files that look like not full papers, paper fragments, foreign-language papers (e.g., French), or pure junk.
headers (title and author information; NOT abstract).
footers (""References"" line and the actual references).
some bad characters (spurious characters).
some page numbers (i.e., a single number appearing on a line, with nothing else attached to it).
significant foreign-language (e.g., French) content in an otherwise English paper.

The cleaned corpus has 10,628 documents.",https://web.eecs.umich.edu/~lahiri/acl_arc.html,EditUnknown,Image,,,,,628 documents,,,"Citation Intent Classification, Sentence Classification, Continual Pretraining, Citation Recommendation","citation-intent-classification-on-acl-arc, citation-recommendation-on-acl-arc-citation, continual-pretraining-on-acl-arc, sentence-classification-on-acl-arc",,See all 844 tasks,Sentence Classification6 bench,Sentence Classification6 bench
Cards_Against_Humanity,Cards Against Humanity Dataset,"A dataset of games played in the card game ""Cards Against Humanity"" (CAH), by human players, derived from the online CAH labs. 
Each round includes the cards presented to users - a ""black"" prompt with a blank or question and 10 ""white"" punchlines as possible responses, and which punchline was picked by a player each round, along with text and metadata.

An example prompt is “TSA guidelines now prohibit ___ on airplanes”. Candidate punchlines are “Goblins”, “BATMAN!!!”, “Poor people”, and “The
right amount of cocaine”. Importantly, many cards are offensive or politically incorrect.

Used to explore human humor preferences.

Available upon request from CAH labs: mail@cardsagainsthumanity.com
Train/test splits and data processing available in the paper/code: ""Cards Against AI: Predicting Humor in a Fill-in-the-blank Party Game"": https://github.com/ddofer/CAH",https://production-media.paperswithcode.com/datasets/c62905bf-ff89-4548-90c3-2d03c2e846b4.webp,EditUnknown,"Image, Text",English,,,,,,,"Sentence Classification, text-based games, Sarcasm Detection, Emotional Intelligence, Text Reranking, Sentence-Embedding, Abusive Language, Classification, Card Games, Humor Detection, Dark Humor Detection, Board Games, Text Classification, Sentiment Analysis",,,See all 844 tasks,Sentence Classification6 bench,Sentence Classification6 bench
CSPubSum,CSPubSum Dataset,"CSPubSum is a dataset for summarisation of computer science publications, created by exploiting a large resource of author provided summaries and show straightforward ways of extending it further.",/paper/a-supervised-approach-to-extractive,EditUnknown,Image,,,,,,,,Sentence Classification,,,See all 844 tasks,Sentence Classification6 bench,Sentence Classification6 bench
DEFT_Corpus,DEFT Corpus Dataset,A SemEval shared task in which participants must extract definitions from free text using a term-definition pair corpus that reflects the complex reality of definitions in natural language.,/paper/semeval-2020-task-6-definition-extraction,EditUnknown,"Graph, Image",,,,,,,,"Relation Extraction, Definition Extraction, Sentence Classification",,,See all 844 tasks,Sentence Classification6 bench,Sentence Classification6 bench
IndoNLU_Benchmark,IndoNLU Benchmark Dataset,"The IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for Bahasa Indonesia. It is a joint venture from many Indonesia NLP enthusiasts from different institutions such as Gojek, Institut Teknologi Bandung, HKUST, Universitas Multimedia Nusantara, Prosa.ai, and Universitas Indonesia.",/paper/indonlu-benchmark-and-resources-for,EditUnknown,"Image, Text",English,,,,,,,"Language Modelling, Sentence Classification, Natural Language Understanding",,,See all 844 tasks,Sentence Classification6 bench,Sentence Classification6 bench
Paper_Field,Paper Field Dataset,"Paper Field is built from the Microsoft Academic Graph and maps paper titles to one of 7 fields of study. Each field of study - geography, politics, economics, business, sociology, medicine, and psychology - has approximately 12K training examples.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Sentence Classification, Text Classification",sentence-classification-on-paper-field,,See all 844 tasks,Sentence Classification6 bench,Sentence Classification6 bench
PcMSP,PcMSP Dataset,"PcMSP is a dataset annotated  from 305 open access scientific articles for material science information extraction that simultaneously contains the synthesis sentences extracted from the experimental paragraphs, as well as the entity mentions and intra-sentence relations.",https://arxiv.org/pdf/2210.12401v1.pdf,EditMIT License,"Graph, Image, Text",English,,,,,,,"Sentence Classification, Relation Classification, Named Entity Recognition (NER)",,,See all 844 tasks,Sentence Classification6 bench,Sentence Classification6 bench
PubMed_RCT,PubMed RCT Dataset,"PubMed 200k RCT is new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: the authors hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field.",https://github.com/Franck-Dernoncourt/pubmed-rct,EditUnknown,"Image, Text",English,,,,,,,"Sentence Classification, Text Classification",,,See all 844 tasks,Sentence Classification6 bench,Sentence Classification6 bench
SciCite,SciCite Dataset,SciCite is a dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL-ARC.,https://arxiv.org/abs/1904.01608,EditUnknown,Image,,,,,,,,"Citation Intent Classification, Sentence Classification","citation-intent-classification-on-scicite, sentence-classification-on-sciencecite, sentence-classification-on-scicite",,See all 844 tasks,Sentence Classification6 bench,Sentence Classification6 bench
AuxAD,AuxAD Dataset,AuxAD is a a distantly supervised dataset for acronym disambiguation.,https://github.com/PrimerAI/sdu-data,EditUnknown,,,,,,,,,Sentence Embeddings,,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
AuxAI,AuxAI Dataset,AuxAI is a distantly supervised dataset for acronym identification.,https://github.com/PrimerAI/sdu-data,EditUnknown,,,,,,,,,Sentence Embeddings,,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
COSTRA_1.0,COSTRA 1.0 Dataset,"COSTRA 1.0 is a dataset of complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing. The first version of the dataset is limited to sentences in Czech but the construction method is universal and the authors plan to use it also for other languages. The dataset consist of 4,262 unique sentences with average length of 10 words, illustrating 15 types of modifications such as simplification, generalization, or formal and informal language variation.",/paper/costra-10-a-dataset-of-complex-sentence,EditUnknown,,,,,,,,,"Sentence Embedding, Sentence Embeddings",,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
Discovery,Discovery Dataset,"The Discovery datasets consists of adjacent sentence pairs (s1,s2) with a discourse marker (y) that occurred at the beginning of s2. They were extracted from the depcc web corpus.

Markers prediction can be used in order to train a sentence encoders. Discourse markers can be considered as noisy labels for various semantic tasks, such as entailment (y=therefore), subjectivity analysis (y=personally) or sentiment analysis (y=sadly), similarity (y=similarly), typicality, (y=curiously) ...

The specificity of this dataset is the diversity of the markers, since previously used data used only ~10 imbalanced classes. The author of the dataset provide:


a list of the 174 discourse markers
a Base version of the dataset with 1.74 million pairs (10k examples per marker)
a Big version with 3.4 million pairs
a Hard version with 1.74 million pairs where the connective couldn't be predicted with a fastText linear model",https://github.com/synapse-developpement/Discovery,EditApache 2.0,"Graph, Image",,,,,10k examples,,,"Sentence Embeddings, Relation Classification",relation-classification-on-discovery-dataset,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
French_CASS_dataset,French CASS dataset Dataset,Composed of judgments from the French Court of cassation and their corresponding summaries.,/paper/strass-a-light-and-effective-method-for,EditUnknown,Text,English,,,,,,,"Sentence Embedding, Sentence Embeddings, Document Embedding",,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
GeoCoV19,GeoCoV19 Dataset,"GeoCoV19 is a large-scale Twitter dataset containing more than 524 million multilingual tweets. The dataset contains around 378K geotagged tweets and 5.4 million tweets with Place information. The annotations include toponyms from the user location field and tweet content and resolve them to geolocations such as country, state, or city level. In this case, 297 million tweets are annotated with geolocation using the user location field and 452 million tweets using tweet content.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Misinformation, Sentence Embeddings, Sentiment Analysis",,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
PIT,PIT Dataset,"Paraphrase and Semantic Similarity in Twitter (PIT) presents a constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs.",https://www.aclweb.org/anthology/S15-2001.pdf,EditUnknown,,,,,,,,,"Sentence Embeddings, Paraphrase Identification, Semantic Textual Similarity",paraphrase-identification-on-pit,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
ScanRefer_Dataset,ScanRefer Dataset Dataset,"Contains 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.",/paper/scanrefer-3d-object-localization-in-rgb-d,EditUnknown,Image,,,,,,,,"Object Localization, Object Detection, Sentence Embeddings",,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
SemEval-2014_Task-10,SemEval-2014 Task-10 Dataset,"SemEval 2014 is a collection of datasets used for the Semantic Evaluation (SemEval) workshop, an annual event that focuses on the evaluation and comparison of systems that can analyze diverse semantic phenomena in text. The datasets from SemEval 2014 are used for various tasks, including but not limited to:


Aspect-Based Sentiment Analysis (ABSA): This task is based on laptop and restaurant reviews. It involves identifying the aspects or features mentioned in a review and determining the sentiment expressed towards each aspect.
Text Classification: This task involves classifying text into predefined categories. Sub-tasks include text-scoring, natural language inference, and semantic-similarity-scoring.",https://production-media.paperswithcode.com/datasets/STS_2014-0000003579-a222dcdb.jpg,EditUnknown,,,2014,,,,,,"Word Embeddings, Sentence Embeddings, Semantic Textual Similarity",,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
WikiMatrix,WikiMatrix Dataset,"WikiMatrix is a dataset of parallel sentences in the textual content of Wikipedia for all possible language pairs. The mined data consists of:


85 different languages, 1620 language pairs
134M parallel sentences, out of which 34M are aligned with English",/paper/wikimatrix-mining-135m-parallel-sentences-in,EditCC BY-SA 4.0,,,,,,,,,"Word Embeddings, Sentence Embeddings",,,See all 844 tasks,Sentence Embeddings3 benchmark,Sentence Embeddings3 benchmark
Sentence_Pair_Modeling26_benchmarks6_papers_with_c,Sentence Pair Modeling26 benchmarks6 papers with code Dataset,,https://paperswithcode.com/dataset/sentence-pair-modeling,,,,,,,,,,,,,See all 844 tasks,Sentence Pair Modeling26 bench,Sentence Pair Modeling26 bench
ASTD,ASTD Dataset,"Arabic Sentiment Tweets Dataset (ASTD) is an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classified as objective, subjective positive, subjective negative, and subjective mixed.",/paper/astd-arabic-sentiment-tweets-dataset,EditGPL-2.0,Text,English,,,,,,,Sentiment Analysis,sentiment-analysis-on-astd,,See all 844 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
Multi-Domain_Sentiment,Multi-Domain Sentiment Dataset,The Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains). Some domains (books and dvds) have hundreds of thousands of reviews. Others (musical instruments) have only a few hundred. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed.,https://www.cs.jhu.edu/~mdredze/datasets/sentiment/,EditUnknown,Text,English,,,,,,,Sentiment Analysis,sentiment-analysis-on-multi-domain-sentiment,,See all 844 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
SST-5,SST-5 Dataset,"The SST-5, also known as the Stanford Sentiment Treebank with 5 labels, is a dataset used for sentiment analysis. The SST-5 dataset consists of 11,855 single sentences extracted from movie reviews¹. It includes a total of 215,154 unique phrases from parse trees, each annotated by 3 human judges¹. Each phrase is labeled as either negative, somewhat negative, neutral, somewhat positive, or positive. This is why it's referred to as SST-5 or SST fine-grained.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,5,"Few-Shot Text Classification, Explanation Fidelity Evaluation, Sentiment Analysis","explanation-fidelity-evaluation-on-sst-5, sentiment-analysis-on-sst-5-fine-grained, few-shot-text-classification-on-sst-5",,See all 844 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
TweetEval,TweetEval Dataset,TweetEval introduces an evaluation framework consisting of seven heterogeneous Twitter-specific classification tasks.,/paper/tweeteval-unified-benchmark-and-comparative,EditUnknown,Text,English,,,,,,,"Language Modelling, Sentiment Analysis",sentiment-analysis-on-tweeteval,,See all 844 tasks,Sentiment Analysis95 benchmark,Sentiment Analysis95 benchmark
Creative_Writing,Creative Writing Dataset,"A creative writing task where the input is 4 random sentences and the output should be a coherent passage with 4 paragraphs that end in the 4 input sentences respectively. Such a task is open-ended and exploratory, and challenges creative thinking as well as high-level planning.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,4 paragraphs,,,Story Generation,,,See all 844 tasks,Story Generation8 benchmarks10,Story Generation8 benchmarks10
HANNA,HANNA Dataset,"HANNA, a large annotated dataset of Human-ANnotated NArratives for Automatic Story Generation (ASG) evaluation, has been designed for the benchmarking of automatic metrics for ASG. HANNA contains 1,056 stories generated from 96 prompts from the WritingPrompts dataset. Each prompt is linked to a human story and to 10 stories generated by different ASG systems. Each story was annotated on six human criteria (Relevance, Coherence, Empathy, Surprise, Engagement and Complexity) by three raters. HANNA also contains the scores produced by 72 automatic metrics on each story.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditMIT,Text,English,,,,,,,Story Generation,,,See all 844 tasks,Story Generation8 benchmarks10,Story Generation8 benchmarks10
MoviePlotEvents,MoviePlotEvents Dataset,"A version of the CMU Movie Summary Corpus (http://www.cs.cmu.edu/~ark/personas/), which was originally scraped from plot summaries from Wikipedia, with some cleaning and sentences turned into events & sorted into ""genres"" (via LDA).",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,Story Generation,,,See all 844 tasks,Story Generation8 benchmarks10,Story Generation8 benchmarks10
Scifi_TV_Shows,Scifi TV Shows Dataset,"A collection of long-running (80+ episodes) science fiction TV show synopses, scraped from Fandom.com wikis. Collected Nov 2017. Each episode is considered a ""story"".

Contains plot summaries from :


Babylon 5 (https://babylon5.fandom.com/wiki/Main_Page) - 84 stories
Doctor Who (https://tardis.fandom.com/wiki/Doctor_Who_Wiki) - 311 stories
Doctor Who spin-offs - 95 stories
Farscape (https://farscape.fandom.com/wiki/Farscape_Encyclopedia_Project:Main_Page) - 90 stories
Fringe (https://fringe.fandom.com/wiki/FringeWiki) - 87 stories
Futurama (https://futurama.fandom.com/wiki/Futurama_Wiki) - 87 stories
Stargate (https://stargate.fandom.com/wiki/Stargate_Wiki) - 351 stories
Star Trek (https://memory-alpha.fandom.com/wiki/Star_Trek) - 701 stories
Star Wars books (https://starwars.fandom.com/wiki/Main_Page) - 205 stories
Star Wars Rebels - 65 stories
X-Files (https://x-files.fandom.com/wiki/Main_Page) - 200 stories

Total: 2276 stories

Dataset is ""eventified"" and generalized (see LJ Martin, P Ammanabrolu, X Wang, W Hancock, S Singh, B Harrison, and MO Riedl. Event Representations for Automated Story Generation with Deep Neural Nets, Thirty-Second AAAI Conference on Artificial Intelligence (AAAI), 2018. for details on these processes.) and split into train-test-validation sets for converting events into full sentences.",https://huggingface.co/datasets/lara-martin/Scifi_TV_Shows,EditCreative Commons Attribution 4.0 International License,Text,English,2017,,,,,,"Story Generation, Event Expansion, Story Completion",event-expansion-on-scifi-tv-shows,,See all 844 tasks,Story Generation8 benchmarks10,Story Generation8 benchmarks10
TVRecap,TVRecap Dataset,"TVRecap a story generation dataset that requires generating detailed TV show episode recaps from a brief summary and a set of documents describing the characters involved. Unlike other story generation datasets, TVRecap contains stories that are authored by professional screenwriters and that feature complex interactions among multiple characters. Generating stories in TVRecap requires drawing relevant information from the lengthy provided documents about characters based on the brief summary. In addition, by swapping the input and output, TVRecap can serve as a challenging testbed for abstractive summarization.",https://production-media.paperswithcode.com/datasets/ca603ff2-2dc5-49cc-af70-dd9d6c2f774f.png,EditUnknown,Text,English,,,,,,,Story Generation,"story-generation-on-tvmegasite-test, story-generation-on-fandom-test, story-generation-on-fandom-dev, story-generation-on-tvmegasite-dev",,See all 844 tasks,Story Generation8 benchmarks10,Story Generation8 benchmarks10
Visual_Writing_Prompts,Visual Writing Prompts Dataset,"Hugging Face Datasets (New!)  | Website | Github Repository | arXiv e-Print

<!-- Provide a quick summary of the dataset. -->


The Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of
movie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.

Dataset Details
Links
<!-- Provide the basic links for the dataset. -->



TACL 2023 Paper: Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences

Dataset Description
<!-- Provide a longer summary of what this dataset is. -->


The Visual Writing Prompts (VWP) dataset is designed to facilitate the development and testing of natural language processing models that generate stories based on sequences of images. This dataset comprises nearly 2,000 curated sequences of movie shots, each sequence containing between 5 to 10 images. These images are meticulously selected to ensure they depict coherent plots centered around one or more main characters, enhancing the visual narrative structure for story generation. Aligned with these image sequences are approximately 12,000 stories, which were written by crowd workers using Amazon Mechanical Turk. This setup aims to provide a rich, visually grounded storytelling context that helps models generate more coherent, diverse, and engaging stories.


Curated by: Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele
Funded by: See Acknowledgments in our paper
Language(s) (NLP): English
License: Apache License 2.0

Dataset Structure
<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->


The dataset is in a CSV file. The explanation of each column is in this table.

Uses
<!-- Address questions about how the dataset is intended to be used. -->


Direct Use
<!-- This section describes suitable use cases for the dataset. -->


The dataset is intended for use in natural language processing tasks, particularly for the development and evaluation of models designed to generate coherent and visually grounded stories from sequences of images.

Out-of-Scope Use
<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->


The copyrights of all movie shots belong to the original copyright holders which can be found in the IMDb page of each movie. The IMDb page is indicated by the index in the imdb_id column. For example, for the first row of our data, the imdb_id is tt0112573 so the corresponding imdb page is https://www.imdb.com/title/tt0112573/companycredits/. Do not violate the copyrights while using these images. The usage of these images is limited to academic purposes.

Dataset Creation
Curation Rationale
<!-- Motivation for the creation of this dataset. -->


The dataset was curated to improve the quality of text stories generated from image sequences, focusing on visual storytelling with coherent plots and character grounding.

Source Data
<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->


Data Collection and Processing
<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->


The source data consists of image sequences extracted from the movie shots from the MovieNet dataset (https://opendatalab.com/OpenDataLab/MovieNet/tree/main/raw), ensuring a coherent plot around one or more main characters.

Who are the source data producers?
<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->


The images were initially produced by movie production companies and extracted by authors of MovieNet. The stories are written by crowd workers. Then the stories are compiled and refined by the authors.

Annotations
<!-- If the dataset contains annotations that are not part of the initial data collection, use this section to describe them. -->


Annotation process
<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->


Crowdworkers were asked to write stories that fit the provided image sequences. The annotation process included reviewing these stories for coherence, grammatical correctness, and alignment with the images. More details are in our paper.

Who are the annotators?
<!-- This section describes the people or systems who created the annotations. -->


The annotators were five graduate students from Saarland University. Two are native English speakers. The other three are proficient in English.

Personal and Sensitive Information
<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->


We do not collect personal or sensitive information. Personal information like worker IDs are not released. Our anonymization process is described in our paper.

Bias, Risks, and Limitations
<!-- This section is meant to convey both technical and sociotechnical limitations. -->


The stories in this dataset are in English only. Although we have tried our best to filter the images and review the stories, it is not possible to go through all the stories. There could still be biased or harmful content. Please use the dataset carefully.

Citation
Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, and Bernt Schiele. 2023. Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences. Transactions of the Association for Computational Linguistics, 11:565–581.

BibTeX:

latex
@article{10.1162/tacl_a_00553,
author = {Hong, Xudong and Sayeed, Asad and Mehra, Khushboo and Demberg, Vera and Schiele, Bernt},
title = ""{Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences}"",
journal = {Transactions of the Association for Computational Linguistics},
volume = {11},
pages = {565-581},
year = {2023},
month = {06},
issn = {2307-387X},
doi = {10.1162/tacl_a_00553},
url = {[https://doi.org/10.1162/tacl\\\\_a\\\\_00553](https://doi.org/10.1162/tacl%5C%5C%5C%5C_a%5C%5C%5C%5C_00553)},
eprint = {[https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\\\_a\\\\_00553/2134487/tacl\\\\_a\\\\_00553.pdf](https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl%5C%5C%5C%5C_a%5C%5C%5C%5C_00553/2134487/tacl%5C%5C%5C%5C_a%5C%5C%5C%5C_00553.pdf)},
}

Dataset Card Authors
Xudong Hong

Dataset Card Contact
xLASTNAME@coli.uni-saarland.de

Disclaimer:
All the images are extracted from the movie shots from the MovieNet dataset (https://opendatalab.com/OpenDataLab/MovieNet/tree/main/raw). The copyrights of all movie shots belong to the original copyright holders which can be found in the IMDb page of each movie. The IMDb page is indicated by the index in the imdb_id column. For example, for the first row of our data, the imdb_id is tt0112573 so the corresponding imdb page is https://www.imdb.com/title/tt0112573/companycredits/. Do not violate the copyrights while using these images. We only use these images for academic purposes. Please contact the author if you have any questions.",https://huggingface.co/datasets/tonyhong/vwp,EditApache License 2.0,"Image, Text",English,2023,,,10 images,"testing of natural language processing models that generate stories based on sequences of images. This dataset comprises nearly 2,000 curated sequences of movie shots, each sequence containing between 5 to 10 images",,"Text Generation, Story Generation, Visual Storytelling",,,See all 844 tasks,Story Generation8 benchmarks10,Story Generation8 benchmarks10
WritingPrompts,WritingPrompts Dataset,WritingPrompts is a large dataset of 300K human-written stories paired with writing prompts from an online forum.,https://arxiv.org/pdf/1805.04833v1.pdf,EditMIT,Text,English,,,,,,,"Text Generation, Language Modelling, Natural Language Understanding, Story Generation",story-generation-on-writingprompts,,See all 844 tasks,Story Generation8 benchmarks10,Story Generation8 benchmarks10
DukeMTMC-reID,DukeMTMC-reID Dataset,"The DukeMTMC-reID (Duke Multi-Tracking Multi-Camera ReIDentification) dataset is a subset of the DukeMTMC for image-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian image datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images.

NOTE: This dataset has been retracted.",https://arxiv.org/abs/1804.11027,EditUnknown,Image,,,,,,"training images of 702 identities, 2,228 query images",,"Unsupervised Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification, Style Transfer",,,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
GYAFC,GYAFC Dataset,"Grammarly’s Yahoo Answers Formality Corpus (GYAFC) is the largest dataset for any style containing a total of 110K informal / formal sentence pairs.

Yahoo Answers is a question answering forum, contains a large number of informal sentences and allows redistribution of data. The authors used the Yahoo Answers L6 corpus to create the GYAFC dataset of informal and formal sentence pairs. In order to ensure a uniform distribution of data, they removed sentences that are questions, contain URLs, and are shorter than 5 words or longer than 25. After these preprocessing steps, 40 million sentences remain. 

The Yahoo Answers corpus consists of several different domains like Business, Entertainment & Music, Travel, Food, etc. Pavlick and Tetreault formality classifier (PT16) shows that the formality level varies significantly
across different genres. In order to control for this variation, the authors work with two specific domains that contain the most informal sentences and show results on training and testing within those categories. The authors use the formality classifier from PT16 to identify informal sentences and train this classifier on the Answers genre of the PT16 corpus
which consists of nearly 5,000 randomly selected sentences from Yahoo Answers manually annotated on a scale of -3 (very informal) to 3 (very formal). They find that the domains of Entertainment & Music and Family & Relationships contain the most informal sentences and create the GYAFC dataset using these domains.",https://arxiv.org/pdf/1803.06535v2.pdf,EditCustom (research-only),Text,English,,,,,,,"Unsupervised Text Style Transfer, Formality Style Transfer, Style Transfer","formality-style-transfer-on-gyafc, unsupervised-text-style-transfer-on-gyafc, style-transfer-on-gyafc",,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
iKala,iKala Dataset,"The iKala dataset is a singing voice separation dataset that comprises of 252 30-second excerpts sampled from 206 iKala songs (plus 100 hidden excerpts reserved for MIREX data mining contest). The music accompaniment and the singing voice are recorded at the left and right channels respectively. Additionally, the human-labeled pitch contours and timestamped lyrics are provided.

This dataset is not available anymore.


""T"" as a sofa:

The ""T"" horizontal strip can mimic the back of a sofa with a delicate cushion or details of the uphols or appliances with the color button.

The ""T"" vertical strip can show a feet or arm of the sofa, shiny, yet firm.


Merge ""P"":

Put ""P"" next to ""T"", your curve to delicately with the top ""T."" It is intertwined. The circular part of ""P"" can show a cushion or a curved chair and synchronize the subject of furniture.

Make sure ""P"" is visually relying on ""T"", which reflects the relationship of cohesion and balance.


Coherence of ""B"" and ""I"":

""B"" can be aligned as a pair of cushions or a modern chair, with mild curves with glossy and modern aesthetics.

""I"" can be a symbol of a shiny furniture or a vertical light bar and completes the shapes without overburdess them.

Color palette 4:

Includes soft soil colors such as beige, top and gray shades, along with silent or silver gold tips to touch elegance.

Consider a slope effect to enhance modernity, to keep colors elegant and complex.


Connect the letters:

Use the overlap or intertwined edges that the letters meet for the symbol of unity.

The plan should allow viewers to distinguish each letter while feeling part of the same ""structure"".


Background patterns:

Use delicate geometric patterns or textures that mimic fabrics or furniture materials such as wood seeds or woven fibers.

These patterns must remain minimalist and focus on highlighting the logo, while maintaining communication.

While it deals with the subject of furniture and design, this concept conveys modernity, creativity and professional. If you like, I can create a draft design for better visualization.",http://mac.citi.sinica.edu.tw/ikala/,EditUnknown,Audio,,,,,,,,"Speech Separation, Style Transfer, Music Information Retrieval, Information Retrieval",speech-separation-on-ikala,,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
LaMem,LaMem Dataset,"An annotated image memorability dataset to date (with 60,000 labeled images from a diverse array of sources).",/paper/understanding-and-predicting-image,EditUnknown,"Image, Text",English,,,,,,,"Image Classification, Image-to-Image Translation, Style Transfer",,,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
MPIIGaze,MPIIGaze Dataset,"MPIIGaze is a dataset for appearance-based gaze estimation in the wild. It contains 213,659 images collected from 15 participants during natural everyday laptop use over more than three months. It has a large variability in appearance and illumination.",https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild,EditCC BY-NC-SA 4.0,Image,English,,,,659 images,,,"Style Transfer, Gaze Estimation","gaze-estimation-on-mpii-gaze, gaze-estimation-on-mpiigaze-1",,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
MPI_Sintel,MPI Sintel Dataset,MPI (Max Planck Institute) Sintel is a dataset for optical flow evaluation that has 1064 synthesized stereo images and ground truth data for disparity. Sintel is derived from open-source 3D animated short film Sintel. The dataset has 23 different scenes. The stereo images are RGB while the disparity is grayscale. Both have resolution of 1024×436 pixels and 8-bit per channel.,https://arxiv.org/abs/1805.07499,EditUnknown,"Image, Time Series, Video",,,,,,valuation that has 1064 synthesized stereo images,,"Optical Flow Estimation, Video Prediction, Intrinsic Image Decomposition, Style Transfer, Temporal View Synthesis","optical-flow-estimation-on-sintel-final-2, optical-flow-estimation-on-sintel-final, video-prediction-on-mpi-sintel, optical-flow-estimation-on-sintel-clean-2, temporal-view-synthesis-on-mpi-sintel, optical-flow-estimation-on-sintel-clean",,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
POP909,POP909 Dataset,"POP909 is a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, annotations are provided of tempo, beat, key, and chords, where the tempo curves are hand-labelled and others are done by MIR algorithms.",https://arxiv.org/pdf/2008.07142.pdf,EditUnknown,"Audio, Text",English,,,,,,,"Music Generation, Style Transfer",,,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
TextSeg,TextSeg Dataset,"TextSeg is a large-scale fine-annotated and multi-purpose text detection and segmentation dataset, collecting scene and design text with six types of annotations: word- and character-wise bounding polygons, masks and transcriptions.",https://github.com/SHI-Labs/Rethinking-Text-Segmentation,EditUnknown,"Image, Text",English,,,,,,,"Text Style Transfer, Style Transfer, self-supervised scene text recognition",self-supervised-scene-text-recognition-on-1,,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
Touchdown_Dataset,Touchdown Dataset Dataset,"Touchdown is a corpus for executing navigation instructions and resolving spatial descriptions in visual real-world environments. The task is to follow instruction to a goal position and there find a hidden object, Touchdown the bear.",https://github.com/lil-lab/touchdown,EditUnknown,Text,English,,,,,,,"Text Style Transfer, Style Transfer, Vision and Language Navigation",vision-and-language-navigation-on-touchdown,,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
WikiArt,WikiArt Dataset,WikiArt contains painting from 195 different artists. The dataset has 42129 images for training and 10628 images for testing.,https://arxiv.org/abs/1805.11119,EditCustom (non-commercial),,,,,,42129 images,training and 10628 images,,"Continual Learning, Style Transfer","style-transfer-on-wikiart, continual-learning-on-wikiart-fine-grained-6",,See all 844 tasks,Style Transfer3 benchmarks741 ,Style Transfer3 benchmarks741 
CompMix-IR,CompMix-IR Dataset,"CompMix-IR Dataset Overview:

Characteristics: CompMix-IR is a heterogeneous knowledge retrieval benchmark dataset, featuring four knowledge types (text, knowledge graphs, tables, and infoboxes), 9,400+ QA pairs, and a corpus of 10 million entries. It supports two retrieval scenarios: retrieving across all knowledge types or retrieving specific types based on user instructions.

Motivation: It addresses the limitations of existing benchmarks by providing a more comprehensive and realistic dataset that reflects real-world retrieval needs with diverse knowledge sources and user intents.

Potential Use Cases: Ideal for developing and evaluating heterogeneous IR models, instruction-aware retrieval systems, and open-domain QA systems. It can also be used for benchmarking, cross-domain IR research, and enhancing the adaptability and robustness of retrieval models.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Editcc-by-4.0,"Tabular, Text",English,,,,,,,"Table Retrieval, RAG, Knowledge Graphs, Information Retrieval, Text Retrieval, Question Answering",,,See all 844 tasks,Table Retrieval1 benchmark14 p,Table Retrieval1 benchmark14 p
Statcan_Dialogue_Dataset,Statcan Dialogue Dataset Dataset,"The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents

Xing Han Lu, Siva Reddy, Harm de Vries

EACL 2023


| | | | | |
| :--: | :--: | :--: | :--: | :--: |
| Code | Huggingface | Request on Dataverse | Paper | Website |",https://production-media.paperswithcode.com/datasets/b40d683a-08bd-4420-80ce-a40176c36c5e.png,EditCustom,"Tabular, Text",English,2023,,,,,,"Dialogue Generation, Table Retrieval, Retrieval",table-retrieval-on-statcan-dialogue-dataset,,See all 844 tasks,Table Retrieval1 benchmark14 p,Table Retrieval1 benchmark14 p
Conceptual_Captions,Conceptual Captions Dataset,"Automatic image captioning is the task of producing a natural-language utterance (usually a sentence) that correctly reflects the visual content of an image. Up to this point, the resource most used for this task was the MS-COCO dataset, containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).

Google's Conceptual Captions dataset has more than 3 million images, paired with natural-language captions. In contrast with the curated style of the MS-COCO images, Conceptual Captions images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. The raw descriptions are harvested from the Alt-text HTML attribute associated with web images. The authors developed an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.",https://github.com/google-research-datasets/conceptual-captions,EditCustom,"Image, Text",English,,,,000 images,,,"Text-to-Image Generation, Visual Question Answering (VQA), Image Captioning, Question Answering","image-captioning-on-conceptual-captions, text-to-image-generation-on-conceptual",,See all 844 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Fashion-Gen,Fashion-Gen Dataset,"Fashion-Gen consists of 293,008 high definition (1360 x 1360 pixels) fashion images paired with item descriptions provided by professional stylists. Each item is photographed from a variety of angles.",https://arxiv.org/pdf/1806.08317v2.pdf,EditCustom,"Image, Text",English,,,,,,,"Image Inpainting, Image Generation, Text-to-Image Generation, Image Retrieval",,,See all 844 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Human-Art,Human-Art Dataset,"Human-Art is a versatile human-centric dataset to bridge the gap between natural and artificial scenes. It includes twenty high-quality human scenes, including natural and artificial humans in both 2D representation and 3D representation. It includes 50,000 images including more than 123,000 human figures in 20 scenarios, with annotations of human bounding box, 21 2D human keypoints, human self-contact keypoints, and description text.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,Edithttps://docs.google.com/document/d/19j-6GFOCYBDU4CxwRSKgORndse_j5iHGK0RCJ2TvXNQ/edit?usp=sharing,"3D, Image, Text",English,,,,000 images,,,"Conditional Image Generation, Text-to-Image Generation, 2D Human Pose Estimation, 3D Human Pose Estimation",2d-human-pose-estimation-on-human-art,,See all 844 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
LHQ,LHQ Dataset,"A dataset of 90,000 high-resolution nature landscape images, crawled from Unsplash and Flickr and preprocessed with Mask R-CNN and Inception V3.",https://production-media.paperswithcode.com/datasets/lhq.jpg,EditCC BY 2.0,"Image, Text",English,,,,,,,"Text-to-Image Generation, Infinite Image Generation, Image Generation, Perpetual View Generation, Image Outpainting","perpetual-view-generation-on-lhq, text-to-image-generation-on-lhqc, image-outpainting-on-lhqc, infinite-image-generation-on-lhq",,See all 844 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Multi-Modal_CelebA-HQ,Multi-Modal CelebA-HQ Dataset,"Multi-Modal-CelebA-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has high-quality segmentation mask, sketch, descriptive text, and image with transparent background.

Multi-Modal-CelebA-HQ can be used to train and evaluate algorithms of text-to-image-generation, text-guided image manipulation, sketch-to-image generation, and GANs for face generation and editing.",https://github.com/weihaox/Multi-Modal-CelebA-HQ-Dataset,EditUnknown,"Image, Text",English,,,,,,,"Text-to-Image Generation, Image Generation, Face Sketch Synthesis, multimodal generation","face-sketch-synthesis-on-multi-modal-celeba, text-to-image-generation-on-multi-modal, multimodal-generation-on-multi-modal-celeba",,See all 844 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Pick-a-Pic,Pick-a-Pic Dataset,"Pick-a-Pic dataset was created by logging user interactions with the Pick-a-Pic web application for text-to image generation. Overall, the Pick-a-Pic dataset contains over 500,000 examples and 35,000 distinct prompts. Each example contains a prompt, two generated images, and a label for which image is preferred, or if there is a tie when no image is significantly preferred over the other.",https://arxiv.org/pdf/2305.01569v1.pdf,EditUnknown,"Image, Text",English,,,,000 examples,,,Text-to-Image Generation,,,See all 844 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
T2I-CompBench,T2I-CompBench Dataset,"T2I-CompBench is a comprehensive benchmark for open-world compositional text-to-image generation, consisting of 6,000 compositional textual prompts from 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions).",https://production-media.paperswithcode.com/datasets/6947ca2f-be02-4895-ac5e-32b045d0ec0a.jpeg,EditMIT License,"Image, Text",English,,,,,,3,Text-to-Image Generation,,,See all 844 tasks,Text-to-Image Generation16 ben,Text-to-Image Generation16 ben
Text_Augmentation40_papers_with_code_Dataset,Text Augmentation40 papers with code Dataset,,https://paperswithcode.com/dataset/text-augmentation,,,,,,,,,,,,,See all 844 tasks,Text Augmentation40 papers wit,Text Augmentation40 papers wit
AG_News,AG News Dataset,"AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.",https://arxiv.org/pdf/1509.01626.pdf,EditCustom (non-commercial),"Image, Text",English,,,,,"training and 1,900 test samples",,"Zero-Shot Text Classification, Topic Models, Short Text Clustering, Stochastic Optimization, Continual Pretraining, Semi-Supervised Text Classification, Anomaly Detection, Unsupervised Text Classification, Text Classification","semi-supervised-text-classification-on-ag-1, unsupervised-text-classification-on-ag-news, text-classification-on-ag-news, zero-shot-text-classification-on-ag-news, stochastic-optimization-on-ag-news, short-text-clustering-on-ag-news, topic-models-on-ag-news, anomaly-detection-on-ag-news, continual-pretraining-on-ag-news",,See all 844 tasks,Text Classification172 benchma,Text Classification172 benchma
CoNLL_2003,CoNLL 2003 Dataset,"CoNLL-2003 is a named entity recognition dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition.
The data consists of eight files covering two languages: English and German.
For each of the languages there is a training file, a development file, a test file and a large file with unannotated data.

The English data was taken from the Reuters Corpus. This corpus consists of Reuters news stories between August 1996 and August 1997.
For the training and development set, ten days worth of data were taken from the files representing the end of August 1996.
For the test set, the texts were from December 1996. The preprocessed raw data covers the month of September 1996.

The text for the German data was taken from the ECI Multilingual Text Corpus. This corpus consists of texts in many languages. The portion of data that
was used for this task, was extracted from the German newspaper Frankfurter Rundshau. All three of the training, development and test sets were taken
from articles written in one week at the end of August 1992.
The raw data were taken from the months of September to December 1992.

| English      data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |
|-------------------|----------|-----------|---------|------|------|------|------|
| Training     set  | 946      | 14,987    | 203,621 | 7140 | 3438 | 6321 | 6600 |
| Development  set  | 216      | 3,466     | 51,362  | 1837 | 922  | 1341 | 1842 |
| Test         set  | 231      | 3,684     | 46,435  | 1668 | 702  | 1661 | 1617 |

Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in English data files.

| German       data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |
|-------------------|----------|-----------|---------|------|------|------|------|
| Training     set  | 553      | 12,705    | 206,931 | 4363 | 2288 | 2427 | 2773 |
| Development  set  | 201      | 3,068     | 51,444  | 1181 | 1010 | 1241 | 1401 |
| Test         set  | 155      | 3,160     | 51,943  | 1035 | 670  | 773  | 1195 |

Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in German data files.",https://huggingface.co/datasets/conll2003,EditUnknown,"Image, Text",English,2003,,,,,,"FG-1-PG-1, Weakly-Supervised Named Entity Recognition, Named Entity Recognition (NER), Chunking, Token Classification, Cross-Lingual NER, Semantic Similarity, Low Resource Named Entity Recognition, Knowledge Distillation, POS, UIE, Text Classification, Named Entity Recognition, NER","chunking-on-conll-2003-english, text-classification-on-unknown, fg-1-pg-1-on-conll2003, named-entity-recognition-on-conll-2003-german-1, semantic-similarity-on-unknown, named-entity-recognition-on-conll03, named-entity-recognition-on-conll-2003-german, uie-on-conll-2003, ner-on-conll-2003-1, named-entity-recognition-ner-on-conll-2003, cross-lingual-ner-on-conll-2003, token-classification-on-conll2003, low-resource-named-entity-recognition-on-4, knowledge-distillation-on-unknown, named-entity-recognition-on-conll-2003-3, weakly-supervised-named-entity-recognition-on, chunking-on-conll-2003-german, pos-on-conll-2003, chunking-on-conll-2003",,See all 844 tasks,Text Classification172 benchma,Text Classification172 benchma
DBpedia,DBpedia Dataset,"DBpedia (from ""DB"" for ""database"") is a project aiming to extract structured content from the information created in the Wikipedia project. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.",https://en.wikipedia.org/wiki/DBpedia,EditCC BY-SA 3.0,"Image, Text",English,,,,,,,"Text Retrieval, Text Classification, Zero-shot Text Search, Open Intent Discovery","text-classification-on-dbpedia, text-retrieval-on-dbpedia, zero-shot-text-search-on-dbpedia, open-intent-discovery-on-dbpedia",,See all 844 tasks,Text Classification172 benchma,Text Classification172 benchma
OpenWebText,OpenWebText Dataset,OpenWebText is an open-source recreation of the WebText corpus. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).,https://arxiv.org/abs/1907.11692,EditCustom,"Image, Text",English,,,,,,,"Text Generation, Language Modelling, Text Classification","text-generation-on-openwebtext, language-modelling-on-openwebtext",,See all 844 tasks,Text Classification172 benchma,Text Classification172 benchma
RCV1,RCV1 Dataset,"The RCV1 dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.",https://arxiv.org/abs/1211.6085,EditCustom,"Image, Text",English,1996,,,,,,"Text Classification, Hierarchical Multi-label Classification, Multi-Label Text Classification, Cross-Lingual Document Classification","hierarchical-multi-label-classification-on-17, text-classification-on-rcv1, multi-label-text-classification-on-rcv1, multi-label-text-classification-on-rcv1-v2-1, cross-lingual-document-classification-on-12, cross-lingual-document-classification-on-13",,See all 844 tasks,Text Classification172 benchma,Text Classification172 benchma
CNN_Daily_Mail,CNN/Daily Mail Dataset,"CNN/Daily Mail is a dataset for text summarization. Human generated abstractive summary bullets were generated from news stories in CNN and Daily Mail websites as questions (with one of the entities hidden), and stories as the corresponding passages from which the system is expected to answer the fill-in the-blank question. The authors released the scripts that crawl, extract and generate pairs of passages and questions from these websites.

In all, the corpus has 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs, as defined by their scripts. The source documents in the training set have 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences.",https://arxiv.org/pdf/1602.06023v5.pdf,EditMIT,Text,English,,,,74 sentences,,,"Text Summarization, Extractive Text Summarization, Summarization, Text Generation, Extractive Document Summarization, Document Summarization, Abstractive Text Summarization, Question Answering","summarization-on-cnn-dailymail, text-summarization-on-cnn-daily-mail-2, document-summarization-on-cnn-daily-mail, extractive-document-summarization-on-cnn, extractive-document-summarization-on-cnn-1, question-answering-on-cnn-daily-mail, text-generation-on-cnn-daily-mail, abstractive-text-summarization-on-cnn-daily, abstractive-text-summarization-on-cnn-daily-2",,See all 844 tasks,Text Generation179 benchmarks1,Text Generation179 benchmarks1
DailyDialog,DailyDialog Dataset,"DailyDialog is a high-quality multi-turn open-domain English dialog dataset. It contains 13,118 dialogues split into a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. On average there are around 8 speaker turns per dialogue with around 15 tokens per turn.",http://yanran.li/dailydialog,"EditCustom (research-only, non-commercial)","Image, Text",English,,,,,,,"Text Generation, Emotion Recognition in Conversation","emotion-recognition-in-conversation-on-3, text-generation-on-dailydialog",,See all 844 tasks,Text Generation179 benchmarks1,Text Generation179 benchmarks1
GSM8K,GSM8K Dataset,"GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − ×÷) to reach the final answer. A bright middle school student should be able to solve every problem. It can be used for multi-step mathematical reasoning.",https://production-media.paperswithcode.com/datasets/1c104a87-b074-4e24-87c4-5122e00d74a9.png,EditUnknown,Text,English,,,,,,,"Text Generation, Arithmetic Reasoning, GSM8K","text-generation-on-gsm8k-tr, arithmetic-reasoning-on-gsm8k, text-generation-on-gsm8k-5-shot, gsm8k-on-gsm8k",,See all 844 tasks,Text Generation179 benchmarks1,Text Generation179 benchmarks1
MT-Bench,MT-Bench Dataset,"This dataset contains 3.3K expert-level pairwise human preferences for model responses generated by 6 models in response to 80 MT-bench questions. The 6 models are GPT-4, GPT-3.5, Claud-v1, Vicuna-13B, Alpaca-13B, and LLaMA-13B. The annotators are mostly graduate students with expertise in the topic areas of each of the questions.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Text Generation, Human Judgment Correlation",text-generation-on-mt-bench,,See all 844 tasks,Text Generation179 benchmarks1,Text Generation179 benchmarks1
TruthfulQA,TruthfulQA Dataset,"TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. The authors crafted questions that some humans would answer falsely due to a false belief or misconception.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-09-17_at_09.47.38.png,EditUnknown,Text,English,,,,,,38,"Text Generation, Question Answering","question-answering-on-truthfulqa, text-generation-on-truthfulqa-0-shot, text-generation-on-truthfulqa-tr, text-generation-on-truthfulqa",,See all 844 tasks,Text Generation179 benchmarks1,Text Generation179 benchmarks1
WinoGrande,WinoGrande Dataset,"WinoGrande is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations.",/paper/winogrande-an-adversarial-winograd-schema,EditCC-BY,Text,English,,,,,,,"Common Sense Reasoning, Natural Language Understanding, Text Generation, parameter-efficient fine-tuning, Natural Language Inference, Question Answering","parameter-efficient-fine-tuning-on-winogrande, common-sense-reasoning-on-winogrande, text-generation-on-winogrande-5-shot, text-generation-on-winogrande-tr",,See all 844 tasks,Text Generation179 benchmarks1,Text Generation179 benchmarks1
FEVER,FEVER Dataset,"FEVER is a publicly available dataset for fact extraction and verification against textual sources.

It consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGHINFO. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim.

The claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering. The verification of each claim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was
extracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.",https://arxiv.org/pdf/1803.05355v3.pdf,EditCustom,Text,English,,,,,,,"Fact Verification, Text Retrieval, Zero-shot Text Search, Question Answering","fact-verification-on-fever, question-answering-on-fever, zero-shot-text-search-on-fever, text-retrieval-on-fever",,See all 844 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
MS_MARCO,MS MARCO Dataset,"The MS MARCO (Microsoft MAchine Reading Comprehension) is a collection of datasets focused on deep learning in search.
The first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. Over time the collection was extended with a 1,000,000 question dataset, a natural language generation dataset, a passage ranking dataset, keyphrase extraction dataset, crawling dataset, and a conversational search.",https://microsoft.github.io/msmarco/,"EditCustom (research-only, non-commercial)",Text,English,,,,,,,"Passage Ranking, TREC 2019 Passage Ranking, Passage Retrieval, Passage Re-Ranking, Information Retrieval, Reading Comprehension, Text Retrieval, Question Answering","passage-ranking-on-ms-marco, passage-retrieval-on-ms-marco-1, information-retrieval-on-msmarco, question-answering-on-ms-marco, information-retrieval-on-ms-marco, text-retrieval-on-ms-marco, passage-re-ranking-on-ms-marco, trec-2019-passage-ranking-on-msmarco",,See all 844 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
MTEB,MTEB Dataset,"MTEB is a benchmark that spans 8 embedding tasks covering a total of 56 datasets and 112 languages. The 8 task types are Bitext mining, Classification, Clustering, Pair Classification, Reranking, Retrieval, Semantic Textual Similarity and Summarisation. The 56 datasets contain varying text lengths and they are grouped into three categories: Sentence to sentence, Paragraph to paragraph, and Sentence to paragraph.

Check the latest leaderboards at HuggingFace.",https://production-media.paperswithcode.com/datasets/5c793a66-bcc1-4aae-afed-061d31ee66fc.png,EditApache-2.0 license,"Image, Text",English,,,,,,,"Text Summarization, STS, Text Reranking, Semantic Textual Similarity, Text Pair Classification, Text Clustering, Information Retrieval, Text Classification, Text Retrieval","semantic-textual-similarity-on-mteb, text-reranking-on-mteb, information-retrieval-on-mteb, text-classification-on-mteb, text-clustering-on-mteb, text-pair-classification-on-mteb, text-summarization-on-mteb, text-retrieval-on-mteb",,See all 844 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
Reuters-21578,Reuters-21578 Dataset,"The Reuters-21578 dataset is a collection of documents with news articles. The original corpus has 10,369 documents and a vocabulary of 29,930 words.",https://arxiv.org/abs/1604.00783,"EditCustom (research-only, attribution)","Image, Text",English,,,,369 documents,,,"Unsupervised Anomaly Detection, Multi-Modal Document Classification, Text Retrieval, Multi-Label Text Classification, Supervised Text Retrieval, Document Classification","text-retrieval-on-reuters-21578, multi-label-text-classification-on-reuters-1, multi-modal-document-classification-on-1, supervised-text-retrieval-on-reuters-21578, unsupervised-anomaly-detection-on-reuters-1, document-classification-on-reuters-21578",,See all 844 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
RSICD,RSICD Dataset,"The Remote Sensing Image Captioning Dataset (RSICD) is a dataset for remote sensing image captioning task. It contains more than ten thousands remote sensing images which are collected from Google Earth, Baidu Map, MapABC and Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images is 10921, with five sentences descriptions per image.",https://github.com/201528014227051/RSICD_optimal,EditUnknown,"Image, Text",English,,,,,,,"Cross-Modal Retrieval, Image-to-Text Retrieval, Image Captioning, Scene Classification, Text Retrieval","text-retrieval-on-rsicd, cross-modal-retrieval-on-rsicd, image-to-text-retrieval-on-rsicd",,See all 844 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
SciFact,SciFact Dataset,"SciFact is a dataset of 1.4K expert-written claims, paired with evidence-containing abstracts annotated with veracity labels and rationales.",https://huggingface.co/datasets/allenai/scifact_entailment,EditCC BY-NC 2.0,Text,English,,,,,,,"Text Retrieval, Zero-shot Text Search","text-retrieval-on-scifact, zero-shot-text-search-on-scifact",,See all 844 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
TREC-COVID,TREC-COVID Dataset,"TREC-COVID is a community evaluation designed to build a test collection that captures the information needs of biomedical researchers using the scientific literature during a pandemic. One of the key characteristics of pandemic search is the accelerated rate of change: the topics of interest evolve as the pandemic progresses and the scientific literature in the area explodes. The COVID-19 pandemic provides an opportunity to capture this progression as it happens. TREC-COVID, in creating a test collection around COVID-19 literature, is building infrastructure to support new research and technologies in pandemic search.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Text,English,,,,,,,"Text Retrieval, Zero-shot Text Search, Information Retrieval","text-retrieval-on-trec-covid, zero-shot-text-search-on-trec-covid",,See all 844 tasks,Text Retrieval16 benchmarks310,Text Retrieval16 benchmarks310
StyleGallery,StyleGallery Dataset,"We construct a style-balanced dataset, called StyleGallery, covering several open source datasets. Specifically, StyleGallery includes JourneyDB, a dataset comprising a broad spectrum of diverse styles derived from MidJourney, and WIKIART, with extensive fine-grained painting styles, such as pointillism and ink drawing, and a subset of stylized images from LAION-Aesthetics.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text, Video",English,,,,,,,"Text Style Transfer, Image Generation, Video Style Transfer, Style Transfer",style-transfer-on-stylebench,,See all 844 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
StylePTB,StylePTB Dataset,"StylePTB is a fine-grained text style transfer benchmark. It consists of paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as compositions of multiple transfers which allow modelling of fine-grained stylistic changes as building blocks for more complex, high-level transfers.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-04-14_at_10.05.07.png,EditUnknown,Text,English,,,,,,,Text Style Transfer,,,See all 844 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
TextBox_2.0,TextBox 2.0 Dataset,"TextBox 2.0 is a comprehensive and unified library for text generation, focusing on the use of pre-trained language models (PLMs). The library covers 13 common text generation tasks and their corresponding 83 datasets and further incorporates 45 PLMs covering general, translation, Chinese, dialogue, controllable, distilled, prompting, and lightweight PLMs.",https://arxiv.org/pdf/2212.13005v1.pdf,EditMIT License,Text,English,,,,,,,"Text Summarization, Text Generation, Text Simplification, Question Generation, Paraphrase Generation, Text Style Transfer, Machine Translation, Question Answering",,,See all 844 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
XFORMAL,XFORMAL Dataset,"XFORMAL is a multilingual formal style transfer benchmark of multiple formal reformulations of informal text in Brazilian Portuguese, French, and Italian.",https://production-media.paperswithcode.com/datasets/Screenshot_2021-04-14_at_10.57.58.png,EditUnknown,Text,English,,,,,,,Text Style Transfer,,,See all 844 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
Yelp,Yelp Dataset,"The Yelp Dataset is a valuable resource for academic research, teaching, and learning. It provides a rich collection of real-world data related to businesses, reviews, and user interactions. Here are the key details about the Yelp Dataset:
Reviews: A whopping 6,990,280 reviews from users.
Businesses: Information on 150,346 businesses.
Pictures: A collection of 200,100 pictures.
Metropolitan Areas: Data from 11 metropolitan areas.
Tips: Over 908,915 tips provided by 1,987,897 users.
Business Attributes: Details like hours, parking availability, and ambiance for more than 1.2 million businesses.
Aggregated Check-ins: Historical check-in data for each of the 131,930 businesses.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Graph, Image, Text, Time Series",English,,,,,,,"Sentiment Classification, Sequential Recommendation, Anomaly Detection, Sentiment Analysis, Collaborative Filtering, Unsupervised Opinion Summarization, Paraphrase Identification, Fraud Detection, Aspect Extraction, SQL Parsing, Text Style Transfer, Text Classification, Node Classification, Unsupervised Text Style Transfer, Link Prediction, Graph Mining, Cross-Domain Document Classification, Recommendation Systems, Multibehavior Recommendation, Document Classification","text-classification-on-yelp-2, sentiment-analysis-on-yelp-binary, fraud-detection-on-yelp-fraud, link-prediction-on-yelp, multibehavior-recommendation-on-yelp, sql-parsing-on-yelp, unsupervised-opinion-summarization-on-yelp, text-style-transfer-on-yelp-review-dataset, cross-domain-document-classification-on-yelp, sentiment-analysis-on-yelp-fine-grained, document-classification-on-yelp-14, unsupervised-text-style-transfer-on-yelp2018, paraphrase-identification-on-yelp, node-classification-on-yelpchi, text-style-transfer-on-yelp-review-dataset-1, recommendation-systems-on-yelp2018, aspect-extraction-on-yaso-yelp, unsupervised-text-style-transfer-on-yelp, recommendation-systems-on-yelp, sequential-recommendation-on-yelp, collaborative-filtering-on-yelp2018, text-classification-on-yelp-5",,See all 844 tasks,Text Style Transfer3 benchmark,Text Style Transfer3 benchmark
20NewsGroups,20NewsGroups Dataset,"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text",English,,,,,,,"Unsupervised Text Classification, Intrusion Detection, Topic Models","topic-models-on-20newsgroups, unsupervised-text-classification-on-1, intrusion-detection-on-20newsgroups",,See all 844 tasks,Topic Models9 benchmarks227 pa,Topic Models9 benchmarks227 pa
20_Newsgroups,20 Newsgroups Dataset,"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.",https://huggingface.co/datasets/google-research-datasets/newsgroup,EditUnknown,"Image, Text",English,,,,,,,"Topic Models, Text Clustering, Out-of-Distribution Detection, Text Classification, Text Retrieval, Supervised Text Retrieval","out-of-distribution-detection-on-20, text-clustering-on-20-newsgroups, text-classification-on-20-newsgroups, topic-models-on-20-newsgroups, supervised-text-retrieval-on-20-newsgroups-1, text-retrieval-on-20-newsgroups",,See all 844 tasks,Topic Models9 benchmarks227 pa,Topic Models9 benchmarks227 pa
Arxiv_HEP-TH_citation_graph,Arxiv HEP-TH citation graph Dataset,"Arxiv HEP-TH (high energy physics theory) citation graph is from the e-print arXiv and covers all the citations within a dataset of 27,770 papers with 352,807 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this.
The data covers papers in the period from January 1993 to April 2003 (124 months).",https://snap.stanford.edu/data/cit-HepTh.html,EditUnknown,"Image, Text, Time Series",English,1993,,,,,,"Text Summarization, Topic Models, Extended Summarization, Language Modelling, Clique Prediction, Text Classification, Document Summarization","clique-prediction-on-arxiv-astroph-3-clique, clique-prediction-on-arxiv-grqc-4-clique, extended-summarization-on-arxiv-long-test, language-modelling-on-arxiv, topic-models-on-arxiv, text-summarization-on-arxiv, text-classification-on-arxiv, extended-summarization-on-arxiv-long-val, document-summarization-on-arxiv",,See all 844 tasks,Topic Models9 benchmarks227 pa,Topic Models9 benchmarks227 pa
COVID-19_Twitter_Chatter_Dataset,COVID-19 Twitter Chatter Dataset Dataset,"A large-scale curated dataset of over 152 million tweets, growing daily, related to COVID-19 chatter generated from January 1st to April 4th at the time of writing.",/paper/a-large-scale-covid-19-twitter-chatter,EditOther (Public Domain),"Image, Text",English,,,,,,,"Topic Models, Misinformation, Text Matching, Text Clustering, Out-of-Distribution Detection, Text Classification, Text Retrieval, Supervised Text Retrieval","out-of-distribution-detection-on-20, text-clustering-on-20-newsgroups, text-classification-on-20-newsgroups, topic-models-on-20-newsgroups, supervised-text-retrieval-on-20-newsgroups-1, text-retrieval-on-20-newsgroups",,See all 844 tasks,Topic Models9 benchmarks227 pa,Topic Models9 benchmarks227 pa
Fashion_144K,Fashion 144K Dataset,"Fashion 144K is a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information.",/paper/neuroaesthetics-in-fashion-modeling-the-1,EditUnknown,"Image, Text",English,,,,,,,"Image Clustering, Semantic Parsing, Topic Models",,,See all 844 tasks,Topic Models9 benchmarks227 pa,Topic Models9 benchmarks227 pa
New_York_Times_Annotated_Corpus,New York Times Annotated Corpus Dataset,"The New York Times Annotated Corpus contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. The corpus includes:


Over 1.8 million articles (excluding wire services articles that appeared during the covered period).
Over 650,000 article summaries written by library scientists.
Over 1,500,000 articles manually tagged by library scientists with tags drawn from a normalized indexing vocabulary of people, organizations, locations and topic descriptors.
Over 275,000 algorithmically-tagged articles that have been hand verified by the online production staff at nytimes.com.
As part of the New York Times' indexing procedures, most articles are manually summarized and tagged by a staff of library scientists. This collection contains over 650,000 article-summary pairs which may prove to be useful in the development and evaluation of algorithms for automated document summarization. Also, over 1.5 million documents have at least one tag. Articles are tagged for persons, places, organizations, titles and topics using a controlled vocabulary that is applied consistently across articles. For instance if one article mentions ""Bill Clinton"" and another refers to ""President William Jefferson Clinton"", both articles will be tagged with ""CLINTON, BILL"".",https://catalog.ldc.upenn.edu/LDC2008T19,"EditCustom (research-only, non-commercial)","Graph, Image, Text",English,1987,,,,,,"Text Summarization, Hierarchical Multi-label Classification, Topic Models, Document Dating, Relationship Extraction (Distant Supervised), Relation Extraction, Joint Entity and Relation Extraction, UIE, Open Information Extraction, Document Summarization, Abstractive Text Summarization","uie-on-nyt, hierarchical-multi-label-classification-on-18, relationship-extraction-distant-supervised-on-2, document-dating-on-nyt, relation-extraction-on-nyt, joint-entity-and-relation-extraction-on-nyt, open-information-extraction-on-nyt, relationship-extraction-distant-supervised-on, topic-models-on-nyt",,See all 844 tasks,Topic Models9 benchmarks227 pa,Topic Models9 benchmarks227 pa
RuWiki-Good,RuWiki-Good Dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,,,,,,,,,Topic Models,,,See all 844 tasks,Topic Models9 benchmarks227 pa,Topic Models9 benchmarks227 pa
ACES,ACES Dataset,ACES a dataset consisting of 68 phenomena ranging from simple perturbations at the word/character level to more complex errors based on discourse and real-world knowledge. It can be used to evaluate a wide range of Machine Translation metrics.,https://production-media.paperswithcode.com/datasets/944d625c-8ac3-4f6e-9ead-be0fd455fe6d.png,EditUnknown,Text,English,,,,,,,"Translation, Machine Translation",machine-translation-on-aces,,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
CVSS,CVSS Dataset,"CVSS is a massively multilingual-to-English speech to speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice  speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY 4.0,"Audio, Text",English,,,,,,,"fr-en, de-en, Text-To-Speech Synthesis, Speech-to-Speech Translation, es-en, Translation","fr-en-on-cvss, speech-to-speech-translation-on-cvss, de-en-on-cvss, es-en-on-cvss",,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
Demetr,Demetr Dataset,"Demetr is a diagnostic dataset with 31K English examples (translated from 10 source languages) for evaluating the sensitivity of MT evaluation metrics to 35 different linguistic perturbations spanning semantic, syntactic, and morphological error categories.",https://arxiv.org/pdf/2210.13746v1.pdf,EditMIT License,Text,English,,,,,,,"Translation, Machine Translation",,,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
Duolingo_STAPLE_Shared_Task,Duolingo STAPLE Shared Task Dataset,"This is the dataset for the 2020 Duolingo shared task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). Sentence prompts, along with automatic translations, and high-coverage sets of translation paraphrases weighted by user response are provided in 5 language pairs. Starter code for this task can be found here: github.com/duolingo/duolingo-sharedtask-2020/. More details on the data set and task are available at: sharedtask.duolingo.com",https://production-media.paperswithcode.com/datasets/45ebbfc5-0707-4b29-9e18-cffc6c3d4603.png,EditCC BY-NC 4.0,Text,English,2020,,,,,,"Paraphrase Generation, Translation, Machine Translation, Multilingual NLP",,,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
FRMT,FRMT Dataset,"FRMT is a dataset and evaluation benchmark for Few-shot Region-aware Machine Translation, a type of style-targeted translation. The dataset consists of human translations of a few thousand English Wikipedia sentences into regional variants of Portuguese and Mandarin. Source documents are selected to enable detailed analysis of phenomena of interest, including lexically distinct terms and distractor terms.

Source:",https://production-media.paperswithcode.com/datasets/0d5d0773-7de5-47df-b45a-4c8de9dfd67d.png,EditCC BY-SA 3.0,Text,English,,,,,,,"Translation, Machine Translation","machine-translation-on-frmt-portuguese-brazil, machine-translation-on-frmt-chinese-mainland, machine-translation-on-frmt-portuguese, machine-translation-on-frmt-chinese-taiwan",,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
GigaST,GigaST Dataset,"GigaST is a large-scale pseudo speech translation (ST) corpus. The corpus was created by translating the text in GigaSpeech, an English ASR corpus, into German and Chinese. The training set is translated by a strong machine translation system and the test set was translated by human. ST models trained with an addition of the corpus obtain new state-of-the-art results on the MuST-C English-German benchmark test set.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY-NC 4.0,Text,English,,,,,,,"Translation, Machine Translation",,,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
Oxford_Radar_RobotCar_Dataset,Oxford Radar RobotCar Dataset Dataset,"The Oxford Radar RobotCar Dataset is a radar extension to The Oxford RobotCar Dataset. It has been extended with data from a Navtech CTS350-X Millimetre-Wave FMCW radar and Dual Velodyne HDL-32E LIDARs with optimised ground truth radar odometry for 280 km of driving around Oxford, UK (in addition to all sensors in the original Oxford RobotCar Dataset).",/paper/the-oxford-radar-robotcar-dataset-a-radar,EditUnknown,"Text, Time Series",English,,,,,,,"Radar odometry, Weather Forecasting, Translation","translation-on-oxford-radar-robotcar-dataset, radar-odometry-on-oxford-radar-robotcar",,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
PhoMT,PhoMT Dataset,PhoMT is a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs for machine translation.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCustom,Text,English,,,,,,,"Translation, Machine Translation",translation-on-phomt,,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
SpeechMatrix,SpeechMatrix Dataset,SpeechMatrix is a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech.,https://scontent-lhr8-2.xx.fbcdn.net/v/t39.8562-6/310002966_605149234737289_5204270723809834290_n.pdf?_nc_cat=102&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=FN2KnupyKI0AX90B5UO&_nc_ht=scontent-lhr8-2.xx&oh=00_AT9iFWHchGOnkzVTmwiYIDElIXSnwilSGhDwRQdFh99rlA&oe=63560915,EditCC-BY-NC 4.0,"Audio, Text",English,,,,,,,"Speech-to-Speech Translation, Translation",,,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
xP3,xP3 Dataset,xP3 is a multilingual dataset for multitask prompted finetuning. It is a composite of supervised datasets in 46 languages with English and machine-translated prompts.,https://arxiv.org/pdf/2211.01786v1.pdf,EditUnknown,Text,English,,,,,,,"Sentence Completion, Text Summarization, Coreference Resolution, Translation",,,See all 844 tasks,Translation6 benchmarks3501 pa,Translation6 benchmarks3501 pa
ANETAC,ANETAC Dataset,"An English-Arabic named entity transliteration and classification dataset built from freely available parallel translation corpora. The dataset contains 79,924 instances, each instance is a triplet (e, a, c), where e is the English named entity, a is its Arabic transliteration and c is its class that can be either a Person, a Location, or an Organization. The ANETAC dataset is mainly aimed for the researchers that are working on Arabic named entity transliteration, but it can also be used for named entity classification purposes.",/paper/anetac-arabic-named-entity-transliteration,EditUnknown,,,,,,924 instances,,,Transliteration,,,See all 844 tasks,Transliteration54 papers with ,Transliteration54 papers with 
ArzEn,ArzEn Dataset,"Corpus of Egyptian Arabic-English Code-switching (ArzEn) is a spontaneous conversational speech corpus, obtained through informal interviews held at the German University in Cairo. The participants discussed broad topics, including education, hobbies, work, and life experiences. The corpus currently contains 12 hours of speech, having 6,216 utterances. The recordings were transcribed and translated into monolingual Egyptian Arabic and monolingual English.",https://arxiv.org/pdf/2211.16319v1.pdf,EditUnknown,"Audio, Image, Text",English,,,,,,,"ArzEn Speech Recognition, Automatic Speech Recognition (ASR), Transliteration, ArzEn Code-switched Translation to eng, Speech Recognition, ArzEn Code-switched Translation to ara","arzen-code-switched-translation-to-eng-on-1, arzen-speech-recognition-on-arzen, arzen-code-switched-translation-to-ara-on",,See all 844 tasks,Transliteration54 papers with ,Transliteration54 papers with 
Bianet,Bianet Dataset,"Bianet is a parallel news corpus in Turkish, Kurdish and English
It contains 3,214 Turkish articles with their sentence-aligned Kurdish or English translations from the Bianet online newspaper.",/paper/bianet-a-parallel-news-corpus-in-turkish,EditUnknown,,,,,,,,,Transliteration,,,See all 844 tasks,Transliteration54 papers with ,Transliteration54 papers with 
Dakshina,Dakshina Dataset,"The Dakshina dataset is a collection of text in both Latin and native scripts for 12 South Asian languages. For each language, the dataset includes a large collection of native script Wikipedia text, a romanization lexicon which consists of words in the native script with attested romanizations, and some full sentence parallel data in both a native script of the language and the basic Latin alphabet.",https://github.com/google-research-datasets/dakshina,EditUnknown,Text,English,,,,,,,"Transliteration, Language Modelling, Language Identification",,,See all 844 tasks,Transliteration54 papers with ,Transliteration54 papers with 
TSAC,TSAC Dataset,Tunisian Sentiment Analysis Corpus (TSAC) is a Tunisian Dialect corpus of 17.000 comments from Facebook.,/paper/sentiment-analysis-of-tunisian-dialects,EditUnknown,Text,English,,,,,,,"Transliteration, Sentiment Analysis",,,See all 844 tasks,Transliteration54 papers with ,Transliteration54 papers with 
CoDEx_Large,CoDEx Large Dataset,"CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false.",/paper/codex-a-comprehensive-knowledge-graph,EditMIT,"Graph, Image, Time Series",,,,,,,,"Link Prediction, Triple Classification, Knowledge Graph Completion",link-prediction-on-codex-large,,See all 844 tasks,Triple Classification1 benchma,Triple Classification1 benchma
CoDEx_Medium,CoDEx Medium Dataset,"CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false.",/paper/codex-a-comprehensive-knowledge-graph,EditMIT,"Graph, Image, Time Series",,,,,,,,"Link Prediction, Triple Classification, Knowledge Graph Completion",link-prediction-on-codex-medium,,See all 844 tasks,Triple Classification1 benchma,Triple Classification1 benchma
CoDEx_Small,CoDEx Small Dataset,"CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false.",/paper/codex-a-comprehensive-knowledge-graph,EditMIT,"Graph, Image, Time Series",,,,,,,,"Link Prediction, Triple Classification, Knowledge Graph Completion",link-prediction-on-codex,,See all 844 tasks,Triple Classification1 benchma,Triple Classification1 benchma
YAGO,YAGO Dataset,"Yet Another Great Ontology (YAGO) is a Knowledge Graph that augments WordNet with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base. YAGO originally consisted of more than 1 million entities and 5 million facts describing relationships between these entities. YAGO2 grounded entities, facts, and events in time and space, contained 446 million facts about 9.8 million entities, while YAGO3 added about 1 million more entities from non-English Wikipedia articles. YAGO3-10 a subset of YAGO3, containing entities which have a minimum of 10 relations each.",https://arxiv.org/abs/1904.01172,EditCC BY 3.0,"Image, Time Series",,,,,,,,"Link Prediction, Time-interval Prediction, Triple Classification","time-interval-prediction-on-yago11k, link-prediction-on-yago37, triple-classification-on-yago39k, link-prediction-on-yago3-10, link-prediction-on-yago11k, link-prediction-on-yago39k, link-prediction-on-yago15k-1",,See all 844 tasks,Triple Classification1 benchma,Triple Classification1 benchma
ArSen-20,ArSen-20 Dataset,"Sentiment detection remains a pivotal task in natural language processing, yet its development in Arabic lags due to a scarcity of training materials compared to English. Addressing this gap, we present ArSen-20, a benchmark dataset tailored to propel Arabic sentiment detection forward. ArSen-20 comprises 20,000 professionally labeled tweets sourced from Twitter, focusing on the theme of COVID-19 and spanning the period from 2020 to 2023. Beyond tweet content, the dataset incorporates metadata associated with the user, enriching the contextual understanding. ArSen-20 offers a comprehensive resource to foster advancements in Arabic sentiment analysis and facilitate research in this critical domain.

The ArSen-20 dataset statistics:

| Statistics   |   Num  | 
|:-------------:|:-----:|
| Training set size | 16000 |
| Validation set size| 2000 |
| Testing set size | 2000 |
| Neutral | 17262 |
| Positive | 878 |
| Negative | 1860 |

Features
The dataset has the following features:

| Field   |  Type  |  Description  |
|:-----------:| :--------: |:----------------: |
| tweet id     | string     | The unique identifier of the requested Tweet.     |
| label   | string     | Sentiment Classification of this tweet.     |
| author id   | string    |The unique identifier of this user.     |
| created_at  | data     | Creation time of the Tweet.    |
| lang  | string     | Language of the Tweet, if detected by Twitter.    |
| like_count  | int     |The number of likes on this tweet.|
|quote_count  | int    | The number of times this tweet has been quoted.    |
| reply_count   | int     | The number of replies to this tweet.    |
| retweet_count| int    | The number of retweets to this tweet.    |
| tweet   | string     | The actual UTF-8 text of the Tweet.    |
|user_verified  | boolean     | Indicates if this user is a verified Twitter User.     |
|followers_count  | int     |The number of followers of the author.     |
| following_count  | int     | The number of following of the author.    |
| tweet_count  | int     | Total number of tweets by the author.    |
| listed_count | int     |The number of public lists that this user is a member of.    |
|name | string     | The name of the user.    |
| username   | string     | The Twitter screen name, handle, or alias.    |
| user_created_at| data     | The UTC datetime that the user account was created.     |
| description  | string     | The text of this user’s profile description (bio).     |

DownLoad
You can download the dataset from here.



ArSen-20_publish.csv - Contains all features.



ArSen-20_id_only.csv - Contains only tweets and their author's id.



Citation
If you use this dataset in your research, please cite the following papers:
bibtex
@inproceedings{fang2024arsen,
title={ArSen-20: A New Benchmark for Arabic Sentiment Detection},
author={Yang Fang and Cheng Xu},
booktitle={5th Workshop on African Natural Language Processing},
year={2024},
url={https://openreview.net/forum?id=GgsRUF5kJt}
}

bibtex
@inproceedings{fang2024advancing,
    title = ""Advancing {A}rabic Sentiment Analysis: {A}r{S}en Benchmark and the Improved Fuzzy Deep Hybrid Network"",
    author = ""Fang, Yang  and
      Xu, Cheng  and
      Guan, Shuhao  and
      Yan, Nan  and
      Mei, Yuke"",
    editor = ""Barak, Libby  and
      Alikhani, Malihe"",
    booktitle = ""Proceedings of the 28th Conference on Computational Natural Language Learning"",
    month = nov,
    year = ""2024"",
    address = ""Miami, FL, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.conll-1.39"",
    pages = ""507--516"",
}

contact
If you have any questions or comments about the dataset, please contact Yang Fang (20211209024@chnu.edu.cn).

Potential cooperation in related fields is also welcome. :)",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditApache-2.0 license,"Image, Text",English,2020,,,,,,"Arabic Sentiment Analysis, Twitter Sentiment Analysis, Sentiment Classification, Sentiment Analysis",,,See all 844 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
Crypto_related_tweets_from_10.10.2020_to_3.3.2021,Crypto related tweets from 10.10.2020 to 3.3.2021 Dataset,The dataset contains 30 million cryptocurrency-related tweets from 10.10.2020 to 3.3.2021. See https://github.com/meakbiyik/ask-who-not-what for more details.,https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY 4.0,"Image, Text",English,2020,,,,,,"Twitter Bot Detection, Twitter Event Detection, Twitter Sentiment Analysis",,,See all 844 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
RETWEET,RETWEET Dataset,"RETWEET is a dataset of tweets and overall predominant sentiment of their replies.

SUMMARY
WHAT: Message-level Polarity Classification.

GOAL: To predict the predominant sentiment among (potential) first-order replies to a given tweet.

IDEA: Mitigate the problem of lacking labeled training data wi treating the unsupervised nature of the problem as a supervised learning case.

APPROACH:

Train a tweet classifier. 
Automatically label the replies using the classifier trained in the first part.
Choose a final label representing the general predominant sentiment of the replies of every tweet.

DATA COLLECTION
To download all of the replies to a tweet, the Search API should be used. However, the Search API is limited to 75000 requests per hour, which causes the mining and downloading process to be slow.
Furthermore, using the Twitter API, there is no possibility of downloading absolute random data. Therefore, we try to make the procedure as random as possible by utilizing two different strategies for data downloading and using them in an intermixed manner.



Our first strategy is based on a sample of English tweets obtained by filtering the Twitter stream via a list of cultural keywords. This list consists of 147 words that are deemed to play a ""pivotal role in discussions of culture and society"", covering diverse words such as aesthetics, environment, feminism, power, tourism, or youth. We extracted all tweets in 2019 that have a minimum of 20 first-order replies in the dataset. The data come with an obvious caveat: Both the source tweet as well as all the replies must contain at least one word from the list of keywords. Therewith, it is highly unlikely that the list of replies for any given source is exhaustive, i.e. there might be many more first-order replies to the source tweet that are not in the dataset.



As our second approach, we use the GetOldTweets3 library to download all the replies corresponding to every tweet. We define few restrictions to add randomization to the process. Firstly, every tweet and also every reply should contain at least 20 strings. This is due to the fact that our automatic tweet classifier, explsined in the paper, is optimized based on the message-level classification paradigm. Therefore, it operates optimal when the input contains at least a sufficient number of words. The second constraint is that every tweet should contain at least 20 first-order replies. In order to increase randomness, in this strategy, instead of referencing to a list of keywords, we manually choose some keywords, which are most likely to include long discussions, such as Coronavirus and football or the ones, which are most likely to include strong opinions such as birthday, war, or racism in order to account for the easy-to-guess examples. 



MANUAL ANNOTATIONS FOR THE RETWEET (TEST GOLD DATASET)
5,015 tweets with their corresponding replies, collected as a combination of the two different collection strategies, were given to three different students. Each of them had to read all the replies corresponding to every tweet, without observing the original tweet in order to avoid having a prior knowledge, and decide on ONE final sentiment for the replies. The assigned sentiment can only be one of the positive, negative, or neutral labels.

Considering the fact that this is a really challenging task for the machine, to prevent human mistakes, we correlated the results of the three annotators and only chose the tweets, in which all of the annotators had the same opinion on the labels, as the final gold standard test data. Therefore, we finally, ended up with a test set consisting of 1,519 human labeled tweets, with the labels being the sentiment of the replies of a tweet and not the tweet itself. 

DATASET CONTENTS
1. Training raw dataset: 34,953 unique tweets in total and individual automatic labels for all of their corresponding replies (1,519,504 total replies). Including,


./RETWEET_data/train_reply_labels_set1.txt
./RETWEET_data/train_reply_labels_set2.txt

2. Training autamtically-labeled dataset: 34,953 unique tweets and ONE final automatic label (chosen based on the algorithm 1 of our paper) for every tweet. Including,


./RETWEET_data/train_final_label.txt

3. Gold standard test dataset (RETWEET): 1,519 unique tweets with their manual labels for replies. ONE final label, which states the predominant overall polarity of all its replies, is assigned to every tweet. Including,


./RETWEET_data/test_gold.txt

NOTES


Please note that by downloading the Twitter data you agree to abide by the Twitter terms of service, and in particular you agree not to redistribute the data and to delete tweets that are marked deleted in the future.



The ""neutral"" label in the annotations stands for objective or neutral.



The distribution consists of a set of Twitter unique tweet IDs with annotations (overall polarity of replies). As for data privacy, the texts of the tweets and replies are not distributed. But as all the utilized resources in this dataset are taken from public tweets, having the tweet unique IDs, you can download the tweet and its replies.
You can use the Semeval Twitter data downloading script to obtain the corresponding tweets:  

https://github.com/seirasto/twitter_download/



The dataset URL:

https://kaggle.com/soroosharasteh/retweet/ 



LICENSE
The accompanying dataset is released under a Creative Commons Attribution 4.0 International License.

SOURCE CODE
The official source code of the paper: https://github.com/starasteh/retweet

In case you use this dataset, please cite the original paper:
S. Tayebi Arasteh, M. Monajem, V. Christlein, P. Heinrich, A. Nicolaou, H.N. Boldaji, M. Lotfinia,  S. Evert. ""How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies"". Proceedings of the 2021 IEEE 15th International Conference on Semantic Computing (ICSC), Laguna Hills, CA, USA, January 2021.

BibTex
@inproceedings{RETWEET,
  title = ""How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies"",
  author = ""Tayebi Arasteh, Soroosh and Monajem, Mehrpad and Christlein, Vincent and
  Heinrich, Philipp and Nicolaou, Anguelos and Naderi Boldaji, Hamidreza and Lotfinia, Mahshad and Evert, Stefan"",
  booktitle = ""Proceedings of the 2021 IEEE 15th International Conference on Semantic Computing (ICSC)"",
  address = ""Laguna Hills, CA, USA"",
  pages = ""370-373"",
  doi = ""10.1109/ICSC50631.2021.00068"",
  url = ""https://ieeexplore.ieee.org/document/9364527/"",
  month = ""01"",       
  year = ""2021""
  }


Dataset DOI: 10.34740/kaggle/ds/736988
Paper: https://ieeexplore.ieee.org/document/9364527
Paper DOI: 10.1109/ICSC50631.2021.00068

CONTACT
E-mail: soroosh.arasteh@fau.de

DATA FORMAT FOR ALL THE FILES
label TAB id

where, ""label"" can be positive, neutral or negative, corresponding to the overall message-level polarity of the replies of the tweet and ""id"" corresponds to the Twitter unique ID for the tweets.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCC BY 4.0,"Image, Text",English,2019,,,,"traint is that every tweet should contain at least 20 first-order replies. In order to increase randomness, in this strategy, instead of referencing to a list of keywords, we manually choose some keywords, which are most likely to include long discussions, such as Coronavirus and football or the ones, which are most likely to include strong opinions such as birthday, war, or racism in order to account for the easy-to-guess examples",,"Multi-Domain Sentiment Classification, Point Processes, Tweet-Reply Sentiment Analysis, Twitter Sentiment Analysis, Sentiment Analysis","point-processes-on-retweet, tweet-reply-sentiment-analysis-on-retweet",,See all 844 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
SentimentArcs__Sentiment_Reference_Corpus_for_Nove,SentimentArcs: Sentiment Reference Corpus for Novels Dataset,"SentimentArcs’ reference corpus for novels consists of 25 narratives selected to create a diverse set of well recognized novels that can serve as a benchmark for future studies. The composition of the corpora was limited by the effect of copyright laws as well as historical imbalances. Most works were obtained from US and Australian Gutenberg Projects. The corpora is expected to grow in size and diversity over time.  

Several dimensions of diversity were considered for inclusion including popularity, period, genre, topic, style and author diversity. The first version of our corpus includes only English, although Proust and Homer are included in translation. SentimentArcs has processed a larger set of novels, including some in foreign languages. The initial reference corpus is in English since performance across all ensemble models was uneven in less resourced languages

In sum, the corpora includes (1) the two most popular novels on Gutenberg.org (Project Gutenberg, 2021b), (2) eight of the fifteen most assigned novels at top US universities (EAB, 2021), and (3) three works that have sold over 20 million copies (Books, 2021). There are eight works by women, two by African-Americans and five works by two LGBTQ authors. Britain leads with 15 authors followed by 6 Americans and one each from France, Russia, North Africa and Ancient Greece.",https://production-media.paperswithcode.com/datasets/14bd0ec6-6779-49ee-a7cf-c9989d3410b0.jpg,EditMIT,Text,English,2021,,,,,,"Twitter Sentiment Analysis, Sentiment Analysis",,,See all 844 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
_chinahate,#chinahate Dataset,"#chinahate dataset contains a total of 2,172,333 tweets hashtagged #china posted during the time it was collected. It is designed for the task of hate speech detection.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Audio, Image, Text",English,,,,,,,"Twitter Sentiment Analysis, Hate Speech Detection",,,See all 844 tasks,Twitter Sentiment Analysis1 be,Twitter Sentiment Analysis1 be
Unsupervised_Few-Shot_Learning13_papers_with_code_,Unsupervised Few-Shot Learning13 papers with code Dataset,,https://paperswithcode.com/dataset/unsupervised-few-shot-learning,,,,,,,,,,,,,See all 844 tasks,Unsupervised Few-Shot Learning,Unsupervised Few-Shot Learning
CCMatrix,CCMatrix Dataset,"CCMatrix uses ten snapshots of a curated common crawl corpus (Wenzek et al., 2019) totalling 32.7 billion unique sentences.",/paper/ccmatrix-mining-billions-of-high-quality,EditUnknown,Text,English,2019,,,,,,"Text Generation, Unsupervised Machine Translation",,,See all 844 tasks,Unsupervised Machine Translati,Unsupervised Machine Translati
WMT_2016_News,WMT 2016 News Dataset,"News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, Romanian, Russian, Turkish) and additional 1500 sentences from each of the 5 languages translated to English. For Romanian a third of the test set were released as a development set instead. For Turkish additional 500 sentence development set was released. The sentences were selected from dozens of news websites and translated by professional translators.
The training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.
Some training corpora were identical from WMT 2015 (Europarl, United Nations, French-English 10⁹ corpus, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (CzEng v1.6pre, News Commentary v11, monolingual news data). Additionally, the following new corpora were added: Romanian Europarl, SETIMES2 from OPUS for Romanian-English and Turkish-English, Monolingual data sets from CommonCrawl.",https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/,EditUnknown,Text,English,2015,,,1500 sentences,,,"Automatic Post-Editing, Unsupervised Machine Translation, Word Alignment, Machine Translation","unsupervised-machine-translation-on-wmt2016-2, machine-translation-on-wmt2016-russian, machine-translation-on-wmt2016-english-1, machine-translation-on-wmt2016-finnish, unsupervised-machine-translation-on-wmt2016-3, machine-translation-on-wmt2016-romanian, unsupervised-machine-translation-on-wmt2016-5, machine-translation-on-wmt2016-english",,See all 844 tasks,Unsupervised Machine Translati,Unsupervised Machine Translati
MathVista,MathVista Dataset,"MathVista is a consolidated Mathematical reasoning benchmark within Visual contexts. It consists of three newly created datasets, IQTest, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logical reasoning on puzzle test figures, algebraic reasoning over functional plots, and scientific reasoning with academic paper figures, respectively. It also incorporates 9 MathQA datasets and 19 VQA datasets from the literature, which significantly enrich the diversity and complexity of visual perception and mathematical reasoning challenges within our benchmark. In total, MathVista includes 6,141 examples collected from 31 different datasets.


Project: https://mathvista.github.io/
Visualization: https://mathvista.github.io/#visualization
Leaderboard: https://mathvista.github.io/#leaderboard
Paper: https://arxiv.org/abs/2310.02255
Data: https://huggingface.co/datasets/AI4Math/MathVista
Code: https://github.com/lupantech/MathVista",https://production-media.paperswithcode.com/datasets/5f2b02d1-a1bf-4759-98b5-01f3890eeec9.png,EditCC BY-SA 4.0,"Image, Text",English,,,,141 examples,"Test, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logical reasoning on puzzle test figures, algebraic reasoning over functional plots, and scientific reasoning with academic paper figures, respectively. It also incorporates 9 MathQA datasets and 19 VQA datasets from the literature, which significantly enrich the diversity and complexity of visual perception and mathematical reasoning challenges within our benchmark. In total, MathVista includes 6,141 examples",,"Math Word Problem Solving, Visual Reasoning, Visual Question Answering, Multiple-choice, Mathematical Reasoning, Visual Question Answering (VQA), Question Answering",,,See all 844 tasks,Visual Question Answering31 be,Visual Question Answering31 be
MM-Vet,MM-Vet Dataset,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,https://production-media.paperswithcode.com/datasets/3357518a-4594-44bf-8a58-dc611de92210.jpeg,EditCC BY-NC 4.0,"Image, Text",English,,,,,,,"Visual Question Answering, Visual Question Answering (VQA)","visual-question-answering-on-mm-vet, visual-question-answering-vqa-on-mm-vet",,See all 844 tasks,Visual Question Answering31 be,Visual Question Answering31 be
MMBench,MMBench Dataset,"MMBench is a multi-modality benchmark. It methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluation questions and abilities. The second element introduces a novel CircularEval strategy and incorporates the use of ChatGPT. This implementation is designed to convert free-form predictions into pre-defined choices, thereby facilitating a more robust evaluation of the model's predictions.",https://production-media.paperswithcode.com/datasets/5fee3fd8-23d7-4741-9c2a-6d9e5963e5a8.jpeg,EditApache-2.0 license,"Image, Text",English,,,,,,,Visual Question Answering,visual-question-answering-on-mmbench,,See all 844 tasks,Visual Question Answering31 be,Visual Question Answering31 be
ActivityNet,ActivityNet Dataset,"The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.",https://arxiv.org/abs/1808.02536,EditUnknown,"Image, Text, Time Series, Video",English,,,,,,,"Zero-Shot Video Retrieval, Zero-Shot Action Recognition, Temporal Action Localization, ZSL Video Classification, Action Detection, Semi-Supervised Action Detection, Action Classification, Temporal Action Proposal Generation, Zero-Shot Action Detection, Visual Question Answering (VQA), Action Recognition In Videos, Video Retrieval, Action Recognition, GZSL Video Classification, Few Shot Temporal Action Localization, Weakly Supervised Action Localization","semi-supervised-action-detection-on, action-recognition-in-videos-on-activitynet-1, zsl-video-classification-on-activitynet-gzsl, action-classification-on-activitynet, action-classification-on-activitynet-12, gzsl-video-classification-on-activitynet-gzsl-1, few-shot-temporal-action-localization-on, zero-shot-action-detection-on-activitynet-1-3, temporal-action-localization-on-activitynet-1, temporal-action-proposal-generation-on, temporal-action-localization-on-activitynet, zero-shot-action-recognition-on-activitynet, weakly-supervised-action-localization-on-2, action-recognition-in-videos-on-activitynet, weakly-supervised-action-localization-on-1, zero-shot-video-retrieval-on-activitynet, visual-question-answering-vqa-on-activitynet-1, gzsl-video-classification-on-activitynet-gzsl, video-retrieval-on-activitynet",,See all 844 tasks,Visual Question Answering  VQA,Visual Question Answering  VQA
CLEVR,CLEVR Dataset,"CLEVR (Compositional Language and Elementary Visual Reasoning) is a synthetic Visual Question Answering dataset. It contains images of 3D-rendered objects; each image comes with a number of highly compositional questions that fall into different categories. Those categories fall into 5 classes of tasks: Exist, Count, Compare Integer, Query Attribute and Compare Attribute. The CLEVR dataset consists of: a training set of 70k images and 700k questions, a validation set of 15k images and 150k questions, a test set of 15k images and 150k questions about objects, answers, scene graphs and functional programs for all train and validation images and questions. Each object present in the scene, aside of position, is characterized by a set of four attributes: 2 sizes: large, small, 3 shapes: square, cylinder, sphere, 2 material types: rubber, metal, 8 color types: gray, blue, brown, yellow, red, green, purple, cyan, resulting in 96 unique combinations.",https://arxiv.org/abs/1811.06529,EditCC BY 4.0,"Image, Text",English,,,,70k images,training set of 70k images,5,"Visual Question Answering, Visual Question Answering (VQA) Split B, Image Generation, Visual Question Answering (VQA), Visual Question Answering (VQA) Split A","visual-question-answering-vqa-split-b-on, visual-question-answering-vqa-split-a-on, image-generation-on-clevr, visual-question-answering-on-clevr-1, visual-question-answering-on-clevr",,See all 844 tasks,Visual Question Answering  VQA,Visual Question Answering  VQA
TextVQA,TextVQA Dataset,"TextVQA is a dataset to benchmark visual reasoning based on text in images.
TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions.

Statistics
* 28,408 images from OpenImages
* 45,336 questions
* 453,360 ground truth answers",https://production-media.paperswithcode.com/datasets/f5989fab-b0bc-414f-bd10-feafda85c61b.png,EditCC BY 4.0,"Image, Text",English,,,,408 images,,,"Visual Question Answering, Visual Question Answering (VQA), Question Answering","visual-question-answering-on-textvqa-test-1, visual-question-answering-on-textvqa-test-2, visual-question-answering-vqa-on-textvqa",,See all 844 tasks,Visual Question Answering  VQA,Visual Question Answering  VQA
Visual_Question_Answering_v2.0,Visual Question Answering v2.0 Dataset,"Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. It is the second version of the VQA dataset.


265,016 images (COCO and abstract scenes)
At least 3 questions (5.4 questions on average) per image
10 ground truth answers per question
3 plausible (but likely incorrect) answers per question
Automatic evaluation metric

The first version of the dataset was released in October 2015.",https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_1.57.28_PM.png,EditUnknown,"Image, Text",English,2015,,,016 images,,,"Visual Question Answering, Visual Question Answering (VQA)","visual-question-answering-on-vqa-v2-val-1, visual-question-answering-on-vqa-v2-val, visual-question-answering-on-vqa-v2-test-std-1, visual-question-answering-on-vqa-v2-1, visual-question-answering-on-vqa-v2-test-dev, visual-question-answering-on-vqa-v2-test-std, visual-question-answering-on-vqa-v2-test-dev-1",,See all 844 tasks,Visual Question Answering  VQA,Visual Question Answering  VQA
ESD,ESD Dataset,"ESD is an Emotional Speech Database for voice conversion research. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 hours of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies.",https://production-media.paperswithcode.com/datasets/esd.png,EditCustom (research-only),Audio,,,,,,,,Voice Conversion,,,See all 844 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
GneutralSpeech_Female,GneutralSpeech Female Dataset,"A Brazilian Portuguese TTS dataset featuring a female voice
recorded with high quality in a controlled environment, with
neutral emotion and more than 20 hours of recordings.
 with
neutral emotion and more than 20 hours of recordings. Our
dataset aims to facilitate transfer learning for researchers and
developers working on TTS applications: a highly professional
neutral female voice can serve as a good warm-up stage for
learning language-specific structures, pronunciation and other
non-individual characteristics of speech, leaving to further
training procedures only to learn the specific adaptations
needed (e.g. timbre, emotion and prosody). This can surely
help enabling the accommodation of a more diverse range
of female voices in Brazilian Portuguese. By doing so, we
also hope to contribute to the development of accessible and
high-quality TTS systems for several use cases such as virtual
assistants, audiobooks, language learning tools and accessibility solutions.

Possible  use cases: 
TTS;
Voice Conversion;
ASR;
Speech Enhancement",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCustom,"Audio, Image, Text",English,,,,,,,"Text-To-Speech Synthesis, Automatic Speech Recognition (ASR), Speech Enhancement, Voice Cloning, Voice Conversion",,,See all 844 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
GneutralSpeech_Male,GneutralSpeech Male Dataset,"A database containing high sampling rate recordings of a single speaker reading sentences in Brazilian
Portuguese with neutral voice, along with the corresponding text corpus.
Intended for speech synthesis and automatic speech recognition applications, the dataset contains text extracted from a popular Brazilian news
TV program, totalling roughly 20 h of audio spoken by a trained individual in a controlled environment. The text was normalized in the
recording process and special textual occurrences (e.g. acronyms, numbers, foreign names etc.) were replaced by their phonetic translation to
a readable text in Portuguese. There are no noticeable accidental sounds
and background noise has been kept to a minimum in all audio samples.

Intended for:
TTS
ASR
Speech Enhancement
Voice Conversion",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditCustom,"Audio, Image, Text",English,,,,,,,"Text-To-Speech Synthesis, Automatic Speech Recognition (ASR), Speech Enhancement, Speech Synthesis, Voice Cloning, Voice Conversion",,,See all 844 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
LibriSpeech,LibriSpeech Dataset,"The LibriSpeech corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ’clean’ and ’other’ categories, respectively, depending upon how well or challenging Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.",https://arxiv.org/abs/1910.00716,EditCC BY 4.0,"Audio, Image, Text",English,,,,,,,"Automatic Speech Recognition, Resynthesis, Voice Conversion, Speech Recognition","automatic-speech-recognition-on-librispeech-9, voice-conversion-on-librispeech-test-clean, automatic-speech-recognition-on-librispeech-8, automatic-speech-recognition-on-librispeech-10, speech-recognition-on-librispeech-test-clean, automatic-speech-recognition-on-librispeech-11, automatic-speech-recognition-on-librispeech-7, resynthesis-on-librispeech-1, speech-recognition-on-librispeech-test-other",,See all 844 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
VCTK,VCTK Dataset,"This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald & Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, ""The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,"" https://doi.org/10.1109/ICSDA.2013.6709856. The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf. All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed.",https://datashare.is.ed.ac.uk/handle/10283/3443,EditCreative Commons License: Attribution 4.0 International,Audio,,2013,,,400 sentences,,,"Directional Hearing, Real-time Directional Hearing, Bandwidth Extension, Audio Super-Resolution, Voice Conversion","audio-super-resolution-on-voice-bank-corpus-1, voice-conversion-on-vctk, real-time-directional-hearing-on-vctk, bandwidth-extension-on-vctk, directional-hearing-on-vctk, audio-super-resolution-on-vctk-multi-speaker-1",,See all 844 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
VESUS,VESUS Dataset,"The Varied Emotion in Syntactically Uniform Speech (VESUS) repository is a lexically controlled database collected by the NSA lab. Here, actors read a semantically neutral script of words, phrases, and sentences with different emotional inflections. VESUS contains 252 distinct phrases, each read by 10 actors in 5 emotional states (neutral, angry, happy, sad, fearful).",https://engineering.jhu.edu/nsa/vesus/,EditUnknown,"Audio, Image",,,,,,,,"Emotion Recognition, Voice Conversion",,,See all 844 tasks,Voice Conversion3 benchmarks16,Voice Conversion3 benchmarks16
Belebele,Belebele Dataset,"Belebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the FLORES-200 dataset. The human annotation procedure was carefully curated to create questions that discriminate between different levels of generalizable language comprehension and is reinforced by extensive quality checks. While all questions directly relate to the passage, the English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. Belebele opens up new avenues for evaluating and analyzing the multilingual abilities of language models and NLP systems.",https://production-media.paperswithcode.com/datasets/f054f9d9-0344-4cfd-b15c-069f294c5fea.jpg,EditCC BY-SA,"Image, Text",English,,,,,,,"Multilingual Machine Comprehension in English Hindi, Reading Comprehension (Zero-Shot), Reading Comprehension (Few-Shot), Multiple-choice, Machine Reading Comprehension, Multilingual NLP, Natural Language Understanding, Multilingual text classification, Pretrained Multilingual Language Models, Reading Comprehension (One-Shot), Natural Questions, Vietnamese Machine Reading Comprehension, XLM-R, Reading Comprehension, Chinese Reading Comprehension, Question Answering",,,See all 844 tasks,XLM-R99 papers with code,XLM-R99 papers with code
ActivityNet-QA,ActivityNet-QA Dataset,"The ActivityNet-QA dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benchmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning.",/paper/activitynet-qa-a-dataset-for-understanding,EditUnknown,"Image, Text, Video",English,,,,,,,"Video Question Answering, Visual Question Answering (VQA), Zero-Shot Video Question Answer, Question Answering","video-question-answering-on-activitynet-qa, zeroshot-video-question-answer-on-activitynet",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
EgoSchema,EgoSchema Dataset,"EgoSchema is very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text, Video",English,,,,,,,"Visual Question Answering (VQA), Zero-Shot Video Question Answer","zero-shot-video-question-answer-on-egoschema, visual-question-answering-vqa-on-egoschema, zero-shot-video-question-answer-on-egoschema-1",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
MSRVTT-QA,MSRVTT-QA Dataset,"The MSR-VTT-QA dataset is a benchmark for the task of Visual Question Answering (VQA) on the MSR-VTT (Microsoft Research Video to Text) dataset. The MSR-VTT-QA benchmark is used to evaluate models on their ability to answer questions based on these videos. It's part of the tasks that this dataset is used for, along with Video Retrieval, Video Captioning, Zero-Shot Video Question Answering, Zero-Shot Video Retrieval, and Text-to-Video Generation.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text, Video",English,,,,,,,"Video Question Answering, Visual Question Answering, Zero-Shot Learning, Visual Question Answering (VQA), Zero-Shot Video Question Answer, Zeroshot Video Question Answer","visual-question-answering-on-msrvtt-qa-2, visual-question-answering-on-msrvtt-qa-1, video-question-answering-on-msrvtt-qa, zeroshot-video-question-answer-on-msrvtt-qa-1, zeroshot-video-question-answer-on-msrvtt-qa, zero-shot-learning-on-msrvtt-qa",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
MSVD-QA,MSVD-QA Dataset,"The MSVD-QA dataset is a Video Question Answering (VideoQA) dataset. It is based on the existing Microsoft Research Video Description (MSVD) dataset, which consists of about 120K sentences describing more than 2,000 video snippets. In the MSVD-QA dataset, Question-Answer (QA) pairs are generated from these descriptions. The dataset is mainly used in video captioning experiments but due to its large data size, it is also used for VideoQA. It contains 1970 video clips and approximately 50.5K QA pairs.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text, Video",English,1970,,,120K sentences,,,"Video Question Answering, Visual Question Answering, Zero-Shot Learning, Visual Question Answering (VQA), Zero-Shot Video Question Answer, Zeroshot Video Question Answer","zero-shot-learning-on-msvd-qa, visual-question-answering-on-msvd-qa-2, video-question-answering-on-msvd-qa, zeroshot-video-question-answer-on-msvd-qa-1, visual-question-answering-on-msvd-qa-1, zeroshot-video-question-answer-on-msvd-qa",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
MVBench,MVBench Dataset,"MVBench is a comprehensive Multi-modal Video understanding Benchmark. It was introduced to evaluate the comprehension capabilities of Multi-modal Large Language Models (MLLMs), particularly their temporal understanding in dynamic video tasks. MVBench covers 20 challenging video tasks that cannot be effectively solved with a single frame. It introduces a novel static-to-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, it enables the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,"Image, Text, Video",English,,,,,,,"Visual Question Answering (VQA), Zero-Shot Video Question Answer, Video Question Answering","video-question-answering-on-mvbench, zero-shot-video-question-answer-on-mvbench, visual-question-answering-vqa-on-mvbench",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
NExT-QA,NExT-QA Dataset,"NExT-QA is a VideoQA benchmark targeting the explanation of video contents. It challenges QA models to reason about the causal and temporal actions and understand the rich object interactions in daily activities, e.g., ""why is the boy crying?"" and ""How does the lady react after the boy fall backward?"". It supports both multi-choice and generative open-ended QA tasks. The videos are untrimmed and the questions usually invoke local video contents for answers.",https://production-media.paperswithcode.com/datasets/4a25daeb-5315-427b-8d3c-ced512a3cee2.png,EditMIT,"Text, Time Series, Video",English,,,,,,,"Video Question Answering, Zero-Shot Video Question Answer, Temporal/Casual QA, Question Answering","temporal-casual-qa-on-next-qa, video-question-answering-on-next-qa, zero-shot-video-question-answer-on-next-gqa, zero-shot-video-question-answer-on-next-qa, question-answering-on-next-qa-open-ended",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
TGIF-QA,TGIF-QA Dataset,The TGIF-QA dataset contains 165K QA pairs for the animated GIFs from the TGIF dataset [Li et al. CVPR 2016]. The question & answer pairs are collected via crowdsourcing with a carefully designed user interface to ensure quality. The dataset can be used to evaluate video-based Visual Question Answering techniques.,https://github.com/YunseokJANG/tgif-qa,EditCustom (research-only),"Image, Text, Video",English,2016,,,,,,"Video Question Answering, TGIF-Action, TGIF-Frame, Visual Question Answering (VQA), Zero-Shot Video Question Answer, TGIF-Transition, Zeroshot Video Question Answer, Question Answering","video-question-answering-on-tgif-qa, tgif-action-on-tgif-qa, zeroshot-video-question-answer-on-tgif-qa-1, visual-question-answering-on-tgif-qa, tgif-transition-on-tgif-qa, zeroshot-video-question-answer-on-tgif-qa, tgif-frame-on-tgif-qa",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
TVQA,TVQA Dataset,"The TVQA dataset is a large-scale video dataset for video question answering. It is based on 6 popular TV shows (Friends, The Big Bang Theory, How I Met Your Mother, House M.D., Grey's Anatomy, Castle). It includes 152,545 QA pairs from 21,793 TV show clips. The QA pairs are split into the ratio of 8:1:1 for training, validation, and test sets. The TVQA dataset provides the sequence of video frames extracted at 3 FPS, the corresponding subtitles with the video clips, and the query consisting of a question and four answer candidates. Among the four answer candidates, there is only one correct answer.",https://arxiv.org/abs/1907.05006,EditUnknown,"Text, Video",English,,,,,,,"Video Question Answering, Zero-Shot Video Question Answer, Zero-Shot Learning","video-question-answering-on-tvqa, zero-shot-video-question-answer-on-tvqa, zero-shot-learning-on-tvqa",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
TVQA_,TVQA+ Dataset,"TVQA+ contains 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers.",https://arxiv.org/pdf/1904.11574v2.pdf,EditUnknown,"Image, Text, Video",English,,,,,,,"Video Question Answering, Zero-Shot Learning, Visual Question Answering (VQA), Zero-Shot Video Question Answer, Question Answering","video-question-answering-on-tvqa, zero-shot-video-question-answer-on-tvqa, zero-shot-learning-on-tvqa",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
Video-MME,Video-MME Dataset,"Video-MME stands for Video Multi-Modal Evaluation. It is the first-ever comprehensive evaluation benchmark specifically designed for Multi-modal Large Language Models (MLLMs) in video analysis¹. This benchmark is significant because it addresses the need for a high-quality assessment of MLLMs' performance in processing sequential visual data, which has been less explored compared to their capabilities in static image understanding.

The Video-MME benchmark is characterized by its:
1. Diversity in video types, covering 6 primary visual domains with 30 subfields for broad scenario generalizability.
2. Duration in the temporal dimension, including short-, medium-, and long-term videos ranging from 11 seconds to 1 hour, to assess robust contextual dynamics.
3. Breadth in data modalities, integrating multi-modal inputs such as video frames, subtitles, and audios.
4. Quality in annotations, with rigorous manual labeling by expert annotators for precise and reliable model assessment¹.

The benchmark includes 900 videos totaling 256 hours, manually selected and annotated, resulting in 2,700 question-answer pairs. It has been used to evaluate various state-of-the-art MLLMs, including the GPT-4 series and Gemini 1.5 Pro, as well as open-source image and video models¹. The findings from Video-MME highlight the need for further improvements in handling longer sequences and multi-modal data, which is crucial for the advancement of MLLMs¹.

(1) [2405.21075] Video-MME: The First-Ever Comprehensive Evaluation .... https://arxiv.org/abs/2405.21075.
(2) Video-MME. https://video-mme.github.io/home_page.html.
(3) Video-MME: Welcome. https://video-mme.github.io/.
(4) undefined. https://doi.org/10.48550/arXiv.2405.21075.",https://production-assets.paperswithcode.com/perf/images/default_dataset-efab89e8.jpeg,EditUnknown,Video,,,,,,,,Zero-Shot Video Question Answer,"zero-shot-video-question-answer-on-video-mme-1, zero-shot-video-question-answer-on-video-mme",,See all 844 tasks,Zero-Shot Video Question Answe,Zero-Shot Video Question Answe
